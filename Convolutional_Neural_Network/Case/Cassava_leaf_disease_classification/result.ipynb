{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "from tqdm.notebook import tqdm\n",
    "from utils.model import Resnet\n",
    "from utils.dataset import *\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "参数量: 404480\n"
     ]
    }
   ],
   "source": [
    "full_data = Image_dataset(r'D:\\Folder\\Vscode\\Git\\Example\\Data\\Cassava_leaf_disease_classification',mode='train')\n",
    "batch_size = 12\n",
    "train_data,val_data,test_data = random_split(full_data,[0.8,0.15,0.05])\n",
    "train_loader = DataLoader(train_data,batch_size=batch_size,shuffle=True)\n",
    "val_loader = DataLoader(val_data,batch_size=batch_size,shuffle=False)\n",
    "test_loader = DataLoader(test_data,batch_size=batch_size,shuffle=False)\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = Resnet()\n",
    "model = model.to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=1e-4)\n",
    "\n",
    "params_num = 0\n",
    "for i in model.parameters():\n",
    "    params_num += i.numel()\n",
    "print('参数量:',params_num)\n",
    "\n",
    "def val_acc():\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in val_loader:\n",
    "            inputs,labels = data\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _,predicted = torch.max(outputs.data,1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebf5e75078dd40f5a859c9bf2366f82d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1427 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0, Loss: 1.5580545663833618, Accuracy: 0.3333333333333333\n",
      "Step: 1, Loss: 1.4920635223388672, Accuracy: 0.4166666666666667\n",
      "Step: 2, Loss: 1.5511740446090698, Accuracy: 0.3888888888888889\n",
      "Step: 3, Loss: 1.466092586517334, Accuracy: 0.375\n",
      "Step: 4, Loss: 1.578878402709961, Accuracy: 0.35\n",
      "Step: 5, Loss: 1.5488241910934448, Accuracy: 0.3611111111111111\n",
      "Step: 6, Loss: 1.4900994300842285, Accuracy: 0.38095238095238093\n",
      "Step: 7, Loss: 1.409048080444336, Accuracy: 0.40625\n",
      "Step: 8, Loss: 1.562271237373352, Accuracy: 0.4074074074074074\n",
      "Step: 9, Loss: 1.3928823471069336, Accuracy: 0.43333333333333335\n",
      "Step: 10, Loss: 1.4524726867675781, Accuracy: 0.45454545454545453\n",
      "Step: 11, Loss: 1.4625669717788696, Accuracy: 0.4583333333333333\n",
      "Step: 12, Loss: 1.4560036659240723, Accuracy: 0.46153846153846156\n",
      "Step: 13, Loss: 1.4997342824935913, Accuracy: 0.4642857142857143\n",
      "Step: 14, Loss: 1.6117496490478516, Accuracy: 0.45\n",
      "Step: 15, Loss: 1.459421157836914, Accuracy: 0.4583333333333333\n",
      "Step: 16, Loss: 1.4262785911560059, Accuracy: 0.46568627450980393\n",
      "Step: 17, Loss: 1.450888991355896, Accuracy: 0.4675925925925926\n",
      "Step: 18, Loss: 1.471215844154358, Accuracy: 0.4692982456140351\n",
      "Step: 19, Loss: 1.4818528890609741, Accuracy: 0.4708333333333333\n",
      "Step: 20, Loss: 1.3427457809448242, Accuracy: 0.4801587301587302\n",
      "Step: 21, Loss: 1.3519004583358765, Accuracy: 0.48484848484848486\n",
      "Step: 22, Loss: 1.253251075744629, Accuracy: 0.4963768115942029\n",
      "Step: 23, Loss: 1.3561959266662598, Accuracy: 0.5034722222222222\n",
      "Step: 24, Loss: 1.4680105447769165, Accuracy: 0.5033333333333333\n",
      "Step: 25, Loss: 1.5456361770629883, Accuracy: 0.5032051282051282\n",
      "Step: 26, Loss: 1.3522535562515259, Accuracy: 0.5061728395061729\n",
      "Step: 27, Loss: 1.4923957586288452, Accuracy: 0.5029761904761905\n",
      "Step: 28, Loss: 1.4607605934143066, Accuracy: 0.5028735632183908\n",
      "Step: 29, Loss: 1.478156566619873, Accuracy: 0.5\n",
      "Step: 30, Loss: 1.1848492622375488, Accuracy: 0.510752688172043\n",
      "Step: 31, Loss: 1.4278349876403809, Accuracy: 0.5104166666666666\n",
      "Step: 32, Loss: 1.1580824851989746, Accuracy: 0.5202020202020202\n",
      "Step: 33, Loss: 1.2264236211776733, Accuracy: 0.5269607843137255\n",
      "Step: 34, Loss: 1.2707258462905884, Accuracy: 0.530952380952381\n",
      "Step: 35, Loss: 1.2467814683914185, Accuracy: 0.5370370370370371\n",
      "Step: 36, Loss: 1.4785505533218384, Accuracy: 0.5337837837837838\n",
      "Step: 37, Loss: 1.3511457443237305, Accuracy: 0.5350877192982456\n",
      "Step: 38, Loss: 1.3056749105453491, Accuracy: 0.5384615384615384\n",
      "Step: 39, Loss: 1.3200418949127197, Accuracy: 0.5416666666666666\n",
      "Step: 40, Loss: 1.2360923290252686, Accuracy: 0.5467479674796748\n",
      "Step: 41, Loss: 1.4875515699386597, Accuracy: 0.5436507936507936\n",
      "Step: 42, Loss: 1.5142192840576172, Accuracy: 0.5387596899224806\n",
      "Step: 43, Loss: 1.3875479698181152, Accuracy: 0.5397727272727273\n",
      "Step: 44, Loss: 1.5035141706466675, Accuracy: 0.5370370370370371\n",
      "Step: 45, Loss: 1.1579164266586304, Accuracy: 0.5434782608695652\n",
      "Step: 46, Loss: 1.2017828226089478, Accuracy: 0.5478723404255319\n",
      "Step: 47, Loss: 1.3259384632110596, Accuracy: 0.5486111111111112\n",
      "Step: 48, Loss: 1.393627643585205, Accuracy: 0.5476190476190477\n",
      "Step: 49, Loss: 1.407135009765625, Accuracy: 0.5466666666666666\n",
      "Step: 50, Loss: 1.4518280029296875, Accuracy: 0.5441176470588235\n",
      "Step: 51, Loss: 1.143355131149292, Accuracy: 0.5480769230769231\n",
      "Step: 52, Loss: 1.2073842287063599, Accuracy: 0.5518867924528302\n",
      "Step: 53, Loss: 1.4084380865097046, Accuracy: 0.5509259259259259\n",
      "Step: 54, Loss: 1.4786220788955688, Accuracy: 0.5484848484848485\n",
      "Step: 55, Loss: 1.1910794973373413, Accuracy: 0.5520833333333334\n",
      "Step: 56, Loss: 1.2733479738235474, Accuracy: 0.554093567251462\n",
      "Step: 57, Loss: 1.3201947212219238, Accuracy: 0.5545977011494253\n",
      "Step: 58, Loss: 1.2577239274978638, Accuracy: 0.556497175141243\n",
      "Step: 59, Loss: 1.177331566810608, Accuracy: 0.5597222222222222\n",
      "Step: 60, Loss: 1.2468576431274414, Accuracy: 0.5614754098360656\n",
      "Step: 61, Loss: 1.2721518278121948, Accuracy: 0.5631720430107527\n",
      "Step: 62, Loss: 1.3562833070755005, Accuracy: 0.5634920634920635\n",
      "Step: 63, Loss: 1.291239619255066, Accuracy: 0.5638020833333334\n",
      "Step: 64, Loss: 1.132927417755127, Accuracy: 0.5679487179487179\n",
      "Step: 65, Loss: 1.1018010377883911, Accuracy: 0.571969696969697\n",
      "Step: 66, Loss: 1.3103408813476562, Accuracy: 0.5733830845771144\n",
      "Step: 67, Loss: 1.272831678390503, Accuracy: 0.5747549019607843\n",
      "Step: 68, Loss: 1.4888931512832642, Accuracy: 0.572463768115942\n",
      "Step: 69, Loss: 1.4688692092895508, Accuracy: 0.5702380952380952\n",
      "Step: 70, Loss: 1.0782753229141235, Accuracy: 0.573943661971831\n",
      "Step: 71, Loss: 1.20796799659729, Accuracy: 0.5763888888888888\n",
      "Step: 72, Loss: 1.3865891695022583, Accuracy: 0.5753424657534246\n",
      "Step: 73, Loss: 1.4050039052963257, Accuracy: 0.5743243243243243\n",
      "Step: 74, Loss: 1.2450554370880127, Accuracy: 0.5755555555555556\n",
      "Step: 75, Loss: 1.1816784143447876, Accuracy: 0.5778508771929824\n",
      "Step: 76, Loss: 1.3070143461227417, Accuracy: 0.577922077922078\n",
      "Step: 77, Loss: 1.4692093133926392, Accuracy: 0.5758547008547008\n",
      "Step: 78, Loss: 1.165932059288025, Accuracy: 0.5791139240506329\n",
      "Step: 79, Loss: 1.1012849807739258, Accuracy: 0.5822916666666667\n",
      "Step: 80, Loss: 1.1058014631271362, Accuracy: 0.5853909465020576\n",
      "Step: 81, Loss: 1.359018325805664, Accuracy: 0.5853658536585366\n",
      "Step: 82, Loss: 1.4147933721542358, Accuracy: 0.5843373493975904\n",
      "Step: 83, Loss: 1.2449297904968262, Accuracy: 0.5853174603174603\n",
      "Step: 84, Loss: 1.2547231912612915, Accuracy: 0.5862745098039216\n",
      "Step: 85, Loss: 1.218016266822815, Accuracy: 0.5872093023255814\n",
      "Step: 86, Loss: 1.2581290006637573, Accuracy: 0.5881226053639846\n",
      "Step: 87, Loss: 1.1796536445617676, Accuracy: 0.5899621212121212\n",
      "Step: 88, Loss: 1.3571171760559082, Accuracy: 0.5898876404494382\n",
      "Step: 89, Loss: 1.453330159187317, Accuracy: 0.5879629629629629\n",
      "Step: 90, Loss: 1.0431188344955444, Accuracy: 0.5915750915750916\n",
      "Step: 91, Loss: 1.638861060142517, Accuracy: 0.5878623188405797\n",
      "Step: 92, Loss: 1.2433768510818481, Accuracy: 0.5887096774193549\n",
      "Step: 93, Loss: 1.3252111673355103, Accuracy: 0.5886524822695035\n",
      "Step: 94, Loss: 1.1720637083053589, Accuracy: 0.5903508771929824\n",
      "Step: 95, Loss: 1.409858226776123, Accuracy: 0.5894097222222222\n",
      "Step: 96, Loss: 1.2379518747329712, Accuracy: 0.5902061855670103\n",
      "Step: 97, Loss: 1.1060000658035278, Accuracy: 0.592687074829932\n",
      "Step: 98, Loss: 1.4739937782287598, Accuracy: 0.5909090909090909\n",
      "Step: 99, Loss: 1.5695290565490723, Accuracy: 0.5883333333333334\n",
      "Step: 100, Loss: 1.1948885917663574, Accuracy: 0.5899339933993399\n",
      "Step: 101, Loss: 1.1362820863723755, Accuracy: 0.5915032679738562\n",
      "Step: 102, Loss: 1.341690182685852, Accuracy: 0.5914239482200647\n",
      "Step: 103, Loss: 1.4398998022079468, Accuracy: 0.5905448717948718\n",
      "Step: 104, Loss: 1.2464103698730469, Accuracy: 0.5912698412698413\n",
      "Step: 105, Loss: 1.3212910890579224, Accuracy: 0.5911949685534591\n",
      "Step: 106, Loss: 1.1402236223220825, Accuracy: 0.5926791277258567\n",
      "Step: 107, Loss: 1.340807318687439, Accuracy: 0.5925925925925926\n",
      "Step: 108, Loss: 1.3901258707046509, Accuracy: 0.591743119266055\n",
      "Step: 109, Loss: 1.4778882265090942, Accuracy: 0.5901515151515152\n",
      "Step: 110, Loss: 1.1499253511428833, Accuracy: 0.5915915915915916\n",
      "Step: 111, Loss: 1.3510388135910034, Accuracy: 0.5915178571428571\n",
      "Step: 112, Loss: 1.37172269821167, Accuracy: 0.5907079646017699\n",
      "Step: 113, Loss: 1.4190291166305542, Accuracy: 0.5899122807017544\n",
      "Step: 114, Loss: 1.0059796571731567, Accuracy: 0.5927536231884057\n",
      "Step: 115, Loss: 1.5440912246704102, Accuracy: 0.5905172413793104\n",
      "Step: 116, Loss: 1.3302744626998901, Accuracy: 0.5904558404558404\n",
      "Step: 117, Loss: 1.0819463729858398, Accuracy: 0.5925141242937854\n",
      "Step: 118, Loss: 1.1599783897399902, Accuracy: 0.5938375350140056\n",
      "Step: 119, Loss: 1.232035517692566, Accuracy: 0.5944444444444444\n",
      "Step: 120, Loss: 1.2590587139129639, Accuracy: 0.5950413223140496\n",
      "Step: 121, Loss: 1.2445862293243408, Accuracy: 0.5956284153005464\n",
      "Step: 122, Loss: 1.2299622297286987, Accuracy: 0.5962059620596206\n",
      "Step: 123, Loss: 1.3371752500534058, Accuracy: 0.5961021505376344\n",
      "Step: 124, Loss: 1.2590676546096802, Accuracy: 0.5966666666666667\n",
      "Step: 125, Loss: 1.388065218925476, Accuracy: 0.5958994708994709\n",
      "Step: 126, Loss: 1.0644116401672363, Accuracy: 0.597769028871391\n",
      "Step: 127, Loss: 1.2175930738449097, Accuracy: 0.5983072916666666\n",
      "Step: 128, Loss: 1.3166829347610474, Accuracy: 0.5981912144702842\n",
      "Step: 129, Loss: 1.369723916053772, Accuracy: 0.5980769230769231\n",
      "Step: 130, Loss: 1.558634638786316, Accuracy: 0.5960559796437659\n",
      "Step: 131, Loss: 1.25377357006073, Accuracy: 0.5965909090909091\n",
      "Step: 132, Loss: 1.39155113697052, Accuracy: 0.5958646616541353\n",
      "Step: 133, Loss: 1.4052990674972534, Accuracy: 0.5951492537313433\n",
      "Step: 134, Loss: 1.3457196950912476, Accuracy: 0.5950617283950618\n",
      "Step: 135, Loss: 1.3127011060714722, Accuracy: 0.5949754901960784\n",
      "Step: 136, Loss: 1.1514371633529663, Accuracy: 0.5961070559610706\n",
      "Step: 137, Loss: 1.3243459463119507, Accuracy: 0.5960144927536232\n",
      "Step: 138, Loss: 1.2606465816497803, Accuracy: 0.5965227817745803\n",
      "Step: 139, Loss: 1.2348285913467407, Accuracy: 0.5970238095238095\n",
      "Step: 140, Loss: 1.2526183128356934, Accuracy: 0.5975177304964538\n",
      "Step: 141, Loss: 1.329222321510315, Accuracy: 0.5974178403755869\n",
      "Step: 142, Loss: 1.3856096267700195, Accuracy: 0.5967365967365967\n",
      "Step: 143, Loss: 1.4233508110046387, Accuracy: 0.5960648148148148\n",
      "Step: 144, Loss: 1.1904596090316772, Accuracy: 0.5971264367816091\n",
      "Step: 145, Loss: 1.479817271232605, Accuracy: 0.5958904109589042\n",
      "Step: 146, Loss: 1.3378616571426392, Accuracy: 0.5958049886621315\n",
      "Step: 147, Loss: 1.3322473764419556, Accuracy: 0.5957207207207207\n",
      "Step: 148, Loss: 1.314780354499817, Accuracy: 0.5956375838926175\n",
      "Step: 149, Loss: 1.2556129693984985, Accuracy: 0.5961111111111111\n",
      "Step: 150, Loss: 1.1489877700805664, Accuracy: 0.5971302428256071\n",
      "Step: 151, Loss: 1.3330557346343994, Accuracy: 0.5970394736842105\n",
      "Step: 152, Loss: 1.1554521322250366, Accuracy: 0.5980392156862745\n",
      "Step: 153, Loss: 1.0452141761779785, Accuracy: 0.6001082251082251\n",
      "Step: 154, Loss: 1.0593236684799194, Accuracy: 0.6021505376344086\n",
      "Step: 155, Loss: 1.181505799293518, Accuracy: 0.6025641025641025\n",
      "Step: 156, Loss: 1.261144757270813, Accuracy: 0.6029723991507431\n",
      "Step: 157, Loss: 1.4614261388778687, Accuracy: 0.6017932489451476\n",
      "Step: 158, Loss: 1.3188170194625854, Accuracy: 0.6022012578616353\n",
      "Step: 159, Loss: 1.1580243110656738, Accuracy: 0.603125\n",
      "Step: 160, Loss: 1.396950125694275, Accuracy: 0.6024844720496895\n",
      "Step: 161, Loss: 1.408695101737976, Accuracy: 0.6018518518518519\n",
      "Step: 162, Loss: 1.2740482091903687, Accuracy: 0.6022494887525562\n",
      "Step: 163, Loss: 1.2426623106002808, Accuracy: 0.6026422764227642\n",
      "Step: 164, Loss: 1.4672112464904785, Accuracy: 0.6015151515151516\n",
      "Step: 165, Loss: 1.405677318572998, Accuracy: 0.6009036144578314\n",
      "Step: 166, Loss: 1.405735969543457, Accuracy: 0.6002994011976048\n",
      "Step: 167, Loss: 1.49150550365448, Accuracy: 0.5992063492063492\n",
      "Step: 168, Loss: 1.3986482620239258, Accuracy: 0.5986193293885601\n",
      "Step: 169, Loss: 1.3107109069824219, Accuracy: 0.5985294117647059\n",
      "Step: 170, Loss: 1.4756879806518555, Accuracy: 0.5974658869395711\n",
      "Step: 171, Loss: 1.2417443990707397, Accuracy: 0.5978682170542635\n",
      "Step: 172, Loss: 1.2452601194381714, Accuracy: 0.5982658959537572\n",
      "Step: 173, Loss: 1.104963779449463, Accuracy: 0.5996168582375478\n",
      "Step: 174, Loss: 1.5458921194076538, Accuracy: 0.5980952380952381\n",
      "Step: 175, Loss: 1.2395578622817993, Accuracy: 0.5984848484848485\n",
      "Step: 176, Loss: 1.3993390798568726, Accuracy: 0.5979284369114878\n",
      "Step: 177, Loss: 1.3226011991500854, Accuracy: 0.5978464419475655\n",
      "Step: 178, Loss: 1.337981104850769, Accuracy: 0.5977653631284916\n",
      "Step: 179, Loss: 1.1648098230361938, Accuracy: 0.5986111111111111\n",
      "Step: 180, Loss: 1.1943049430847168, Accuracy: 0.5994475138121547\n",
      "Step: 181, Loss: 1.168117880821228, Accuracy: 0.6002747252747253\n",
      "Step: 182, Loss: 1.3391798734664917, Accuracy: 0.6001821493624773\n",
      "Step: 183, Loss: 1.4289296865463257, Accuracy: 0.5996376811594203\n",
      "Step: 184, Loss: 1.165020227432251, Accuracy: 0.6004504504504504\n",
      "Step: 185, Loss: 1.4409054517745972, Accuracy: 0.5994623655913979\n",
      "Step: 186, Loss: 1.3936511278152466, Accuracy: 0.5989304812834224\n",
      "Step: 187, Loss: 1.1569186449050903, Accuracy: 0.5997340425531915\n",
      "Step: 188, Loss: 1.0166980028152466, Accuracy: 0.6014109347442681\n",
      "Step: 189, Loss: 1.3242172002792358, Accuracy: 0.6013157894736842\n",
      "Step: 190, Loss: 1.3235220909118652, Accuracy: 0.6012216404886562\n",
      "Step: 191, Loss: 1.5451440811157227, Accuracy: 0.5998263888888888\n",
      "Step: 192, Loss: 1.2989474534988403, Accuracy: 0.5997409326424871\n",
      "Step: 193, Loss: 1.3938676118850708, Accuracy: 0.5992268041237113\n",
      "Step: 194, Loss: 1.4585100412368774, Accuracy: 0.5982905982905983\n",
      "Step: 195, Loss: 1.2494510412216187, Accuracy: 0.5986394557823129\n",
      "Step: 196, Loss: 1.1532496213912964, Accuracy: 0.5994077834179357\n",
      "Step: 197, Loss: 1.3775253295898438, Accuracy: 0.5989057239057239\n",
      "Step: 198, Loss: 1.3002861738204956, Accuracy: 0.5988274706867671\n",
      "Step: 199, Loss: 1.3138878345489502, Accuracy: 0.59875\n",
      "Step: 200, Loss: 1.160859227180481, Accuracy: 0.599502487562189\n",
      "Step: 201, Loss: 1.3900370597839355, Accuracy: 0.599009900990099\n",
      "Step: 202, Loss: 1.281366229057312, Accuracy: 0.5989326765188834\n",
      "Step: 203, Loss: 1.4005709886550903, Accuracy: 0.5984477124183006\n",
      "Step: 204, Loss: 1.2673146724700928, Accuracy: 0.5983739837398374\n",
      "Step: 205, Loss: 1.385185718536377, Accuracy: 0.5983009708737864\n",
      "Step: 206, Loss: 1.4681166410446167, Accuracy: 0.5974235104669887\n",
      "Step: 207, Loss: 1.3169379234313965, Accuracy: 0.5973557692307693\n",
      "Step: 208, Loss: 1.2560783624649048, Accuracy: 0.5976874003189793\n",
      "Step: 209, Loss: 1.4571595191955566, Accuracy: 0.5968253968253968\n",
      "Step: 210, Loss: 1.3980861902236938, Accuracy: 0.5963665086887836\n",
      "Step: 211, Loss: 1.2581583261489868, Accuracy: 0.5966981132075472\n",
      "Step: 212, Loss: 1.7087503671646118, Accuracy: 0.594679186228482\n",
      "Step: 213, Loss: 1.3823696374893188, Accuracy: 0.5942367601246106\n",
      "Step: 214, Loss: 1.3169280290603638, Accuracy: 0.5941860465116279\n",
      "Step: 215, Loss: 1.162453293800354, Accuracy: 0.5949074074074074\n",
      "Step: 216, Loss: 1.2026199102401733, Accuracy: 0.5956221198156681\n",
      "Step: 217, Loss: 1.4084235429763794, Accuracy: 0.5951834862385321\n",
      "Step: 218, Loss: 1.1432400941848755, Accuracy: 0.5958904109589042\n",
      "Step: 219, Loss: 1.3995347023010254, Accuracy: 0.5954545454545455\n",
      "Step: 220, Loss: 1.213496446609497, Accuracy: 0.5957767722473605\n",
      "Step: 221, Loss: 1.0802700519561768, Accuracy: 0.5968468468468469\n",
      "Step: 222, Loss: 1.1714890003204346, Accuracy: 0.5975336322869955\n",
      "Step: 223, Loss: 1.0922250747680664, Accuracy: 0.5985863095238095\n",
      "Step: 224, Loss: 1.3914169073104858, Accuracy: 0.5981481481481481\n",
      "Step: 225, Loss: 1.2936404943466187, Accuracy: 0.5980825958702065\n",
      "Step: 226, Loss: 1.5246309041976929, Accuracy: 0.5969162995594713\n",
      "Step: 227, Loss: 1.2382453680038452, Accuracy: 0.5972222222222222\n",
      "Step: 228, Loss: 1.1861201524734497, Accuracy: 0.5978893740902474\n",
      "Step: 229, Loss: 1.3789993524551392, Accuracy: 0.597463768115942\n",
      "Step: 230, Loss: 1.2786756753921509, Accuracy: 0.5974025974025974\n",
      "Step: 231, Loss: 1.1589850187301636, Accuracy: 0.5980603448275862\n",
      "Step: 232, Loss: 1.2869162559509277, Accuracy: 0.5983547925608012\n",
      "Step: 233, Loss: 1.4390426874160767, Accuracy: 0.5975783475783476\n",
      "Step: 234, Loss: 1.320715069770813, Accuracy: 0.5975177304964538\n",
      "Step: 235, Loss: 1.2503957748413086, Accuracy: 0.5978107344632768\n",
      "Step: 236, Loss: 1.284055471420288, Accuracy: 0.5977496483825597\n",
      "Step: 237, Loss: 1.252498745918274, Accuracy: 0.5976890756302521\n",
      "Step: 238, Loss: 1.3744195699691772, Accuracy: 0.597629009762901\n",
      "Step: 239, Loss: 1.091304063796997, Accuracy: 0.5986111111111111\n",
      "Step: 240, Loss: 1.1634986400604248, Accuracy: 0.5992392807745505\n",
      "Step: 241, Loss: 1.4136356115341187, Accuracy: 0.5988292011019284\n",
      "Step: 242, Loss: 1.3031771183013916, Accuracy: 0.5987654320987654\n",
      "Step: 243, Loss: 1.168200135231018, Accuracy: 0.5993852459016393\n",
      "Step: 244, Loss: 1.4446035623550415, Accuracy: 0.5986394557823129\n",
      "Step: 245, Loss: 1.1443943977355957, Accuracy: 0.5992547425474255\n",
      "Step: 246, Loss: 1.2455447912216187, Accuracy: 0.599527665317139\n",
      "Step: 247, Loss: 1.1200497150421143, Accuracy: 0.6004704301075269\n",
      "Step: 248, Loss: 1.1761629581451416, Accuracy: 0.6010709504685409\n",
      "Step: 249, Loss: 1.1563787460327148, Accuracy: 0.6016666666666667\n",
      "Step: 250, Loss: 1.2366887331008911, Accuracy: 0.601925630810093\n",
      "Step: 251, Loss: 1.258154273033142, Accuracy: 0.6021825396825397\n",
      "Step: 252, Loss: 1.2418923377990723, Accuracy: 0.602437417654809\n",
      "Step: 253, Loss: 1.6364411115646362, Accuracy: 0.6010498687664042\n",
      "Step: 254, Loss: 1.1841074228286743, Accuracy: 0.6016339869281045\n",
      "Step: 255, Loss: 1.3900383710861206, Accuracy: 0.6012369791666666\n",
      "Step: 256, Loss: 1.4109892845153809, Accuracy: 0.6008430609597925\n",
      "Step: 257, Loss: 1.1634674072265625, Accuracy: 0.601421188630491\n",
      "Step: 258, Loss: 1.4790400266647339, Accuracy: 0.6007078507078507\n",
      "Step: 259, Loss: 1.335677981376648, Accuracy: 0.6006410256410256\n",
      "Step: 260, Loss: 1.41448974609375, Accuracy: 0.6002554278416348\n",
      "Step: 261, Loss: 1.1344857215881348, Accuracy: 0.6008269720101781\n",
      "Step: 262, Loss: 1.3242727518081665, Accuracy: 0.6007604562737643\n",
      "Step: 263, Loss: 1.2637062072753906, Accuracy: 0.601010101010101\n",
      "Step: 264, Loss: 1.313246250152588, Accuracy: 0.6009433962264151\n",
      "Step: 265, Loss: 1.2515147924423218, Accuracy: 0.6011904761904762\n",
      "Step: 266, Loss: 1.2456539869308472, Accuracy: 0.6014357053682896\n",
      "Step: 267, Loss: 1.2425581216812134, Accuracy: 0.601679104477612\n",
      "Step: 268, Loss: 1.1608551740646362, Accuracy: 0.6022304832713755\n",
      "Step: 269, Loss: 1.0851857662200928, Accuracy: 0.6030864197530864\n",
      "Step: 270, Loss: 1.402699589729309, Accuracy: 0.6027060270602707\n",
      "Step: 271, Loss: 1.0074384212493896, Accuracy: 0.6038602941176471\n",
      "Step: 272, Loss: 1.2984439134597778, Accuracy: 0.6037851037851037\n",
      "Step: 273, Loss: 1.3083235025405884, Accuracy: 0.6037104622871047\n",
      "Step: 274, Loss: 1.3294554948806763, Accuracy: 0.6036363636363636\n",
      "Step: 275, Loss: 1.0769240856170654, Accuracy: 0.6044685990338164\n",
      "Step: 276, Loss: 1.5636335611343384, Accuracy: 0.6034897713598074\n",
      "Step: 277, Loss: 1.235561490058899, Accuracy: 0.6037170263788969\n",
      "Step: 278, Loss: 1.3188910484313965, Accuracy: 0.6036439665471923\n",
      "Step: 279, Loss: 1.1584004163742065, Accuracy: 0.6041666666666666\n",
      "Step: 280, Loss: 1.2449488639831543, Accuracy: 0.6043890865954923\n",
      "Step: 281, Loss: 1.4708703756332397, Accuracy: 0.6037234042553191\n",
      "Step: 282, Loss: 1.1639760732650757, Accuracy: 0.6042402826855123\n",
      "Step: 283, Loss: 1.1502174139022827, Accuracy: 0.6047535211267606\n",
      "Step: 284, Loss: 1.479122519493103, Accuracy: 0.604093567251462\n",
      "Step: 285, Loss: 1.0589298009872437, Accuracy: 0.6048951048951049\n",
      "Step: 286, Loss: 1.2391959428787231, Accuracy: 0.6051103368176539\n",
      "Step: 287, Loss: 1.0760900974273682, Accuracy: 0.6059027777777778\n",
      "Step: 288, Loss: 1.1910072565078735, Accuracy: 0.606401384083045\n",
      "Step: 289, Loss: 1.394181728363037, Accuracy: 0.6060344827586207\n",
      "Step: 290, Loss: 1.0988109111785889, Accuracy: 0.606815578465063\n",
      "Step: 291, Loss: 1.1147451400756836, Accuracy: 0.6075913242009132\n",
      "Step: 292, Loss: 1.0245808362960815, Accuracy: 0.608646188850967\n",
      "Step: 293, Loss: 1.3142355680465698, Accuracy: 0.6085600907029478\n",
      "Step: 294, Loss: 1.3886221647262573, Accuracy: 0.6081920903954803\n",
      "Step: 295, Loss: 1.2640095949172974, Accuracy: 0.6083896396396397\n",
      "Step: 296, Loss: 1.479319453239441, Accuracy: 0.6077441077441077\n",
      "Step: 297, Loss: 1.0440155267715454, Accuracy: 0.6085011185682326\n",
      "Step: 298, Loss: 1.070057988166809, Accuracy: 0.6092530657748049\n",
      "Step: 299, Loss: 1.1719082593917847, Accuracy: 0.6097222222222223\n",
      "Step: 300, Loss: 1.069328784942627, Accuracy: 0.6104651162790697\n",
      "Step: 301, Loss: 1.5443482398986816, Accuracy: 0.6095474613686535\n",
      "Step: 302, Loss: 1.3139394521713257, Accuracy: 0.6094609460946094\n",
      "Step: 303, Loss: 1.217838168144226, Accuracy: 0.6096491228070176\n",
      "Step: 304, Loss: 1.2416855096817017, Accuracy: 0.6098360655737705\n",
      "Step: 305, Loss: 1.305829405784607, Accuracy: 0.6097494553376906\n",
      "Step: 306, Loss: 1.3024462461471558, Accuracy: 0.6096634093376765\n",
      "Step: 307, Loss: 1.2077112197875977, Accuracy: 0.6101190476190477\n",
      "Step: 308, Loss: 1.1489925384521484, Accuracy: 0.61084142394822\n",
      "Step: 309, Loss: 1.264182686805725, Accuracy: 0.611021505376344\n",
      "Step: 310, Loss: 1.3753395080566406, Accuracy: 0.6106645230439443\n",
      "Step: 311, Loss: 1.3304437398910522, Accuracy: 0.6105769230769231\n",
      "Step: 312, Loss: 1.186144232749939, Accuracy: 0.6110223642172524\n",
      "Step: 313, Loss: 1.1402679681777954, Accuracy: 0.6114649681528662\n",
      "Step: 314, Loss: 1.3247122764587402, Accuracy: 0.6113756613756614\n",
      "Step: 315, Loss: 1.2179538011550903, Accuracy: 0.6115506329113924\n",
      "Step: 316, Loss: 1.5312471389770508, Accuracy: 0.6106729758149316\n",
      "Step: 317, Loss: 1.1839035749435425, Accuracy: 0.6111111111111112\n",
      "Step: 318, Loss: 1.3292022943496704, Accuracy: 0.6110240334378265\n",
      "Step: 319, Loss: 1.2309480905532837, Accuracy: 0.6111979166666667\n",
      "Step: 320, Loss: 1.45774507522583, Accuracy: 0.6105919003115264\n",
      "Step: 321, Loss: 1.3260161876678467, Accuracy: 0.6105072463768116\n",
      "Step: 322, Loss: 1.159101963043213, Accuracy: 0.6109391124871001\n",
      "Step: 323, Loss: 1.1493794918060303, Accuracy: 0.6113683127572016\n",
      "Step: 324, Loss: 1.1855682134628296, Accuracy: 0.6117948717948718\n",
      "Step: 325, Loss: 1.5521892309188843, Accuracy: 0.6109406952965235\n",
      "Step: 326, Loss: 1.2104781866073608, Accuracy: 0.6113659531090724\n",
      "Step: 327, Loss: 1.0934590101242065, Accuracy: 0.6120426829268293\n",
      "Step: 328, Loss: 1.1046794652938843, Accuracy: 0.6127152988855117\n",
      "Step: 329, Loss: 1.22279691696167, Accuracy: 0.6128787878787879\n",
      "Step: 330, Loss: 1.2001537084579468, Accuracy: 0.6132930513595166\n",
      "Step: 331, Loss: 1.220167636871338, Accuracy: 0.6134538152610441\n",
      "Step: 332, Loss: 1.299200177192688, Accuracy: 0.6133633633633634\n",
      "Step: 333, Loss: 1.3262189626693726, Accuracy: 0.6132734530938124\n",
      "Step: 334, Loss: 1.124740719795227, Accuracy: 0.6139303482587065\n",
      "Step: 335, Loss: 1.3779205083847046, Accuracy: 0.6135912698412699\n",
      "Step: 336, Loss: 1.2451412677764893, Accuracy: 0.6137487636003957\n",
      "Step: 337, Loss: 1.2409902811050415, Accuracy: 0.613905325443787\n",
      "Step: 338, Loss: 1.4044371843338013, Accuracy: 0.6135693215339233\n",
      "Step: 339, Loss: 1.2405338287353516, Accuracy: 0.6137254901960785\n",
      "Step: 340, Loss: 1.2445417642593384, Accuracy: 0.613880742913001\n",
      "Step: 341, Loss: 1.3889116048812866, Accuracy: 0.6135477582846004\n",
      "Step: 342, Loss: 1.241699457168579, Accuracy: 0.6137026239067055\n",
      "Step: 343, Loss: 1.21370267868042, Accuracy: 0.6138565891472868\n",
      "Step: 344, Loss: 1.1613879203796387, Accuracy: 0.6142512077294686\n",
      "Step: 345, Loss: 1.159664511680603, Accuracy: 0.6146435452793835\n",
      "Step: 346, Loss: 1.5681720972061157, Accuracy: 0.6138328530259366\n",
      "Step: 347, Loss: 1.330989956855774, Accuracy: 0.6137452107279694\n",
      "Step: 348, Loss: 1.244036078453064, Accuracy: 0.6138968481375359\n",
      "Step: 349, Loss: 1.1269798278808594, Accuracy: 0.6142857142857143\n",
      "Step: 350, Loss: 1.3116618394851685, Accuracy: 0.6141975308641975\n",
      "Step: 351, Loss: 1.171180248260498, Accuracy: 0.6145833333333334\n",
      "Step: 352, Loss: 1.5473518371582031, Accuracy: 0.6137865911237016\n",
      "Step: 353, Loss: 1.2333399057388306, Accuracy: 0.6139359698681732\n",
      "Step: 354, Loss: 1.3075693845748901, Accuracy: 0.613849765258216\n",
      "Step: 355, Loss: 1.257595419883728, Accuracy: 0.6139981273408239\n",
      "Step: 356, Loss: 1.3197040557861328, Accuracy: 0.6139122315592904\n",
      "Step: 357, Loss: 1.0401806831359863, Accuracy: 0.6147579143389199\n",
      "Step: 358, Loss: 1.314927101135254, Accuracy: 0.6146703806870938\n",
      "Step: 359, Loss: 1.085079550743103, Accuracy: 0.6152777777777778\n",
      "Step: 360, Loss: 1.1583832502365112, Accuracy: 0.6156509695290858\n",
      "Step: 361, Loss: 1.2418240308761597, Accuracy: 0.615791896869245\n",
      "Step: 362, Loss: 1.2246272563934326, Accuracy: 0.6159320477502296\n",
      "Step: 363, Loss: 1.4570657014846802, Accuracy: 0.6153846153846154\n",
      "Step: 364, Loss: 1.244996190071106, Accuracy: 0.6155251141552511\n",
      "Step: 365, Loss: 1.2898045778274536, Accuracy: 0.6154371584699454\n",
      "Step: 366, Loss: 1.2346020936965942, Accuracy: 0.6155767484105359\n",
      "Step: 367, Loss: 1.2296996116638184, Accuracy: 0.6157155797101449\n",
      "Step: 368, Loss: 1.1808650493621826, Accuracy: 0.6160794941282746\n",
      "Step: 369, Loss: 1.0988630056381226, Accuracy: 0.6166666666666667\n",
      "Step: 370, Loss: 1.3862295150756836, Accuracy: 0.6163522012578616\n",
      "Step: 371, Loss: 1.4796298742294312, Accuracy: 0.6158154121863799\n",
      "Step: 372, Loss: 1.2449246644973755, Accuracy: 0.6159517426273459\n",
      "Step: 373, Loss: 1.0134865045547485, Accuracy: 0.6167557932263814\n",
      "Step: 374, Loss: 1.260637640953064, Accuracy: 0.6168888888888889\n",
      "Step: 375, Loss: 1.3154020309448242, Accuracy: 0.6167996453900709\n",
      "Step: 376, Loss: 1.4565166234970093, Accuracy: 0.6162687886825818\n",
      "Step: 377, Loss: 1.5561612844467163, Accuracy: 0.6155202821869489\n",
      "Step: 378, Loss: 1.1820639371871948, Accuracy: 0.6158751099384345\n",
      "Step: 379, Loss: 1.4020729064941406, Accuracy: 0.6155701754385965\n",
      "Step: 380, Loss: 1.3329355716705322, Accuracy: 0.615485564304462\n",
      "Step: 381, Loss: 1.3140790462493896, Accuracy: 0.6154013961605584\n",
      "Step: 382, Loss: 1.3699531555175781, Accuracy: 0.6151000870322019\n",
      "Step: 383, Loss: 1.2239233255386353, Accuracy: 0.615234375\n",
      "Step: 384, Loss: 1.2402498722076416, Accuracy: 0.6153679653679653\n",
      "Step: 385, Loss: 1.513540267944336, Accuracy: 0.6146373056994818\n",
      "Step: 386, Loss: 1.2323431968688965, Accuracy: 0.6147717484926787\n",
      "Step: 387, Loss: 1.1640291213989258, Accuracy: 0.6151202749140894\n",
      "Step: 388, Loss: 1.159608244895935, Accuracy: 0.6154670094258783\n",
      "Step: 389, Loss: 1.463955283164978, Accuracy: 0.614957264957265\n",
      "Step: 390, Loss: 1.5706228017807007, Accuracy: 0.6142369991474851\n",
      "Step: 391, Loss: 1.3824577331542969, Accuracy: 0.6139455782312925\n",
      "Step: 392, Loss: 1.3659006357192993, Accuracy: 0.6136556403731976\n",
      "Step: 393, Loss: 1.3653303384780884, Accuracy: 0.6133671742808798\n",
      "Step: 394, Loss: 1.1773778200149536, Accuracy: 0.6137130801687763\n",
      "Step: 395, Loss: 1.3052797317504883, Accuracy: 0.6136363636363636\n",
      "Step: 396, Loss: 1.0756944417953491, Accuracy: 0.6141897565071368\n",
      "Step: 397, Loss: 1.1431516408920288, Accuracy: 0.6145309882747069\n",
      "Step: 398, Loss: 1.0863771438598633, Accuracy: 0.6150793650793651\n",
      "Step: 399, Loss: 1.3037418127059937, Accuracy: 0.615\n",
      "Step: 400, Loss: 1.1704145669937134, Accuracy: 0.6153366583541147\n",
      "Step: 401, Loss: 1.2293094396591187, Accuracy: 0.6154643449419569\n",
      "Step: 402, Loss: 1.2909269332885742, Accuracy: 0.6153846153846154\n",
      "Step: 403, Loss: 1.4082809686660767, Accuracy: 0.6150990099009901\n",
      "Step: 404, Loss: 1.452508807182312, Accuracy: 0.6146090534979424\n",
      "Step: 405, Loss: 1.1553902626037598, Accuracy: 0.6149425287356322\n",
      "Step: 406, Loss: 1.2684143781661987, Accuracy: 0.6148648648648649\n",
      "Step: 407, Loss: 1.2417277097702026, Accuracy: 0.6149918300653595\n",
      "Step: 408, Loss: 1.3363990783691406, Accuracy: 0.6149144254278729\n",
      "Step: 409, Loss: 1.3138511180877686, Accuracy: 0.6148373983739838\n",
      "Step: 410, Loss: 1.3101290464401245, Accuracy: 0.6147607461476075\n",
      "Step: 411, Loss: 1.253579020500183, Accuracy: 0.6148867313915858\n",
      "Step: 412, Loss: 1.388187050819397, Accuracy: 0.6146085552865214\n",
      "Step: 413, Loss: 1.2652491331100464, Accuracy: 0.6147342995169082\n",
      "Step: 414, Loss: 1.1829904317855835, Accuracy: 0.614859437751004\n",
      "Step: 415, Loss: 1.2569555044174194, Accuracy: 0.6149839743589743\n",
      "Step: 416, Loss: 1.2503002882003784, Accuracy: 0.6151079136690647\n",
      "Step: 417, Loss: 1.2265013456344604, Accuracy: 0.6152312599681021\n",
      "Step: 418, Loss: 1.3314369916915894, Accuracy: 0.6151551312649165\n",
      "Step: 419, Loss: 1.0744365453720093, Accuracy: 0.6156746031746032\n",
      "Step: 420, Loss: 1.1113829612731934, Accuracy: 0.6161916072842438\n",
      "Step: 421, Loss: 1.3039387464523315, Accuracy: 0.6161137440758294\n",
      "Step: 422, Loss: 1.2175029516220093, Accuracy: 0.6162332545311269\n",
      "Step: 423, Loss: 1.2126425504684448, Accuracy: 0.6163522012578616\n",
      "Step: 424, Loss: 1.432774543762207, Accuracy: 0.6158823529411764\n",
      "Step: 425, Loss: 1.314573884010315, Accuracy: 0.6158059467918623\n",
      "Step: 426, Loss: 1.3456882238388062, Accuracy: 0.6157298985167837\n",
      "Step: 427, Loss: 1.0862977504730225, Accuracy: 0.6162383177570093\n",
      "Step: 428, Loss: 1.1654412746429443, Accuracy: 0.6165501165501166\n",
      "Step: 429, Loss: 1.1234996318817139, Accuracy: 0.6168604651162791\n",
      "Step: 430, Loss: 1.4718915224075317, Accuracy: 0.6163959783449343\n",
      "Step: 431, Loss: 1.2380646467208862, Accuracy: 0.6165123456790124\n",
      "Step: 432, Loss: 1.0902034044265747, Accuracy: 0.6170130869899924\n",
      "Step: 433, Loss: 1.2136449813842773, Accuracy: 0.6171274961597543\n",
      "Step: 434, Loss: 1.0238230228424072, Accuracy: 0.617816091954023\n",
      "Step: 435, Loss: 1.4758802652359009, Accuracy: 0.6173547400611621\n",
      "Step: 436, Loss: 1.3983322381973267, Accuracy: 0.6170861937452327\n",
      "Step: 437, Loss: 1.2358601093292236, Accuracy: 0.617199391171994\n",
      "Step: 438, Loss: 1.3366413116455078, Accuracy: 0.6171222475322703\n",
      "Step: 439, Loss: 1.4007116556167603, Accuracy: 0.6168560606060606\n",
      "Step: 440, Loss: 1.166406273841858, Accuracy: 0.6171579743008314\n",
      "Step: 441, Loss: 1.2324994802474976, Accuracy: 0.6172699849170438\n",
      "Step: 442, Loss: 0.9577589631080627, Accuracy: 0.6181339352896915\n",
      "Step: 443, Loss: 1.3570650815963745, Accuracy: 0.6178678678678678\n",
      "Step: 444, Loss: 1.3972409963607788, Accuracy: 0.6176029962546816\n",
      "Step: 445, Loss: 1.3951143026351929, Accuracy: 0.617339312406577\n",
      "Step: 446, Loss: 1.1625409126281738, Accuracy: 0.6176360924683072\n",
      "Step: 447, Loss: 1.2383136749267578, Accuracy: 0.6177455357142857\n",
      "Step: 448, Loss: 1.0786676406860352, Accuracy: 0.6182256867112101\n",
      "Step: 449, Loss: 1.0736571550369263, Accuracy: 0.6187037037037038\n",
      "Step: 450, Loss: 1.3835893869400024, Accuracy: 0.618440502586844\n",
      "Step: 451, Loss: 1.5045768022537231, Accuracy: 0.6179941002949852\n",
      "Step: 452, Loss: 1.1325241327285767, Accuracy: 0.6182855040470935\n",
      "Step: 453, Loss: 1.213100790977478, Accuracy: 0.6183920704845814\n",
      "Step: 454, Loss: 1.3414325714111328, Accuracy: 0.6183150183150183\n",
      "Step: 455, Loss: 1.3042640686035156, Accuracy: 0.6182383040935673\n",
      "Step: 456, Loss: 1.3538156747817993, Accuracy: 0.6181619256017505\n",
      "Step: 457, Loss: 1.1677067279815674, Accuracy: 0.6184497816593887\n",
      "Step: 458, Loss: 1.0762683153152466, Accuracy: 0.6189179375453885\n",
      "Step: 459, Loss: 1.40554678440094, Accuracy: 0.6186594202898551\n",
      "Step: 460, Loss: 1.2320667505264282, Accuracy: 0.618763557483731\n",
      "Step: 461, Loss: 1.225014090538025, Accuracy: 0.6188672438672439\n",
      "Step: 462, Loss: 1.326066493988037, Accuracy: 0.6187904967602592\n",
      "Step: 463, Loss: 1.309203863143921, Accuracy: 0.6187140804597702\n",
      "Step: 464, Loss: 1.3472188711166382, Accuracy: 0.6186379928315412\n",
      "Step: 465, Loss: 1.3459091186523438, Accuracy: 0.6183834048640916\n",
      "Step: 466, Loss: 1.305812954902649, Accuracy: 0.6183083511777302\n",
      "Step: 467, Loss: 1.3163131475448608, Accuracy: 0.6182336182336182\n",
      "Step: 468, Loss: 1.290108323097229, Accuracy: 0.6181592039800995\n",
      "Step: 469, Loss: 1.3612152338027954, Accuracy: 0.6179078014184397\n",
      "Step: 470, Loss: 1.2810567617416382, Accuracy: 0.6178343949044586\n",
      "Step: 471, Loss: 1.1580263376235962, Accuracy: 0.618114406779661\n",
      "Step: 472, Loss: 1.2213116884231567, Accuracy: 0.6182170542635659\n",
      "Step: 473, Loss: 1.4130223989486694, Accuracy: 0.6179676511954993\n",
      "Step: 474, Loss: 1.2303963899612427, Accuracy: 0.6180701754385964\n",
      "Step: 475, Loss: 1.254767894744873, Accuracy: 0.618172268907563\n",
      "Step: 476, Loss: 1.0903252363204956, Accuracy: 0.6186233403214535\n",
      "Step: 477, Loss: 1.1933594942092896, Accuracy: 0.6188981868898187\n",
      "Step: 478, Loss: 1.3105436563491821, Accuracy: 0.6188239387613083\n",
      "Step: 479, Loss: 1.1517539024353027, Accuracy: 0.6190972222222222\n",
      "Step: 480, Loss: 1.2186945676803589, Accuracy: 0.6191961191961192\n",
      "Step: 481, Loss: 1.2475826740264893, Accuracy: 0.6192946058091287\n",
      "Step: 482, Loss: 1.2324811220169067, Accuracy: 0.6193926846100759\n",
      "Step: 483, Loss: 1.377068042755127, Accuracy: 0.6191460055096418\n",
      "Step: 484, Loss: 1.247503399848938, Accuracy: 0.6192439862542956\n",
      "Step: 485, Loss: 1.5488990545272827, Accuracy: 0.6186556927297668\n",
      "Step: 486, Loss: 1.146694540977478, Accuracy: 0.6189253935660507\n",
      "Step: 487, Loss: 1.2443493604660034, Accuracy: 0.6190232240437158\n",
      "Step: 488, Loss: 1.320011854171753, Accuracy: 0.6189502385821404\n",
      "Step: 489, Loss: 1.155921220779419, Accuracy: 0.6192176870748299\n",
      "Step: 490, Loss: 1.3263212442398071, Accuracy: 0.6191446028513238\n",
      "Step: 491, Loss: 1.2920280694961548, Accuracy: 0.6190718157181572\n",
      "Step: 492, Loss: 1.2308100461959839, Accuracy: 0.6191683569979716\n",
      "Step: 493, Loss: 1.3395484685897827, Accuracy: 0.6190958164642375\n",
      "Step: 494, Loss: 1.2505320310592651, Accuracy: 0.6191919191919192\n",
      "Step: 495, Loss: 1.2851033210754395, Accuracy: 0.6192876344086021\n",
      "Step: 496, Loss: 1.2370915412902832, Accuracy: 0.619382964453387\n",
      "Step: 497, Loss: 1.1508755683898926, Accuracy: 0.6196452476572959\n",
      "Step: 498, Loss: 1.189164638519287, Accuracy: 0.6197394789579158\n",
      "Step: 499, Loss: 1.171287178993225, Accuracy: 0.62\n",
      "Step: 500, Loss: 1.239548921585083, Accuracy: 0.6200931470392548\n",
      "Step: 501, Loss: 1.3236969709396362, Accuracy: 0.6200199203187251\n",
      "Step: 502, Loss: 1.3798617124557495, Accuracy: 0.6197813121272365\n",
      "Step: 503, Loss: 1.160220742225647, Accuracy: 0.6200396825396826\n",
      "Step: 504, Loss: 1.2660709619522095, Accuracy: 0.6199669966996699\n",
      "Step: 505, Loss: 1.0769593715667725, Accuracy: 0.6203886693017128\n",
      "Step: 506, Loss: 1.3988395929336548, Accuracy: 0.6201512163050624\n",
      "Step: 507, Loss: 1.3966041803359985, Accuracy: 0.6199146981627297\n",
      "Step: 508, Loss: 1.2025331258773804, Accuracy: 0.6201702685003274\n",
      "Step: 509, Loss: 1.2186847925186157, Accuracy: 0.6202614379084967\n",
      "Step: 510, Loss: 1.1571455001831055, Accuracy: 0.620515329419439\n",
      "Step: 511, Loss: 1.237922191619873, Accuracy: 0.62060546875\n",
      "Step: 512, Loss: 1.388526439666748, Accuracy: 0.6203703703703703\n",
      "Step: 513, Loss: 1.3919099569320679, Accuracy: 0.620136186770428\n",
      "Step: 514, Loss: 1.1635574102401733, Accuracy: 0.6203883495145631\n",
      "Step: 515, Loss: 1.5341191291809082, Accuracy: 0.6198320413436692\n",
      "Step: 516, Loss: 1.531760334968567, Accuracy: 0.619277885235332\n",
      "Step: 517, Loss: 1.3110724687576294, Accuracy: 0.6192084942084942\n",
      "Step: 518, Loss: 1.3333683013916016, Accuracy: 0.6191393705844573\n",
      "Step: 519, Loss: 1.2852319478988647, Accuracy: 0.6190705128205128\n",
      "Step: 520, Loss: 1.1615766286849976, Accuracy: 0.619321817018554\n",
      "Step: 521, Loss: 1.2140775918960571, Accuracy: 0.6194125159642401\n",
      "Step: 522, Loss: 1.0866295099258423, Accuracy: 0.6198215423836839\n",
      "Step: 523, Loss: 1.411630630493164, Accuracy: 0.6195928753180662\n",
      "Step: 524, Loss: 1.4275425672531128, Accuracy: 0.6193650793650793\n",
      "Step: 525, Loss: 1.3137377500534058, Accuracy: 0.6192965779467681\n",
      "Step: 526, Loss: 1.1655502319335938, Accuracy: 0.6195445920303605\n",
      "Step: 527, Loss: 1.181991696357727, Accuracy: 0.6197916666666666\n",
      "Step: 528, Loss: 1.3265691995620728, Accuracy: 0.6197227473219912\n",
      "Step: 529, Loss: 1.2340848445892334, Accuracy: 0.6199685534591195\n",
      "Step: 530, Loss: 1.2320713996887207, Accuracy: 0.6200564971751412\n",
      "Step: 531, Loss: 1.1910690069198608, Accuracy: 0.6201441102756893\n",
      "Step: 532, Loss: 1.3973134756088257, Accuracy: 0.6199186991869918\n",
      "Step: 533, Loss: 1.2160435914993286, Accuracy: 0.6200062421972534\n",
      "Step: 534, Loss: 1.194897174835205, Accuracy: 0.6200934579439252\n",
      "Step: 535, Loss: 1.24734365940094, Accuracy: 0.6201803482587065\n",
      "Step: 536, Loss: 1.228527545928955, Accuracy: 0.6202669149596524\n",
      "Step: 537, Loss: 1.3624743223190308, Accuracy: 0.6200433705080545\n",
      "Step: 538, Loss: 1.1532858610153198, Accuracy: 0.6202844774273346\n",
      "Step: 539, Loss: 1.3754504919052124, Accuracy: 0.6200617283950617\n",
      "Step: 540, Loss: 1.3221279382705688, Accuracy: 0.6199938385705484\n",
      "Step: 541, Loss: 1.2410533428192139, Accuracy: 0.620079950799508\n",
      "Step: 542, Loss: 1.3745115995407104, Accuracy: 0.6198588090853284\n",
      "Step: 543, Loss: 1.2530887126922607, Accuracy: 0.6199448529411765\n",
      "Step: 544, Loss: 1.2161486148834229, Accuracy: 0.6200305810397554\n",
      "Step: 545, Loss: 1.0588479042053223, Accuracy: 0.6204212454212454\n",
      "Step: 546, Loss: 1.4526872634887695, Accuracy: 0.6200487507617306\n",
      "Step: 547, Loss: 1.4894965887069702, Accuracy: 0.6196776155717761\n",
      "Step: 548, Loss: 1.2649177312850952, Accuracy: 0.6197632058287796\n",
      "Step: 549, Loss: 1.2764250040054321, Accuracy: 0.6196969696969697\n",
      "Step: 550, Loss: 1.309605360031128, Accuracy: 0.6196309739866909\n",
      "Step: 551, Loss: 1.3168344497680664, Accuracy: 0.6195652173913043\n",
      "Step: 552, Loss: 1.3148255348205566, Accuracy: 0.6194996986136226\n",
      "Step: 553, Loss: 1.387372374534607, Accuracy: 0.6192839951865222\n",
      "Step: 554, Loss: 1.247341513633728, Accuracy: 0.6193693693693694\n",
      "Step: 555, Loss: 1.3211692571640015, Accuracy: 0.6193045563549161\n",
      "Step: 556, Loss: 1.0161298513412476, Accuracy: 0.61983842010772\n",
      "Step: 557, Loss: 1.2427529096603394, Accuracy: 0.6199223416965353\n",
      "Step: 558, Loss: 1.162380337715149, Accuracy: 0.6201550387596899\n",
      "Step: 559, Loss: 1.4422979354858398, Accuracy: 0.6197916666666666\n",
      "Step: 560, Loss: 1.2343084812164307, Accuracy: 0.6198752228163993\n",
      "Step: 561, Loss: 1.2316476106643677, Accuracy: 0.6199584816132859\n",
      "Step: 562, Loss: 1.144973874092102, Accuracy: 0.6201894612196566\n",
      "Step: 563, Loss: 1.3104702234268188, Accuracy: 0.6201241134751773\n",
      "Step: 564, Loss: 1.1628931760787964, Accuracy: 0.620353982300885\n",
      "Step: 565, Loss: 1.3797730207443237, Accuracy: 0.6201413427561837\n",
      "Step: 566, Loss: 1.2296792268753052, Accuracy: 0.6202233980011758\n",
      "Step: 567, Loss: 1.2329788208007812, Accuracy: 0.6203051643192489\n",
      "Step: 568, Loss: 1.5675767660140991, Accuracy: 0.619800820152314\n",
      "Step: 569, Loss: 1.1841676235198975, Accuracy: 0.6200292397660818\n",
      "Step: 570, Loss: 1.30136239528656, Accuracy: 0.6199649737302977\n",
      "Step: 571, Loss: 1.1987979412078857, Accuracy: 0.6201923076923077\n",
      "Step: 572, Loss: 1.2757073640823364, Accuracy: 0.6202734147760326\n",
      "Step: 573, Loss: 1.2110480070114136, Accuracy: 0.620499419279907\n",
      "Step: 574, Loss: 1.348685622215271, Accuracy: 0.6204347826086957\n",
      "Step: 575, Loss: 1.2206989526748657, Accuracy: 0.6205150462962963\n",
      "Step: 576, Loss: 1.0721710920333862, Accuracy: 0.6208838821490468\n",
      "Step: 577, Loss: 1.2257992029190063, Accuracy: 0.6209630911188004\n",
      "Step: 578, Loss: 1.3772934675216675, Accuracy: 0.6207541738629822\n",
      "Step: 579, Loss: 1.1925352811813354, Accuracy: 0.6209770114942529\n",
      "Step: 580, Loss: 1.162676215171814, Accuracy: 0.6211990820424556\n",
      "Step: 581, Loss: 1.2178821563720703, Accuracy: 0.6212772050400917\n",
      "Step: 582, Loss: 1.1263090372085571, Accuracy: 0.6214979988564894\n",
      "Step: 583, Loss: 1.4079991579055786, Accuracy: 0.6212899543378996\n",
      "Step: 584, Loss: 1.3810420036315918, Accuracy: 0.6210826210826211\n",
      "Step: 585, Loss: 1.1266976594924927, Accuracy: 0.6213026166097838\n",
      "Step: 586, Loss: 1.2834160327911377, Accuracy: 0.6212379329926179\n",
      "Step: 587, Loss: 1.2901588678359985, Accuracy: 0.6211734693877551\n",
      "Step: 588, Loss: 1.3683595657348633, Accuracy: 0.6209677419354839\n",
      "Step: 589, Loss: 1.2361640930175781, Accuracy: 0.621045197740113\n",
      "Step: 590, Loss: 1.532896876335144, Accuracy: 0.6205583756345178\n",
      "Step: 591, Loss: 1.0123376846313477, Accuracy: 0.6210585585585585\n",
      "Step: 592, Loss: 1.1984392404556274, Accuracy: 0.6212759977515458\n",
      "Step: 593, Loss: 1.22414231300354, Accuracy: 0.6213524130190797\n",
      "Step: 594, Loss: 1.2131940126419067, Accuracy: 0.6214285714285714\n",
      "Step: 595, Loss: 1.1086183786392212, Accuracy: 0.6216442953020134\n",
      "Step: 596, Loss: 1.0157592296600342, Accuracy: 0.6221384701284198\n",
      "Step: 597, Loss: 1.093254566192627, Accuracy: 0.6223522853957637\n",
      "Step: 598, Loss: 1.3119276762008667, Accuracy: 0.6222871452420701\n",
      "Step: 599, Loss: 1.235166072845459, Accuracy: 0.6223611111111111\n",
      "Step: 600, Loss: 1.1934125423431396, Accuracy: 0.6224348308374931\n",
      "Step: 601, Loss: 1.089791178703308, Accuracy: 0.6227851605758582\n",
      "Step: 602, Loss: 1.0786232948303223, Accuracy: 0.6231343283582089\n",
      "Step: 603, Loss: 1.326127052307129, Accuracy: 0.6230684326710817\n",
      "Step: 604, Loss: 0.9912647604942322, Accuracy: 0.6235537190082645\n",
      "Step: 605, Loss: 1.1740111112594604, Accuracy: 0.6237623762376238\n",
      "Step: 606, Loss: 1.2785643339157104, Accuracy: 0.6236957715540912\n",
      "Step: 607, Loss: 1.082267165184021, Accuracy: 0.6240405701754386\n",
      "Step: 608, Loss: 1.223286747932434, Accuracy: 0.6241105637657361\n",
      "Step: 609, Loss: 1.2437328100204468, Accuracy: 0.6241803278688525\n",
      "Step: 610, Loss: 1.2443581819534302, Accuracy: 0.6242498636115658\n",
      "Step: 611, Loss: 1.3303834199905396, Accuracy: 0.6241830065359477\n",
      "Step: 612, Loss: 1.1935863494873047, Accuracy: 0.6242523110386079\n",
      "Step: 613, Loss: 1.2072709798812866, Accuracy: 0.6243213897937026\n",
      "Step: 614, Loss: 1.2975624799728394, Accuracy: 0.6242547425474255\n",
      "Step: 615, Loss: 1.4654144048690796, Accuracy: 0.6239177489177489\n",
      "Step: 616, Loss: 1.1781927347183228, Accuracy: 0.6241220961642355\n",
      "Step: 617, Loss: 1.3684002161026, Accuracy: 0.6240560949298813\n",
      "Step: 618, Loss: 1.156622052192688, Accuracy: 0.6242595584275713\n",
      "Step: 619, Loss: 1.2998806238174438, Accuracy: 0.6241935483870967\n",
      "Step: 620, Loss: 1.3149421215057373, Accuracy: 0.6241277509393451\n",
      "Step: 621, Loss: 1.5104573965072632, Accuracy: 0.6237942122186495\n",
      "Step: 622, Loss: 1.0839158296585083, Accuracy: 0.6241305510968432\n",
      "Step: 623, Loss: 1.2822450399398804, Accuracy: 0.6240651709401709\n",
      "Step: 624, Loss: 1.2499433755874634, Accuracy: 0.6241333333333333\n",
      "Step: 625, Loss: 1.187379002571106, Accuracy: 0.6243343982960596\n",
      "Step: 626, Loss: 1.3182214498519897, Accuracy: 0.6242690058479532\n",
      "Step: 627, Loss: 1.1795421838760376, Accuracy: 0.6244692144373672\n",
      "Step: 628, Loss: 1.066116213798523, Accuracy: 0.6248012718600954\n",
      "Step: 629, Loss: 1.2179052829742432, Accuracy: 0.6248677248677249\n",
      "Step: 630, Loss: 1.1682883501052856, Accuracy: 0.6250660327522451\n",
      "Step: 631, Loss: 1.3677729368209839, Accuracy: 0.625\n",
      "Step: 632, Loss: 1.183679223060608, Accuracy: 0.6250658241179569\n",
      "Step: 633, Loss: 1.2349334955215454, Accuracy: 0.6251314405888538\n",
      "Step: 634, Loss: 1.3054765462875366, Accuracy: 0.6250656167979003\n",
      "Step: 635, Loss: 1.2315903902053833, Accuracy: 0.6251310272536688\n",
      "Step: 636, Loss: 1.459549069404602, Accuracy: 0.6248037676609105\n",
      "Step: 637, Loss: 1.2310999631881714, Accuracy: 0.6248693834900731\n",
      "Step: 638, Loss: 1.255730390548706, Accuracy: 0.6249347939488784\n",
      "Step: 639, Loss: 1.1323281526565552, Accuracy: 0.6252604166666667\n",
      "Step: 640, Loss: 1.1595085859298706, Accuracy: 0.625455018200728\n",
      "Step: 641, Loss: 1.4887375831604004, Accuracy: 0.6251298026998962\n",
      "Step: 642, Loss: 1.0214542150497437, Accuracy: 0.6255832037325039\n",
      "Step: 643, Loss: 1.2215615510940552, Accuracy: 0.6256469979296067\n",
      "Step: 644, Loss: 1.4490957260131836, Accuracy: 0.6253229974160207\n",
      "Step: 645, Loss: 1.1703945398330688, Accuracy: 0.625515995872033\n",
      "Step: 646, Loss: 1.460997223854065, Accuracy: 0.625193199381762\n",
      "Step: 647, Loss: 1.2442933320999146, Accuracy: 0.6252572016460906\n",
      "Step: 648, Loss: 1.253409504890442, Accuracy: 0.6253210066769389\n",
      "Step: 649, Loss: 1.3654812574386597, Accuracy: 0.6251282051282051\n",
      "Step: 650, Loss: 1.2812081575393677, Accuracy: 0.6250640040962622\n",
      "Step: 651, Loss: 1.4553356170654297, Accuracy: 0.6247443762781186\n",
      "Step: 652, Loss: 1.3320759534835815, Accuracy: 0.6246809596733027\n",
      "Step: 653, Loss: 1.3907923698425293, Accuracy: 0.6244903160040775\n",
      "Step: 654, Loss: 1.3313442468643188, Accuracy: 0.6244274809160305\n",
      "Step: 655, Loss: 1.6354894638061523, Accuracy: 0.6238567073170732\n",
      "Step: 656, Loss: 1.5022320747375488, Accuracy: 0.6234145104008117\n",
      "Step: 657, Loss: 1.2418521642684937, Accuracy: 0.6234802431610942\n",
      "Step: 658, Loss: 1.2187896966934204, Accuracy: 0.6235457764289327\n",
      "Step: 659, Loss: 1.4227863550186157, Accuracy: 0.6232323232323232\n",
      "Step: 660, Loss: 1.2898138761520386, Accuracy: 0.623171961674231\n",
      "Step: 661, Loss: 1.1667550802230835, Accuracy: 0.6233635448136958\n",
      "Step: 662, Loss: 1.2816330194473267, Accuracy: 0.6233031674208145\n",
      "Step: 663, Loss: 1.3967591524124146, Accuracy: 0.6231174698795181\n",
      "Step: 664, Loss: 1.3130155801773071, Accuracy: 0.6230576441102756\n",
      "Step: 665, Loss: 1.2294880151748657, Accuracy: 0.6231231231231231\n",
      "Step: 666, Loss: 1.0993036031723022, Accuracy: 0.6234382808595702\n",
      "Step: 667, Loss: 1.4456175565719604, Accuracy: 0.6231287425149701\n",
      "Step: 668, Loss: 1.400307059288025, Accuracy: 0.6229446935724963\n",
      "Step: 669, Loss: 1.152090311050415, Accuracy: 0.6231343283582089\n",
      "Step: 670, Loss: 1.633714199066162, Accuracy: 0.6225782414307004\n",
      "Step: 671, Loss: 1.1644694805145264, Accuracy: 0.6227678571428571\n",
      "Step: 672, Loss: 1.2381879091262817, Accuracy: 0.6227092620108965\n",
      "Step: 673, Loss: 1.1924302577972412, Accuracy: 0.6228981206726014\n",
      "Step: 674, Loss: 1.2351552248001099, Accuracy: 0.6229629629629629\n",
      "Step: 675, Loss: 1.245162844657898, Accuracy: 0.6230276134122288\n",
      "Step: 676, Loss: 1.2884039878845215, Accuracy: 0.6229689807976366\n",
      "Step: 677, Loss: 1.4035617113113403, Accuracy: 0.6227876106194691\n",
      "Step: 678, Loss: 1.4047712087631226, Accuracy: 0.6226067746686303\n",
      "Step: 679, Loss: 1.1567410230636597, Accuracy: 0.6227941176470588\n",
      "Step: 680, Loss: 1.2311586141586304, Accuracy: 0.622858541360744\n",
      "Step: 681, Loss: 1.226272702217102, Accuracy: 0.6229227761485826\n",
      "Step: 682, Loss: 1.3229678869247437, Accuracy: 0.6228648121034651\n",
      "Step: 683, Loss: 1.3776479959487915, Accuracy: 0.6226851851851852\n",
      "Step: 684, Loss: 1.3179904222488403, Accuracy: 0.6226277372262774\n",
      "Step: 685, Loss: 1.3249164819717407, Accuracy: 0.6225704567541303\n",
      "Step: 686, Loss: 1.0629466772079468, Accuracy: 0.6228772440562833\n",
      "Step: 687, Loss: 1.2905867099761963, Accuracy: 0.6229408914728682\n",
      "Step: 688, Loss: 1.4417847394943237, Accuracy: 0.6226415094339622\n",
      "Step: 689, Loss: 1.3498066663742065, Accuracy: 0.6224637681159421\n",
      "Step: 690, Loss: 1.514646053314209, Accuracy: 0.6220453449107574\n",
      "Step: 691, Loss: 1.1842577457427979, Accuracy: 0.6222302504816956\n",
      "Step: 692, Loss: 1.146782636642456, Accuracy: 0.6224146224146224\n",
      "Step: 693, Loss: 1.340706467628479, Accuracy: 0.6223583093179635\n",
      "Step: 694, Loss: 1.0648667812347412, Accuracy: 0.6226618705035971\n",
      "Step: 695, Loss: 1.0800024271011353, Accuracy: 0.6229645593869731\n",
      "Step: 696, Loss: 1.3880524635314941, Accuracy: 0.6227881396461024\n",
      "Step: 697, Loss: 1.3220150470733643, Accuracy: 0.6227316141356256\n",
      "Step: 698, Loss: 1.0744889974594116, Accuracy: 0.623032904148784\n",
      "Step: 699, Loss: 1.3339905738830566, Accuracy: 0.6229761904761905\n",
      "Step: 700, Loss: 1.2514657974243164, Accuracy: 0.6229196386115073\n",
      "Step: 701, Loss: 1.463788628578186, Accuracy: 0.6226258309591642\n",
      "Step: 702, Loss: 1.2385501861572266, Accuracy: 0.6226884779516358\n",
      "Step: 703, Loss: 1.5696005821228027, Accuracy: 0.6222774621212122\n",
      "Step: 704, Loss: 1.2500351667404175, Accuracy: 0.6223404255319149\n",
      "Step: 705, Loss: 1.1605134010314941, Accuracy: 0.6225212464589235\n",
      "Step: 706, Loss: 1.2448042631149292, Accuracy: 0.6225836869401226\n",
      "Step: 707, Loss: 1.5594402551651, Accuracy: 0.6221751412429378\n",
      "Step: 708, Loss: 1.4460808038711548, Accuracy: 0.6218852844381758\n",
      "Step: 709, Loss: 1.2812992334365845, Accuracy: 0.6218309859154929\n",
      "Step: 710, Loss: 1.0842502117156982, Accuracy: 0.6221284575714956\n",
      "Step: 711, Loss: 1.2048954963684082, Accuracy: 0.6221910112359551\n",
      "Step: 712, Loss: 1.1377488374710083, Accuracy: 0.6223702664796634\n",
      "Step: 713, Loss: 1.251060128211975, Accuracy: 0.6224323062558357\n",
      "Step: 714, Loss: 1.2317603826522827, Accuracy: 0.6224941724941725\n",
      "Step: 715, Loss: 1.3686546087265015, Accuracy: 0.6223230912476723\n",
      "Step: 716, Loss: 1.517660140991211, Accuracy: 0.6219200371920037\n",
      "Step: 717, Loss: 1.1899099349975586, Accuracy: 0.6220984215413184\n",
      "Step: 718, Loss: 1.196825623512268, Accuracy: 0.622160407974038\n",
      "Step: 719, Loss: 1.5593112707138062, Accuracy: 0.6217592592592592\n",
      "Step: 720, Loss: 1.2275137901306152, Accuracy: 0.6218215441516413\n",
      "Step: 721, Loss: 1.3136183023452759, Accuracy: 0.6217682363804248\n",
      "Step: 722, Loss: 1.3235783576965332, Accuracy: 0.6215998155832181\n",
      "Step: 723, Loss: 1.2684155702590942, Accuracy: 0.6216620626151013\n",
      "Step: 724, Loss: 1.1853631734848022, Accuracy: 0.6218390804597701\n",
      "Step: 725, Loss: 1.0963683128356934, Accuracy: 0.6221303948576676\n",
      "Step: 726, Loss: 1.3214970827102661, Accuracy: 0.6220770288858322\n",
      "Step: 727, Loss: 1.2774430513381958, Accuracy: 0.6220238095238095\n",
      "Step: 728, Loss: 1.1312471628189087, Accuracy: 0.6223136716963877\n",
      "Step: 729, Loss: 1.4797416925430298, Accuracy: 0.6220319634703196\n",
      "Step: 730, Loss: 1.4237085580825806, Accuracy: 0.6218650250797993\n",
      "Step: 731, Loss: 1.2970808744430542, Accuracy: 0.6218123861566485\n",
      "Step: 732, Loss: 1.2740761041641235, Accuracy: 0.621987266939518\n",
      "Step: 733, Loss: 1.0773733854293823, Accuracy: 0.622275204359673\n",
      "Step: 734, Loss: 1.1443274021148682, Accuracy: 0.6224489795918368\n",
      "Step: 735, Loss: 1.3874636888504028, Accuracy: 0.6222826086956522\n",
      "Step: 736, Loss: 1.248779535293579, Accuracy: 0.6223428312980552\n",
      "Step: 737, Loss: 1.1775892972946167, Accuracy: 0.6225158084914183\n",
      "Step: 738, Loss: 1.244062900543213, Accuracy: 0.622575552548489\n",
      "Step: 739, Loss: 1.322287917137146, Accuracy: 0.6225225225225225\n",
      "Step: 740, Loss: 1.404003620147705, Accuracy: 0.622357174988754\n",
      "Step: 741, Loss: 1.159748911857605, Accuracy: 0.6225292003593891\n",
      "Step: 742, Loss: 1.3337985277175903, Accuracy: 0.6224764468371468\n",
      "Step: 743, Loss: 1.3688284158706665, Accuracy: 0.6223118279569892\n",
      "Step: 744, Loss: 1.4657224416732788, Accuracy: 0.6220357941834452\n",
      "Step: 745, Loss: 1.2313803434371948, Accuracy: 0.6220956210902592\n",
      "Step: 746, Loss: 1.1506026983261108, Accuracy: 0.6222668451584115\n",
      "Step: 747, Loss: 1.2125718593597412, Accuracy: 0.6223262032085561\n",
      "Step: 748, Loss: 1.154262661933899, Accuracy: 0.6224966622162884\n",
      "Step: 749, Loss: 1.25077486038208, Accuracy: 0.6225555555555555\n",
      "Step: 750, Loss: 1.3930912017822266, Accuracy: 0.6223923657345761\n",
      "Step: 751, Loss: 1.3157376050949097, Accuracy: 0.6223404255319149\n",
      "Step: 752, Loss: 1.2141244411468506, Accuracy: 0.6225099601593626\n",
      "Step: 753, Loss: 1.3086020946502686, Accuracy: 0.6224580017683466\n",
      "Step: 754, Loss: 1.2033568620681763, Accuracy: 0.6225165562913907\n",
      "Step: 755, Loss: 1.23439621925354, Accuracy: 0.6225749559082893\n",
      "Step: 756, Loss: 1.4019370079040527, Accuracy: 0.6224130339057684\n",
      "Step: 757, Loss: 1.2502387762069702, Accuracy: 0.622471416007036\n",
      "Step: 758, Loss: 1.3877954483032227, Accuracy: 0.6223100570926657\n",
      "Step: 759, Loss: 1.2786781787872314, Accuracy: 0.6223684210526316\n",
      "Step: 760, Loss: 1.455381989479065, Accuracy: 0.6220981165133597\n",
      "Step: 761, Loss: 1.237573266029358, Accuracy: 0.6221566054243219\n",
      "Step: 762, Loss: 1.1485713720321655, Accuracy: 0.6223241590214067\n",
      "Step: 763, Loss: 1.1820677518844604, Accuracy: 0.6224912739965096\n",
      "Step: 764, Loss: 1.3687515258789062, Accuracy: 0.6223311546840958\n",
      "Step: 765, Loss: 1.3017579317092896, Accuracy: 0.6222802436901653\n",
      "Step: 766, Loss: 1.2884886264801025, Accuracy: 0.6222294654498044\n",
      "Step: 767, Loss: 1.2359482049942017, Accuracy: 0.6222873263888888\n",
      "Step: 768, Loss: 1.4056354761123657, Accuracy: 0.6221283051582142\n",
      "Step: 769, Loss: 1.2427164316177368, Accuracy: 0.6221861471861472\n",
      "Step: 770, Loss: 1.168619990348816, Accuracy: 0.6223519239083442\n",
      "Step: 771, Loss: 1.1538926362991333, Accuracy: 0.6225172711571675\n",
      "Step: 772, Loss: 1.2252074480056763, Accuracy: 0.6225743855109961\n",
      "Step: 773, Loss: 1.2820340394973755, Accuracy: 0.6225236864771748\n",
      "Step: 774, Loss: 1.2552591562271118, Accuracy: 0.6225806451612903\n",
      "Step: 775, Loss: 1.295760154724121, Accuracy: 0.6226374570446735\n",
      "Step: 776, Loss: 1.3061585426330566, Accuracy: 0.6225868725868726\n",
      "Step: 777, Loss: 1.2344615459442139, Accuracy: 0.6226435304198801\n",
      "Step: 778, Loss: 1.1344364881515503, Accuracy: 0.6228070175438597\n",
      "Step: 779, Loss: 1.3816510438919067, Accuracy: 0.6226495726495727\n",
      "Step: 780, Loss: 1.3845640420913696, Accuracy: 0.6224925309432352\n",
      "Step: 781, Loss: 1.1775789260864258, Accuracy: 0.6226555839727195\n",
      "Step: 782, Loss: 1.1613608598709106, Accuracy: 0.62281822051937\n",
      "Step: 783, Loss: 1.5237165689468384, Accuracy: 0.6224489795918368\n",
      "Step: 784, Loss: 0.9965997338294983, Accuracy: 0.622823779193206\n",
      "Step: 785, Loss: 1.2246328592300415, Accuracy: 0.6228795589482612\n",
      "Step: 786, Loss: 1.4769762754440308, Accuracy: 0.6226175349428208\n",
      "Step: 787, Loss: 1.1736172437667847, Accuracy: 0.6227791878172588\n",
      "Step: 788, Loss: 1.227547526359558, Accuracy: 0.6228348119983101\n",
      "Step: 789, Loss: 1.2189489603042603, Accuracy: 0.6228902953586498\n",
      "Step: 790, Loss: 1.0348213911056519, Accuracy: 0.6232616940581542\n",
      "Step: 791, Loss: 1.302270770072937, Accuracy: 0.6232112794612794\n",
      "Step: 792, Loss: 1.2887264490127563, Accuracy: 0.623160992013451\n",
      "Step: 793, Loss: 1.319750428199768, Accuracy: 0.6231108312342569\n",
      "Step: 794, Loss: 1.2397810220718384, Accuracy: 0.6231656184486373\n",
      "Step: 795, Loss: 1.2883601188659668, Accuracy: 0.6231155778894473\n",
      "Step: 796, Loss: 1.2357908487319946, Accuracy: 0.6231702216645755\n",
      "Step: 797, Loss: 1.3950616121292114, Accuracy: 0.623015873015873\n",
      "Step: 798, Loss: 1.3976935148239136, Accuracy: 0.6228619107217355\n",
      "Step: 799, Loss: 1.3169820308685303, Accuracy: 0.6228125\n",
      "Step: 800, Loss: 1.107059359550476, Accuracy: 0.6230753225135247\n",
      "Step: 801, Loss: 1.3066439628601074, Accuracy: 0.6230257689110557\n",
      "Step: 802, Loss: 1.237972378730774, Accuracy: 0.6230801162308012\n",
      "Step: 803, Loss: 1.373456597328186, Accuracy: 0.6229270315091211\n",
      "Step: 804, Loss: 1.0826830863952637, Accuracy: 0.6231884057971014\n",
      "Step: 805, Loss: 1.3969006538391113, Accuracy: 0.6230355665839536\n",
      "Step: 806, Loss: 1.4697798490524292, Accuracy: 0.6227798430400661\n",
      "Step: 807, Loss: 1.3068281412124634, Accuracy: 0.6227310231023102\n",
      "Step: 808, Loss: 1.1398086547851562, Accuracy: 0.622991347342398\n",
      "Step: 809, Loss: 1.2026854753494263, Accuracy: 0.6230452674897119\n",
      "Step: 810, Loss: 1.2502704858779907, Accuracy: 0.6230990546650226\n",
      "Step: 811, Loss: 1.399908185005188, Accuracy: 0.6229474548440066\n",
      "Step: 812, Loss: 1.3206177949905396, Accuracy: 0.6228987289872898\n",
      "Step: 813, Loss: 1.286068320274353, Accuracy: 0.6228501228501229\n",
      "Step: 814, Loss: 1.3427222967147827, Accuracy: 0.62280163599182\n",
      "Step: 815, Loss: 1.5629359483718872, Accuracy: 0.6224468954248366\n",
      "Step: 816, Loss: 1.320244550704956, Accuracy: 0.6223990208078335\n",
      "Step: 817, Loss: 1.1254782676696777, Accuracy: 0.6225550122249389\n",
      "Step: 818, Loss: 1.337966799736023, Accuracy: 0.6225071225071225\n",
      "Step: 819, Loss: 1.0705077648162842, Accuracy: 0.6228658536585366\n",
      "Step: 820, Loss: 1.4504810571670532, Accuracy: 0.6226146975233455\n",
      "Step: 821, Loss: 1.2960067987442017, Accuracy: 0.6225669099756691\n",
      "Step: 822, Loss: 1.0602918863296509, Accuracy: 0.6228230052652896\n",
      "Step: 823, Loss: 1.3801178932189941, Accuracy: 0.6226739482200647\n",
      "Step: 824, Loss: 1.2358975410461426, Accuracy: 0.6227272727272727\n",
      "Step: 825, Loss: 1.070645809173584, Accuracy: 0.6229822437449556\n",
      "Step: 826, Loss: 1.2869677543640137, Accuracy: 0.6230350665054414\n",
      "Step: 827, Loss: 1.1109027862548828, Accuracy: 0.6232890499194848\n",
      "Step: 828, Loss: 1.1071306467056274, Accuracy: 0.6235424205870527\n",
      "Step: 829, Loss: 1.170154094696045, Accuracy: 0.6236947791164659\n",
      "Step: 830, Loss: 1.0952399969100952, Accuracy: 0.6239470517448856\n",
      "Step: 831, Loss: 1.2353001832962036, Accuracy: 0.6239983974358975\n",
      "Step: 832, Loss: 1.1527668237686157, Accuracy: 0.6241496598639455\n",
      "Step: 833, Loss: 1.2227591276168823, Accuracy: 0.6242006394884093\n",
      "Step: 834, Loss: 1.434771180152893, Accuracy: 0.6239520958083832\n",
      "Step: 835, Loss: 1.5448966026306152, Accuracy: 0.6236044657097288\n",
      "Step: 836, Loss: 1.3042805194854736, Accuracy: 0.6235563520509757\n",
      "Step: 837, Loss: 1.3051670789718628, Accuracy: 0.6235083532219571\n",
      "Step: 838, Loss: 1.3068360090255737, Accuracy: 0.6234604688120778\n",
      "Step: 839, Loss: 1.2412736415863037, Accuracy: 0.6235119047619048\n",
      "Step: 840, Loss: 1.394170880317688, Accuracy: 0.6233650416171225\n",
      "Step: 841, Loss: 1.5379104614257812, Accuracy: 0.6230205859065716\n",
      "Step: 842, Loss: 1.2461811304092407, Accuracy: 0.6230723606168446\n",
      "Step: 843, Loss: 1.3439573049545288, Accuracy: 0.6229265402843602\n",
      "Step: 844, Loss: 1.1855682134628296, Accuracy: 0.6229783037475345\n",
      "Step: 845, Loss: 1.1667829751968384, Accuracy: 0.6231284475965327\n",
      "Step: 846, Loss: 1.3768092393875122, Accuracy: 0.6229830775285321\n",
      "Step: 847, Loss: 1.5010738372802734, Accuracy: 0.6227397798742138\n",
      "Step: 848, Loss: 1.3184289932250977, Accuracy: 0.6226933647428347\n",
      "Step: 849, Loss: 1.326457142829895, Accuracy: 0.6226470588235294\n",
      "Step: 850, Loss: 1.494132399559021, Accuracy: 0.6223070896983941\n",
      "Step: 851, Loss: 1.4825081825256348, Accuracy: 0.6220657276995305\n",
      "Step: 852, Loss: 1.4450116157531738, Accuracy: 0.6218249316139117\n",
      "Step: 853, Loss: 1.3961817026138306, Accuracy: 0.6216822794691648\n",
      "Step: 854, Loss: 1.1745942831039429, Accuracy: 0.621832358674464\n",
      "Step: 855, Loss: 1.3276602029800415, Accuracy: 0.6217873831775701\n",
      "Step: 856, Loss: 1.5372084379196167, Accuracy: 0.6214507973551148\n",
      "Step: 857, Loss: 1.310247540473938, Accuracy: 0.6214063714063714\n",
      "Step: 858, Loss: 1.0811587572097778, Accuracy: 0.6216530849825378\n",
      "Step: 859, Loss: 1.4845422506332397, Accuracy: 0.6214147286821705\n",
      "Step: 860, Loss: 1.3164777755737305, Accuracy: 0.6213704994192799\n",
      "Step: 861, Loss: 0.9871807098388672, Accuracy: 0.6218097447795824\n",
      "Step: 862, Loss: 1.2937884330749512, Accuracy: 0.6217651602935497\n",
      "Step: 863, Loss: 1.2156301736831665, Accuracy: 0.6218171296296297\n",
      "Step: 864, Loss: 1.4855380058288574, Accuracy: 0.6215799614643546\n",
      "Step: 865, Loss: 1.175451636314392, Accuracy: 0.6217282525019245\n",
      "Step: 866, Loss: 1.218065619468689, Accuracy: 0.6217800845828527\n",
      "Step: 867, Loss: 1.1609283685684204, Accuracy: 0.6219278033794163\n",
      "Step: 868, Loss: 1.2520920038223267, Accuracy: 0.6219792865362486\n",
      "Step: 869, Loss: 1.2789629697799683, Accuracy: 0.6220306513409962\n",
      "Step: 870, Loss: 1.1580625772476196, Accuracy: 0.622177573670111\n",
      "Step: 871, Loss: 1.2067145109176636, Accuracy: 0.6222285932721713\n",
      "Step: 872, Loss: 1.0035878419876099, Accuracy: 0.6225658648339061\n",
      "Step: 873, Loss: 1.3274531364440918, Accuracy: 0.6225209763539283\n",
      "Step: 874, Loss: 1.155220866203308, Accuracy: 0.6226666666666667\n",
      "Step: 875, Loss: 1.248304009437561, Accuracy: 0.6226217656012176\n",
      "Step: 876, Loss: 1.082611322402954, Accuracy: 0.6228620296465223\n",
      "Step: 877, Loss: 1.2977726459503174, Accuracy: 0.6228170083523159\n",
      "Step: 878, Loss: 1.1122504472732544, Accuracy: 0.6230565036025787\n",
      "Step: 879, Loss: 1.0780137777328491, Accuracy: 0.6232954545454545\n",
      "Step: 880, Loss: 1.4170527458190918, Accuracy: 0.623155505107832\n",
      "Step: 881, Loss: 1.1968823671340942, Accuracy: 0.6232048374905518\n",
      "Step: 882, Loss: 1.1143020391464233, Accuracy: 0.6234428086070215\n",
      "Step: 883, Loss: 1.2392498254776, Accuracy: 0.6234917043740573\n",
      "Step: 884, Loss: 1.1641019582748413, Accuracy: 0.6236346516007533\n",
      "Step: 885, Loss: 1.40837824344635, Accuracy: 0.6234951091045899\n",
      "Step: 886, Loss: 1.7834582328796387, Accuracy: 0.6228861330326945\n",
      "Step: 887, Loss: 1.1467275619506836, Accuracy: 0.6230292792792793\n",
      "Step: 888, Loss: 1.2323198318481445, Accuracy: 0.6230783652043494\n",
      "Step: 889, Loss: 1.081307291984558, Accuracy: 0.623314606741573\n",
      "Step: 890, Loss: 1.3387088775634766, Accuracy: 0.6232697343808455\n",
      "Step: 891, Loss: 1.2325783967971802, Accuracy: 0.6233183856502242\n",
      "Step: 892, Loss: 1.5494670867919922, Accuracy: 0.6229936543486375\n",
      "Step: 893, Loss: 1.310591220855713, Accuracy: 0.6229492915734527\n",
      "Step: 894, Loss: 1.0750426054000854, Accuracy: 0.6232774674115457\n",
      "Step: 895, Loss: 1.3091176748275757, Accuracy: 0.6232328869047619\n",
      "Step: 896, Loss: 1.5615510940551758, Accuracy: 0.6229096989966555\n",
      "Step: 897, Loss: 1.309842586517334, Accuracy: 0.6228656273199703\n",
      "Step: 898, Loss: 1.3290294408798218, Accuracy: 0.6228216536892844\n",
      "Step: 899, Loss: 1.3870278596878052, Accuracy: 0.6226851851851852\n",
      "Step: 900, Loss: 1.1610618829727173, Accuracy: 0.6228264890862005\n",
      "Step: 901, Loss: 1.1667920351028442, Accuracy: 0.6229674796747967\n",
      "Step: 902, Loss: 1.3269057273864746, Accuracy: 0.6229235880398671\n",
      "Step: 903, Loss: 1.350072979927063, Accuracy: 0.6228797935103245\n",
      "Step: 904, Loss: 1.0804494619369507, Accuracy: 0.6231123388581952\n",
      "Step: 905, Loss: 1.4685053825378418, Accuracy: 0.6228844738778514\n",
      "Step: 906, Loss: 1.275539517402649, Accuracy: 0.6228408673281882\n",
      "Step: 907, Loss: 1.4023700952529907, Accuracy: 0.6227055800293686\n",
      "Step: 908, Loss: 1.379134178161621, Accuracy: 0.6225705903923726\n",
      "Step: 909, Loss: 1.4497748613357544, Accuracy: 0.6223443223443224\n",
      "Step: 910, Loss: 1.2023643255233765, Accuracy: 0.6224844493230882\n",
      "Step: 911, Loss: 1.31635320186615, Accuracy: 0.6224415204678363\n",
      "Step: 912, Loss: 1.0991698503494263, Accuracy: 0.6226725082146769\n",
      "Step: 913, Loss: 1.3857293128967285, Accuracy: 0.6226294675419402\n",
      "Step: 914, Loss: 1.2091552019119263, Accuracy: 0.6226775956284153\n",
      "Step: 915, Loss: 1.3909183740615845, Accuracy: 0.6225436681222707\n",
      "Step: 916, Loss: 1.0795025825500488, Accuracy: 0.6227735368956743\n",
      "Step: 917, Loss: 1.3938740491867065, Accuracy: 0.6226397966594045\n",
      "Step: 918, Loss: 1.260814905166626, Accuracy: 0.6226877040261154\n",
      "Step: 919, Loss: 1.3308838605880737, Accuracy: 0.6226449275362319\n",
      "Step: 920, Loss: 1.2286278009414673, Accuracy: 0.6226927252985885\n",
      "Step: 921, Loss: 1.3364843130111694, Accuracy: 0.62265003615329\n",
      "Step: 922, Loss: 1.2608305215835571, Accuracy: 0.6226977248104009\n",
      "Step: 923, Loss: 1.2700729370117188, Accuracy: 0.6227453102453102\n",
      "Step: 924, Loss: 1.5273551940917969, Accuracy: 0.6224324324324324\n",
      "Step: 925, Loss: 1.2534879446029663, Accuracy: 0.6224802015838733\n",
      "Step: 926, Loss: 1.3849509954452515, Accuracy: 0.6223480762315714\n",
      "Step: 927, Loss: 1.4019650220870972, Accuracy: 0.6222162356321839\n",
      "Step: 928, Loss: 1.3548070192337036, Accuracy: 0.6221743810548978\n",
      "Step: 929, Loss: 1.7199956178665161, Accuracy: 0.6216845878136201\n",
      "Step: 930, Loss: 0.9982433915138245, Accuracy: 0.6220014321518081\n",
      "Step: 931, Loss: 1.3946720361709595, Accuracy: 0.6218705293276109\n",
      "Step: 932, Loss: 1.3769279718399048, Accuracy: 0.6217399071096821\n",
      "Step: 933, Loss: 1.1556190252304077, Accuracy: 0.6218772305496074\n",
      "Step: 934, Loss: 1.0923875570297241, Accuracy: 0.6221033868092691\n",
      "Step: 935, Loss: 1.3747035264968872, Accuracy: 0.6219729344729344\n",
      "Step: 936, Loss: 1.1855663061141968, Accuracy: 0.6220206332266097\n",
      "Step: 937, Loss: 1.2387701272964478, Accuracy: 0.6220682302771855\n",
      "Step: 938, Loss: 1.166176199913025, Accuracy: 0.6222044728434505\n",
      "Step: 939, Loss: 1.4743815660476685, Accuracy: 0.6219858156028368\n",
      "Step: 940, Loss: 1.3969472646713257, Accuracy: 0.6218561813673397\n",
      "Step: 941, Loss: 1.317506194114685, Accuracy: 0.6218152866242038\n",
      "Step: 942, Loss: 1.595513939857483, Accuracy: 0.6214209968186638\n",
      "Step: 943, Loss: 1.217238426208496, Accuracy: 0.6215572033898306\n",
      "Step: 944, Loss: 1.2378441095352173, Accuracy: 0.6216049382716049\n",
      "Step: 945, Loss: 1.3195394277572632, Accuracy: 0.6215644820295984\n",
      "Step: 946, Loss: 1.0769201517105103, Accuracy: 0.6217881027807111\n",
      "Step: 947, Loss: 1.4426922798156738, Accuracy: 0.6215717299578059\n",
      "Step: 948, Loss: 1.1287075281143188, Accuracy: 0.6217070600632244\n",
      "Step: 949, Loss: 1.2511237859725952, Accuracy: 0.6217543859649123\n",
      "Step: 950, Loss: 1.0694352388381958, Accuracy: 0.6219768664563617\n",
      "Step: 951, Loss: 1.2473632097244263, Accuracy: 0.6220238095238095\n",
      "Step: 952, Loss: 1.1619952917099, Accuracy: 0.622158097236796\n",
      "Step: 953, Loss: 1.2344897985458374, Accuracy: 0.6222047519217331\n",
      "Step: 954, Loss: 1.2500590085983276, Accuracy: 0.6222513089005236\n",
      "Step: 955, Loss: 1.2635383605957031, Accuracy: 0.6222977684797768\n",
      "Step: 956, Loss: 1.251740574836731, Accuracy: 0.6223441309648207\n",
      "Step: 957, Loss: 1.3255929946899414, Accuracy: 0.622303409881698\n",
      "Step: 958, Loss: 1.3111180067062378, Accuracy: 0.6222627737226277\n",
      "Step: 959, Loss: 1.12940514087677, Accuracy: 0.6223958333333334\n",
      "Step: 960, Loss: 1.3197470903396606, Accuracy: 0.6223551855705862\n",
      "Step: 961, Loss: 1.1893795728683472, Accuracy: 0.6224878724878725\n",
      "Step: 962, Loss: 1.1574840545654297, Accuracy: 0.6226202838352372\n",
      "Step: 963, Loss: 1.213280439376831, Accuracy: 0.6226659751037344\n",
      "Step: 964, Loss: 1.2360364198684692, Accuracy: 0.6227115716753022\n",
      "Step: 965, Loss: 1.2754162549972534, Accuracy: 0.6227570738440303\n",
      "Step: 966, Loss: 1.3213480710983276, Accuracy: 0.6227163047225095\n",
      "Step: 967, Loss: 1.3517111539840698, Accuracy: 0.6226756198347108\n",
      "Step: 968, Loss: 1.1525206565856934, Accuracy: 0.6228070175438597\n",
      "Step: 969, Loss: 1.2037919759750366, Accuracy: 0.622852233676976\n",
      "Step: 970, Loss: 1.3630318641662598, Accuracy: 0.6227257123240645\n",
      "Step: 971, Loss: 1.4020614624023438, Accuracy: 0.622599451303155\n",
      "Step: 972, Loss: 1.3143950700759888, Accuracy: 0.6225590955806783\n",
      "Step: 973, Loss: 1.2973476648330688, Accuracy: 0.6225188227241615\n",
      "Step: 974, Loss: 1.4282822608947754, Accuracy: 0.6223076923076923\n",
      "Step: 975, Loss: 1.3642301559448242, Accuracy: 0.6222677595628415\n",
      "Step: 976, Loss: 1.4055795669555664, Accuracy: 0.6221426134425111\n",
      "Step: 977, Loss: 1.0882853269577026, Accuracy: 0.6223585548738922\n",
      "Step: 978, Loss: 1.4021989107131958, Accuracy: 0.622233571671774\n",
      "Step: 979, Loss: 1.3931983709335327, Accuracy: 0.6221088435374149\n",
      "Step: 980, Loss: 1.2297066450119019, Accuracy: 0.6221542643560992\n",
      "Step: 981, Loss: 1.2381407022476196, Accuracy: 0.6221995926680245\n",
      "Step: 982, Loss: 1.3947662115097046, Accuracy: 0.6220752797558494\n",
      "Step: 983, Loss: 1.3292816877365112, Accuracy: 0.6220359078590786\n",
      "Step: 984, Loss: 1.2381998300552368, Accuracy: 0.6220812182741117\n",
      "Step: 985, Loss: 1.224113941192627, Accuracy: 0.6221264367816092\n",
      "Step: 986, Loss: 1.405676245689392, Accuracy: 0.6220027017899359\n",
      "Step: 987, Loss: 1.224905252456665, Accuracy: 0.6220479082321188\n",
      "Step: 988, Loss: 1.1265758275985718, Accuracy: 0.6221772834512976\n",
      "Step: 989, Loss: 1.3836182355880737, Accuracy: 0.6220538720538721\n",
      "Step: 990, Loss: 1.1712929010391235, Accuracy: 0.6221829801547258\n",
      "Step: 991, Loss: 1.476044774055481, Accuracy: 0.6219758064516129\n",
      "Step: 992, Loss: 1.238113284111023, Accuracy: 0.6220208123531387\n",
      "Step: 993, Loss: 1.2400991916656494, Accuracy: 0.6220657276995305\n",
      "Step: 994, Loss: 1.0122321844100952, Accuracy: 0.6223618090452261\n",
      "Step: 995, Loss: 1.2112243175506592, Accuracy: 0.6224062918340026\n",
      "Step: 996, Loss: 1.164484977722168, Accuracy: 0.6225342694750919\n",
      "Step: 997, Loss: 1.4864674806594849, Accuracy: 0.6223279893119572\n",
      "Step: 998, Loss: 1.070377230644226, Accuracy: 0.6225392058725392\n",
      "Step: 999, Loss: 1.2835601568222046, Accuracy: 0.6225833333333334\n",
      "Step: 1000, Loss: 1.4223155975341797, Accuracy: 0.6224608724608724\n",
      "Step: 1001, Loss: 1.5286494493484497, Accuracy: 0.6221723220226214\n",
      "Step: 1002, Loss: 1.2086706161499023, Accuracy: 0.6222166832834829\n",
      "Step: 1003, Loss: 1.3113430738449097, Accuracy: 0.6222609561752988\n",
      "Step: 1004, Loss: 1.2640970945358276, Accuracy: 0.6223051409618574\n",
      "Step: 1005, Loss: 1.400250792503357, Accuracy: 0.6221835652750166\n",
      "Step: 1006, Loss: 1.0839518308639526, Accuracy: 0.6223932472691162\n",
      "Step: 1007, Loss: 1.3624286651611328, Accuracy: 0.6223544973544973\n",
      "Step: 1008, Loss: 1.2269254922866821, Accuracy: 0.622398414271556\n",
      "Step: 1009, Loss: 1.2065186500549316, Accuracy: 0.6224422442244224\n",
      "Step: 1010, Loss: 1.377400517463684, Accuracy: 0.6223211341905704\n",
      "Step: 1011, Loss: 1.1129661798477173, Accuracy: 0.6225296442687747\n",
      "Step: 1012, Loss: 1.3178155422210693, Accuracy: 0.622490950970714\n",
      "Step: 1013, Loss: 1.6981910467147827, Accuracy: 0.6220414201183432\n",
      "Step: 1014, Loss: 1.315499186515808, Accuracy: 0.6220032840722496\n",
      "Step: 1015, Loss: 1.3901134729385376, Accuracy: 0.6218832020997376\n",
      "Step: 1016, Loss: 1.3982657194137573, Accuracy: 0.6217633562766306\n",
      "Step: 1017, Loss: 1.204084038734436, Accuracy: 0.6218074656188605\n",
      "Step: 1018, Loss: 1.5143756866455078, Accuracy: 0.6215243702976775\n",
      "Step: 1019, Loss: 1.4437881708145142, Accuracy: 0.6213235294117647\n",
      "Step: 1020, Loss: 1.4355155229568481, Accuracy: 0.6211230819458048\n",
      "Step: 1021, Loss: 1.392654538154602, Accuracy: 0.6210045662100456\n",
      "Step: 1022, Loss: 1.4719423055648804, Accuracy: 0.6208048224177256\n",
      "Step: 1023, Loss: 1.4325047731399536, Accuracy: 0.62060546875\n",
      "Step: 1024, Loss: 1.2549577951431274, Accuracy: 0.620650406504065\n",
      "Step: 1025, Loss: 1.1213535070419312, Accuracy: 0.6208576998050682\n",
      "Step: 1026, Loss: 1.3146806955337524, Accuracy: 0.6209023044466082\n",
      "Step: 1027, Loss: 1.655510425567627, Accuracy: 0.620541504539559\n",
      "Step: 1028, Loss: 1.3682284355163574, Accuracy: 0.6204243602202786\n",
      "Step: 1029, Loss: 1.0869405269622803, Accuracy: 0.6206310679611651\n",
      "Step: 1030, Loss: 1.5573889017105103, Accuracy: 0.6203524086647269\n",
      "Step: 1031, Loss: 1.1263502836227417, Accuracy: 0.6204780361757106\n",
      "Step: 1032, Loss: 1.4056459665298462, Accuracy: 0.6202807357212003\n",
      "Step: 1033, Loss: 1.3741534948349, Accuracy: 0.620164410058027\n",
      "Step: 1034, Loss: 1.3095409870147705, Accuracy: 0.6201288244766505\n",
      "Step: 1035, Loss: 1.1714938879013062, Accuracy: 0.6202541827541828\n",
      "Step: 1036, Loss: 1.1686670780181885, Accuracy: 0.6203792992606879\n",
      "Step: 1037, Loss: 1.2648578882217407, Accuracy: 0.6203436095054592\n",
      "Step: 1038, Loss: 1.3269253969192505, Accuracy: 0.6203079884504331\n",
      "Step: 1039, Loss: 1.269285798072815, Accuracy: 0.6203525641025641\n",
      "Step: 1040, Loss: 1.3670789003372192, Accuracy: 0.6202369516490553\n",
      "Step: 1041, Loss: 1.2738851308822632, Accuracy: 0.6202815099168266\n",
      "Step: 1042, Loss: 1.216455101966858, Accuracy: 0.6203259827420902\n",
      "Step: 1043, Loss: 1.507303237915039, Accuracy: 0.6201309067688378\n",
      "Step: 1044, Loss: 1.472781777381897, Accuracy: 0.6199362041467305\n",
      "Step: 1045, Loss: 1.2575740814208984, Accuracy: 0.619980879541109\n",
      "Step: 1046, Loss: 1.1441318988800049, Accuracy: 0.6201050620821394\n",
      "Step: 1047, Loss: 1.3182772397994995, Accuracy: 0.6200699745547074\n",
      "Step: 1048, Loss: 1.081421971321106, Accuracy: 0.6202732761360026\n",
      "Step: 1049, Loss: 1.5540885925292969, Accuracy: 0.62\n",
      "Step: 1050, Loss: 1.205190896987915, Accuracy: 0.6200444021566762\n",
      "Step: 1051, Loss: 1.2987290620803833, Accuracy: 0.6200095057034221\n",
      "Step: 1052, Loss: 1.3117904663085938, Accuracy: 0.6199746755302311\n",
      "Step: 1053, Loss: 1.3009759187698364, Accuracy: 0.6199399114484504\n",
      "Step: 1054, Loss: 1.1842294931411743, Accuracy: 0.6200631911532386\n",
      "Step: 1055, Loss: 1.1582789421081543, Accuracy: 0.6201862373737373\n",
      "Step: 1056, Loss: 1.347359538078308, Accuracy: 0.620151371807001\n",
      "Step: 1057, Loss: 1.3704148530960083, Accuracy: 0.6200378071833649\n",
      "Step: 1058, Loss: 1.2374593019485474, Accuracy: 0.6200818382121498\n",
      "Step: 1059, Loss: 1.211248517036438, Accuracy: 0.6202044025157233\n",
      "Step: 1060, Loss: 1.2069025039672852, Accuracy: 0.6203267357838517\n",
      "Step: 1061, Loss: 1.4804121255874634, Accuracy: 0.6201349654739485\n",
      "Step: 1062, Loss: 1.1628443002700806, Accuracy: 0.6202571338977736\n",
      "Step: 1063, Loss: 1.1764112710952759, Accuracy: 0.6203790726817042\n",
      "Step: 1064, Loss: 1.245137333869934, Accuracy: 0.6204225352112676\n",
      "Step: 1065, Loss: 1.3800827264785767, Accuracy: 0.6203095684803002\n",
      "Step: 1066, Loss: 1.0818917751312256, Accuracy: 0.6205092158700406\n",
      "Step: 1067, Loss: 1.2319780588150024, Accuracy: 0.6205524344569289\n",
      "Step: 1068, Loss: 1.3839842081069946, Accuracy: 0.6204396632366698\n",
      "Step: 1069, Loss: 1.1684027910232544, Accuracy: 0.6205607476635514\n",
      "Step: 1070, Loss: 1.2278045415878296, Accuracy: 0.6206037970743853\n",
      "Step: 1071, Loss: 1.0812926292419434, Accuracy: 0.6208022388059702\n",
      "Step: 1072, Loss: 1.3087209463119507, Accuracy: 0.6207673190431812\n",
      "Step: 1073, Loss: 0.9959583878517151, Accuracy: 0.6210428305400373\n",
      "Step: 1074, Loss: 1.4824881553649902, Accuracy: 0.6208527131782946\n",
      "Step: 1075, Loss: 1.0535098314285278, Accuracy: 0.6210501858736059\n",
      "Step: 1076, Loss: 1.2600752115249634, Accuracy: 0.6210151655834107\n",
      "Step: 1077, Loss: 1.1555477380752563, Accuracy: 0.621134817563389\n",
      "Step: 1078, Loss: 1.2235687971115112, Accuracy: 0.621177015755329\n",
      "Step: 1079, Loss: 1.1633446216583252, Accuracy: 0.6212962962962963\n",
      "Step: 1080, Loss: 1.4042166471481323, Accuracy: 0.6211840888066605\n",
      "Step: 1081, Loss: 1.4703360795974731, Accuracy: 0.6209950708564387\n",
      "Step: 1082, Loss: 0.9914082884788513, Accuracy: 0.621268082486919\n",
      "Step: 1083, Loss: 1.294400930404663, Accuracy: 0.6212330873308733\n",
      "Step: 1084, Loss: 1.258076548576355, Accuracy: 0.6212749615975423\n",
      "Step: 1085, Loss: 1.2359241247177124, Accuracy: 0.621316758747698\n",
      "Step: 1086, Loss: 1.2295668125152588, Accuracy: 0.6213584789941735\n",
      "Step: 1087, Loss: 1.1482354402542114, Accuracy: 0.6214767156862745\n",
      "Step: 1088, Loss: 1.2311257123947144, Accuracy: 0.6215182124273033\n",
      "Step: 1089, Loss: 1.0691367387771606, Accuracy: 0.6217125382262997\n",
      "Step: 1090, Loss: 1.278435468673706, Accuracy: 0.6217537427436602\n",
      "Step: 1091, Loss: 1.4298127889633179, Accuracy: 0.6216422466422467\n",
      "Step: 1092, Loss: 1.3599597215652466, Accuracy: 0.6216071973162549\n",
      "Step: 1093, Loss: 1.3127719163894653, Accuracy: 0.6215722120658135\n",
      "Step: 1094, Loss: 1.5198687314987183, Accuracy: 0.6213089802130898\n",
      "Step: 1095, Loss: 1.3271983861923218, Accuracy: 0.6212743309002433\n",
      "Step: 1096, Loss: 1.1028367280960083, Accuracy: 0.6214676390154968\n",
      "Step: 1097, Loss: 1.3252681493759155, Accuracy: 0.6214329083181542\n",
      "Step: 1098, Loss: 1.162807822227478, Accuracy: 0.6215498938428875\n",
      "Step: 1099, Loss: 1.0973467826843262, Accuracy: 0.6217424242424242\n",
      "Step: 1100, Loss: 1.3351038694381714, Accuracy: 0.6217075386012716\n",
      "Step: 1101, Loss: 1.4785213470458984, Accuracy: 0.6215214761040533\n",
      "Step: 1102, Loss: 1.4008865356445312, Accuracy: 0.6214113025083107\n",
      "Step: 1103, Loss: 1.5478938817977905, Accuracy: 0.6211503623188406\n",
      "Step: 1104, Loss: 1.469988465309143, Accuracy: 0.6209653092006033\n",
      "Step: 1105, Loss: 1.2713446617126465, Accuracy: 0.6209312839059674\n",
      "Step: 1106, Loss: 1.5409642457962036, Accuracy: 0.6206714844926227\n",
      "Step: 1107, Loss: 1.3962197303771973, Accuracy: 0.6205625752105897\n",
      "Step: 1108, Loss: 1.0628775358200073, Accuracy: 0.6207544334235047\n",
      "Step: 1109, Loss: 1.6423131227493286, Accuracy: 0.6204204204204204\n",
      "Step: 1110, Loss: 1.3881274461746216, Accuracy: 0.6203120312031203\n",
      "Step: 1111, Loss: 1.5461063385009766, Accuracy: 0.6200539568345323\n",
      "Step: 1112, Loss: 1.0788708925247192, Accuracy: 0.6202455825097335\n",
      "Step: 1113, Loss: 1.1472452878952026, Accuracy: 0.6203620586475165\n",
      "Step: 1114, Loss: 1.218836784362793, Accuracy: 0.6204035874439462\n",
      "Step: 1115, Loss: 1.0120207071304321, Accuracy: 0.6206690561529271\n",
      "Step: 1116, Loss: 1.0666999816894531, Accuracy: 0.6209340495374515\n",
      "Step: 1117, Loss: 1.3105415105819702, Accuracy: 0.6209004174120453\n",
      "Step: 1118, Loss: 1.1080821752548218, Accuracy: 0.6210902591599643\n",
      "Step: 1119, Loss: 1.0255308151245117, Accuracy: 0.6212797619047619\n",
      "Step: 1120, Loss: 1.3945326805114746, Accuracy: 0.6211715730002974\n",
      "Step: 1121, Loss: 1.3969697952270508, Accuracy: 0.6210635769459298\n",
      "Step: 1122, Loss: 1.3207097053527832, Accuracy: 0.6210299792223212\n",
      "Step: 1123, Loss: 1.4763349294662476, Accuracy: 0.6208481613285883\n",
      "Step: 1124, Loss: 1.2356613874435425, Accuracy: 0.6208888888888889\n",
      "Step: 1125, Loss: 1.3134082555770874, Accuracy: 0.6208555358200119\n",
      "Step: 1126, Loss: 1.1767946481704712, Accuracy: 0.6209701271813073\n",
      "Step: 1127, Loss: 1.3268929719924927, Accuracy: 0.6209367612293144\n",
      "Step: 1128, Loss: 1.254870057106018, Accuracy: 0.6209772660171243\n",
      "Step: 1129, Loss: 1.2357248067855835, Accuracy: 0.6210176991150442\n",
      "Step: 1130, Loss: 1.2438675165176392, Accuracy: 0.6210580607132331\n",
      "Step: 1131, Loss: 1.2667903900146484, Accuracy: 0.6210983510011778\n",
      "Step: 1132, Loss: 1.082421898841858, Accuracy: 0.6212856722565461\n",
      "Step: 1133, Loss: 1.4008903503417969, Accuracy: 0.6211787184009406\n",
      "Step: 1134, Loss: 1.2297695875167847, Accuracy: 0.6212187958883995\n",
      "Step: 1135, Loss: 1.119348406791687, Accuracy: 0.6213321596244131\n",
      "Step: 1136, Loss: 1.493639588356018, Accuracy: 0.6210788625036646\n",
      "Step: 1137, Loss: 1.4754924774169922, Accuracy: 0.6208992384299942\n",
      "Step: 1138, Loss: 1.4880183935165405, Accuracy: 0.62071992976295\n",
      "Step: 1139, Loss: 1.20000422000885, Accuracy: 0.6207602339181286\n",
      "Step: 1140, Loss: 1.2288082838058472, Accuracy: 0.6208004674262343\n",
      "Step: 1141, Loss: 1.074574589729309, Accuracy: 0.6209865732632808\n",
      "Step: 1142, Loss: 1.2041336297988892, Accuracy: 0.621026538349373\n",
      "Step: 1143, Loss: 1.1562957763671875, Accuracy: 0.6211392773892774\n",
      "Step: 1144, Loss: 1.0618027448654175, Accuracy: 0.6213245997088792\n",
      "Step: 1145, Loss: 1.2685117721557617, Accuracy: 0.6212914485165794\n",
      "Step: 1146, Loss: 1.4151898622512817, Accuracy: 0.6211857018308631\n",
      "Step: 1147, Loss: 1.3917025327682495, Accuracy: 0.6210801393728222\n",
      "Step: 1148, Loss: 1.2416876554489136, Accuracy: 0.6211198143313026\n",
      "Step: 1149, Loss: 1.0357111692428589, Accuracy: 0.6213768115942029\n",
      "Step: 1150, Loss: 1.476770043373108, Accuracy: 0.6211989574283232\n",
      "Step: 1151, Loss: 1.238594889640808, Accuracy: 0.6212384259259259\n",
      "Step: 1152, Loss: 1.1457785367965698, Accuracy: 0.6213501011853136\n",
      "Step: 1153, Loss: 1.1173628568649292, Accuracy: 0.6215337954939342\n",
      "Step: 1154, Loss: 1.16597318649292, Accuracy: 0.6216450216450217\n",
      "Step: 1155, Loss: 1.3054594993591309, Accuracy: 0.6216118800461361\n",
      "Step: 1156, Loss: 1.3965339660644531, Accuracy: 0.6215067703831749\n",
      "Step: 1157, Loss: 1.3111978769302368, Accuracy: 0.6214738054116292\n",
      "Step: 1158, Loss: 1.4019886255264282, Accuracy: 0.6213689962611446\n",
      "Step: 1159, Loss: 1.2930516004562378, Accuracy: 0.6213362068965518\n",
      "Step: 1160, Loss: 1.2250577211380005, Accuracy: 0.6213752512202124\n",
      "Step: 1161, Loss: 1.0719295740127563, Accuracy: 0.6215576592082617\n",
      "Step: 1162, Loss: 1.0622355937957764, Accuracy: 0.6217397535110347\n",
      "Step: 1163, Loss: 1.2198282480239868, Accuracy: 0.6218499427262314\n",
      "Step: 1164, Loss: 1.1893115043640137, Accuracy: 0.6219599427753935\n",
      "Step: 1165, Loss: 1.394282341003418, Accuracy: 0.6218553459119497\n",
      "Step: 1166, Loss: 1.3406896591186523, Accuracy: 0.6217509283061983\n",
      "Step: 1167, Loss: 1.250382423400879, Accuracy: 0.6217893835616438\n",
      "Step: 1168, Loss: 1.296406865119934, Accuracy: 0.6217564870259481\n",
      "Step: 1169, Loss: 1.263326644897461, Accuracy: 0.6217236467236468\n",
      "Step: 1170, Loss: 1.0887333154678345, Accuracy: 0.6219043552519214\n",
      "Step: 1171, Loss: 1.224552035331726, Accuracy: 0.6219425483503982\n",
      "Step: 1172, Loss: 1.2895931005477905, Accuracy: 0.6219096334185849\n",
      "Step: 1173, Loss: 1.0982470512390137, Accuracy: 0.6220897217490062\n",
      "Step: 1174, Loss: 1.25571870803833, Accuracy: 0.6221276595744681\n",
      "Step: 1175, Loss: 1.476977825164795, Accuracy: 0.621952947845805\n",
      "Step: 1176, Loss: 1.1514934301376343, Accuracy: 0.6220617388841688\n",
      "Step: 1177, Loss: 1.2224963903427124, Accuracy: 0.6220996038483305\n",
      "Step: 1178, Loss: 1.2531557083129883, Accuracy: 0.6221374045801527\n",
      "Step: 1179, Loss: 0.9253279566764832, Accuracy: 0.622457627118644\n",
      "Step: 1180, Loss: 1.2357181310653687, Accuracy: 0.622495060683037\n",
      "Step: 1181, Loss: 1.215701699256897, Accuracy: 0.6225324309080654\n",
      "Step: 1182, Loss: 1.4488238096237183, Accuracy: 0.6223584108199492\n",
      "Step: 1183, Loss: 1.3305164575576782, Accuracy: 0.6223254504504504\n",
      "Step: 1184, Loss: 1.7095445394515991, Accuracy: 0.6219409282700422\n",
      "Step: 1185, Loss: 1.256087303161621, Accuracy: 0.6219786396852164\n",
      "Step: 1186, Loss: 1.330215573310852, Accuracy: 0.6218758775624824\n",
      "Step: 1187, Loss: 1.2309774160385132, Accuracy: 0.6219135802469136\n",
      "Step: 1188, Loss: 1.2938995361328125, Accuracy: 0.6218811326044295\n",
      "Step: 1189, Loss: 1.1123319864273071, Accuracy: 0.6220588235294118\n",
      "Step: 1190, Loss: 1.1596752405166626, Accuracy: 0.6221662468513854\n",
      "Step: 1191, Loss: 1.4670518636703491, Accuracy: 0.6219938478747203\n",
      "Step: 1192, Loss: 1.3808141946792603, Accuracy: 0.6218915898295613\n",
      "Step: 1193, Loss: 1.1534770727157593, Accuracy: 0.621998883305416\n",
      "Step: 1194, Loss: 1.1581608057022095, Accuracy: 0.6221059972105997\n",
      "Step: 1195, Loss: 1.4695361852645874, Accuracy: 0.6219342251950948\n",
      "Step: 1196, Loss: 1.4807209968566895, Accuracy: 0.6217627401837929\n",
      "Step: 1197, Loss: 1.3111640214920044, Accuracy: 0.6217306622148024\n",
      "Step: 1198, Loss: 1.325583815574646, Accuracy: 0.6216986377536836\n",
      "Step: 1199, Loss: 1.3624539375305176, Accuracy: 0.6215972222222222\n",
      "Step: 1200, Loss: 1.155852198600769, Accuracy: 0.6217041354426867\n",
      "Step: 1201, Loss: 1.4714187383651733, Accuracy: 0.6215335551858014\n",
      "Step: 1202, Loss: 1.3869271278381348, Accuracy: 0.6214325297866445\n",
      "Step: 1203, Loss: 1.2391092777252197, Accuracy: 0.6214700996677741\n",
      "Step: 1204, Loss: 1.2184420824050903, Accuracy: 0.6215076071922545\n",
      "Step: 1205, Loss: 1.1505881547927856, Accuracy: 0.6216141514648977\n",
      "Step: 1206, Loss: 1.2570630311965942, Accuracy: 0.6216514774924055\n",
      "Step: 1207, Loss: 1.2820744514465332, Accuracy: 0.6216197571743929\n",
      "Step: 1208, Loss: 1.2183177471160889, Accuracy: 0.6215880893300249\n",
      "Step: 1209, Loss: 1.1508928537368774, Accuracy: 0.621694214876033\n",
      "Step: 1210, Loss: 1.2760382890701294, Accuracy: 0.6216625378475089\n",
      "Step: 1211, Loss: 1.3031505346298218, Accuracy: 0.6216309130913091\n",
      "Step: 1212, Loss: 1.2690626382827759, Accuracy: 0.6215993404781534\n",
      "Step: 1213, Loss: 1.0762230157852173, Accuracy: 0.621773750686436\n",
      "Step: 1214, Loss: 1.3746892213821411, Accuracy: 0.6216735253772291\n",
      "Step: 1215, Loss: 1.2695066928863525, Accuracy: 0.6217105263157895\n",
      "Step: 1216, Loss: 1.1393753290176392, Accuracy: 0.6218159408381265\n",
      "Step: 1217, Loss: 1.163699984550476, Accuracy: 0.6219211822660099\n",
      "Step: 1218, Loss: 1.2758463621139526, Accuracy: 0.6218895269346458\n",
      "Step: 1219, Loss: 1.210011601448059, Accuracy: 0.6219262295081968\n",
      "Step: 1220, Loss: 1.1395392417907715, Accuracy: 0.622031122031122\n",
      "Step: 1221, Loss: 1.3023595809936523, Accuracy: 0.6219994544462629\n",
      "Step: 1222, Loss: 1.1828045845031738, Accuracy: 0.6221041155628236\n",
      "Step: 1223, Loss: 1.3458150625228882, Accuracy: 0.6220043572984749\n",
      "Step: 1224, Loss: 1.373925805091858, Accuracy: 0.621904761904762\n",
      "Step: 1225, Loss: 1.3898946046829224, Accuracy: 0.621805328983143\n",
      "Step: 1226, Loss: 1.3892616033554077, Accuracy: 0.621706058136376\n",
      "Step: 1227, Loss: 1.3644624948501587, Accuracy: 0.6216069489685125\n",
      "Step: 1228, Loss: 1.2680565118789673, Accuracy: 0.6216436126932465\n",
      "Step: 1229, Loss: 1.4906424283981323, Accuracy: 0.6214092140921409\n",
      "Step: 1230, Loss: 1.4579087495803833, Accuracy: 0.621242891957758\n",
      "Step: 1231, Loss: 1.2977908849716187, Accuracy: 0.6212121212121212\n",
      "Step: 1232, Loss: 1.2281079292297363, Accuracy: 0.621316572046499\n",
      "Step: 1233, Loss: 1.5895136594772339, Accuracy: 0.6210831982712047\n",
      "Step: 1234, Loss: 1.2810943126678467, Accuracy: 0.6210526315789474\n",
      "Step: 1235, Loss: 1.3216687440872192, Accuracy: 0.6210221143473571\n",
      "Step: 1236, Loss: 1.3883990049362183, Accuracy: 0.620924279170035\n",
      "Step: 1237, Loss: 1.0984729528427124, Accuracy: 0.6210958535271944\n",
      "Step: 1238, Loss: 1.0895206928253174, Accuracy: 0.6212671509281679\n",
      "Step: 1239, Loss: 1.23298180103302, Accuracy: 0.6213037634408602\n",
      "Step: 1240, Loss: 1.1072221994400024, Accuracy: 0.6214074670964276\n",
      "Step: 1241, Loss: 1.2580984830856323, Accuracy: 0.6214439076757917\n",
      "Step: 1242, Loss: 1.0928694009780884, Accuracy: 0.6216143738267632\n",
      "Step: 1243, Loss: 1.1398876905441284, Accuracy: 0.6217175777063236\n",
      "Step: 1244, Loss: 1.3472503423690796, Accuracy: 0.6216867469879518\n",
      "Step: 1245, Loss: 1.3089592456817627, Accuracy: 0.6216559657570894\n",
      "Step: 1246, Loss: 1.245552897453308, Accuracy: 0.621692060946271\n",
      "Step: 1247, Loss: 1.2120614051818848, Accuracy: 0.6217280982905983\n",
      "Step: 1248, Loss: 1.2947168350219727, Accuracy: 0.6217640779290099\n",
      "Step: 1249, Loss: 1.2250357866287231, Accuracy: 0.6218\n",
      "Step: 1250, Loss: 1.117806315422058, Accuracy: 0.6219024780175859\n",
      "Step: 1251, Loss: 1.397818922996521, Accuracy: 0.6218051118210862\n",
      "Step: 1252, Loss: 1.3100849390029907, Accuracy: 0.6217744080872573\n",
      "Step: 1253, Loss: 1.1884578466415405, Accuracy: 0.621943115364168\n",
      "Step: 1254, Loss: 1.2551002502441406, Accuracy: 0.6219123505976095\n",
      "Step: 1255, Loss: 1.1680949926376343, Accuracy: 0.6220143312101911\n",
      "Step: 1256, Loss: 1.368424415588379, Accuracy: 0.6219172633253779\n",
      "Step: 1257, Loss: 1.4194055795669556, Accuracy: 0.6217541070482246\n",
      "Step: 1258, Loss: 1.3260588645935059, Accuracy: 0.6217235901509134\n",
      "Step: 1259, Loss: 1.301446795463562, Accuracy: 0.6216931216931217\n",
      "Step: 1260, Loss: 1.1521039009094238, Accuracy: 0.6217948717948718\n",
      "Step: 1261, Loss: 1.247685432434082, Accuracy: 0.6218304278922345\n",
      "Step: 1262, Loss: 1.4116734266281128, Accuracy: 0.6217339667458432\n",
      "Step: 1263, Loss: 1.1037544012069702, Accuracy: 0.6218354430379747\n",
      "Step: 1264, Loss: 1.358665108680725, Accuracy: 0.6218050065876153\n",
      "Step: 1265, Loss: 1.163473129272461, Accuracy: 0.6219062664560295\n",
      "Step: 1266, Loss: 1.3086063861846924, Accuracy: 0.6218758221520653\n",
      "Step: 1267, Loss: 1.3025941848754883, Accuracy: 0.6218454258675079\n",
      "Step: 1268, Loss: 1.1469547748565674, Accuracy: 0.621946414499606\n",
      "Step: 1269, Loss: 1.152124285697937, Accuracy: 0.6220472440944882\n",
      "Step: 1270, Loss: 1.1969660520553589, Accuracy: 0.6221479150275374\n",
      "Step: 1271, Loss: 1.1314597129821777, Accuracy: 0.622248427672956\n",
      "Step: 1272, Loss: 1.2141443490982056, Accuracy: 0.6222833202409007\n",
      "Step: 1273, Loss: 1.3505686521530151, Accuracy: 0.6221873364730508\n",
      "Step: 1274, Loss: 1.15493643283844, Accuracy: 0.6222875816993464\n",
      "Step: 1275, Loss: 1.07217276096344, Accuracy: 0.6224529780564263\n",
      "Step: 1276, Loss: 1.197228193283081, Accuracy: 0.6225528582615505\n",
      "Step: 1277, Loss: 1.5774911642074585, Accuracy: 0.6223265519040166\n",
      "Step: 1278, Loss: 1.1982799768447876, Accuracy: 0.6223612197028929\n",
      "Step: 1279, Loss: 1.2912009954452515, Accuracy: 0.6223958333333334\n",
      "Step: 1280, Loss: 1.3268457651138306, Accuracy: 0.6223653395784543\n",
      "Step: 1281, Loss: 1.3094717264175415, Accuracy: 0.6223348933957358\n",
      "Step: 1282, Loss: 1.3363547325134277, Accuracy: 0.6222395427383736\n",
      "Step: 1283, Loss: 1.0494277477264404, Accuracy: 0.6224688473520249\n",
      "Step: 1284, Loss: 1.1647213697433472, Accuracy: 0.622568093385214\n",
      "Step: 1285, Loss: 1.2524967193603516, Accuracy: 0.6226023846552617\n",
      "Step: 1286, Loss: 1.10880708694458, Accuracy: 0.6227013727013727\n",
      "Step: 1287, Loss: 1.401243805885315, Accuracy: 0.6226061076604554\n",
      "Step: 1288, Loss: 1.4777334928512573, Accuracy: 0.6224463408326868\n",
      "Step: 1289, Loss: 1.3253194093704224, Accuracy: 0.6224160206718347\n",
      "Step: 1290, Loss: 1.3068774938583374, Accuracy: 0.6223857474825717\n",
      "Step: 1291, Loss: 1.2275859117507935, Accuracy: 0.6224200206398349\n",
      "Step: 1292, Loss: 1.225071668624878, Accuracy: 0.6224542407837071\n",
      "Step: 1293, Loss: 1.3607667684555054, Accuracy: 0.6224240082431737\n",
      "Step: 1294, Loss: 1.3263095617294312, Accuracy: 0.6223938223938223\n",
      "Step: 1295, Loss: 1.2693753242492676, Accuracy: 0.6224279835390947\n",
      "Step: 1296, Loss: 1.1296826601028442, Accuracy: 0.622526342842457\n",
      "Step: 1297, Loss: 1.1419843435287476, Accuracy: 0.6226245505906522\n",
      "Step: 1298, Loss: 1.0746362209320068, Accuracy: 0.6227867590454196\n",
      "Step: 1299, Loss: 1.3584707975387573, Accuracy: 0.6227564102564103\n",
      "Step: 1300, Loss: 1.482399582862854, Accuracy: 0.6225980015372791\n",
      "Step: 1301, Loss: 1.254858136177063, Accuracy: 0.6226318484383001\n",
      "Step: 1302, Loss: 1.2753158807754517, Accuracy: 0.6226016884113584\n",
      "Step: 1303, Loss: 1.0683369636535645, Accuracy: 0.6227632924335378\n",
      "Step: 1304, Loss: 1.2901731729507446, Accuracy: 0.6227330779054917\n",
      "Step: 1305, Loss: 1.2877018451690674, Accuracy: 0.6227029096477795\n",
      "Step: 1306, Loss: 1.310226559638977, Accuracy: 0.6226727875541953\n",
      "Step: 1307, Loss: 1.22733736038208, Accuracy: 0.6227064220183486\n",
      "Step: 1308, Loss: 1.288083791732788, Accuracy: 0.6226763432645785\n",
      "Step: 1309, Loss: 1.2750211954116821, Accuracy: 0.62264631043257\n",
      "Step: 1310, Loss: 1.3099908828735352, Accuracy: 0.6226163234172387\n",
      "Step: 1311, Loss: 1.3973153829574585, Accuracy: 0.6225228658536586\n",
      "Step: 1312, Loss: 1.3614085912704468, Accuracy: 0.6224930185326225\n",
      "Step: 1313, Loss: 1.141188383102417, Accuracy: 0.6225900558092339\n",
      "Step: 1314, Loss: 1.2409238815307617, Accuracy: 0.6226235741444867\n",
      "Step: 1315, Loss: 1.2516330480575562, Accuracy: 0.6226570415400202\n",
      "Step: 1316, Loss: 1.6417781114578247, Accuracy: 0.6223740825107568\n",
      "Step: 1317, Loss: 1.0819766521453857, Accuracy: 0.6225341426403642\n",
      "Step: 1318, Loss: 1.267660140991211, Accuracy: 0.6225676017184736\n",
      "Step: 1319, Loss: 1.499595046043396, Accuracy: 0.6224116161616161\n",
      "Step: 1320, Loss: 1.5491647720336914, Accuracy: 0.6221927832450164\n",
      "Step: 1321, Loss: 1.3345741033554077, Accuracy: 0.6221003530005043\n",
      "Step: 1322, Loss: 1.3307164907455444, Accuracy: 0.6220710506424793\n",
      "Step: 1323, Loss: 1.299978256225586, Accuracy: 0.6220417925478349\n",
      "Step: 1324, Loss: 1.3813036680221558, Accuracy: 0.6219496855345912\n",
      "Step: 1325, Loss: 1.0728940963745117, Accuracy: 0.6221091000502765\n",
      "Step: 1326, Loss: 1.3185713291168213, Accuracy: 0.6220798794272796\n",
      "Step: 1327, Loss: 1.1546616554260254, Accuracy: 0.6221762048192772\n",
      "Step: 1328, Loss: 1.0812026262283325, Accuracy: 0.622335089039378\n",
      "Step: 1329, Loss: 1.2396055459976196, Accuracy: 0.6223684210526316\n",
      "Step: 1330, Loss: 1.2567808628082275, Accuracy: 0.6224017029802154\n",
      "Step: 1331, Loss: 1.3422075510025024, Accuracy: 0.6223723723723724\n",
      "Step: 1332, Loss: 1.1244213581085205, Accuracy: 0.6225306326581646\n",
      "Step: 1333, Loss: 1.2643402814865112, Accuracy: 0.6225637181409296\n",
      "Step: 1334, Loss: 1.2731279134750366, Accuracy: 0.6225967540574282\n",
      "Step: 1335, Loss: 1.3060944080352783, Accuracy: 0.6225673652694611\n",
      "Step: 1336, Loss: 1.1347674131393433, Accuracy: 0.6226626776364996\n",
      "Step: 1337, Loss: 1.1468654870986938, Accuracy: 0.6227578475336323\n",
      "Step: 1338, Loss: 1.4591764211654663, Accuracy: 0.6226039332835449\n",
      "Step: 1339, Loss: 1.2707459926605225, Accuracy: 0.622636815920398\n",
      "Step: 1340, Loss: 1.5373010635375977, Accuracy: 0.6224210787969178\n",
      "Step: 1341, Loss: 1.430018424987793, Accuracy: 0.6223298559364133\n",
      "Step: 1342, Loss: 1.3020139932632446, Accuracy: 0.6223008190618019\n",
      "Step: 1343, Loss: 1.4301414489746094, Accuracy: 0.6221478174603174\n",
      "Step: 1344, Loss: 1.4103657007217407, Accuracy: 0.6220570012391574\n",
      "Step: 1345, Loss: 1.1863845586776733, Accuracy: 0.6220901436354631\n",
      "Step: 1346, Loss: 1.1546021699905396, Accuracy: 0.6221851026973522\n",
      "Step: 1347, Loss: 1.1456503868103027, Accuracy: 0.6222799208704253\n",
      "Step: 1348, Loss: 1.4684724807739258, Accuracy: 0.6221275018532246\n",
      "Step: 1349, Loss: 1.230168104171753, Accuracy: 0.6221604938271605\n",
      "Step: 1350, Loss: 1.3972982168197632, Accuracy: 0.6220700715519368\n",
      "Step: 1351, Loss: 1.1593120098114014, Accuracy: 0.6221646942800789\n",
      "Step: 1352, Loss: 1.3183952569961548, Accuracy: 0.6221359940872137\n",
      "Step: 1353, Loss: 1.6193310022354126, Accuracy: 0.621861152141802\n",
      "Step: 1354, Loss: 1.1390081644058228, Accuracy: 0.6219557195571955\n",
      "Step: 1355, Loss: 1.234832763671875, Accuracy: 0.6219886922320551\n",
      "Step: 1356, Loss: 1.31485116481781, Accuracy: 0.6219602063375093\n",
      "Step: 1357, Loss: 1.3595317602157593, Accuracy: 0.6219317623956799\n",
      "Step: 1358, Loss: 1.1551566123962402, Accuracy: 0.6220259995094433\n",
      "Step: 1359, Loss: 1.0391496419906616, Accuracy: 0.6222426470588235\n",
      "Step: 1360, Loss: 1.190886378288269, Accuracy: 0.6222752877785942\n",
      "Step: 1361, Loss: 1.5316330194473267, Accuracy: 0.6220631424375918\n",
      "Step: 1362, Loss: 1.104561686515808, Accuracy: 0.6222181462460259\n",
      "Step: 1363, Loss: 1.0717823505401611, Accuracy: 0.6223729227761485\n",
      "Step: 1364, Loss: 1.2730270624160767, Accuracy: 0.6223443223443224\n",
      "Step: 1365, Loss: 1.151964545249939, Accuracy: 0.6224377745241582\n",
      "Step: 1366, Loss: 1.3762174844741821, Accuracy: 0.6223482077542063\n",
      "Step: 1367, Loss: 1.3646186590194702, Accuracy: 0.6222587719298246\n",
      "Step: 1368, Loss: 1.1813881397247314, Accuracy: 0.6223520818115412\n",
      "Step: 1369, Loss: 1.4698171615600586, Accuracy: 0.6222019464720194\n",
      "Step: 1370, Loss: 1.2768702507019043, Accuracy: 0.6222343787989302\n",
      "Step: 1371, Loss: 1.2467833757400513, Accuracy: 0.6222667638483965\n",
      "Step: 1372, Loss: 1.329908847808838, Accuracy: 0.6222384073804321\n",
      "Step: 1373, Loss: 1.384961724281311, Accuracy: 0.6221494420184377\n",
      "Step: 1374, Loss: 1.2149574756622314, Accuracy: 0.6221818181818182\n",
      "Step: 1375, Loss: 1.0918327569961548, Accuracy: 0.6223352713178295\n",
      "Step: 1376, Loss: 1.1914623975753784, Accuracy: 0.6224279835390947\n",
      "Step: 1377, Loss: 1.4740146398544312, Accuracy: 0.622278664731495\n",
      "Step: 1378, Loss: 1.1291853189468384, Accuracy: 0.6223712835387962\n",
      "Step: 1379, Loss: 1.251090407371521, Accuracy: 0.6223429951690821\n",
      "Step: 1380, Loss: 1.525666356086731, Accuracy: 0.6221940622737147\n",
      "Step: 1381, Loss: 1.2010345458984375, Accuracy: 0.6222262421611191\n",
      "Step: 1382, Loss: 1.438122272491455, Accuracy: 0.6221378645456737\n",
      "Step: 1383, Loss: 1.4607359170913696, Accuracy: 0.6219894026974951\n",
      "Step: 1384, Loss: 1.4368816614151, Accuracy: 0.621841155234657\n",
      "Step: 1385, Loss: 1.5134830474853516, Accuracy: 0.6216931216931217\n",
      "Step: 1386, Loss: 1.159854531288147, Accuracy: 0.6217856284546984\n",
      "Step: 1387, Loss: 1.1684917211532593, Accuracy: 0.6218780019212296\n",
      "Step: 1388, Loss: 1.2216157913208008, Accuracy: 0.6219102471802256\n",
      "Step: 1389, Loss: 1.3227146863937378, Accuracy: 0.6218824940047961\n",
      "Step: 1390, Loss: 1.4139772653579712, Accuracy: 0.6217948717948718\n",
      "Step: 1391, Loss: 1.3872443437576294, Accuracy: 0.6217073754789272\n",
      "Step: 1392, Loss: 1.302024483680725, Accuracy: 0.6216798277099784\n",
      "Step: 1393, Loss: 1.3133705854415894, Accuracy: 0.6216523194643712\n",
      "Step: 1394, Loss: 1.332377552986145, Accuracy: 0.6216248506571087\n",
      "Step: 1395, Loss: 1.407081961631775, Accuracy: 0.6215377268385864\n",
      "Step: 1396, Loss: 1.3648706674575806, Accuracy: 0.6215103793843951\n",
      "Step: 1397, Loss: 1.2653812170028687, Accuracy: 0.6214830710538866\n",
      "Step: 1398, Loss: 1.169769287109375, Accuracy: 0.6215749344770074\n",
      "Step: 1399, Loss: 1.1573467254638672, Accuracy: 0.6216666666666667\n",
      "Step: 1400, Loss: 1.2323055267333984, Accuracy: 0.6216987865810135\n",
      "Step: 1401, Loss: 1.3158754110336304, Accuracy: 0.6216714217784118\n",
      "Step: 1402, Loss: 1.2218976020812988, Accuracy: 0.6217034925160371\n",
      "Step: 1403, Loss: 1.1174923181533813, Accuracy: 0.6218542260208927\n",
      "Step: 1404, Loss: 1.0516613721847534, Accuracy: 0.6220047449584816\n",
      "Step: 1405, Loss: 1.1607569456100464, Accuracy: 0.6220957799905168\n",
      "Step: 1406, Loss: 1.2247207164764404, Accuracy: 0.6221274579483534\n",
      "Step: 1407, Loss: 1.349276065826416, Accuracy: 0.6220407196969697\n",
      "Step: 1408, Loss: 1.2101036310195923, Accuracy: 0.6220723917672107\n",
      "Step: 1409, Loss: 0.9962772727012634, Accuracy: 0.6222813238770686\n",
      "Step: 1410, Loss: 1.3241304159164429, Accuracy: 0.6222537207654146\n",
      "Step: 1411, Loss: 1.3104431629180908, Accuracy: 0.6222261567516525\n",
      "Step: 1412, Loss: 1.4309760332107544, Accuracy: 0.6220806794055201\n",
      "Step: 1413, Loss: 1.301027536392212, Accuracy: 0.6221122112211221\n",
      "Step: 1414, Loss: 1.336262583732605, Accuracy: 0.6220848056537103\n",
      "Step: 1415, Loss: 1.402275562286377, Accuracy: 0.6219985875706214\n",
      "Step: 1416, Loss: 1.1841753721237183, Accuracy: 0.6220889202540578\n",
      "Step: 1417, Loss: 1.39506196975708, Accuracy: 0.6220028208744711\n",
      "Step: 1418, Loss: 1.2330381870269775, Accuracy: 0.6220342964529011\n",
      "Step: 1419, Loss: 1.3605766296386719, Accuracy: 0.6219483568075117\n",
      "Step: 1420, Loss: 1.1120408773422241, Accuracy: 0.6220971147079521\n",
      "Step: 1421, Loss: 1.1722216606140137, Accuracy: 0.6221870604781997\n",
      "Step: 1422, Loss: 1.0111380815505981, Accuracy: 0.6223940032794566\n",
      "Step: 1423, Loss: 1.2211017608642578, Accuracy: 0.6224250936329588\n",
      "Step: 1424, Loss: 1.329978346824646, Accuracy: 0.6223976608187135\n",
      "Step: 1425, Loss: 1.557064414024353, Accuracy: 0.6221949509116409\n",
      "Step: 1426, Loss: 1.5257891416549683, Accuracy: 0.6218757299696333\n",
      "Epoch: 0, Val_Accuracy: 0.6299065420560748\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8ffce01f5224f2b93e36d8c1f25750b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1427 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0, Loss: 1.188383936882019, Accuracy: 0.75\n",
      "Step: 1, Loss: 1.0209969282150269, Accuracy: 0.8333333333333334\n",
      "Step: 2, Loss: 1.4193825721740723, Accuracy: 0.7222222222222222\n",
      "Step: 3, Loss: 1.295035481452942, Accuracy: 0.6875\n",
      "Step: 4, Loss: 1.1507552862167358, Accuracy: 0.7\n",
      "Step: 5, Loss: 1.1542855501174927, Accuracy: 0.7083333333333334\n",
      "Step: 6, Loss: 1.2335125207901, Accuracy: 0.7023809523809523\n",
      "Step: 7, Loss: 1.1537567377090454, Accuracy: 0.7083333333333334\n",
      "Step: 8, Loss: 1.2135430574417114, Accuracy: 0.7037037037037037\n",
      "Step: 9, Loss: 1.3749476671218872, Accuracy: 0.6833333333333333\n",
      "Step: 10, Loss: 1.3811321258544922, Accuracy: 0.6666666666666666\n",
      "Step: 11, Loss: 1.359480381011963, Accuracy: 0.6527777777777778\n",
      "Step: 12, Loss: 1.21206796169281, Accuracy: 0.6602564102564102\n",
      "Step: 13, Loss: 1.4647070169448853, Accuracy: 0.6428571428571429\n",
      "Step: 14, Loss: 1.3950835466384888, Accuracy: 0.6333333333333333\n",
      "Step: 15, Loss: 1.1963226795196533, Accuracy: 0.6354166666666666\n",
      "Step: 16, Loss: 1.082614541053772, Accuracy: 0.6470588235294118\n",
      "Step: 17, Loss: 1.4282315969467163, Accuracy: 0.6342592592592593\n",
      "Step: 18, Loss: 1.1847072839736938, Accuracy: 0.6359649122807017\n",
      "Step: 19, Loss: 1.1768730878829956, Accuracy: 0.6416666666666667\n",
      "Step: 20, Loss: 1.1615595817565918, Accuracy: 0.6468253968253969\n",
      "Step: 21, Loss: 1.0933407545089722, Accuracy: 0.6553030303030303\n",
      "Step: 22, Loss: 1.1867676973342896, Accuracy: 0.6594202898550725\n",
      "Step: 23, Loss: 1.1785656213760376, Accuracy: 0.6631944444444444\n",
      "Step: 24, Loss: 1.4468169212341309, Accuracy: 0.6566666666666666\n",
      "Step: 25, Loss: 1.361491322517395, Accuracy: 0.6506410256410257\n",
      "Step: 26, Loss: 1.3912068605422974, Accuracy: 0.6450617283950617\n",
      "Step: 27, Loss: 1.177493929862976, Accuracy: 0.6458333333333334\n",
      "Step: 28, Loss: 1.1452327966690063, Accuracy: 0.6494252873563219\n",
      "Step: 29, Loss: 1.127146601676941, Accuracy: 0.6555555555555556\n",
      "Step: 30, Loss: 1.2233555316925049, Accuracy: 0.6559139784946236\n",
      "Step: 31, Loss: 1.2483652830123901, Accuracy: 0.65625\n",
      "Step: 32, Loss: 1.299340844154358, Accuracy: 0.6540404040404041\n",
      "Step: 33, Loss: 1.3234903812408447, Accuracy: 0.6519607843137255\n",
      "Step: 34, Loss: 1.2856920957565308, Accuracy: 0.6523809523809524\n",
      "Step: 35, Loss: 1.2392421960830688, Accuracy: 0.6527777777777778\n",
      "Step: 36, Loss: 1.2321853637695312, Accuracy: 0.6531531531531531\n",
      "Step: 37, Loss: 1.317828893661499, Accuracy: 0.6513157894736842\n",
      "Step: 38, Loss: 1.0456233024597168, Accuracy: 0.655982905982906\n",
      "Step: 39, Loss: 1.3923330307006836, Accuracy: 0.6520833333333333\n",
      "Step: 40, Loss: 1.116973876953125, Accuracy: 0.6565040650406504\n",
      "Step: 41, Loss: 1.0841118097305298, Accuracy: 0.6607142857142857\n",
      "Step: 42, Loss: 1.1205329895019531, Accuracy: 0.6647286821705426\n",
      "Step: 43, Loss: 1.2435978651046753, Accuracy: 0.6647727272727273\n",
      "Step: 44, Loss: 1.199573040008545, Accuracy: 0.6648148148148149\n",
      "Step: 45, Loss: 1.087486743927002, Accuracy: 0.6684782608695652\n",
      "Step: 46, Loss: 1.3109406232833862, Accuracy: 0.6666666666666666\n",
      "Step: 47, Loss: 0.9915896058082581, Accuracy: 0.671875\n",
      "Step: 48, Loss: 1.2519713640213013, Accuracy: 0.6717687074829932\n",
      "Step: 49, Loss: 1.2229124307632446, Accuracy: 0.6716666666666666\n",
      "Step: 50, Loss: 1.3671340942382812, Accuracy: 0.6699346405228758\n",
      "Step: 51, Loss: 1.2416468858718872, Accuracy: 0.6682692307692307\n",
      "Step: 52, Loss: 1.3551125526428223, Accuracy: 0.6666666666666666\n",
      "Step: 53, Loss: 1.1619316339492798, Accuracy: 0.6682098765432098\n",
      "Step: 54, Loss: 1.1665637493133545, Accuracy: 0.6696969696969697\n",
      "Step: 55, Loss: 1.2541090250015259, Accuracy: 0.6696428571428571\n",
      "Step: 56, Loss: 1.4016575813293457, Accuracy: 0.6666666666666666\n",
      "Step: 57, Loss: 1.0939663648605347, Accuracy: 0.6681034482758621\n",
      "Step: 58, Loss: 1.2323664426803589, Accuracy: 0.6680790960451978\n",
      "Step: 59, Loss: 1.3014880418777466, Accuracy: 0.6666666666666666\n",
      "Step: 60, Loss: 1.2308052778244019, Accuracy: 0.6666666666666666\n",
      "Step: 61, Loss: 1.2908295392990112, Accuracy: 0.6653225806451613\n",
      "Step: 62, Loss: 1.1518208980560303, Accuracy: 0.6666666666666666\n",
      "Step: 63, Loss: 1.0827256441116333, Accuracy: 0.6692708333333334\n",
      "Step: 64, Loss: 1.1733368635177612, Accuracy: 0.6705128205128205\n",
      "Step: 65, Loss: 1.4395846128463745, Accuracy: 0.6679292929292929\n",
      "Step: 66, Loss: 1.2589203119277954, Accuracy: 0.667910447761194\n",
      "Step: 67, Loss: 1.0672059059143066, Accuracy: 0.6703431372549019\n",
      "Step: 68, Loss: 1.1294792890548706, Accuracy: 0.6714975845410628\n",
      "Step: 69, Loss: 1.1662219762802124, Accuracy: 0.6726190476190477\n",
      "Step: 70, Loss: 1.455492377281189, Accuracy: 0.6690140845070423\n",
      "Step: 71, Loss: 1.1842906475067139, Accuracy: 0.6701388888888888\n",
      "Step: 72, Loss: 1.2749401330947876, Accuracy: 0.6689497716894978\n",
      "Step: 73, Loss: 1.4708045721054077, Accuracy: 0.6655405405405406\n",
      "Step: 74, Loss: 1.3869978189468384, Accuracy: 0.6633333333333333\n",
      "Step: 75, Loss: 1.0246866941452026, Accuracy: 0.6666666666666666\n",
      "Step: 76, Loss: 1.3349552154541016, Accuracy: 0.6645021645021645\n",
      "Step: 77, Loss: 1.1823126077651978, Accuracy: 0.6645299145299145\n",
      "Step: 78, Loss: 1.3204851150512695, Accuracy: 0.6635021097046413\n",
      "Step: 79, Loss: 1.3252356052398682, Accuracy: 0.6625\n",
      "Step: 80, Loss: 1.386765480041504, Accuracy: 0.6604938271604939\n",
      "Step: 81, Loss: 1.1971516609191895, Accuracy: 0.6615853658536586\n",
      "Step: 82, Loss: 1.3535019159317017, Accuracy: 0.6606425702811245\n",
      "Step: 83, Loss: 1.2375050783157349, Accuracy: 0.6607142857142857\n",
      "Step: 84, Loss: 1.2487235069274902, Accuracy: 0.6607843137254902\n",
      "Step: 85, Loss: 0.9981028437614441, Accuracy: 0.6637596899224806\n",
      "Step: 86, Loss: 1.2282243967056274, Accuracy: 0.6637931034482759\n",
      "Step: 87, Loss: 1.364004135131836, Accuracy: 0.6619318181818182\n",
      "Step: 88, Loss: 1.4663305282592773, Accuracy: 0.6591760299625468\n",
      "Step: 89, Loss: 1.207402229309082, Accuracy: 0.6592592592592592\n",
      "Step: 90, Loss: 1.1815210580825806, Accuracy: 0.6602564102564102\n",
      "Step: 91, Loss: 1.4726533889770508, Accuracy: 0.657608695652174\n",
      "Step: 92, Loss: 1.1070313453674316, Accuracy: 0.6594982078853047\n",
      "Step: 93, Loss: 1.403719425201416, Accuracy: 0.6569148936170213\n",
      "Step: 94, Loss: 1.457729697227478, Accuracy: 0.6543859649122807\n",
      "Step: 95, Loss: 1.1920959949493408, Accuracy: 0.6545138888888888\n",
      "Step: 96, Loss: 1.347599983215332, Accuracy: 0.6529209621993127\n",
      "Step: 97, Loss: 1.384377121925354, Accuracy: 0.6513605442176871\n",
      "Step: 98, Loss: 1.1795507669448853, Accuracy: 0.6523569023569024\n",
      "Step: 99, Loss: 1.0903223752975464, Accuracy: 0.6541666666666667\n",
      "Step: 100, Loss: 1.366745948791504, Accuracy: 0.6534653465346535\n",
      "Step: 101, Loss: 1.156707525253296, Accuracy: 0.6544117647058824\n",
      "Step: 102, Loss: 1.0691403150558472, Accuracy: 0.6561488673139159\n",
      "Step: 103, Loss: 1.222988247871399, Accuracy: 0.657051282051282\n",
      "Step: 104, Loss: 1.4388728141784668, Accuracy: 0.6547619047619048\n",
      "Step: 105, Loss: 1.345274567604065, Accuracy: 0.6540880503144654\n",
      "Step: 106, Loss: 1.1787084341049194, Accuracy: 0.6549844236760125\n",
      "Step: 107, Loss: 1.0703808069229126, Accuracy: 0.6566358024691358\n",
      "Step: 108, Loss: 1.2515933513641357, Accuracy: 0.6567278287461774\n",
      "Step: 109, Loss: 1.5497527122497559, Accuracy: 0.6537878787878788\n",
      "Step: 110, Loss: 1.248336672782898, Accuracy: 0.6531531531531531\n",
      "Step: 111, Loss: 1.1847552061080933, Accuracy: 0.6532738095238095\n",
      "Step: 112, Loss: 1.3756896257400513, Accuracy: 0.6519174041297935\n",
      "Step: 113, Loss: 1.1604429483413696, Accuracy: 0.6527777777777778\n",
      "Step: 114, Loss: 1.3211718797683716, Accuracy: 0.6521739130434783\n",
      "Step: 115, Loss: 1.3446651697158813, Accuracy: 0.6508620689655172\n",
      "Step: 116, Loss: 1.0419985055923462, Accuracy: 0.6524216524216524\n",
      "Step: 117, Loss: 1.0409420728683472, Accuracy: 0.6546610169491526\n",
      "Step: 118, Loss: 1.4025176763534546, Accuracy: 0.6533613445378151\n",
      "Step: 119, Loss: 1.3945717811584473, Accuracy: 0.6520833333333333\n",
      "Step: 120, Loss: 1.185326337814331, Accuracy: 0.6528925619834711\n",
      "Step: 121, Loss: 1.0790748596191406, Accuracy: 0.6543715846994536\n",
      "Step: 122, Loss: 1.2757490873336792, Accuracy: 0.6537940379403794\n",
      "Step: 123, Loss: 1.167336106300354, Accuracy: 0.6545698924731183\n",
      "Step: 124, Loss: 1.1682170629501343, Accuracy: 0.6553333333333333\n",
      "Step: 125, Loss: 1.3043440580368042, Accuracy: 0.6554232804232805\n",
      "Step: 126, Loss: 1.101058840751648, Accuracy: 0.6568241469816273\n",
      "Step: 127, Loss: 1.1644357442855835, Accuracy: 0.6575520833333334\n",
      "Step: 128, Loss: 1.416141390800476, Accuracy: 0.6563307493540051\n",
      "Step: 129, Loss: 1.1828911304473877, Accuracy: 0.657051282051282\n",
      "Step: 130, Loss: 1.3684271574020386, Accuracy: 0.655852417302799\n",
      "Step: 131, Loss: 1.3751519918441772, Accuracy: 0.6546717171717171\n",
      "Step: 132, Loss: 1.3838075399398804, Accuracy: 0.6535087719298246\n",
      "Step: 133, Loss: 1.3982142210006714, Accuracy: 0.652363184079602\n",
      "Step: 134, Loss: 1.1241300106048584, Accuracy: 0.6530864197530865\n",
      "Step: 135, Loss: 1.404207706451416, Accuracy: 0.6519607843137255\n",
      "Step: 136, Loss: 1.2798019647598267, Accuracy: 0.6514598540145985\n",
      "Step: 137, Loss: 1.4782692193984985, Accuracy: 0.6497584541062802\n",
      "Step: 138, Loss: 1.157152771949768, Accuracy: 0.6504796163069544\n",
      "Step: 139, Loss: 1.375484824180603, Accuracy: 0.6494047619047619\n",
      "Step: 140, Loss: 1.3597527742385864, Accuracy: 0.6483451536643026\n",
      "Step: 141, Loss: 1.080392837524414, Accuracy: 0.6496478873239436\n",
      "Step: 142, Loss: 1.4356287717819214, Accuracy: 0.6480186480186481\n",
      "Step: 143, Loss: 1.0903030633926392, Accuracy: 0.6493055555555556\n",
      "Step: 144, Loss: 1.218109130859375, Accuracy: 0.6494252873563219\n",
      "Step: 145, Loss: 1.493569016456604, Accuracy: 0.6478310502283106\n",
      "Step: 146, Loss: 1.244486689567566, Accuracy: 0.6479591836734694\n",
      "Step: 147, Loss: 1.324859857559204, Accuracy: 0.6475225225225225\n",
      "Step: 148, Loss: 1.216140627861023, Accuracy: 0.6476510067114094\n",
      "Step: 149, Loss: 1.1739445924758911, Accuracy: 0.6483333333333333\n",
      "Step: 150, Loss: 1.2728112936019897, Accuracy: 0.6484547461368654\n",
      "Step: 151, Loss: 1.3532133102416992, Accuracy: 0.6474780701754386\n",
      "Step: 152, Loss: 1.3965935707092285, Accuracy: 0.6465141612200436\n",
      "Step: 153, Loss: 1.2185598611831665, Accuracy: 0.6466450216450217\n",
      "Step: 154, Loss: 1.307329773902893, Accuracy: 0.646236559139785\n",
      "Step: 155, Loss: 1.5020469427108765, Accuracy: 0.6447649572649573\n",
      "Step: 156, Loss: 1.3934346437454224, Accuracy: 0.6438428874734607\n",
      "Step: 157, Loss: 1.142959713935852, Accuracy: 0.6445147679324894\n",
      "Step: 158, Loss: 1.292066216468811, Accuracy: 0.6441299790356394\n",
      "Step: 159, Loss: 1.5357674360275269, Accuracy: 0.6421875\n",
      "Step: 160, Loss: 1.3091877698898315, Accuracy: 0.6418219461697723\n",
      "Step: 161, Loss: 1.2195497751235962, Accuracy: 0.6419753086419753\n",
      "Step: 162, Loss: 1.3428831100463867, Accuracy: 0.6416155419222904\n",
      "Step: 163, Loss: 1.4023948907852173, Accuracy: 0.6407520325203252\n",
      "Step: 164, Loss: 1.5654808282852173, Accuracy: 0.6383838383838384\n",
      "Step: 165, Loss: 1.3645448684692383, Accuracy: 0.6375502008032129\n",
      "Step: 166, Loss: 1.4348516464233398, Accuracy: 0.6362275449101796\n",
      "Step: 167, Loss: 1.2775238752365112, Accuracy: 0.6359126984126984\n",
      "Step: 168, Loss: 1.064524531364441, Accuracy: 0.6370808678500987\n",
      "Step: 169, Loss: 1.1603782176971436, Accuracy: 0.6377450980392156\n",
      "Step: 170, Loss: 1.352583885192871, Accuracy: 0.6374269005847953\n",
      "Step: 171, Loss: 1.247820496559143, Accuracy: 0.6375968992248062\n",
      "Step: 172, Loss: 1.2450034618377686, Accuracy: 0.6377649325626205\n",
      "Step: 173, Loss: 1.417336106300354, Accuracy: 0.6369731800766284\n",
      "Step: 174, Loss: 1.2597395181655884, Accuracy: 0.6371428571428571\n",
      "Step: 175, Loss: 1.1979719400405884, Accuracy: 0.6377840909090909\n",
      "Step: 176, Loss: 1.2096149921417236, Accuracy: 0.6379472693032016\n",
      "Step: 177, Loss: 1.2043399810791016, Accuracy: 0.6385767790262172\n",
      "Step: 178, Loss: 1.046534538269043, Accuracy: 0.6396648044692738\n",
      "Step: 179, Loss: 1.5316100120544434, Accuracy: 0.637962962962963\n",
      "Step: 180, Loss: 1.3828792572021484, Accuracy: 0.6372007366482505\n",
      "Step: 181, Loss: 1.275955080986023, Accuracy: 0.6369047619047619\n",
      "Step: 182, Loss: 1.3323323726654053, Accuracy: 0.6361566484517304\n",
      "Step: 183, Loss: 1.0756443738937378, Accuracy: 0.6372282608695652\n",
      "Step: 184, Loss: 1.1640931367874146, Accuracy: 0.6378378378378379\n",
      "Step: 185, Loss: 1.3685132265090942, Accuracy: 0.6375448028673835\n",
      "Step: 186, Loss: 1.2988123893737793, Accuracy: 0.6372549019607843\n",
      "Step: 187, Loss: 1.2630887031555176, Accuracy: 0.636968085106383\n",
      "Step: 188, Loss: 1.2551838159561157, Accuracy: 0.6371252204585538\n",
      "Step: 189, Loss: 1.1117440462112427, Accuracy: 0.637719298245614\n",
      "Step: 190, Loss: 1.0882816314697266, Accuracy: 0.6387434554973822\n",
      "Step: 191, Loss: 1.194792628288269, Accuracy: 0.6393229166666666\n",
      "Step: 192, Loss: 1.2022367715835571, Accuracy: 0.6398963730569949\n",
      "Step: 193, Loss: 1.1820236444473267, Accuracy: 0.6404639175257731\n",
      "Step: 194, Loss: 1.2583905458450317, Accuracy: 0.6405982905982905\n",
      "Step: 195, Loss: 1.186519980430603, Accuracy: 0.641156462585034\n",
      "Step: 196, Loss: 1.5314946174621582, Accuracy: 0.6395939086294417\n",
      "Step: 197, Loss: 1.3213615417480469, Accuracy: 0.6397306397306397\n",
      "Step: 198, Loss: 1.308097243309021, Accuracy: 0.6394472361809045\n",
      "Step: 199, Loss: 1.4543920755386353, Accuracy: 0.6383333333333333\n",
      "Step: 200, Loss: 1.1602648496627808, Accuracy: 0.6388888888888888\n",
      "Step: 201, Loss: 1.0521928071975708, Accuracy: 0.6398514851485149\n",
      "Step: 202, Loss: 1.1620866060256958, Accuracy: 0.6403940886699507\n",
      "Step: 203, Loss: 1.171120047569275, Accuracy: 0.6409313725490197\n",
      "Step: 204, Loss: 1.2245981693267822, Accuracy: 0.6410569105691057\n",
      "Step: 205, Loss: 1.0792831182479858, Accuracy: 0.6419902912621359\n",
      "Step: 206, Loss: 1.387924313545227, Accuracy: 0.6417069243156199\n",
      "Step: 207, Loss: 1.2959142923355103, Accuracy: 0.641426282051282\n",
      "Step: 208, Loss: 1.0687066316604614, Accuracy: 0.6423444976076556\n",
      "Step: 209, Loss: 1.2476855516433716, Accuracy: 0.6424603174603175\n",
      "Step: 210, Loss: 1.253532886505127, Accuracy: 0.6425750394944708\n",
      "Step: 211, Loss: 1.0079913139343262, Accuracy: 0.6438679245283019\n",
      "Step: 212, Loss: 1.4309736490249634, Accuracy: 0.6428012519561815\n",
      "Step: 213, Loss: 1.1202304363250732, Accuracy: 0.6436915887850467\n",
      "Step: 214, Loss: 1.3662689924240112, Accuracy: 0.6430232558139535\n",
      "Step: 215, Loss: 1.1959543228149414, Accuracy: 0.6431327160493827\n",
      "Step: 216, Loss: 1.2565028667449951, Accuracy: 0.6432411674347158\n",
      "Step: 217, Loss: 1.0327104330062866, Accuracy: 0.6441131498470948\n",
      "Step: 218, Loss: 1.1442996263504028, Accuracy: 0.6445966514459666\n",
      "Step: 219, Loss: 1.145851731300354, Accuracy: 0.6450757575757575\n",
      "Step: 220, Loss: 1.3005908727645874, Accuracy: 0.6447963800904978\n",
      "Step: 221, Loss: 1.229091763496399, Accuracy: 0.6448948948948949\n",
      "Step: 222, Loss: 1.4421626329421997, Accuracy: 0.6438714499252616\n",
      "Step: 223, Loss: 1.2172850370407104, Accuracy: 0.6439732142857143\n",
      "Step: 224, Loss: 1.2799396514892578, Accuracy: 0.644074074074074\n",
      "Step: 225, Loss: 1.3060308694839478, Accuracy: 0.6441740412979351\n",
      "Step: 226, Loss: 1.0705538988113403, Accuracy: 0.645007342143906\n",
      "Step: 227, Loss: 1.333616852760315, Accuracy: 0.6447368421052632\n",
      "Step: 228, Loss: 1.3659719228744507, Accuracy: 0.6444687045123726\n",
      "Step: 229, Loss: 1.2123881578445435, Accuracy: 0.6445652173913043\n",
      "Step: 230, Loss: 1.2416666746139526, Accuracy: 0.6446608946608947\n",
      "Step: 231, Loss: 1.2397903203964233, Accuracy: 0.6447557471264368\n",
      "Step: 232, Loss: 1.0567116737365723, Accuracy: 0.6455650929899857\n",
      "Step: 233, Loss: 1.1149401664733887, Accuracy: 0.6463675213675214\n",
      "Step: 234, Loss: 1.4181855916976929, Accuracy: 0.6457446808510638\n",
      "Step: 235, Loss: 1.0691282749176025, Accuracy: 0.6465395480225988\n",
      "Step: 236, Loss: 1.3619476556777954, Accuracy: 0.6462728551336147\n",
      "Step: 237, Loss: 1.2823007106781006, Accuracy: 0.6460084033613446\n",
      "Step: 238, Loss: 1.1952407360076904, Accuracy: 0.646094839609484\n",
      "Step: 239, Loss: 1.3753207921981812, Accuracy: 0.6454861111111111\n",
      "Step: 240, Loss: 1.4896751642227173, Accuracy: 0.6445366528354081\n",
      "Step: 241, Loss: 1.2566033601760864, Accuracy: 0.6446280991735537\n",
      "Step: 242, Loss: 1.388759732246399, Accuracy: 0.6440329218106996\n",
      "Step: 243, Loss: 1.161987543106079, Accuracy: 0.6444672131147541\n",
      "Step: 244, Loss: 1.3866691589355469, Accuracy: 0.6438775510204081\n",
      "Step: 245, Loss: 1.471230149269104, Accuracy: 0.6429539295392954\n",
      "Step: 246, Loss: 1.108707070350647, Accuracy: 0.6437246963562753\n",
      "Step: 247, Loss: 1.2674590349197388, Accuracy: 0.6434811827956989\n",
      "Step: 248, Loss: 1.403452754020691, Accuracy: 0.642904953145917\n",
      "Step: 249, Loss: 1.1753782033920288, Accuracy: 0.6433333333333333\n",
      "Step: 250, Loss: 1.133675217628479, Accuracy: 0.6437583001328021\n",
      "Step: 251, Loss: 1.312064528465271, Accuracy: 0.6435185185185185\n",
      "Step: 252, Loss: 1.090575098991394, Accuracy: 0.6442687747035574\n",
      "Step: 253, Loss: 1.2344882488250732, Accuracy: 0.6443569553805775\n",
      "Step: 254, Loss: 1.207823395729065, Accuracy: 0.6447712418300654\n",
      "Step: 255, Loss: 1.1414740085601807, Accuracy: 0.6451822916666666\n",
      "Step: 256, Loss: 1.1585737466812134, Accuracy: 0.6455901426718548\n",
      "Step: 257, Loss: 1.0728545188903809, Accuracy: 0.6463178294573644\n",
      "Step: 258, Loss: 1.421244502067566, Accuracy: 0.6454311454311454\n",
      "Step: 259, Loss: 1.3832387924194336, Accuracy: 0.6448717948717949\n",
      "Step: 260, Loss: 1.2380245923995972, Accuracy: 0.644955300127714\n",
      "Step: 261, Loss: 1.2354305982589722, Accuracy: 0.6450381679389313\n",
      "Step: 262, Loss: 1.1089270114898682, Accuracy: 0.6457541191381495\n",
      "Step: 263, Loss: 1.3835474252700806, Accuracy: 0.6452020202020202\n",
      "Step: 264, Loss: 1.303661823272705, Accuracy: 0.6449685534591195\n",
      "Step: 265, Loss: 1.3212436437606812, Accuracy: 0.6447368421052632\n",
      "Step: 266, Loss: 1.1164331436157227, Accuracy: 0.6451310861423221\n",
      "Step: 267, Loss: 1.1835787296295166, Accuracy: 0.6452114427860697\n",
      "Step: 268, Loss: 1.3150140047073364, Accuracy: 0.6449814126394052\n",
      "Step: 269, Loss: 1.4858050346374512, Accuracy: 0.6441358024691358\n",
      "Step: 270, Loss: 1.1558467149734497, Accuracy: 0.6445264452644527\n",
      "Step: 271, Loss: 1.153549313545227, Accuracy: 0.6449142156862745\n",
      "Step: 272, Loss: 1.1518219709396362, Accuracy: 0.6452991452991453\n",
      "Step: 273, Loss: 1.3058351278305054, Accuracy: 0.6450729927007299\n",
      "Step: 274, Loss: 1.087024450302124, Accuracy: 0.6457575757575758\n",
      "Step: 275, Loss: 1.3996652364730835, Accuracy: 0.6452294685990339\n",
      "Step: 276, Loss: 1.2532988786697388, Accuracy: 0.6453068592057761\n",
      "Step: 277, Loss: 1.165125846862793, Accuracy: 0.64568345323741\n",
      "Step: 278, Loss: 1.2493896484375, Accuracy: 0.6460573476702509\n",
      "Step: 279, Loss: 1.2624036073684692, Accuracy: 0.6461309523809524\n",
      "Step: 280, Loss: 1.227721095085144, Accuracy: 0.6465005931198102\n",
      "Step: 281, Loss: 1.257106900215149, Accuracy: 0.6465721040189125\n",
      "Step: 282, Loss: 1.4294792413711548, Accuracy: 0.6460541813898705\n",
      "Step: 283, Loss: 1.332903265953064, Accuracy: 0.6458333333333334\n",
      "Step: 284, Loss: 1.1274608373641968, Accuracy: 0.6461988304093568\n",
      "Step: 285, Loss: 1.3904114961624146, Accuracy: 0.6456876456876457\n",
      "Step: 286, Loss: 1.305261492729187, Accuracy: 0.6454703832752613\n",
      "Step: 287, Loss: 1.1391539573669434, Accuracy: 0.6458333333333334\n",
      "Step: 288, Loss: 1.3108347654342651, Accuracy: 0.6456170703575548\n",
      "Step: 289, Loss: 1.175308346748352, Accuracy: 0.6459770114942529\n",
      "Step: 290, Loss: 1.1623402833938599, Accuracy: 0.6463344788087056\n",
      "Step: 291, Loss: 1.2788738012313843, Accuracy: 0.646404109589041\n",
      "Step: 292, Loss: 1.2438793182373047, Accuracy: 0.6464732650739476\n",
      "Step: 293, Loss: 1.2061777114868164, Accuracy: 0.6465419501133787\n",
      "Step: 294, Loss: 1.1627435684204102, Accuracy: 0.6468926553672316\n",
      "Step: 295, Loss: 1.2940045595169067, Accuracy: 0.6466779279279279\n",
      "Step: 296, Loss: 1.1507488489151, Accuracy: 0.6470258136924804\n",
      "Step: 297, Loss: 1.3484822511672974, Accuracy: 0.6468120805369127\n",
      "Step: 298, Loss: 1.2137458324432373, Accuracy: 0.6468784838350056\n",
      "Step: 299, Loss: 1.1420626640319824, Accuracy: 0.6472222222222223\n",
      "Step: 300, Loss: 1.2025662660598755, Accuracy: 0.647563676633444\n",
      "Step: 301, Loss: 1.380042552947998, Accuracy: 0.647075055187638\n",
      "Step: 302, Loss: 1.3024853467941284, Accuracy: 0.6468646864686468\n",
      "Step: 303, Loss: 1.2020834684371948, Accuracy: 0.6469298245614035\n",
      "Step: 304, Loss: 1.2319378852844238, Accuracy: 0.6469945355191257\n",
      "Step: 305, Loss: 1.3051427602767944, Accuracy: 0.6467864923747276\n",
      "Step: 306, Loss: 1.2231261730194092, Accuracy: 0.6468512486427795\n",
      "Step: 307, Loss: 1.3863121271133423, Accuracy: 0.6463744588744589\n",
      "Step: 308, Loss: 1.0882216691970825, Accuracy: 0.6469795037756203\n",
      "Step: 309, Loss: 1.3260560035705566, Accuracy: 0.646774193548387\n",
      "Step: 310, Loss: 1.2185014486312866, Accuracy: 0.6468381564844587\n",
      "Step: 311, Loss: 1.090969443321228, Accuracy: 0.6477029914529915\n",
      "Step: 312, Loss: 1.1349892616271973, Accuracy: 0.6480298189563365\n",
      "Step: 313, Loss: 1.3926037549972534, Accuracy: 0.6475583864118896\n",
      "Step: 314, Loss: 1.1990349292755127, Accuracy: 0.6478835978835978\n",
      "Step: 315, Loss: 1.3293386697769165, Accuracy: 0.6476793248945147\n",
      "Step: 316, Loss: 1.1276150941848755, Accuracy: 0.6480021030494216\n",
      "Step: 317, Loss: 1.2133551836013794, Accuracy: 0.6480607966457023\n",
      "Step: 318, Loss: 1.4058986902236938, Accuracy: 0.6475966562173459\n",
      "Step: 319, Loss: 1.4533251523971558, Accuracy: 0.646875\n",
      "Step: 320, Loss: 1.217444896697998, Accuracy: 0.6469366562824507\n",
      "Step: 321, Loss: 1.2863441705703735, Accuracy: 0.6467391304347826\n",
      "Step: 322, Loss: 1.2121936082839966, Accuracy: 0.6468008255933952\n",
      "Step: 323, Loss: 1.2295669317245483, Accuracy: 0.6468621399176955\n",
      "Step: 324, Loss: 1.1551264524459839, Accuracy: 0.6471794871794871\n",
      "Step: 325, Loss: 1.238923192024231, Accuracy: 0.647239263803681\n",
      "Step: 326, Loss: 1.3718323707580566, Accuracy: 0.6467889908256881\n",
      "Step: 327, Loss: 1.0552021265029907, Accuracy: 0.6476117886178862\n",
      "Step: 328, Loss: 1.4572926759719849, Accuracy: 0.6471631205673759\n",
      "Step: 329, Loss: 1.254456639289856, Accuracy: 0.6472222222222223\n",
      "Step: 330, Loss: 1.2390763759613037, Accuracy: 0.6470292044310171\n",
      "Step: 331, Loss: 1.3488879203796387, Accuracy: 0.6468373493975904\n",
      "Step: 332, Loss: 1.1934374570846558, Accuracy: 0.6471471471471472\n",
      "Step: 333, Loss: 1.2396550178527832, Accuracy: 0.6472055888223552\n",
      "Step: 334, Loss: 1.2859681844711304, Accuracy: 0.6472636815920398\n",
      "Step: 335, Loss: 1.2768269777297974, Accuracy: 0.6473214285714286\n",
      "Step: 336, Loss: 1.4297665357589722, Accuracy: 0.6468842729970327\n",
      "Step: 337, Loss: 1.3074597120285034, Accuracy: 0.6466962524654832\n",
      "Step: 338, Loss: 1.0324106216430664, Accuracy: 0.6474926253687315\n",
      "Step: 339, Loss: 1.3115383386611938, Accuracy: 0.6473039215686275\n",
      "Step: 340, Loss: 1.2098047733306885, Accuracy: 0.6473607038123167\n",
      "Step: 341, Loss: 1.3549598455429077, Accuracy: 0.6471734892787524\n",
      "Step: 342, Loss: 1.1305699348449707, Accuracy: 0.6477162293488824\n",
      "Step: 343, Loss: 1.3399842977523804, Accuracy: 0.6472868217054264\n",
      "Step: 344, Loss: 1.1025255918502808, Accuracy: 0.6478260869565218\n",
      "Step: 345, Loss: 1.4214097261428833, Accuracy: 0.6473988439306358\n",
      "Step: 346, Loss: 1.3990002870559692, Accuracy: 0.6469740634005764\n",
      "Step: 347, Loss: 1.1330937147140503, Accuracy: 0.6475095785440613\n",
      "Step: 348, Loss: 1.5245434045791626, Accuracy: 0.6466093600764088\n",
      "Step: 349, Loss: 1.1353899240493774, Accuracy: 0.6471428571428571\n",
      "Step: 350, Loss: 1.2527928352355957, Accuracy: 0.6471984805318138\n",
      "Step: 351, Loss: 1.1579415798187256, Accuracy: 0.6474905303030303\n",
      "Step: 352, Loss: 0.9986111521720886, Accuracy: 0.6482530689329556\n",
      "Step: 353, Loss: 1.3411561250686646, Accuracy: 0.6480696798493408\n",
      "Step: 354, Loss: 1.4695239067077637, Accuracy: 0.6474178403755868\n",
      "Step: 355, Loss: 1.297918438911438, Accuracy: 0.6472378277153558\n",
      "Step: 356, Loss: 1.2977055311203003, Accuracy: 0.6470588235294118\n",
      "Step: 357, Loss: 1.191217303276062, Accuracy: 0.6471135940409684\n",
      "Step: 358, Loss: 1.2282689809799194, Accuracy: 0.6471680594243269\n",
      "Step: 359, Loss: 1.325278401374817, Accuracy: 0.6469907407407407\n",
      "Step: 360, Loss: 1.4961563348770142, Accuracy: 0.6461218836565097\n",
      "Step: 361, Loss: 1.331870675086975, Accuracy: 0.6459484346224678\n",
      "Step: 362, Loss: 1.2304311990737915, Accuracy: 0.6460055096418733\n",
      "Step: 363, Loss: 1.2314609289169312, Accuracy: 0.6462912087912088\n",
      "Step: 364, Loss: 1.2237200736999512, Accuracy: 0.6463470319634703\n",
      "Step: 365, Loss: 1.3623815774917603, Accuracy: 0.6461748633879781\n",
      "Step: 366, Loss: 1.61533784866333, Accuracy: 0.6450953678474114\n",
      "Step: 367, Loss: 1.2885181903839111, Accuracy: 0.644927536231884\n",
      "Step: 368, Loss: 1.2417412996292114, Accuracy: 0.6449864498644986\n",
      "Step: 369, Loss: 1.2901519536972046, Accuracy: 0.6448198198198198\n",
      "Step: 370, Loss: 1.2452665567398071, Accuracy: 0.644878706199461\n",
      "Step: 371, Loss: 1.4829250574111938, Accuracy: 0.6442652329749103\n",
      "Step: 372, Loss: 1.253059983253479, Accuracy: 0.644325290437891\n",
      "Step: 373, Loss: 1.2771037817001343, Accuracy: 0.6443850267379679\n",
      "Step: 374, Loss: 1.4045766592025757, Accuracy: 0.644\n",
      "Step: 375, Loss: 1.1192432641983032, Accuracy: 0.6445035460992907\n",
      "Step: 376, Loss: 1.2933381795883179, Accuracy: 0.6443412908930151\n",
      "Step: 377, Loss: 1.1560739278793335, Accuracy: 0.644620811287478\n",
      "Step: 378, Loss: 1.115881085395813, Accuracy: 0.6451187335092349\n",
      "Step: 379, Loss: 1.1736868619918823, Accuracy: 0.6453947368421052\n",
      "Step: 380, Loss: 1.1417127847671509, Accuracy: 0.6456692913385826\n",
      "Step: 381, Loss: 1.123762845993042, Accuracy: 0.6459424083769634\n",
      "Step: 382, Loss: 1.3124903440475464, Accuracy: 0.6457789382071366\n",
      "Step: 383, Loss: 1.0959068536758423, Accuracy: 0.6462673611111112\n",
      "Step: 384, Loss: 1.2557438611984253, Accuracy: 0.6463203463203463\n",
      "Step: 385, Loss: 1.1145082712173462, Accuracy: 0.646804835924007\n",
      "Step: 386, Loss: 1.346408486366272, Accuracy: 0.646640826873385\n",
      "Step: 387, Loss: 1.2144229412078857, Accuracy: 0.6469072164948454\n",
      "Step: 388, Loss: 1.3099552392959595, Accuracy: 0.6467437874892887\n",
      "Step: 389, Loss: 1.1763030290603638, Accuracy: 0.647008547008547\n",
      "Step: 390, Loss: 1.2774790525436401, Accuracy: 0.646845694799659\n",
      "Step: 391, Loss: 1.2399574518203735, Accuracy: 0.6468962585034014\n",
      "Step: 392, Loss: 1.168606162071228, Accuracy: 0.64715860899067\n",
      "Step: 393, Loss: 1.4933056831359863, Accuracy: 0.6465736040609137\n",
      "Step: 394, Loss: 1.308819055557251, Accuracy: 0.6464135021097046\n",
      "Step: 395, Loss: 1.2139570713043213, Accuracy: 0.6464646464646465\n",
      "Step: 396, Loss: 1.272396445274353, Accuracy: 0.6463056255247691\n",
      "Step: 397, Loss: 1.3770771026611328, Accuracy: 0.6459380234505863\n",
      "Step: 398, Loss: 1.4239832162857056, Accuracy: 0.6453634085213033\n",
      "Step: 399, Loss: 1.384625792503357, Accuracy: 0.645\n",
      "Step: 400, Loss: 1.4214062690734863, Accuracy: 0.6444305901911886\n",
      "Step: 401, Loss: 1.3490694761276245, Accuracy: 0.6442786069651741\n",
      "Step: 402, Loss: 1.1695348024368286, Accuracy: 0.6445409429280397\n",
      "Step: 403, Loss: 1.161091923713684, Accuracy: 0.6448019801980198\n",
      "Step: 404, Loss: 1.4000916481018066, Accuracy: 0.6444444444444445\n",
      "Step: 405, Loss: 1.2798744440078735, Accuracy: 0.6442939244663383\n",
      "Step: 406, Loss: 1.6341586112976074, Accuracy: 0.6433251433251433\n",
      "Step: 407, Loss: 1.1554244756698608, Accuracy: 0.6435866013071896\n",
      "Step: 408, Loss: 1.3742138147354126, Accuracy: 0.6432355338223309\n",
      "Step: 409, Loss: 1.3366022109985352, Accuracy: 0.6430894308943089\n",
      "Step: 410, Loss: 1.0100743770599365, Accuracy: 0.6437550689375506\n",
      "Step: 411, Loss: 1.4027525186538696, Accuracy: 0.643406148867314\n",
      "Step: 412, Loss: 1.0253874063491821, Accuracy: 0.6440677966101694\n",
      "Step: 413, Loss: 1.4534155130386353, Accuracy: 0.6435185185185185\n",
      "Step: 414, Loss: 1.2418736219406128, Accuracy: 0.643574297188755\n",
      "Step: 415, Loss: 1.325857162475586, Accuracy: 0.6434294871794872\n",
      "Step: 416, Loss: 1.372594952583313, Accuracy: 0.6430855315747402\n",
      "Step: 417, Loss: 1.0703445672988892, Accuracy: 0.6435406698564593\n",
      "Step: 418, Loss: 1.265843152999878, Accuracy: 0.6433969769291965\n",
      "Step: 419, Loss: 1.3077170848846436, Accuracy: 0.6432539682539683\n",
      "Step: 420, Loss: 1.4290947914123535, Accuracy: 0.6429136975455265\n",
      "Step: 421, Loss: 1.0867654085159302, Accuracy: 0.6433649289099526\n",
      "Step: 422, Loss: 1.2362574338912964, Accuracy: 0.6434200157604413\n",
      "Step: 423, Loss: 1.4007667303085327, Accuracy: 0.6430817610062893\n",
      "Step: 424, Loss: 1.2039529085159302, Accuracy: 0.6433333333333333\n",
      "Step: 425, Loss: 1.050139307975769, Accuracy: 0.6437793427230047\n",
      "Step: 426, Loss: 1.3388915061950684, Accuracy: 0.6436377829820453\n",
      "Step: 427, Loss: 1.3668729066848755, Accuracy: 0.6433021806853583\n",
      "Step: 428, Loss: 1.0427173376083374, Accuracy: 0.6439393939393939\n",
      "Step: 429, Loss: 1.1669334173202515, Accuracy: 0.6441860465116279\n",
      "Step: 430, Loss: 1.3107247352600098, Accuracy: 0.6440448569218871\n",
      "Step: 431, Loss: 1.483649730682373, Accuracy: 0.6435185185185185\n",
      "Step: 432, Loss: 1.2942014932632446, Accuracy: 0.6433795227097767\n",
      "Step: 433, Loss: 1.1365848779678345, Accuracy: 0.6436251920122887\n",
      "Step: 434, Loss: 1.1402472257614136, Accuracy: 0.6440613026819924\n",
      "Step: 435, Loss: 1.2824225425720215, Accuracy: 0.6439220183486238\n",
      "Step: 436, Loss: 1.1655774116516113, Accuracy: 0.6441647597254004\n",
      "Step: 437, Loss: 1.1514308452606201, Accuracy: 0.644406392694064\n",
      "Step: 438, Loss: 1.1115293502807617, Accuracy: 0.6448367501898253\n",
      "Step: 439, Loss: 1.4522086381912231, Accuracy: 0.6443181818181818\n",
      "Step: 440, Loss: 1.1552947759628296, Accuracy: 0.6445578231292517\n",
      "Step: 441, Loss: 1.4178298711776733, Accuracy: 0.6442307692307693\n",
      "Step: 442, Loss: 0.9124694466590881, Accuracy: 0.6450338600451467\n",
      "Step: 443, Loss: 1.3341017961502075, Accuracy: 0.6448948948948949\n",
      "Step: 444, Loss: 1.3974062204360962, Accuracy: 0.6445692883895131\n",
      "Step: 445, Loss: 1.13461434841156, Accuracy: 0.6448056801195815\n",
      "Step: 446, Loss: 1.0069695711135864, Accuracy: 0.645413870246085\n",
      "Step: 447, Loss: 1.3027385473251343, Accuracy: 0.6452752976190477\n",
      "Step: 448, Loss: 1.2271950244903564, Accuracy: 0.6453229398663697\n",
      "Step: 449, Loss: 1.3178757429122925, Accuracy: 0.6451851851851852\n",
      "Step: 450, Loss: 1.166797161102295, Accuracy: 0.6454175905395417\n",
      "Step: 451, Loss: 1.2378350496292114, Accuracy: 0.6454646017699115\n",
      "Step: 452, Loss: 1.1566540002822876, Accuracy: 0.6456953642384106\n",
      "Step: 453, Loss: 1.2329925298690796, Accuracy: 0.645741556534508\n",
      "Step: 454, Loss: 1.322164535522461, Accuracy: 0.6456043956043956\n",
      "Step: 455, Loss: 1.2013474702835083, Accuracy: 0.6456505847953217\n",
      "Step: 456, Loss: 1.437354564666748, Accuracy: 0.6453318745441283\n",
      "Step: 457, Loss: 1.1050699949264526, Accuracy: 0.6457423580786026\n",
      "Step: 458, Loss: 1.202786922454834, Accuracy: 0.6457879448075526\n",
      "Step: 459, Loss: 1.3299092054367065, Accuracy: 0.6456521739130435\n",
      "Step: 460, Loss: 1.391292929649353, Accuracy: 0.6453362255965293\n",
      "Step: 461, Loss: 1.3209654092788696, Accuracy: 0.6452020202020202\n",
      "Step: 462, Loss: 1.2889528274536133, Accuracy: 0.6452483801295896\n",
      "Step: 463, Loss: 1.3752870559692383, Accuracy: 0.6449353448275862\n",
      "Step: 464, Loss: 1.2385882139205933, Accuracy: 0.6449820788530466\n",
      "Step: 465, Loss: 1.3669694662094116, Accuracy: 0.6446709585121603\n",
      "Step: 466, Loss: 1.4412845373153687, Accuracy: 0.644361170592434\n",
      "Step: 467, Loss: 1.6287097930908203, Accuracy: 0.6435185185185185\n",
      "Step: 468, Loss: 1.3930217027664185, Accuracy: 0.6430348258706468\n",
      "Step: 469, Loss: 1.0935213565826416, Accuracy: 0.6434397163120568\n",
      "Step: 470, Loss: 1.3600101470947266, Accuracy: 0.6431351733899504\n",
      "Step: 471, Loss: 1.0920145511627197, Accuracy: 0.6435381355932204\n",
      "Step: 472, Loss: 1.1779242753982544, Accuracy: 0.6437632135306554\n",
      "Step: 473, Loss: 1.2915921211242676, Accuracy: 0.6436357243319268\n",
      "Step: 474, Loss: 1.3253114223480225, Accuracy: 0.6436842105263157\n",
      "Step: 475, Loss: 1.3259373903274536, Accuracy: 0.6435574229691877\n",
      "Step: 476, Loss: 1.271317720413208, Accuracy: 0.6436058700209644\n",
      "Step: 477, Loss: 1.3589235544204712, Accuracy: 0.643305439330544\n",
      "Step: 478, Loss: 1.155804991722107, Accuracy: 0.6435281837160751\n",
      "Step: 479, Loss: 1.2438496351242065, Accuracy: 0.6435763888888889\n",
      "Step: 480, Loss: 1.3847633600234985, Accuracy: 0.6432778932778933\n",
      "Step: 481, Loss: 1.0991493463516235, Accuracy: 0.6436721991701245\n",
      "Step: 482, Loss: 1.0103081464767456, Accuracy: 0.6442374051069704\n",
      "Step: 483, Loss: 1.507789969444275, Accuracy: 0.643595041322314\n",
      "Step: 484, Loss: 1.4205241203308105, Accuracy: 0.643298969072165\n",
      "Step: 485, Loss: 1.2301582098007202, Accuracy: 0.6433470507544582\n",
      "Step: 486, Loss: 1.133949637413025, Accuracy: 0.6437371663244353\n",
      "Step: 487, Loss: 1.2464523315429688, Accuracy: 0.6437841530054644\n",
      "Step: 488, Loss: 1.4140912294387817, Accuracy: 0.643490115882754\n",
      "Step: 489, Loss: 1.3709691762924194, Accuracy: 0.6431972789115646\n",
      "Step: 490, Loss: 1.2908320426940918, Accuracy: 0.6430753564154786\n",
      "Step: 491, Loss: 1.393933892250061, Accuracy: 0.6427845528455285\n",
      "Step: 492, Loss: 1.0924184322357178, Accuracy: 0.6431710615280595\n",
      "Step: 493, Loss: 1.3357715606689453, Accuracy: 0.6428812415654521\n",
      "Step: 494, Loss: 1.4611090421676636, Accuracy: 0.6424242424242425\n",
      "Step: 495, Loss: 1.3183954954147339, Accuracy: 0.6423051075268817\n",
      "Step: 496, Loss: 1.1581348180770874, Accuracy: 0.6425217974513749\n",
      "Step: 497, Loss: 1.3026106357574463, Accuracy: 0.6424029451137885\n",
      "Step: 498, Loss: 1.4380369186401367, Accuracy: 0.6421175684702739\n",
      "Step: 499, Loss: 1.3310537338256836, Accuracy: 0.6418333333333334\n",
      "Step: 500, Loss: 1.1370601654052734, Accuracy: 0.6420492348636061\n",
      "Step: 501, Loss: 1.282163381576538, Accuracy: 0.6419322709163346\n",
      "Step: 502, Loss: 1.3350120782852173, Accuracy: 0.6418157720344599\n",
      "Step: 503, Loss: 1.1464803218841553, Accuracy: 0.6421957671957672\n",
      "Step: 504, Loss: 1.2300169467926025, Accuracy: 0.6422442244224422\n",
      "Step: 505, Loss: 1.266270637512207, Accuracy: 0.642292490118577\n",
      "Step: 506, Loss: 1.167943000793457, Accuracy: 0.6425049309664694\n",
      "Step: 507, Loss: 1.235784888267517, Accuracy: 0.6425524934383202\n",
      "Step: 508, Loss: 1.1438740491867065, Accuracy: 0.6427635887360839\n",
      "Step: 509, Loss: 1.3820899724960327, Accuracy: 0.642483660130719\n",
      "Step: 510, Loss: 1.4028372764587402, Accuracy: 0.642204827136334\n",
      "Step: 511, Loss: 1.1604987382888794, Accuracy: 0.6424153645833334\n",
      "Step: 512, Loss: 1.2975109815597534, Accuracy: 0.6423001949317739\n",
      "Step: 513, Loss: 0.9934666752815247, Accuracy: 0.642833981841764\n",
      "Step: 514, Loss: 1.1412559747695923, Accuracy: 0.643042071197411\n",
      "Step: 515, Loss: 1.468030571937561, Accuracy: 0.6426033591731266\n",
      "Step: 516, Loss: 1.4633487462997437, Accuracy: 0.6421663442940039\n",
      "Step: 517, Loss: 1.1644407510757446, Accuracy: 0.6423745173745173\n",
      "Step: 518, Loss: 1.1317410469055176, Accuracy: 0.6425818882466281\n",
      "Step: 519, Loss: 1.2024781703948975, Accuracy: 0.6426282051282052\n",
      "Step: 520, Loss: 1.3380013704299927, Accuracy: 0.6425143953934741\n",
      "Step: 521, Loss: 1.29511559009552, Accuracy: 0.6424010217113666\n",
      "Step: 522, Loss: 1.6085575819015503, Accuracy: 0.6416507329509241\n",
      "Step: 523, Loss: 1.297390103340149, Accuracy: 0.6415394402035624\n",
      "Step: 524, Loss: 1.1788283586502075, Accuracy: 0.6417460317460317\n",
      "Step: 525, Loss: 1.179972767829895, Accuracy: 0.6419518377693283\n",
      "Step: 526, Loss: 1.0719691514968872, Accuracy: 0.642314990512334\n",
      "Step: 527, Loss: 1.223641037940979, Accuracy: 0.6423611111111112\n",
      "Step: 528, Loss: 1.2026110887527466, Accuracy: 0.6424070573408948\n",
      "Step: 529, Loss: 0.9915500283241272, Accuracy: 0.6429245283018868\n",
      "Step: 530, Loss: 1.313963532447815, Accuracy: 0.642812303829253\n",
      "Step: 531, Loss: 1.1055235862731934, Accuracy: 0.643170426065163\n",
      "Step: 532, Loss: 1.265640377998352, Accuracy: 0.6432145090681676\n",
      "Step: 533, Loss: 1.2948919534683228, Accuracy: 0.6431023720349563\n",
      "Step: 534, Loss: 1.1800962686538696, Accuracy: 0.6433021806853583\n",
      "Step: 535, Loss: 1.3113495111465454, Accuracy: 0.6431902985074627\n",
      "Step: 536, Loss: 1.0851067304611206, Accuracy: 0.643544382371198\n",
      "Step: 537, Loss: 1.0037903785705566, Accuracy: 0.6440520446096655\n",
      "Step: 538, Loss: 1.3203368186950684, Accuracy: 0.6439393939393939\n",
      "Step: 539, Loss: 1.368057131767273, Accuracy: 0.6436728395061728\n",
      "Step: 540, Loss: 1.1945407390594482, Accuracy: 0.6438693776956254\n",
      "Step: 541, Loss: 1.0929814577102661, Accuracy: 0.6442189421894219\n",
      "Step: 542, Loss: 1.3075684309005737, Accuracy: 0.6441068139963168\n",
      "Step: 543, Loss: 1.168669581413269, Accuracy: 0.6443014705882353\n",
      "Step: 544, Loss: 1.2882070541381836, Accuracy: 0.6441896024464832\n",
      "Step: 545, Loss: 1.331164002418518, Accuracy: 0.6440781440781441\n",
      "Step: 546, Loss: 1.37874174118042, Accuracy: 0.6438147471054235\n",
      "Step: 547, Loss: 1.354188323020935, Accuracy: 0.6437043795620438\n",
      "Step: 548, Loss: 1.3605866432189941, Accuracy: 0.6434426229508197\n",
      "Step: 549, Loss: 1.628544807434082, Accuracy: 0.6427272727272727\n",
      "Step: 550, Loss: 1.2643555402755737, Accuracy: 0.6427707199032063\n",
      "Step: 551, Loss: 1.3405495882034302, Accuracy: 0.642512077294686\n",
      "Step: 552, Loss: 1.1655951738357544, Accuracy: 0.6425557564798071\n",
      "Step: 553, Loss: 1.3754301071166992, Accuracy: 0.6422984356197352\n",
      "Step: 554, Loss: 1.208497405052185, Accuracy: 0.6423423423423423\n",
      "Step: 555, Loss: 1.307599425315857, Accuracy: 0.642236211031175\n",
      "Step: 556, Loss: 1.0488353967666626, Accuracy: 0.6425792938360263\n",
      "Step: 557, Loss: 1.4002405405044556, Accuracy: 0.6423237753882916\n",
      "Step: 558, Loss: 1.19071364402771, Accuracy: 0.6425163983303518\n",
      "Step: 559, Loss: 1.2034481763839722, Accuracy: 0.6425595238095239\n",
      "Step: 560, Loss: 1.1480380296707153, Accuracy: 0.6427510398098634\n",
      "Step: 561, Loss: 1.2512060403823853, Accuracy: 0.6427935943060499\n",
      "Step: 562, Loss: 1.3890156745910645, Accuracy: 0.6425399644760214\n",
      "Step: 563, Loss: 1.2367714643478394, Accuracy: 0.6425827423167849\n",
      "Step: 564, Loss: 1.1302490234375, Accuracy: 0.6427728613569321\n",
      "Step: 565, Loss: 1.1981706619262695, Accuracy: 0.6428150765606596\n",
      "Step: 566, Loss: 1.3717279434204102, Accuracy: 0.6425631981187536\n",
      "Step: 567, Loss: 1.162577509880066, Accuracy: 0.6427523474178404\n",
      "Step: 568, Loss: 1.2501524686813354, Accuracy: 0.6427943760984183\n",
      "Step: 569, Loss: 1.4113696813583374, Accuracy: 0.6425438596491229\n",
      "Step: 570, Loss: 1.1990193128585815, Accuracy: 0.6427320490367776\n",
      "Step: 571, Loss: 1.565792441368103, Accuracy: 0.6420454545454546\n",
      "Step: 572, Loss: 1.409087061882019, Accuracy: 0.6417975567190227\n",
      "Step: 573, Loss: 1.3913764953613281, Accuracy: 0.6415505226480837\n",
      "Step: 574, Loss: 1.0765869617462158, Accuracy: 0.6418840579710144\n",
      "Step: 575, Loss: 1.2593929767608643, Accuracy: 0.6417824074074074\n",
      "Step: 576, Loss: 1.3040918111801147, Accuracy: 0.641681109185442\n",
      "Step: 577, Loss: 1.2519359588623047, Accuracy: 0.6417243367935409\n",
      "Step: 578, Loss: 1.221614956855774, Accuracy: 0.6417674150834772\n",
      "Step: 579, Loss: 1.4703079462051392, Accuracy: 0.6413793103448275\n",
      "Step: 580, Loss: 1.1042860746383667, Accuracy: 0.6417096959265634\n",
      "Step: 581, Loss: 1.202835202217102, Accuracy: 0.6417525773195877\n",
      "Step: 582, Loss: 1.041046380996704, Accuracy: 0.6422241280731846\n",
      "Step: 583, Loss: 1.2608935832977295, Accuracy: 0.6422659817351598\n",
      "Step: 584, Loss: 1.2611243724822998, Accuracy: 0.6423076923076924\n",
      "Step: 585, Loss: 1.232490062713623, Accuracy: 0.642349260523322\n",
      "Step: 586, Loss: 1.2134904861450195, Accuracy: 0.6423906871095968\n",
      "Step: 587, Loss: 1.183821439743042, Accuracy: 0.6425736961451247\n",
      "Step: 588, Loss: 1.18393075466156, Accuracy: 0.6426146010186757\n",
      "Step: 589, Loss: 1.1391311883926392, Accuracy: 0.6427966101694915\n",
      "Step: 590, Loss: 1.0925403833389282, Accuracy: 0.6431190073322053\n",
      "Step: 591, Loss: 1.3345659971237183, Accuracy: 0.6430180180180181\n",
      "Step: 592, Loss: 1.2372523546218872, Accuracy: 0.6430578976953345\n",
      "Step: 593, Loss: 1.2779724597930908, Accuracy: 0.6430976430976431\n",
      "Step: 594, Loss: 1.3107147216796875, Accuracy: 0.6429971988795519\n",
      "Step: 595, Loss: 1.4675121307373047, Accuracy: 0.6426174496644296\n",
      "Step: 596, Loss: 1.272847056388855, Accuracy: 0.6426577331099944\n",
      "Step: 597, Loss: 1.1295808553695679, Accuracy: 0.6428372352285395\n",
      "Step: 598, Loss: 1.1671866178512573, Accuracy: 0.6430161380077908\n",
      "Step: 599, Loss: 1.186916470527649, Accuracy: 0.6430555555555556\n",
      "Step: 600, Loss: 1.3661069869995117, Accuracy: 0.6428175263449806\n",
      "Step: 601, Loss: 1.4078203439712524, Accuracy: 0.6425802879291251\n",
      "Step: 602, Loss: 1.1344738006591797, Accuracy: 0.642758430071863\n",
      "Step: 603, Loss: 1.1569403409957886, Accuracy: 0.6429359823399559\n",
      "Step: 604, Loss: 1.151065468788147, Accuracy: 0.6431129476584022\n",
      "Step: 605, Loss: 1.330674648284912, Accuracy: 0.643014301430143\n",
      "Step: 606, Loss: 1.3069509267807007, Accuracy: 0.6429159802306426\n",
      "Step: 607, Loss: 1.3382925987243652, Accuracy: 0.6428179824561403\n",
      "Step: 608, Loss: 1.1235466003417969, Accuracy: 0.6431308155446086\n",
      "Step: 609, Loss: 1.2766458988189697, Accuracy: 0.6431693989071038\n",
      "Step: 610, Loss: 1.1360028982162476, Accuracy: 0.6433442444080743\n",
      "Step: 611, Loss: 1.269357442855835, Accuracy: 0.6433823529411765\n",
      "Step: 612, Loss: 1.6262632608413696, Accuracy: 0.6427406199021207\n",
      "Step: 613, Loss: 1.010249137878418, Accuracy: 0.6431867535287731\n",
      "Step: 614, Loss: 1.3773722648620605, Accuracy: 0.6429539295392954\n",
      "Step: 615, Loss: 1.1455599069595337, Accuracy: 0.6431277056277056\n",
      "Step: 616, Loss: 1.4175291061401367, Accuracy: 0.6427606699081577\n",
      "Step: 617, Loss: 1.1688406467437744, Accuracy: 0.642799352750809\n",
      "Step: 618, Loss: 1.3371003866195679, Accuracy: 0.6427032848680668\n",
      "Step: 619, Loss: 1.1337131261825562, Accuracy: 0.6428763440860215\n",
      "Step: 620, Loss: 1.2016105651855469, Accuracy: 0.6430488459473966\n",
      "Step: 621, Loss: 1.2242287397384644, Accuracy: 0.6430868167202572\n",
      "Step: 622, Loss: 1.482190728187561, Accuracy: 0.6427233814874265\n",
      "Step: 623, Loss: 1.3830432891845703, Accuracy: 0.6424946581196581\n",
      "Step: 624, Loss: 1.047624111175537, Accuracy: 0.6429333333333334\n",
      "Step: 625, Loss: 1.341668725013733, Accuracy: 0.6428381256656017\n",
      "Step: 626, Loss: 1.2884939908981323, Accuracy: 0.6427432216905901\n",
      "Step: 627, Loss: 1.37856924533844, Accuracy: 0.642515923566879\n",
      "Step: 628, Loss: 1.1489711999893188, Accuracy: 0.6426868044515104\n",
      "Step: 629, Loss: 1.534109115600586, Accuracy: 0.6421957671957672\n",
      "Step: 630, Loss: 1.2347978353500366, Accuracy: 0.6422345483359746\n",
      "Step: 631, Loss: 1.3714157342910767, Accuracy: 0.6420094936708861\n",
      "Step: 632, Loss: 1.4044419527053833, Accuracy: 0.641785150078989\n",
      "Step: 633, Loss: 1.0690838098526, Accuracy: 0.642087276550999\n",
      "Step: 634, Loss: 1.392922043800354, Accuracy: 0.6418635170603675\n",
      "Step: 635, Loss: 1.1565481424331665, Accuracy: 0.6420335429769392\n",
      "Step: 636, Loss: 1.3580464124679565, Accuracy: 0.641810570381999\n",
      "Step: 637, Loss: 1.3799046277999878, Accuracy: 0.6415882967607106\n",
      "Step: 638, Loss: 1.2279471158981323, Accuracy: 0.6416275430359938\n",
      "Step: 639, Loss: 1.2303215265274048, Accuracy: 0.6416666666666667\n",
      "Step: 640, Loss: 1.2214292287826538, Accuracy: 0.641705668226729\n",
      "Step: 641, Loss: 1.3050034046173096, Accuracy: 0.6416147455867082\n",
      "Step: 642, Loss: 1.0956124067306519, Accuracy: 0.6419129082426127\n",
      "Step: 643, Loss: 1.5134152173995972, Accuracy: 0.6415631469979296\n",
      "Step: 644, Loss: 1.2606260776519775, Accuracy: 0.6416020671834626\n",
      "Step: 645, Loss: 1.3897522687911987, Accuracy: 0.6413828689370485\n",
      "Step: 646, Loss: 1.1586605310440063, Accuracy: 0.6415507470376095\n",
      "Step: 647, Loss: 1.1758356094360352, Accuracy: 0.6417181069958847\n",
      "Step: 648, Loss: 1.3937439918518066, Accuracy: 0.6414997431946584\n",
      "Step: 649, Loss: 1.4584627151489258, Accuracy: 0.6411538461538462\n",
      "Step: 650, Loss: 1.243698239326477, Accuracy: 0.6411930363543267\n",
      "Step: 651, Loss: 1.1474201679229736, Accuracy: 0.641359918200409\n",
      "Step: 652, Loss: 1.1748783588409424, Accuracy: 0.6415262889229199\n",
      "Step: 653, Loss: 1.1334353685379028, Accuracy: 0.6416921508664628\n",
      "Step: 654, Loss: 1.3185843229293823, Accuracy: 0.6416030534351145\n",
      "Step: 655, Loss: 1.3239065408706665, Accuracy: 0.6415142276422764\n",
      "Step: 656, Loss: 1.300046682357788, Accuracy: 0.6414256722475901\n",
      "Step: 657, Loss: 1.0873030424118042, Accuracy: 0.6417173252279635\n",
      "Step: 658, Loss: 1.2045087814331055, Accuracy: 0.6417551846231664\n",
      "Step: 659, Loss: 1.4931772947311401, Accuracy: 0.6414141414141414\n",
      "Step: 660, Loss: 1.3261070251464844, Accuracy: 0.6413262733232477\n",
      "Step: 661, Loss: 1.154326319694519, Accuracy: 0.6414904330312186\n",
      "Step: 662, Loss: 1.1465128660202026, Accuracy: 0.6416540975364505\n",
      "Step: 663, Loss: 1.2492908239364624, Accuracy: 0.6416917670682731\n",
      "Step: 664, Loss: 1.0876866579055786, Accuracy: 0.6419799498746868\n",
      "Step: 665, Loss: 1.2133363485336304, Accuracy: 0.642017017017017\n",
      "Step: 666, Loss: 1.2787158489227295, Accuracy: 0.6420539730134932\n",
      "Step: 667, Loss: 1.2333121299743652, Accuracy: 0.6420908183632734\n",
      "Step: 668, Loss: 1.1400961875915527, Accuracy: 0.6422521175884405\n",
      "Step: 669, Loss: 1.4242796897888184, Accuracy: 0.6420398009950249\n",
      "Step: 670, Loss: 1.1372984647750854, Accuracy: 0.642200695479384\n",
      "Step: 671, Loss: 1.2682808637619019, Accuracy: 0.6422371031746031\n",
      "Step: 672, Loss: 1.22670316696167, Accuracy: 0.6422734026745914\n",
      "Step: 673, Loss: 1.2137399911880493, Accuracy: 0.6423095944609297\n",
      "Step: 674, Loss: 1.1394511461257935, Accuracy: 0.6424691358024691\n",
      "Step: 675, Loss: 1.3356794118881226, Accuracy: 0.6422583826429981\n",
      "Step: 676, Loss: 1.271424651145935, Accuracy: 0.6422944362383063\n",
      "Step: 677, Loss: 0.9958376288414001, Accuracy: 0.6426991150442478\n",
      "Step: 678, Loss: 1.3178575038909912, Accuracy: 0.6426116838487973\n",
      "Step: 679, Loss: 1.234757423400879, Accuracy: 0.6426470588235295\n",
      "Step: 680, Loss: 1.0979994535446167, Accuracy: 0.6429270680372002\n",
      "Step: 681, Loss: 1.1603227853775024, Accuracy: 0.6430840664711632\n",
      "Step: 682, Loss: 1.1983412504196167, Accuracy: 0.6432406051732552\n",
      "Step: 683, Loss: 1.3686890602111816, Accuracy: 0.6430311890838206\n",
      "Step: 684, Loss: 1.2185643911361694, Accuracy: 0.6430656934306569\n",
      "Step: 685, Loss: 1.31768000125885, Accuracy: 0.6429786200194364\n",
      "Step: 686, Loss: 1.3228167295455933, Accuracy: 0.6428918000970403\n",
      "Step: 687, Loss: 1.2647539377212524, Accuracy: 0.6429263565891473\n",
      "Step: 688, Loss: 1.0598559379577637, Accuracy: 0.6432027092404451\n",
      "Step: 689, Loss: 1.1832590103149414, Accuracy: 0.6433574879227053\n",
      "Step: 690, Loss: 1.307824969291687, Accuracy: 0.6432706222865412\n",
      "Step: 691, Loss: 1.0172847509384155, Accuracy: 0.6436657032755299\n",
      "Step: 692, Loss: 1.2988862991333008, Accuracy: 0.6435786435786436\n",
      "Step: 693, Loss: 1.4154976606369019, Accuracy: 0.6432516810758886\n",
      "Step: 694, Loss: 1.1487611532211304, Accuracy: 0.6434052757793765\n",
      "Step: 695, Loss: 1.4692293405532837, Accuracy: 0.6430795019157088\n",
      "Step: 696, Loss: 1.3274552822113037, Accuracy: 0.6428742228598756\n",
      "Step: 697, Loss: 1.4249253273010254, Accuracy: 0.6426695319961796\n",
      "Step: 698, Loss: 1.219812273979187, Accuracy: 0.6427038626609443\n",
      "Step: 699, Loss: 1.4902464151382446, Accuracy: 0.6423809523809524\n",
      "Step: 700, Loss: 1.157515048980713, Accuracy: 0.6425344745601521\n",
      "Step: 701, Loss: 1.378387451171875, Accuracy: 0.6423314339981007\n",
      "Step: 702, Loss: 1.2088724374771118, Accuracy: 0.6423660502607871\n",
      "Step: 703, Loss: 1.148016095161438, Accuracy: 0.6425189393939394\n",
      "Step: 704, Loss: 1.1411858797073364, Accuracy: 0.6426713947990543\n",
      "Step: 705, Loss: 1.0092288255691528, Accuracy: 0.6430594900849859\n",
      "Step: 706, Loss: 1.386454701423645, Accuracy: 0.6428571428571429\n",
      "Step: 707, Loss: 1.334680438041687, Accuracy: 0.6427730696798494\n",
      "Step: 708, Loss: 1.2488476037979126, Accuracy: 0.6428067700987306\n",
      "Step: 709, Loss: 1.2427104711532593, Accuracy: 0.6428403755868545\n",
      "Step: 710, Loss: 1.1940386295318604, Accuracy: 0.6428738865447726\n",
      "Step: 711, Loss: 1.0873602628707886, Accuracy: 0.6431413857677902\n",
      "Step: 712, Loss: 1.4432915449142456, Accuracy: 0.6428237494156148\n",
      "Step: 713, Loss: 1.3987447023391724, Accuracy: 0.642623716153128\n",
      "Step: 714, Loss: 1.249066710472107, Accuracy: 0.6426573426573426\n",
      "Step: 715, Loss: 1.361824631690979, Accuracy: 0.6424581005586593\n",
      "Step: 716, Loss: 1.4544636011123657, Accuracy: 0.6421431892143189\n",
      "Step: 717, Loss: 1.0893255472183228, Accuracy: 0.6424094707520891\n",
      "Step: 718, Loss: 1.267612099647522, Accuracy: 0.6424432081594807\n",
      "Step: 719, Loss: 1.1416889429092407, Accuracy: 0.6425925925925926\n",
      "Step: 720, Loss: 1.3329702615737915, Accuracy: 0.6425104022191401\n",
      "Step: 721, Loss: 1.2801330089569092, Accuracy: 0.6425438596491229\n",
      "Step: 722, Loss: 1.390188217163086, Accuracy: 0.6423467035500231\n",
      "Step: 723, Loss: 1.2666383981704712, Accuracy: 0.6423802946593001\n",
      "Step: 724, Loss: 1.1431221961975098, Accuracy: 0.6425287356321839\n",
      "Step: 725, Loss: 1.318925142288208, Accuracy: 0.6424471992653811\n",
      "Step: 726, Loss: 1.2955576181411743, Accuracy: 0.6423658872077029\n",
      "Step: 727, Loss: 1.0880906581878662, Accuracy: 0.6426282051282052\n",
      "Step: 728, Loss: 1.2965830564498901, Accuracy: 0.6426611796982168\n",
      "Step: 729, Loss: 1.3509896993637085, Accuracy: 0.6425799086757991\n",
      "Step: 730, Loss: 1.321208119392395, Accuracy: 0.6424988600091199\n",
      "Step: 731, Loss: 1.2021526098251343, Accuracy: 0.6425318761384335\n",
      "Step: 732, Loss: 1.4577234983444214, Accuracy: 0.6422237380627558\n",
      "Step: 733, Loss: 1.2241344451904297, Accuracy: 0.6422570390554042\n",
      "Step: 734, Loss: 1.2248696088790894, Accuracy: 0.6422902494331065\n",
      "Step: 735, Loss: 1.0923048257827759, Accuracy: 0.6425498188405797\n",
      "Step: 736, Loss: 1.1582223176956177, Accuracy: 0.6425825418362732\n",
      "Step: 737, Loss: 1.3199599981307983, Accuracy: 0.6425022583559169\n",
      "Step: 738, Loss: 1.2138381004333496, Accuracy: 0.6425349571493009\n",
      "Step: 739, Loss: 1.1451700925827026, Accuracy: 0.6426801801801801\n",
      "Step: 740, Loss: 1.0619536638259888, Accuracy: 0.6429374718848403\n",
      "Step: 741, Loss: 1.5031094551086426, Accuracy: 0.6426325247079964\n",
      "Step: 742, Loss: 1.2008845806121826, Accuracy: 0.642664872139973\n",
      "Step: 743, Loss: 1.25202476978302, Accuracy: 0.6426971326164874\n",
      "Step: 744, Loss: 1.42777681350708, Accuracy: 0.6423937360178971\n",
      "Step: 745, Loss: 1.1321934461593628, Accuracy: 0.6425379803395889\n",
      "Step: 746, Loss: 1.30418860912323, Accuracy: 0.642458723784025\n",
      "Step: 747, Loss: 1.3846489191055298, Accuracy: 0.6422682709447415\n",
      "Step: 748, Loss: 1.1510733366012573, Accuracy: 0.6424121050289274\n",
      "Step: 749, Loss: 1.6717724800109863, Accuracy: 0.6417777777777778\n",
      "Step: 750, Loss: 1.4267185926437378, Accuracy: 0.6415889924545051\n",
      "Step: 751, Loss: 1.5480270385742188, Accuracy: 0.6411790780141844\n",
      "Step: 752, Loss: 1.1884809732437134, Accuracy: 0.6413235945108455\n",
      "Step: 753, Loss: 1.2149337530136108, Accuracy: 0.6413572060123784\n",
      "Step: 754, Loss: 1.1592539548873901, Accuracy: 0.6415011037527594\n",
      "Step: 755, Loss: 1.2832876443862915, Accuracy: 0.6414241622574955\n",
      "Step: 756, Loss: 1.368944525718689, Accuracy: 0.6412373403786878\n",
      "Step: 757, Loss: 1.1696280241012573, Accuracy: 0.6413808267370272\n",
      "Step: 758, Loss: 1.3661469221115112, Accuracy: 0.6411945542380325\n",
      "Step: 759, Loss: 1.198747158050537, Accuracy: 0.6413377192982456\n",
      "Step: 760, Loss: 1.4375592470169067, Accuracy: 0.6410424879544458\n",
      "Step: 761, Loss: 1.2152198553085327, Accuracy: 0.6410761154855643\n",
      "Step: 762, Loss: 1.1953237056732178, Accuracy: 0.641218872870249\n",
      "Step: 763, Loss: 1.4616674184799194, Accuracy: 0.6409249563699826\n",
      "Step: 764, Loss: 1.0813947916030884, Accuracy: 0.6411764705882353\n",
      "Step: 765, Loss: 1.3073867559432983, Accuracy: 0.641100957354221\n",
      "Step: 766, Loss: 1.2476235628128052, Accuracy: 0.6411342894393742\n",
      "Step: 767, Loss: 1.194102168083191, Accuracy: 0.6412760416666666\n",
      "Step: 768, Loss: 1.2195194959640503, Accuracy: 0.641309059384482\n",
      "Step: 769, Loss: 1.531957745552063, Accuracy: 0.6409090909090909\n",
      "Step: 770, Loss: 1.107377529144287, Accuracy: 0.6410505836575876\n",
      "Step: 771, Loss: 1.2438379526138306, Accuracy: 0.6410837651122625\n",
      "Step: 772, Loss: 1.1563044786453247, Accuracy: 0.6413324708926261\n",
      "Step: 773, Loss: 1.1822510957717896, Accuracy: 0.641365202411714\n",
      "Step: 774, Loss: 1.2494159936904907, Accuracy: 0.6412903225806451\n",
      "Step: 775, Loss: 1.203210473060608, Accuracy: 0.6413230240549829\n",
      "Step: 776, Loss: 1.170841932296753, Accuracy: 0.6414628914628915\n",
      "Step: 777, Loss: 1.0195502042770386, Accuracy: 0.6417095115681234\n",
      "Step: 778, Loss: 1.1449357271194458, Accuracy: 0.6418485237483954\n",
      "Step: 779, Loss: 1.081344723701477, Accuracy: 0.6420940170940171\n",
      "Step: 780, Loss: 1.075767159461975, Accuracy: 0.6423388817755015\n",
      "Step: 781, Loss: 1.2409486770629883, Accuracy: 0.6423699914748509\n",
      "Step: 782, Loss: 1.287845253944397, Accuracy: 0.6422945934440187\n",
      "Step: 783, Loss: 1.182754635810852, Accuracy: 0.6424319727891157\n",
      "Step: 784, Loss: 1.315595269203186, Accuracy: 0.6423566878980892\n",
      "Step: 785, Loss: 1.2372328042984009, Accuracy: 0.6423876166242578\n",
      "Step: 786, Loss: 1.2038196325302124, Accuracy: 0.6424184667513765\n",
      "Step: 787, Loss: 1.2194839715957642, Accuracy: 0.6424492385786802\n",
      "Step: 788, Loss: 1.171989917755127, Accuracy: 0.6425855513307985\n",
      "Step: 789, Loss: 1.1723442077636719, Accuracy: 0.6427215189873418\n",
      "Step: 790, Loss: 1.412147879600525, Accuracy: 0.6425410872313527\n",
      "Step: 791, Loss: 1.3789267539978027, Accuracy: 0.6423611111111112\n",
      "Step: 792, Loss: 1.2647665739059448, Accuracy: 0.6422866750735603\n",
      "Step: 793, Loss: 1.2793569564819336, Accuracy: 0.6423173803526449\n",
      "Step: 794, Loss: 1.3013179302215576, Accuracy: 0.6422431865828092\n",
      "Step: 795, Loss: 1.2195775508880615, Accuracy: 0.6422738693467337\n",
      "Step: 796, Loss: 1.2768306732177734, Accuracy: 0.6423044751150147\n",
      "Step: 797, Loss: 1.3271572589874268, Accuracy: 0.6422305764411027\n",
      "Step: 798, Loss: 1.3277535438537598, Accuracy: 0.6421568627450981\n",
      "Step: 799, Loss: 1.2531458139419556, Accuracy: 0.6421875\n",
      "Step: 800, Loss: 1.084943175315857, Accuracy: 0.6423220973782772\n",
      "Step: 801, Loss: 1.362866997718811, Accuracy: 0.64214463840399\n",
      "Step: 802, Loss: 1.0956660509109497, Accuracy: 0.6423827314238273\n",
      "Step: 803, Loss: 1.271921992301941, Accuracy: 0.6424129353233831\n",
      "Step: 804, Loss: 1.3290053606033325, Accuracy: 0.6423395445134575\n",
      "Step: 805, Loss: 1.1257983446121216, Accuracy: 0.6424731182795699\n",
      "Step: 806, Loss: 1.2917183637619019, Accuracy: 0.642399834779017\n",
      "Step: 807, Loss: 1.3550353050231934, Accuracy: 0.642223597359736\n",
      "Step: 808, Loss: 1.1674221754074097, Accuracy: 0.642356819118253\n",
      "Step: 809, Loss: 1.2439534664154053, Accuracy: 0.6423868312757202\n",
      "Step: 810, Loss: 1.4041074514389038, Accuracy: 0.6422112618166872\n",
      "Step: 811, Loss: 1.495123267173767, Accuracy: 0.6419334975369458\n",
      "Step: 812, Loss: 1.2507437467575073, Accuracy: 0.6419639196391964\n",
      "Step: 813, Loss: 1.2883145809173584, Accuracy: 0.6418918918918919\n",
      "Step: 814, Loss: 1.3687406778335571, Accuracy: 0.6417177914110429\n",
      "Step: 815, Loss: 1.3640800714492798, Accuracy: 0.6415441176470589\n",
      "Step: 816, Loss: 1.114797592163086, Accuracy: 0.6417788657690738\n",
      "Step: 817, Loss: 1.4115349054336548, Accuracy: 0.6415036674816625\n",
      "Step: 818, Loss: 1.3955411911010742, Accuracy: 0.6413308913308914\n",
      "Step: 819, Loss: 1.0291309356689453, Accuracy: 0.6416666666666667\n",
      "Step: 820, Loss: 1.1518352031707764, Accuracy: 0.6417986195696306\n",
      "Step: 821, Loss: 1.1750742197036743, Accuracy: 0.6419302514193025\n",
      "Step: 822, Loss: 1.3125003576278687, Accuracy: 0.6418590522478737\n",
      "Step: 823, Loss: 1.1674551963806152, Accuracy: 0.6419902912621359\n",
      "Step: 824, Loss: 1.3102866411209106, Accuracy: 0.6419191919191919\n",
      "Step: 825, Loss: 1.31484055519104, Accuracy: 0.6418482647296206\n",
      "Step: 826, Loss: 1.3576840162277222, Accuracy: 0.6416767432486901\n",
      "Step: 827, Loss: 1.4022527933120728, Accuracy: 0.6415056360708534\n",
      "Step: 828, Loss: 1.1951773166656494, Accuracy: 0.6415359871330921\n",
      "Step: 829, Loss: 1.26175057888031, Accuracy: 0.641566265060241\n",
      "Step: 830, Loss: 1.3029484748840332, Accuracy: 0.6414961893301243\n",
      "Step: 831, Loss: 1.3742061853408813, Accuracy: 0.6413261217948718\n",
      "Step: 832, Loss: 1.2048457860946655, Accuracy: 0.6413565426170468\n",
      "Step: 833, Loss: 1.2941155433654785, Accuracy: 0.6412869704236611\n",
      "Step: 834, Loss: 1.1685837507247925, Accuracy: 0.6414171656686627\n",
      "Step: 835, Loss: 1.015778660774231, Accuracy: 0.6417464114832536\n",
      "Step: 836, Loss: 1.534292221069336, Accuracy: 0.6413779370768619\n",
      "Step: 837, Loss: 1.077586054801941, Accuracy: 0.641607000795545\n",
      "Step: 838, Loss: 1.0552948713302612, Accuracy: 0.6418355184743743\n",
      "Step: 839, Loss: 1.4215272665023804, Accuracy: 0.6416666666666667\n",
      "Step: 840, Loss: 1.0876579284667969, Accuracy: 0.6418945699564012\n",
      "Step: 841, Loss: 1.1426893472671509, Accuracy: 0.6420229612034838\n",
      "Step: 842, Loss: 1.3869425058364868, Accuracy: 0.6418544879398972\n",
      "Step: 843, Loss: 1.0900390148162842, Accuracy: 0.6420813586097947\n",
      "Step: 844, Loss: 1.1580721139907837, Accuracy: 0.6422090729783038\n",
      "Step: 845, Loss: 1.2833397388458252, Accuracy: 0.6421394799054374\n",
      "Step: 846, Loss: 1.0077928304672241, Accuracy: 0.6424635970090515\n",
      "Step: 847, Loss: 1.2083613872528076, Accuracy: 0.6424921383647799\n",
      "Step: 848, Loss: 1.1389942169189453, Accuracy: 0.6426187671770711\n",
      "Step: 849, Loss: 1.3742421865463257, Accuracy: 0.6425490196078432\n",
      "Step: 850, Loss: 1.3696656227111816, Accuracy: 0.6423815119467293\n",
      "Step: 851, Loss: 1.4843817949295044, Accuracy: 0.6421165884194053\n",
      "Step: 852, Loss: 1.2420008182525635, Accuracy: 0.6421453692848769\n",
      "Step: 853, Loss: 1.2139939069747925, Accuracy: 0.6421740827478533\n",
      "Step: 854, Loss: 1.3972331285476685, Accuracy: 0.6420077972709551\n",
      "Step: 855, Loss: 1.1613346338272095, Accuracy: 0.6420366043613707\n",
      "Step: 856, Loss: 1.244362235069275, Accuracy: 0.6420653442240374\n",
      "Step: 857, Loss: 1.5338430404663086, Accuracy: 0.6417055167055167\n",
      "Step: 858, Loss: 1.2376705408096313, Accuracy: 0.6417345750873108\n",
      "Step: 859, Loss: 1.386889100074768, Accuracy: 0.6415697674418605\n",
      "Step: 860, Loss: 1.0938960313796997, Accuracy: 0.641792489353465\n",
      "Step: 861, Loss: 1.166579008102417, Accuracy: 0.6419180201082754\n",
      "Step: 862, Loss: 1.5276451110839844, Accuracy: 0.6415604480494399\n",
      "Step: 863, Loss: 1.1019903421401978, Accuracy: 0.6417824074074074\n",
      "Step: 864, Loss: 1.484178066253662, Accuracy: 0.6415221579961464\n",
      "Step: 865, Loss: 1.4589251279830933, Accuracy: 0.6412625096227867\n",
      "Step: 866, Loss: 1.3023618459701538, Accuracy: 0.64119569396386\n",
      "Step: 867, Loss: 1.4321216344833374, Accuracy: 0.6410330261136713\n",
      "Step: 868, Loss: 1.2141834497451782, Accuracy: 0.6411584196394323\n",
      "Step: 869, Loss: 1.0620381832122803, Accuracy: 0.6413793103448275\n",
      "Step: 870, Loss: 1.0939836502075195, Accuracy: 0.6415996938384998\n",
      "Step: 871, Loss: 1.3455041646957397, Accuracy: 0.641532874617737\n",
      "Step: 872, Loss: 1.235332727432251, Accuracy: 0.6415616647575411\n",
      "Step: 873, Loss: 1.2602267265319824, Accuracy: 0.6415903890160183\n",
      "Step: 874, Loss: 1.309411883354187, Accuracy: 0.6415238095238095\n",
      "Step: 875, Loss: 1.5527912378311157, Accuracy: 0.64117199391172\n",
      "Step: 876, Loss: 1.1915924549102783, Accuracy: 0.6412010642341315\n",
      "Step: 877, Loss: 1.0545111894607544, Accuracy: 0.6414198936977981\n",
      "Step: 878, Loss: 1.1589792966842651, Accuracy: 0.6415434205536594\n",
      "Step: 879, Loss: 1.2108978033065796, Accuracy: 0.6415719696969697\n",
      "Step: 880, Loss: 1.2398008108139038, Accuracy: 0.6416004540295119\n",
      "Step: 881, Loss: 1.0765458345413208, Accuracy: 0.6418178382464097\n",
      "Step: 882, Loss: 1.4042857885360718, Accuracy: 0.6416572291430729\n",
      "Step: 883, Loss: 1.231786847114563, Accuracy: 0.641685520361991\n",
      "Step: 884, Loss: 1.1596873998641968, Accuracy: 0.6418079096045197\n",
      "Step: 885, Loss: 1.450143814086914, Accuracy: 0.6415537998495109\n",
      "Step: 886, Loss: 1.2132495641708374, Accuracy: 0.6415821119879744\n",
      "Step: 887, Loss: 1.348749041557312, Accuracy: 0.6415165165165165\n",
      "Step: 888, Loss: 1.1615527868270874, Accuracy: 0.6415448068991376\n",
      "Step: 889, Loss: 1.6300382614135742, Accuracy: 0.6411048689138577\n",
      "Step: 890, Loss: 1.4538758993148804, Accuracy: 0.6408529741863075\n",
      "Step: 891, Loss: 1.5904512405395508, Accuracy: 0.640414798206278\n",
      "Step: 892, Loss: 1.5082769393920898, Accuracy: 0.6400709219858156\n",
      "Step: 893, Loss: 1.4211591482162476, Accuracy: 0.6399142431021626\n",
      "Step: 894, Loss: 1.2816224098205566, Accuracy: 0.6398510242085661\n",
      "Step: 895, Loss: 1.080114722251892, Accuracy: 0.6400669642857143\n",
      "Step: 896, Loss: 0.9869699478149414, Accuracy: 0.6403753251579338\n",
      "Step: 897, Loss: 1.24221670627594, Accuracy: 0.6404046028210839\n",
      "Step: 898, Loss: 1.397929072380066, Accuracy: 0.6402484241750093\n",
      "Step: 899, Loss: 1.2924031019210815, Accuracy: 0.6401851851851852\n",
      "Step: 900, Loss: 1.2678468227386475, Accuracy: 0.6402145763965964\n",
      "Step: 901, Loss: 1.2369146347045898, Accuracy: 0.6402439024390244\n",
      "Step: 902, Loss: 1.3096208572387695, Accuracy: 0.6401808785529716\n",
      "Step: 903, Loss: 1.0856112241744995, Accuracy: 0.6403945427728613\n",
      "Step: 904, Loss: 1.2913731336593628, Accuracy: 0.6403314917127072\n",
      "Step: 905, Loss: 1.2290228605270386, Accuracy: 0.6403605592347315\n",
      "Step: 906, Loss: 1.076536774635315, Accuracy: 0.6405733186328556\n",
      "Step: 907, Loss: 1.3018497228622437, Accuracy: 0.6405102790014684\n",
      "Step: 908, Loss: 1.1109884977340698, Accuracy: 0.6407224055738907\n",
      "Step: 909, Loss: 1.3575440645217896, Accuracy: 0.6405677655677655\n",
      "Step: 910, Loss: 1.3685237169265747, Accuracy: 0.6404134650567143\n",
      "Step: 911, Loss: 1.4152541160583496, Accuracy: 0.6402595029239766\n",
      "Step: 912, Loss: 1.261459231376648, Accuracy: 0.6402884264330048\n",
      "Step: 913, Loss: 1.2935816049575806, Accuracy: 0.6403172866520788\n",
      "Step: 914, Loss: 1.3225852251052856, Accuracy: 0.6401639344262295\n",
      "Step: 915, Loss: 1.1284083127975464, Accuracy: 0.6402838427947598\n",
      "Step: 916, Loss: 1.1518079042434692, Accuracy: 0.6404034896401308\n",
      "Step: 917, Loss: 1.2561570405960083, Accuracy: 0.6404320987654321\n",
      "Step: 918, Loss: 1.2745107412338257, Accuracy: 0.6403699673558215\n",
      "Step: 919, Loss: 1.2199712991714478, Accuracy: 0.6403985507246377\n",
      "Step: 920, Loss: 1.4737714529037476, Accuracy: 0.6401556279406442\n",
      "Step: 921, Loss: 1.3707294464111328, Accuracy: 0.640003615328995\n",
      "Step: 922, Loss: 1.275376319885254, Accuracy: 0.6399422174070062\n",
      "Step: 923, Loss: 1.2318249940872192, Accuracy: 0.63997113997114\n",
      "Step: 924, Loss: 1.627647876739502, Accuracy: 0.6395495495495496\n",
      "Step: 925, Loss: 1.2237701416015625, Accuracy: 0.6395788336933045\n",
      "Step: 926, Loss: 1.389128565788269, Accuracy: 0.6394282632146709\n",
      "Step: 927, Loss: 1.0694653987884521, Accuracy: 0.6396372126436781\n",
      "Step: 928, Loss: 1.071808934211731, Accuracy: 0.6398457122353786\n",
      "Step: 929, Loss: 1.0554124116897583, Accuracy: 0.6400537634408602\n",
      "Step: 930, Loss: 1.2264928817749023, Accuracy: 0.640171858216971\n",
      "Step: 931, Loss: 1.295444130897522, Accuracy: 0.6401108726752504\n",
      "Step: 932, Loss: 1.1143295764923096, Accuracy: 0.6403179707038228\n",
      "Step: 933, Loss: 1.1824290752410889, Accuracy: 0.640435403283369\n",
      "Step: 934, Loss: 1.3334922790527344, Accuracy: 0.6402852049910873\n",
      "Step: 935, Loss: 1.3715038299560547, Accuracy: 0.6401353276353277\n",
      "Step: 936, Loss: 1.2077012062072754, Accuracy: 0.6402525791533262\n",
      "Step: 937, Loss: 1.2820311784744263, Accuracy: 0.6401918976545842\n",
      "Step: 938, Loss: 1.2565641403198242, Accuracy: 0.6402200922967696\n",
      "Step: 939, Loss: 1.4591383934020996, Accuracy: 0.6399822695035461\n",
      "Step: 940, Loss: 1.3309879302978516, Accuracy: 0.6399220687212186\n",
      "Step: 941, Loss: 1.2949026823043823, Accuracy: 0.6398619957537155\n",
      "Step: 942, Loss: 1.166412353515625, Accuracy: 0.6399787910922587\n",
      "Step: 943, Loss: 1.2063630819320679, Accuracy: 0.6400070621468926\n",
      "Step: 944, Loss: 1.3475713729858398, Accuracy: 0.6398589065255732\n",
      "Step: 945, Loss: 1.4803560972213745, Accuracy: 0.6396229739252995\n",
      "Step: 946, Loss: 1.2582144737243652, Accuracy: 0.639563533966913\n",
      "Step: 947, Loss: 1.3040194511413574, Accuracy: 0.6395042194092827\n",
      "Step: 948, Loss: 1.269041657447815, Accuracy: 0.6394450298559887\n",
      "Step: 949, Loss: 1.4187631607055664, Accuracy: 0.639298245614035\n",
      "Step: 950, Loss: 1.1326152086257935, Accuracy: 0.6394146512443042\n",
      "Step: 951, Loss: 1.1131765842437744, Accuracy: 0.6395308123249299\n",
      "Step: 952, Loss: 1.2068623304367065, Accuracy: 0.6395592864637986\n",
      "Step: 953, Loss: 1.1781595945358276, Accuracy: 0.6396750524109015\n",
      "Step: 954, Loss: 1.4393423795700073, Accuracy: 0.6395287958115183\n",
      "Step: 955, Loss: 1.2131776809692383, Accuracy: 0.6395571827057183\n",
      "Step: 956, Loss: 1.2162386178970337, Accuracy: 0.6395855102751654\n",
      "Step: 957, Loss: 1.258877158164978, Accuracy: 0.6396137787056367\n",
      "Step: 958, Loss: 1.1151119470596313, Accuracy: 0.6398157803267293\n",
      "Step: 959, Loss: 1.2376439571380615, Accuracy: 0.6397569444444444\n",
      "Step: 960, Loss: 1.266928791999817, Accuracy: 0.6397849462365591\n",
      "Step: 961, Loss: 1.3227440118789673, Accuracy: 0.6396396396396397\n",
      "Step: 962, Loss: 1.1292498111724854, Accuracy: 0.639840775354794\n",
      "Step: 963, Loss: 1.3283323049545288, Accuracy: 0.6397821576763485\n",
      "Step: 964, Loss: 1.256869912147522, Accuracy: 0.6397236614853196\n",
      "Step: 965, Loss: 1.5323024988174438, Accuracy: 0.6394064872325742\n",
      "Step: 966, Loss: 1.5467373132705688, Accuracy: 0.6390899689762151\n",
      "Step: 967, Loss: 1.1165848970413208, Accuracy: 0.6392906336088154\n",
      "Step: 968, Loss: 1.2348624467849731, Accuracy: 0.6393188854489165\n",
      "Step: 969, Loss: 1.36835515499115, Accuracy: 0.6391752577319587\n",
      "Step: 970, Loss: 1.2461715936660767, Accuracy: 0.6392035702025404\n",
      "Step: 971, Loss: 1.055217981338501, Accuracy: 0.63940329218107\n",
      "Step: 972, Loss: 1.0409691333770752, Accuracy: 0.6396882494004796\n",
      "Step: 973, Loss: 1.2913204431533813, Accuracy: 0.6396303901437371\n",
      "Step: 974, Loss: 1.466870903968811, Accuracy: 0.6394017094017094\n",
      "Step: 975, Loss: 1.3386908769607544, Accuracy: 0.6392588797814208\n",
      "Step: 976, Loss: 1.381460189819336, Accuracy: 0.6391163425452064\n",
      "Step: 977, Loss: 1.2493258714675903, Accuracy: 0.6391445126107703\n",
      "Step: 978, Loss: 1.5087398290634155, Accuracy: 0.6389172625127681\n",
      "Step: 979, Loss: 1.2224422693252563, Accuracy: 0.6389455782312925\n",
      "Step: 980, Loss: 1.23209810256958, Accuracy: 0.6389738362215427\n",
      "Step: 981, Loss: 1.3481336832046509, Accuracy: 0.6388323150033944\n",
      "Step: 982, Loss: 1.2369537353515625, Accuracy: 0.6388606307222787\n",
      "Step: 983, Loss: 1.4010671377182007, Accuracy: 0.6387195121951219\n",
      "Step: 984, Loss: 1.0640841722488403, Accuracy: 0.638917089678511\n",
      "Step: 985, Loss: 1.311058759689331, Accuracy: 0.6388607167004733\n",
      "Step: 986, Loss: 1.1238387823104858, Accuracy: 0.6389733198243837\n",
      "Step: 987, Loss: 1.2205485105514526, Accuracy: 0.6390013495276653\n",
      "Step: 988, Loss: 1.1075286865234375, Accuracy: 0.6391978429389956\n",
      "Step: 989, Loss: 1.3294082880020142, Accuracy: 0.6391414141414141\n",
      "Step: 990, Loss: 1.2904454469680786, Accuracy: 0.6390850992263707\n",
      "Step: 991, Loss: 1.298537015914917, Accuracy: 0.6390288978494624\n",
      "Step: 992, Loss: 1.361362099647522, Accuracy: 0.6389728096676737\n",
      "Step: 993, Loss: 1.1698399782180786, Accuracy: 0.6390845070422535\n",
      "Step: 994, Loss: 1.23043692111969, Accuracy: 0.6391122278056951\n",
      "Step: 995, Loss: 1.1334894895553589, Accuracy: 0.6392235609103079\n",
      "Step: 996, Loss: 1.2443504333496094, Accuracy: 0.6392510865931127\n",
      "Step: 997, Loss: 1.2576395273208618, Accuracy: 0.6392785571142284\n",
      "Step: 998, Loss: 1.3451294898986816, Accuracy: 0.6392225558892226\n",
      "Step: 999, Loss: 1.2415659427642822, Accuracy: 0.63925\n",
      "Step: 1000, Loss: 1.0652681589126587, Accuracy: 0.6394438894438894\n",
      "Step: 1001, Loss: 1.2291454076766968, Accuracy: 0.6394710578842315\n",
      "Step: 1002, Loss: 1.5170202255249023, Accuracy: 0.6391658358258557\n",
      "Step: 1003, Loss: 1.1402175426483154, Accuracy: 0.6392762284196547\n",
      "Step: 1004, Loss: 1.3003910779953003, Accuracy: 0.6392205638474295\n",
      "Step: 1005, Loss: 0.9344653487205505, Accuracy: 0.6395791915175613\n",
      "Step: 1006, Loss: 1.3957160711288452, Accuracy: 0.6394405825885469\n",
      "Step: 1007, Loss: 1.347514033317566, Accuracy: 0.6393849206349206\n",
      "Step: 1008, Loss: 1.3190548419952393, Accuracy: 0.6393293690122234\n",
      "Step: 1009, Loss: 1.302472472190857, Accuracy: 0.6392739273927392\n",
      "Step: 1010, Loss: 1.1293389797210693, Accuracy: 0.6393834487306297\n",
      "Step: 1011, Loss: 1.227558970451355, Accuracy: 0.6394104084321476\n",
      "Step: 1012, Loss: 1.3632055521011353, Accuracy: 0.6392727871010201\n",
      "Step: 1013, Loss: 1.301379919052124, Accuracy: 0.6392176199868508\n",
      "Step: 1014, Loss: 1.2306309938430786, Accuracy: 0.6392446633825944\n",
      "Step: 1015, Loss: 1.0818578004837036, Accuracy: 0.6394356955380578\n",
      "Step: 1016, Loss: 1.2407954931259155, Accuracy: 0.6394624713208784\n",
      "Step: 1017, Loss: 1.4091933965682983, Accuracy: 0.6392436149312377\n",
      "Step: 1018, Loss: 1.3563261032104492, Accuracy: 0.6391069676153092\n",
      "Step: 1019, Loss: 1.2891823053359985, Accuracy: 0.6391339869281045\n",
      "Step: 1020, Loss: 1.4449089765548706, Accuracy: 0.6389977146588313\n",
      "Step: 1021, Loss: 1.176161289215088, Accuracy: 0.6391063274624919\n",
      "Step: 1022, Loss: 1.0988335609436035, Accuracy: 0.6392961876832844\n",
      "Step: 1023, Loss: 1.2379940748214722, Accuracy: 0.6393229166666666\n",
      "Step: 1024, Loss: 1.2627429962158203, Accuracy: 0.639349593495935\n",
      "Step: 1025, Loss: 1.0682731866836548, Accuracy: 0.639538661468486\n",
      "Step: 1026, Loss: 1.357061505317688, Accuracy: 0.6394027913015254\n",
      "Step: 1027, Loss: 1.5194463729858398, Accuracy: 0.6391050583657587\n",
      "Step: 1028, Loss: 1.209119439125061, Accuracy: 0.6392128279883382\n",
      "Step: 1029, Loss: 1.0850110054016113, Accuracy: 0.6394012944983819\n",
      "Step: 1030, Loss: 1.1771974563598633, Accuracy: 0.639508567733592\n",
      "Step: 1031, Loss: 1.251774549484253, Accuracy: 0.6395348837209303\n",
      "Step: 1032, Loss: 1.2346932888031006, Accuracy: 0.6395611487576638\n",
      "Step: 1033, Loss: 1.2998491525650024, Accuracy: 0.6395067698259188\n",
      "Step: 1034, Loss: 1.3591762781143188, Accuracy: 0.6393719806763285\n",
      "Step: 1035, Loss: 1.3789142370224, Accuracy: 0.6392374517374517\n",
      "Step: 1036, Loss: 1.1821211576461792, Accuracy: 0.639344262295082\n",
      "Step: 1037, Loss: 1.3821930885314941, Accuracy: 0.6392100192678227\n",
      "Step: 1038, Loss: 1.344510555267334, Accuracy: 0.6390760346487007\n",
      "Step: 1039, Loss: 1.5853776931762695, Accuracy: 0.6387019230769231\n",
      "Step: 1040, Loss: 1.0188376903533936, Accuracy: 0.6389689401216778\n",
      "Step: 1041, Loss: 1.5327056646347046, Accuracy: 0.6386756238003839\n",
      "Step: 1042, Loss: 1.1437089443206787, Accuracy: 0.6387823585810163\n",
      "Step: 1043, Loss: 1.0001531839370728, Accuracy: 0.6390485312899106\n",
      "Step: 1044, Loss: 1.2219680547714233, Accuracy: 0.6390749601275917\n",
      "Step: 1045, Loss: 1.4492660760879517, Accuracy: 0.6388623326959847\n",
      "Step: 1046, Loss: 1.2507966756820679, Accuracy: 0.6388888888888888\n",
      "Step: 1047, Loss: 1.4118351936340332, Accuracy: 0.638676844783715\n",
      "Step: 1048, Loss: 1.2964621782302856, Accuracy: 0.6386240864315221\n",
      "Step: 1049, Loss: 1.1343895196914673, Accuracy: 0.6387301587301587\n",
      "Step: 1050, Loss: 1.3198343515396118, Accuracy: 0.6386774500475737\n",
      "Step: 1051, Loss: 1.2011762857437134, Accuracy: 0.6387040557667935\n",
      "Step: 1052, Loss: 1.2132742404937744, Accuracy: 0.6387306109528331\n",
      "Step: 1053, Loss: 1.3005545139312744, Accuracy: 0.6386780518659076\n",
      "Step: 1054, Loss: 1.4826353788375854, Accuracy: 0.6384676145339653\n",
      "Step: 1055, Loss: 1.3315922021865845, Accuracy: 0.63833648989899\n",
      "Step: 1056, Loss: 1.3124656677246094, Accuracy: 0.6382844528539893\n",
      "Step: 1057, Loss: 1.192151427268982, Accuracy: 0.6383900441083806\n",
      "Step: 1058, Loss: 1.3990143537521362, Accuracy: 0.638259364180044\n",
      "Step: 1059, Loss: 1.3326016664505005, Accuracy: 0.6382075471698113\n",
      "Step: 1060, Loss: 1.1295210123062134, Accuracy: 0.6383129123468426\n",
      "Step: 1061, Loss: 1.2784889936447144, Accuracy: 0.6382611424984307\n",
      "Step: 1062, Loss: 1.227690577507019, Accuracy: 0.6382878645343368\n",
      "Step: 1063, Loss: 1.2747591733932495, Accuracy: 0.6383145363408521\n",
      "Step: 1064, Loss: 1.158932089805603, Accuracy: 0.6384194053208138\n",
      "Step: 1065, Loss: 1.145682454109192, Accuracy: 0.6384459036898061\n",
      "Step: 1066, Loss: 1.1317843198776245, Accuracy: 0.6387066541705717\n",
      "Step: 1067, Loss: 1.6263141632080078, Accuracy: 0.6383426966292135\n",
      "Step: 1068, Loss: 1.3360704183578491, Accuracy: 0.6382912379170564\n",
      "Step: 1069, Loss: 1.237860083580017, Accuracy: 0.6383177570093458\n",
      "Step: 1070, Loss: 1.1655086278915405, Accuracy: 0.638422035480859\n",
      "Step: 1071, Loss: 1.2769907712936401, Accuracy: 0.6383706467661692\n",
      "Step: 1072, Loss: 1.2495847940444946, Accuracy: 0.6383970177073626\n",
      "Step: 1073, Loss: 1.32414972782135, Accuracy: 0.6383457479826194\n",
      "Step: 1074, Loss: 1.3839277029037476, Accuracy: 0.6382170542635659\n",
      "Step: 1075, Loss: 1.3709993362426758, Accuracy: 0.6380885997521685\n",
      "Step: 1076, Loss: 1.3103188276290894, Accuracy: 0.6380377592076757\n",
      "Step: 1077, Loss: 1.449779987335205, Accuracy: 0.6379097093382807\n",
      "Step: 1078, Loss: 1.2758140563964844, Accuracy: 0.6378591288229842\n",
      "Step: 1079, Loss: 1.0475571155548096, Accuracy: 0.6380401234567902\n",
      "Step: 1080, Loss: 1.5686951875686646, Accuracy: 0.6377582485353068\n",
      "Step: 1081, Loss: 1.2988555431365967, Accuracy: 0.6377079482439926\n",
      "Step: 1082, Loss: 1.0376664400100708, Accuracy: 0.6379655278547245\n",
      "Step: 1083, Loss: 1.3799439668655396, Accuracy: 0.6378382533825339\n",
      "Step: 1084, Loss: 1.334593415260315, Accuracy: 0.6377880184331797\n",
      "Step: 1085, Loss: 1.155633568763733, Accuracy: 0.6378913443830571\n",
      "Step: 1086, Loss: 1.1855171918869019, Accuracy: 0.6379944802207912\n",
      "Step: 1087, Loss: 1.3371071815490723, Accuracy: 0.6379442401960784\n",
      "Step: 1088, Loss: 1.3579168319702148, Accuracy: 0.6378175696357514\n",
      "Step: 1089, Loss: 1.1980270147323608, Accuracy: 0.6378440366972477\n",
      "Step: 1090, Loss: 1.148723840713501, Accuracy: 0.6379468377635197\n",
      "Step: 1091, Loss: 1.4507107734680176, Accuracy: 0.6377442002442002\n",
      "Step: 1092, Loss: 1.2660799026489258, Accuracy: 0.6376944190301921\n",
      "Step: 1093, Loss: 1.50045645236969, Accuracy: 0.6374162096282754\n",
      "Step: 1094, Loss: 1.300911784172058, Accuracy: 0.6373668188736682\n",
      "Step: 1095, Loss: 1.124050259590149, Accuracy: 0.6375456204379562\n",
      "Step: 1096, Loss: 1.2584413290023804, Accuracy: 0.6374962017623823\n",
      "Step: 1097, Loss: 1.352110505104065, Accuracy: 0.637370977534912\n",
      "Step: 1098, Loss: 1.3842164278030396, Accuracy: 0.6372459811950257\n",
      "Step: 1099, Loss: 1.3337551355361938, Accuracy: 0.6371969696969697\n",
      "Step: 1100, Loss: 1.2017064094543457, Accuracy: 0.637223735997578\n",
      "Step: 1101, Loss: 1.3389405012130737, Accuracy: 0.6370992135511192\n",
      "Step: 1102, Loss: 1.281018853187561, Accuracy: 0.6370504684194621\n",
      "Step: 1103, Loss: 1.200058102607727, Accuracy: 0.6371527777777778\n",
      "Step: 1104, Loss: 1.1747419834136963, Accuracy: 0.6372549019607843\n",
      "Step: 1105, Loss: 1.1737743616104126, Accuracy: 0.6373568414707655\n",
      "Step: 1106, Loss: 1.3635751008987427, Accuracy: 0.6373080397470642\n",
      "Step: 1107, Loss: 1.176076889038086, Accuracy: 0.6374849578820698\n",
      "Step: 1108, Loss: 1.1808160543441772, Accuracy: 0.6375112714156899\n",
      "Step: 1109, Loss: 1.1520695686340332, Accuracy: 0.6376126126126126\n",
      "Step: 1110, Loss: 1.3283122777938843, Accuracy: 0.6375637563756376\n",
      "Step: 1111, Loss: 1.166876196861267, Accuracy: 0.637589928057554\n",
      "Step: 1112, Loss: 1.2435216903686523, Accuracy: 0.6376160527103923\n",
      "Step: 1113, Loss: 1.283409595489502, Accuracy: 0.6375673249551167\n",
      "Step: 1114, Loss: 1.3706804513931274, Accuracy: 0.6374439461883408\n",
      "Step: 1115, Loss: 1.1526429653167725, Accuracy: 0.6375448028673835\n",
      "Step: 1116, Loss: 1.406961441040039, Accuracy: 0.6374216651745748\n",
      "Step: 1117, Loss: 1.3180814981460571, Accuracy: 0.6373732856290996\n",
      "Step: 1118, Loss: 1.3773173093795776, Accuracy: 0.6372505212987787\n",
      "Step: 1119, Loss: 1.0904408693313599, Accuracy: 0.6374255952380953\n",
      "Step: 1120, Loss: 1.3967496156692505, Accuracy: 0.6373030032708891\n",
      "Step: 1121, Loss: 1.1430609226226807, Accuracy: 0.6374034462269756\n",
      "Step: 1122, Loss: 1.1857681274414062, Accuracy: 0.6375037102997922\n",
      "Step: 1123, Loss: 1.157485008239746, Accuracy: 0.6376779359430605\n",
      "Step: 1124, Loss: 1.6414450407028198, Accuracy: 0.6373333333333333\n",
      "Step: 1125, Loss: 1.5882712602615356, Accuracy: 0.6370633510953226\n",
      "Step: 1126, Loss: 1.3180642127990723, Accuracy: 0.6370156758355516\n",
      "Step: 1127, Loss: 1.157928228378296, Accuracy: 0.6371158392434988\n",
      "Step: 1128, Loss: 1.242738127708435, Accuracy: 0.6371420135813404\n",
      "Step: 1129, Loss: 1.347368597984314, Accuracy: 0.637094395280236\n",
      "Step: 1130, Loss: 1.1306780576705933, Accuracy: 0.63719422340112\n",
      "Step: 1131, Loss: 0.9774791598320007, Accuracy: 0.6374411071849234\n",
      "Step: 1132, Loss: 1.0001559257507324, Accuracy: 0.6376875551632833\n",
      "Step: 1133, Loss: 1.1904350519180298, Accuracy: 0.6377131099353321\n",
      "Step: 1134, Loss: 1.2189732789993286, Accuracy: 0.6377386196769457\n",
      "Step: 1135, Loss: 1.4608054161071777, Accuracy: 0.6376173708920188\n",
      "Step: 1136, Loss: 1.2804816961288452, Accuracy: 0.6375696276751686\n",
      "Step: 1137, Loss: 1.1365827322006226, Accuracy: 0.637668424135911\n",
      "Step: 1138, Loss: 1.3928570747375488, Accuracy: 0.6375475563359673\n",
      "Step: 1139, Loss: 1.242621898651123, Accuracy: 0.6375730994152047\n",
      "Step: 1140, Loss: 1.1391476392745972, Accuracy: 0.637744668419515\n",
      "Step: 1141, Loss: 1.5204867124557495, Accuracy: 0.637478108581436\n",
      "Step: 1142, Loss: 1.065901279449463, Accuracy: 0.6376494604841062\n",
      "Step: 1143, Loss: 1.4458069801330566, Accuracy: 0.6374562937062938\n",
      "Step: 1144, Loss: 1.486764907836914, Accuracy: 0.6371906841339156\n",
      "Step: 1145, Loss: 1.1547417640686035, Accuracy: 0.6372891215823153\n",
      "Step: 1146, Loss: 1.2250375747680664, Accuracy: 0.6373147340889277\n",
      "Step: 1147, Loss: 1.0902537107467651, Accuracy: 0.6374854819976771\n",
      "Step: 1148, Loss: 1.294199824333191, Accuracy: 0.6374383521903104\n",
      "Step: 1149, Loss: 1.2914811372756958, Accuracy: 0.6373913043478261\n",
      "Step: 1150, Loss: 1.4337822198867798, Accuracy: 0.6371995366348103\n",
      "Step: 1151, Loss: 1.04912531375885, Accuracy: 0.6373697916666666\n",
      "Step: 1152, Loss: 1.3900622129440308, Accuracy: 0.6372506504770165\n",
      "Step: 1153, Loss: 1.0654672384262085, Accuracy: 0.637420566146736\n",
      "Step: 1154, Loss: 1.2331243753433228, Accuracy: 0.6373737373737374\n",
      "Step: 1155, Loss: 1.1684528589248657, Accuracy: 0.6374711649365629\n",
      "Step: 1156, Loss: 1.3462973833084106, Accuracy: 0.6373523480265053\n",
      "Step: 1157, Loss: 1.3345321416854858, Accuracy: 0.6372337363270005\n",
      "Step: 1158, Loss: 1.0304116010665894, Accuracy: 0.6374748346275525\n",
      "Step: 1159, Loss: 1.271929383277893, Accuracy: 0.6374281609195402\n",
      "Step: 1160, Loss: 1.3099299669265747, Accuracy: 0.6373097904105656\n",
      "Step: 1161, Loss: 1.141400933265686, Accuracy: 0.6374784853700516\n",
      "Step: 1162, Loss: 1.38766610622406, Accuracy: 0.6373602751504729\n",
      "Step: 1163, Loss: 1.0844014883041382, Accuracy: 0.6375286368843069\n",
      "Step: 1164, Loss: 1.455039143562317, Accuracy: 0.6373390557939914\n",
      "Step: 1165, Loss: 1.1874744892120361, Accuracy: 0.6374356775300172\n",
      "Step: 1166, Loss: 1.1031229496002197, Accuracy: 0.6376035418451871\n",
      "Step: 1167, Loss: 1.133101224899292, Accuracy: 0.6376997716894978\n",
      "Step: 1168, Loss: 1.2838165760040283, Accuracy: 0.6376532648987738\n",
      "Step: 1169, Loss: 1.237663745880127, Accuracy: 0.6376780626780627\n",
      "Step: 1170, Loss: 1.3546329736709595, Accuracy: 0.6375604896100199\n",
      "Step: 1171, Loss: 1.2581804990768433, Accuracy: 0.6375853242320819\n",
      "Step: 1172, Loss: 1.2903053760528564, Accuracy: 0.6376101165103722\n",
      "Step: 1173, Loss: 1.2776702642440796, Accuracy: 0.6375638841567292\n",
      "Step: 1174, Loss: 1.3935375213623047, Accuracy: 0.6374468085106383\n",
      "Step: 1175, Loss: 1.249038577079773, Accuracy: 0.6374007936507936\n",
      "Step: 1176, Loss: 1.4500064849853516, Accuracy: 0.637213254035684\n",
      "Step: 1177, Loss: 1.2203582525253296, Accuracy: 0.6372382569326542\n",
      "Step: 1178, Loss: 1.5559676885604858, Accuracy: 0.636980491942324\n",
      "Step: 1179, Loss: 1.1751426458358765, Accuracy: 0.6370762711864407\n",
      "Step: 1180, Loss: 1.0666968822479248, Accuracy: 0.6373130115721141\n",
      "Step: 1181, Loss: 1.138671636581421, Accuracy: 0.6374083474337281\n",
      "Step: 1182, Loss: 1.3271814584732056, Accuracy: 0.6373626373626373\n",
      "Step: 1183, Loss: 1.257189393043518, Accuracy: 0.6373170045045045\n",
      "Step: 1184, Loss: 1.1849148273468018, Accuracy: 0.6374120956399437\n",
      "Step: 1185, Loss: 1.2171112298965454, Accuracy: 0.6375070264193367\n",
      "Step: 1186, Loss: 1.2996898889541626, Accuracy: 0.6374613872507723\n",
      "Step: 1187, Loss: 1.3120567798614502, Accuracy: 0.6374158249158249\n",
      "Step: 1188, Loss: 1.096852421760559, Accuracy: 0.6375805999439305\n",
      "Step: 1189, Loss: 1.2150367498397827, Accuracy: 0.6376750700280112\n",
      "Step: 1190, Loss: 1.3271727561950684, Accuracy: 0.6376294430450602\n",
      "Step: 1191, Loss: 1.124076008796692, Accuracy: 0.6377237136465325\n",
      "Step: 1192, Loss: 1.3902472257614136, Accuracy: 0.6376082704666108\n",
      "Step: 1193, Loss: 1.3325990438461304, Accuracy: 0.6375628140703518\n",
      "Step: 1194, Loss: 1.0586453676223755, Accuracy: 0.6377266387726639\n",
      "Step: 1195, Loss: 1.2693943977355957, Accuracy: 0.6377508361204013\n",
      "Step: 1196, Loss: 1.2103351354599, Accuracy: 0.6377749930381509\n",
      "Step: 1197, Loss: 1.1743468046188354, Accuracy: 0.6378686700055648\n",
      "Step: 1198, Loss: 1.3706101179122925, Accuracy: 0.6378231859883235\n",
      "Step: 1199, Loss: 1.2066079378128052, Accuracy: 0.6378472222222222\n",
      "Step: 1200, Loss: 1.0630518198013306, Accuracy: 0.6380099916736053\n",
      "Step: 1201, Loss: 1.2269665002822876, Accuracy: 0.6380338325013866\n",
      "Step: 1202, Loss: 1.3910061120986938, Accuracy: 0.6379190911609864\n",
      "Step: 1203, Loss: 1.4658349752426147, Accuracy: 0.637735326688815\n",
      "Step: 1204, Loss: 1.0115166902542114, Accuracy: 0.6379668049792531\n",
      "Step: 1205, Loss: 1.2793320417404175, Accuracy: 0.6379906025428413\n",
      "Step: 1206, Loss: 1.222959041595459, Accuracy: 0.638014360673847\n",
      "Step: 1207, Loss: 1.2261754274368286, Accuracy: 0.6380380794701986\n",
      "Step: 1208, Loss: 1.3990947008132935, Accuracy: 0.6379239040529363\n",
      "Step: 1209, Loss: 1.4305987358093262, Accuracy: 0.6377410468319559\n",
      "Step: 1210, Loss: 1.2555553913116455, Accuracy: 0.6377649325626205\n",
      "Step: 1211, Loss: 1.3592356443405151, Accuracy: 0.6376512651265126\n",
      "Step: 1212, Loss: 1.0075410604476929, Accuracy: 0.637881286067601\n",
      "Step: 1213, Loss: 1.3778706789016724, Accuracy: 0.6377677100494233\n",
      "Step: 1214, Loss: 1.3195947408676147, Accuracy: 0.6377229080932785\n",
      "Step: 1215, Loss: 1.355125904083252, Accuracy: 0.637609649122807\n",
      "Step: 1216, Loss: 1.423673152923584, Accuracy: 0.6374281018898932\n",
      "Step: 1217, Loss: 1.198167324066162, Accuracy: 0.6374521072796935\n",
      "Step: 1218, Loss: 1.3028727769851685, Accuracy: 0.6374077112387203\n",
      "Step: 1219, Loss: 1.480372428894043, Accuracy: 0.6372267759562842\n",
      "Step: 1220, Loss: 1.4472025632858276, Accuracy: 0.637046137046137\n",
      "Step: 1221, Loss: 1.2181676626205444, Accuracy: 0.6370703764320785\n",
      "Step: 1222, Loss: 1.329622745513916, Accuracy: 0.63702643772145\n",
      "Step: 1223, Loss: 0.9912274479866028, Accuracy: 0.6372549019607843\n",
      "Step: 1224, Loss: 1.2634609937667847, Accuracy: 0.6372108843537415\n",
      "Step: 1225, Loss: 1.2992366552352905, Accuracy: 0.6371669385535618\n",
      "Step: 1226, Loss: 1.2153723239898682, Accuracy: 0.637190980711763\n",
      "Step: 1227, Loss: 1.2356442213058472, Accuracy: 0.637214983713355\n",
      "Step: 1228, Loss: 1.2615801095962524, Accuracy: 0.6371711418497423\n",
      "Step: 1229, Loss: 1.3092515468597412, Accuracy: 0.6371273712737128\n",
      "Step: 1230, Loss: 1.3485112190246582, Accuracy: 0.6370159761711346\n",
      "Step: 1231, Loss: 1.4420019388198853, Accuracy: 0.6369047619047619\n",
      "Step: 1232, Loss: 1.3041173219680786, Accuracy: 0.6368613138686131\n",
      "Step: 1233, Loss: 1.3820949792861938, Accuracy: 0.6367504051863857\n",
      "Step: 1234, Loss: 1.1289016008377075, Accuracy: 0.6368421052631579\n",
      "Step: 1235, Loss: 1.095236897468567, Accuracy: 0.6370010787486515\n",
      "Step: 1236, Loss: 1.2386635541915894, Accuracy: 0.6370250606305577\n",
      "Step: 1237, Loss: 1.3345595598220825, Accuracy: 0.6369816908992999\n",
      "Step: 1238, Loss: 1.2096039056777954, Accuracy: 0.6370056497175142\n",
      "Step: 1239, Loss: 1.196510910987854, Accuracy: 0.6370967741935484\n",
      "Step: 1240, Loss: 1.1563869714736938, Accuracy: 0.637187751813054\n",
      "Step: 1241, Loss: 1.3975547552108765, Accuracy: 0.6370772946859904\n",
      "Step: 1242, Loss: 1.4207383394241333, Accuracy: 0.6369670152855994\n",
      "Step: 1243, Loss: 1.3201099634170532, Accuracy: 0.6369239013933548\n",
      "Step: 1244, Loss: 1.2293180227279663, Accuracy: 0.6369477911646586\n",
      "Step: 1245, Loss: 1.4848636388778687, Accuracy: 0.6367710005350454\n",
      "Step: 1246, Loss: 1.244076132774353, Accuracy: 0.6367949746057204\n",
      "Step: 1247, Loss: 1.1351419687271118, Accuracy: 0.6368856837606838\n",
      "Step: 1248, Loss: 0.9938035011291504, Accuracy: 0.6371096877502002\n",
      "Step: 1249, Loss: 1.263872504234314, Accuracy: 0.6371333333333333\n",
      "Step: 1250, Loss: 1.2823072671890259, Accuracy: 0.6370903277378097\n",
      "Step: 1251, Loss: 1.146411418914795, Accuracy: 0.6371805111821086\n",
      "Step: 1252, Loss: 1.4921118021011353, Accuracy: 0.6370045224793828\n",
      "Step: 1253, Loss: 1.499759554862976, Accuracy: 0.636762360446571\n",
      "Step: 1254, Loss: 1.5282487869262695, Accuracy: 0.6365205843293492\n",
      "Step: 1255, Loss: 1.2008072137832642, Accuracy: 0.6365445859872612\n",
      "Step: 1256, Loss: 1.2011815309524536, Accuracy: 0.6366348448687351\n",
      "Step: 1257, Loss: 1.2447127103805542, Accuracy: 0.6366587175410705\n",
      "Step: 1258, Loss: 1.0666242837905884, Accuracy: 0.6368149324861001\n",
      "Step: 1259, Loss: 1.4666976928710938, Accuracy: 0.6365740740740741\n",
      "Step: 1260, Loss: 1.4139219522476196, Accuracy: 0.6364657679090668\n",
      "Step: 1261, Loss: 1.2718721628189087, Accuracy: 0.6364236661384046\n",
      "Step: 1262, Loss: 1.3140395879745483, Accuracy: 0.636381631037213\n",
      "Step: 1263, Loss: 1.05856454372406, Accuracy: 0.6365374472573839\n",
      "Step: 1264, Loss: 1.1017390489578247, Accuracy: 0.6366271409749671\n",
      "Step: 1265, Loss: 1.0609725713729858, Accuracy: 0.6367825171142707\n",
      "Step: 1266, Loss: 1.2682920694351196, Accuracy: 0.6368061036569324\n",
      "Step: 1267, Loss: 1.3861913681030273, Accuracy: 0.6366982124079916\n",
      "Step: 1268, Loss: 1.2966116666793823, Accuracy: 0.6366561597058051\n",
      "Step: 1269, Loss: 1.2190994024276733, Accuracy: 0.6366797900262468\n",
      "Step: 1270, Loss: 1.240069031715393, Accuracy: 0.6366378179910831\n",
      "Step: 1271, Loss: 1.2942171096801758, Accuracy: 0.6365959119496856\n",
      "Step: 1272, Loss: 1.1835170984268188, Accuracy: 0.6366195339094004\n",
      "Step: 1273, Loss: 1.2568597793579102, Accuracy: 0.6366431187859759\n",
      "Step: 1274, Loss: 1.0428273677825928, Accuracy: 0.636797385620915\n",
      "Step: 1275, Loss: 1.2102500200271606, Accuracy: 0.6368207941483803\n",
      "Step: 1276, Loss: 1.1302345991134644, Accuracy: 0.6369746802401461\n",
      "Step: 1277, Loss: 1.0472923517227173, Accuracy: 0.6371935315597287\n",
      "Step: 1278, Loss: 1.242077112197876, Accuracy: 0.63721657544957\n",
      "Step: 1279, Loss: 1.2057173252105713, Accuracy: 0.6372395833333333\n",
      "Step: 1280, Loss: 1.0840753316879272, Accuracy: 0.637392661982826\n",
      "Step: 1281, Loss: 1.1725568771362305, Accuracy: 0.6374804992199687\n",
      "Step: 1282, Loss: 1.2459796667099, Accuracy: 0.6375032475967783\n",
      "Step: 1283, Loss: 1.4579845666885376, Accuracy: 0.637331256490135\n",
      "Step: 1284, Loss: 1.2273986339569092, Accuracy: 0.6373540856031128\n",
      "Step: 1285, Loss: 1.190802812576294, Accuracy: 0.6374416796267496\n",
      "Step: 1286, Loss: 1.232870101928711, Accuracy: 0.6374643874643875\n",
      "Step: 1287, Loss: 1.1939237117767334, Accuracy: 0.6375517598343685\n",
      "Step: 1288, Loss: 1.2836788892745972, Accuracy: 0.6375096974398758\n",
      "Step: 1289, Loss: 1.3659448623657227, Accuracy: 0.6374677002583979\n",
      "Step: 1290, Loss: 1.208598256111145, Accuracy: 0.6374903175832688\n",
      "Step: 1291, Loss: 1.3719372749328613, Accuracy: 0.6373839009287926\n",
      "Step: 1292, Loss: 1.256080150604248, Accuracy: 0.6374065480794019\n",
      "Step: 1293, Loss: 1.3142008781433105, Accuracy: 0.6373647604327666\n",
      "Step: 1294, Loss: 1.3141803741455078, Accuracy: 0.6373230373230373\n",
      "Step: 1295, Loss: 1.0951378345489502, Accuracy: 0.637474279835391\n",
      "Step: 1296, Loss: 1.5032726526260376, Accuracy: 0.6372397841171935\n",
      "Step: 1297, Loss: 1.1356141567230225, Accuracy: 0.637326656394453\n",
      "Step: 1298, Loss: 1.0066505670547485, Accuracy: 0.6375416987426226\n",
      "Step: 1299, Loss: 1.2804889678955078, Accuracy: 0.6375\n",
      "Step: 1300, Loss: 1.2130632400512695, Accuracy: 0.6375224186523187\n",
      "Step: 1301, Loss: 1.225882887840271, Accuracy: 0.6375448028673835\n",
      "Step: 1302, Loss: 1.0939698219299316, Accuracy: 0.6376950626758762\n",
      "Step: 1303, Loss: 1.4652223587036133, Accuracy: 0.6375255623721882\n",
      "Step: 1304, Loss: 1.2920451164245605, Accuracy: 0.6374840357598979\n",
      "Step: 1305, Loss: 1.0589079856872559, Accuracy: 0.6376339969372129\n",
      "Step: 1306, Loss: 1.3723264932632446, Accuracy: 0.6375286916602907\n",
      "Step: 1307, Loss: 1.2375112771987915, Accuracy: 0.6375509683995922\n",
      "Step: 1308, Loss: 1.1693657636642456, Accuracy: 0.6376368729309906\n",
      "Step: 1309, Loss: 1.1123957633972168, Accuracy: 0.6377862595419848\n",
      "Step: 1310, Loss: 1.1884828805923462, Accuracy: 0.6378082888380371\n",
      "Step: 1311, Loss: 1.2960842847824097, Accuracy: 0.637766768292683\n",
      "Step: 1312, Loss: 1.2551137208938599, Accuracy: 0.6377887788778878\n",
      "Step: 1313, Loss: 1.192622184753418, Accuracy: 0.6378741755454084\n",
      "Step: 1314, Loss: 1.1595799922943115, Accuracy: 0.6379594423320659\n",
      "Step: 1315, Loss: 1.1842223405838013, Accuracy: 0.6379812563323202\n",
      "Step: 1316, Loss: 1.033210039138794, Accuracy: 0.6381928625664389\n",
      "Step: 1317, Loss: 1.2433370351791382, Accuracy: 0.6382144663631766\n",
      "Step: 1318, Loss: 1.22459077835083, Accuracy: 0.6382360374020722\n",
      "Step: 1319, Loss: 1.4202446937561035, Accuracy: 0.6380681818181818\n",
      "Step: 1320, Loss: 1.2537857294082642, Accuracy: 0.6380898309361595\n",
      "Step: 1321, Loss: 1.2496311664581299, Accuracy: 0.6381114473020676\n",
      "Step: 1322, Loss: 1.3803095817565918, Accuracy: 0.6380070546737213\n",
      "Step: 1323, Loss: 1.3166512250900269, Accuracy: 0.6379657603222558\n",
      "Step: 1324, Loss: 1.1729305982589722, Accuracy: 0.6379874213836478\n",
      "Step: 1325, Loss: 1.1120117902755737, Accuracy: 0.6380718954248366\n",
      "Step: 1326, Loss: 1.3759875297546387, Accuracy: 0.6379678472745541\n",
      "Step: 1327, Loss: 1.2174999713897705, Accuracy: 0.6379894578313253\n",
      "Step: 1328, Loss: 1.2613807916641235, Accuracy: 0.6380737396538751\n",
      "Step: 1329, Loss: 1.0786863565444946, Accuracy: 0.6382832080200501\n",
      "Step: 1330, Loss: 1.148236632347107, Accuracy: 0.6383671424993739\n",
      "Step: 1331, Loss: 1.2854219675064087, Accuracy: 0.6383258258258259\n",
      "Step: 1332, Loss: 1.0235109329223633, Accuracy: 0.6385346336584146\n",
      "Step: 1333, Loss: 1.232006311416626, Accuracy: 0.6385557221389305\n",
      "Step: 1334, Loss: 1.2316876649856567, Accuracy: 0.6385767790262172\n",
      "Step: 1335, Loss: 1.074802041053772, Accuracy: 0.6387225548902196\n",
      "Step: 1336, Loss: 1.2219594717025757, Accuracy: 0.6387434554973822\n",
      "Step: 1337, Loss: 1.1538645029067993, Accuracy: 0.6388266068759342\n",
      "Step: 1338, Loss: 1.1998659372329712, Accuracy: 0.6388473985561364\n",
      "Step: 1339, Loss: 1.336196780204773, Accuracy: 0.6388059701492538\n",
      "Step: 1340, Loss: 1.3680280447006226, Accuracy: 0.6387024608501118\n",
      "Step: 1341, Loss: 1.27994966506958, Accuracy: 0.6386612021857924\n",
      "Step: 1342, Loss: 1.3126277923583984, Accuracy: 0.638620004964011\n",
      "Step: 1343, Loss: 1.3613392114639282, Accuracy: 0.6385168650793651\n",
      "Step: 1344, Loss: 1.306138277053833, Accuracy: 0.6384758364312267\n",
      "Step: 1345, Loss: 1.283822774887085, Accuracy: 0.6384348687469044\n",
      "Step: 1346, Loss: 1.4468828439712524, Accuracy: 0.6382702301410542\n",
      "Step: 1347, Loss: 1.2947105169296265, Accuracy: 0.6382294757665677\n",
      "Step: 1348, Loss: 1.231479287147522, Accuracy: 0.6382505559673832\n",
      "Step: 1349, Loss: 1.1697262525558472, Accuracy: 0.6383333333333333\n",
      "Step: 1350, Loss: 1.293005108833313, Accuracy: 0.6382926227485813\n",
      "Step: 1351, Loss: 1.1629291772842407, Accuracy: 0.6383752465483234\n",
      "Step: 1352, Loss: 1.1420449018478394, Accuracy: 0.6384577482138458\n",
      "Step: 1353, Loss: 1.2577139139175415, Accuracy: 0.6384170359428852\n",
      "Step: 1354, Loss: 1.0483165979385376, Accuracy: 0.638560885608856\n",
      "Step: 1355, Loss: 1.5020875930786133, Accuracy: 0.6383357915437562\n",
      "Step: 1356, Loss: 1.2321759462356567, Accuracy: 0.6383566691230655\n",
      "Step: 1357, Loss: 1.2280796766281128, Accuracy: 0.6383161512027491\n",
      "Step: 1358, Loss: 1.221763253211975, Accuracy: 0.6383370125091979\n",
      "Step: 1359, Loss: 1.364571452140808, Accuracy: 0.638235294117647\n",
      "Step: 1360, Loss: 1.2775475978851318, Accuracy: 0.6381949546901788\n",
      "Step: 1361, Loss: 1.1037136316299438, Accuracy: 0.6383382280959373\n",
      "Step: 1362, Loss: 1.2314916849136353, Accuracy: 0.63835901198337\n",
      "Step: 1363, Loss: 1.2075539827346802, Accuracy: 0.6384408602150538\n",
      "Step: 1364, Loss: 1.374254822731018, Accuracy: 0.6383394383394383\n",
      "Step: 1365, Loss: 1.3417798280715942, Accuracy: 0.6382381649585164\n",
      "Step: 1366, Loss: 1.2360128164291382, Accuracy: 0.6382589612289685\n",
      "Step: 1367, Loss: 1.3025537729263306, Accuracy: 0.6382188109161794\n",
      "Step: 1368, Loss: 1.226913332939148, Accuracy: 0.6383004626247869\n",
      "Step: 1369, Loss: 1.2013660669326782, Accuracy: 0.6383211678832117\n",
      "Step: 1370, Loss: 1.528307557106018, Accuracy: 0.638159494286409\n",
      "Step: 1371, Loss: 1.1901803016662598, Accuracy: 0.6381802721088435\n",
      "Step: 1372, Loss: 1.3297674655914307, Accuracy: 0.63814032532168\n",
      "Step: 1373, Loss: 1.2437766790390015, Accuracy: 0.6381610868510432\n",
      "Step: 1374, Loss: 1.3066127300262451, Accuracy: 0.6380606060606061\n",
      "Step: 1375, Loss: 1.380113959312439, Accuracy: 0.6379602713178295\n",
      "Step: 1376, Loss: 1.1972465515136719, Accuracy: 0.6380416364076494\n",
      "Step: 1377, Loss: 1.2560688257217407, Accuracy: 0.6380019351717465\n",
      "Step: 1378, Loss: 1.2605820894241333, Accuracy: 0.6380227217790669\n",
      "Step: 1379, Loss: 1.094302773475647, Accuracy: 0.6381642512077295\n",
      "Step: 1380, Loss: 1.058383584022522, Accuracy: 0.6383659184166063\n",
      "Step: 1381, Loss: 1.124165654182434, Accuracy: 0.6385069946936807\n",
      "Step: 1382, Loss: 1.3693650960922241, Accuracy: 0.6384068450228971\n",
      "Step: 1383, Loss: 1.3227587938308716, Accuracy: 0.6383670520231214\n",
      "Step: 1384, Loss: 1.1136415004730225, Accuracy: 0.6385078219013237\n",
      "Step: 1385, Loss: 1.3277374505996704, Accuracy: 0.6384680134680135\n",
      "Step: 1386, Loss: 1.3290882110595703, Accuracy: 0.6384282624369142\n",
      "Step: 1387, Loss: 1.1279841661453247, Accuracy: 0.6385086455331412\n",
      "Step: 1388, Loss: 1.155274748802185, Accuracy: 0.6385889128869691\n",
      "Step: 1389, Loss: 1.3669661283493042, Accuracy: 0.6384892086330936\n",
      "Step: 1390, Loss: 1.4158344268798828, Accuracy: 0.6383896477354422\n",
      "Step: 1391, Loss: 1.398105263710022, Accuracy: 0.6382902298850575\n",
      "Step: 1392, Loss: 1.2999597787857056, Accuracy: 0.6382507776980139\n",
      "Step: 1393, Loss: 1.1054147481918335, Accuracy: 0.6383309421329507\n",
      "Step: 1394, Loss: 1.296963095664978, Accuracy: 0.6382915173237754\n",
      "Step: 1395, Loss: 1.286794900894165, Accuracy: 0.6382521489971347\n",
      "Step: 1396, Loss: 1.2722793817520142, Accuracy: 0.6382724886661895\n",
      "Step: 1397, Loss: 1.208875298500061, Accuracy: 0.6382927992370052\n",
      "Step: 1398, Loss: 1.4985326528549194, Accuracy: 0.6381343817012152\n",
      "Step: 1399, Loss: 1.312642216682434, Accuracy: 0.638095238095238\n",
      "Step: 1400, Loss: 1.2028146982192993, Accuracy: 0.6381751130145135\n",
      "Step: 1401, Loss: 1.2022048234939575, Accuracy: 0.6382548739895387\n",
      "Step: 1402, Loss: 1.0989679098129272, Accuracy: 0.6383345212639582\n",
      "Step: 1403, Loss: 1.115746021270752, Accuracy: 0.6384140550807218\n",
      "Step: 1404, Loss: 1.3597904443740845, Accuracy: 0.6383155397390273\n",
      "Step: 1405, Loss: 1.1904171705245972, Accuracy: 0.6383949739212897\n",
      "Step: 1406, Loss: 1.2749086618423462, Accuracy: 0.6384150675195451\n",
      "Step: 1407, Loss: 1.1096667051315308, Accuracy: 0.6385535037878788\n",
      "Step: 1408, Loss: 1.3473234176635742, Accuracy: 0.6385143127513603\n",
      "Step: 1409, Loss: 1.2902599573135376, Accuracy: 0.6385342789598109\n",
      "Step: 1410, Loss: 1.3633133172988892, Accuracy: 0.6384360973304984\n",
      "Step: 1411, Loss: 1.3787635564804077, Accuracy: 0.6383380547686497\n",
      "Step: 1412, Loss: 1.559645652770996, Accuracy: 0.6381221986317528\n",
      "Step: 1413, Loss: 1.2032012939453125, Accuracy: 0.6382013201320133\n",
      "Step: 1414, Loss: 1.125590205192566, Accuracy: 0.6382803297997645\n",
      "Step: 1415, Loss: 1.1450941562652588, Accuracy: 0.6383592278719398\n",
      "Step: 1416, Loss: 1.3071812391281128, Accuracy: 0.6383792048929664\n",
      "Step: 1417, Loss: 1.291680932044983, Accuracy: 0.638340385519511\n",
      "Step: 1418, Loss: 1.29055655002594, Accuracy: 0.6382428940568475\n",
      "Step: 1419, Loss: 1.3769625425338745, Accuracy: 0.6381455399061032\n",
      "Step: 1420, Loss: 1.4016509056091309, Accuracy: 0.6380483227773868\n",
      "Step: 1421, Loss: 1.235108733177185, Accuracy: 0.6380098452883263\n",
      "Step: 1422, Loss: 1.137755036354065, Accuracy: 0.6381471070508316\n",
      "Step: 1423, Loss: 1.2171562910079956, Accuracy: 0.6382256554307116\n",
      "Step: 1424, Loss: 1.1276237964630127, Accuracy: 0.6383040935672515\n",
      "Step: 1425, Loss: 1.1382523775100708, Accuracy: 0.6383824216923796\n",
      "Step: 1426, Loss: 0.990589439868927, Accuracy: 0.6382854473253913\n",
      "Epoch: 1, Val_Accuracy: 0.24735202492211839\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d9597e960784fdba301ee4260649c6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1427 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0, Loss: 1.1828513145446777, Accuracy: 0.6666666666666666\n",
      "Step: 1, Loss: 1.222298264503479, Accuracy: 0.6666666666666666\n",
      "Step: 2, Loss: 1.3607488870620728, Accuracy: 0.6111111111111112\n",
      "Step: 3, Loss: 1.0309314727783203, Accuracy: 0.6875\n",
      "Step: 4, Loss: 1.2319201231002808, Accuracy: 0.6833333333333333\n",
      "Step: 5, Loss: 1.2099920511245728, Accuracy: 0.6944444444444444\n",
      "Step: 6, Loss: 1.2119311094284058, Accuracy: 0.6904761904761905\n",
      "Step: 7, Loss: 1.6092419624328613, Accuracy: 0.6354166666666666\n",
      "Step: 8, Loss: 1.3218647241592407, Accuracy: 0.6296296296296297\n",
      "Step: 9, Loss: 1.5279854536056519, Accuracy: 0.6\n",
      "Step: 10, Loss: 1.1661598682403564, Accuracy: 0.6136363636363636\n",
      "Step: 11, Loss: 1.3133727312088013, Accuracy: 0.6111111111111112\n",
      "Step: 12, Loss: 1.3659433126449585, Accuracy: 0.6089743589743589\n",
      "Step: 13, Loss: 1.2697540521621704, Accuracy: 0.6130952380952381\n",
      "Step: 14, Loss: 1.4977415800094604, Accuracy: 0.5944444444444444\n",
      "Step: 15, Loss: 1.3720966577529907, Accuracy: 0.5885416666666666\n",
      "Step: 16, Loss: 1.1217865943908691, Accuracy: 0.5980392156862745\n",
      "Step: 17, Loss: 1.148786187171936, Accuracy: 0.6064814814814815\n",
      "Step: 18, Loss: 1.0426297187805176, Accuracy: 0.618421052631579\n",
      "Step: 19, Loss: 1.2739673852920532, Accuracy: 0.6208333333333333\n",
      "Step: 20, Loss: 1.4643586874008179, Accuracy: 0.6111111111111112\n",
      "Step: 21, Loss: 1.1712357997894287, Accuracy: 0.6174242424242424\n",
      "Step: 22, Loss: 1.3243234157562256, Accuracy: 0.6123188405797102\n",
      "Step: 23, Loss: 1.240334153175354, Accuracy: 0.6145833333333334\n",
      "Step: 24, Loss: 1.2317558526992798, Accuracy: 0.6166666666666667\n",
      "Step: 25, Loss: 1.0650370121002197, Accuracy: 0.6282051282051282\n",
      "Step: 26, Loss: 1.0955016613006592, Accuracy: 0.6358024691358025\n",
      "Step: 27, Loss: 1.0207443237304688, Accuracy: 0.6458333333333334\n",
      "Step: 28, Loss: 1.3630638122558594, Accuracy: 0.6408045977011494\n",
      "Step: 29, Loss: 1.3090367317199707, Accuracy: 0.6388888888888888\n",
      "Step: 30, Loss: 0.9207034707069397, Accuracy: 0.6505376344086021\n",
      "Step: 31, Loss: 1.2190587520599365, Accuracy: 0.6484375\n",
      "Step: 32, Loss: 1.1991809606552124, Accuracy: 0.648989898989899\n",
      "Step: 33, Loss: 1.4613351821899414, Accuracy: 0.6421568627450981\n",
      "Step: 34, Loss: 1.3858026266098022, Accuracy: 0.638095238095238\n",
      "Step: 35, Loss: 1.2513800859451294, Accuracy: 0.6388888888888888\n",
      "Step: 36, Loss: 1.1114120483398438, Accuracy: 0.6441441441441441\n",
      "Step: 37, Loss: 1.0656298398971558, Accuracy: 0.6491228070175439\n",
      "Step: 38, Loss: 1.380269169807434, Accuracy: 0.6452991452991453\n",
      "Step: 39, Loss: 1.1636539697647095, Accuracy: 0.6479166666666667\n",
      "Step: 40, Loss: 1.172254204750061, Accuracy: 0.6504065040650406\n",
      "Step: 41, Loss: 1.2546987533569336, Accuracy: 0.6507936507936508\n",
      "Step: 42, Loss: 1.370526909828186, Accuracy: 0.6472868217054264\n",
      "Step: 43, Loss: 1.232520580291748, Accuracy: 0.6477272727272727\n",
      "Step: 44, Loss: 1.1708847284317017, Accuracy: 0.65\n",
      "Step: 45, Loss: 1.454409122467041, Accuracy: 0.644927536231884\n",
      "Step: 46, Loss: 1.2795813083648682, Accuracy: 0.6436170212765957\n",
      "Step: 47, Loss: 1.0852208137512207, Accuracy: 0.6475694444444444\n",
      "Step: 48, Loss: 1.370270848274231, Accuracy: 0.6445578231292517\n",
      "Step: 49, Loss: 1.3215433359146118, Accuracy: 0.6416666666666667\n",
      "Step: 50, Loss: 1.4344342947006226, Accuracy: 0.6372549019607843\n",
      "Step: 51, Loss: 1.260815978050232, Accuracy: 0.6378205128205128\n",
      "Step: 52, Loss: 1.203794240951538, Accuracy: 0.6383647798742138\n",
      "Step: 53, Loss: 1.3248237371444702, Accuracy: 0.6373456790123457\n",
      "Step: 54, Loss: 1.032846450805664, Accuracy: 0.6409090909090909\n",
      "Step: 55, Loss: 1.2082974910736084, Accuracy: 0.6413690476190477\n",
      "Step: 56, Loss: 1.110935091972351, Accuracy: 0.6432748538011696\n",
      "Step: 57, Loss: 1.2223304510116577, Accuracy: 0.6436781609195402\n",
      "Step: 58, Loss: 1.3030730485916138, Accuracy: 0.6440677966101694\n",
      "Step: 59, Loss: 1.184909701347351, Accuracy: 0.6444444444444445\n",
      "Step: 60, Loss: 1.2078807353973389, Accuracy: 0.6461748633879781\n",
      "Step: 61, Loss: 1.2876698970794678, Accuracy: 0.6451612903225806\n",
      "Step: 62, Loss: 1.3338063955307007, Accuracy: 0.6428571428571429\n",
      "Step: 63, Loss: 1.3038066625595093, Accuracy: 0.6419270833333334\n",
      "Step: 64, Loss: 1.3015198707580566, Accuracy: 0.6410256410256411\n",
      "Step: 65, Loss: 1.1611367464065552, Accuracy: 0.6426767676767676\n",
      "Step: 66, Loss: 1.221328616142273, Accuracy: 0.6430348258706468\n",
      "Step: 67, Loss: 1.3826346397399902, Accuracy: 0.6397058823529411\n",
      "Step: 68, Loss: 1.212009310722351, Accuracy: 0.6400966183574879\n",
      "Step: 69, Loss: 1.108622431755066, Accuracy: 0.6428571428571429\n",
      "Step: 70, Loss: 1.3637775182724, Accuracy: 0.6408450704225352\n",
      "Step: 71, Loss: 1.2537137269973755, Accuracy: 0.6412037037037037\n",
      "Step: 72, Loss: 1.2701696157455444, Accuracy: 0.6415525114155252\n",
      "Step: 73, Loss: 1.3102680444717407, Accuracy: 0.6407657657657657\n",
      "Step: 74, Loss: 1.2880831956863403, Accuracy: 0.6411111111111111\n",
      "Step: 75, Loss: 1.3465875387191772, Accuracy: 0.6403508771929824\n",
      "Step: 76, Loss: 1.4567676782608032, Accuracy: 0.6374458874458875\n",
      "Step: 77, Loss: 1.183684229850769, Accuracy: 0.6378205128205128\n",
      "Step: 78, Loss: 1.2914141416549683, Accuracy: 0.6381856540084389\n",
      "Step: 79, Loss: 1.3727282285690308, Accuracy: 0.6364583333333333\n",
      "Step: 80, Loss: 1.2620972394943237, Accuracy: 0.6368312757201646\n",
      "Step: 81, Loss: 1.3026076555252075, Accuracy: 0.6361788617886179\n",
      "Step: 82, Loss: 1.1035022735595703, Accuracy: 0.6375502008032129\n",
      "Step: 83, Loss: 1.157314658164978, Accuracy: 0.6388888888888888\n",
      "Step: 84, Loss: 1.1498035192489624, Accuracy: 0.6401960784313725\n",
      "Step: 85, Loss: 1.138094186782837, Accuracy: 0.6424418604651163\n",
      "Step: 86, Loss: 1.4079046249389648, Accuracy: 0.6408045977011494\n",
      "Step: 87, Loss: 1.2316640615463257, Accuracy: 0.6410984848484849\n",
      "Step: 88, Loss: 1.4633005857467651, Accuracy: 0.6385767790262172\n",
      "Step: 89, Loss: 1.3126873970031738, Accuracy: 0.637962962962963\n",
      "Step: 90, Loss: 1.2800021171569824, Accuracy: 0.6373626373626373\n",
      "Step: 91, Loss: 1.3151335716247559, Accuracy: 0.6367753623188406\n",
      "Step: 92, Loss: 1.206803798675537, Accuracy: 0.6379928315412187\n",
      "Step: 93, Loss: 1.1798694133758545, Accuracy: 0.6391843971631206\n",
      "Step: 94, Loss: 1.160794734954834, Accuracy: 0.6403508771929824\n",
      "Step: 95, Loss: 1.3874200582504272, Accuracy: 0.6388888888888888\n",
      "Step: 96, Loss: 1.2871203422546387, Accuracy: 0.6383161512027491\n",
      "Step: 97, Loss: 1.0343621969223022, Accuracy: 0.641156462585034\n",
      "Step: 98, Loss: 1.1288222074508667, Accuracy: 0.6422558922558923\n",
      "Step: 99, Loss: 1.174638032913208, Accuracy: 0.6433333333333333\n",
      "Step: 100, Loss: 1.1671093702316284, Accuracy: 0.6443894389438944\n",
      "Step: 101, Loss: 1.1974323987960815, Accuracy: 0.6454248366013072\n",
      "Step: 102, Loss: 1.0920650959014893, Accuracy: 0.6472491909385113\n",
      "Step: 103, Loss: 1.0864108800888062, Accuracy: 0.6490384615384616\n",
      "Step: 104, Loss: 1.3158842325210571, Accuracy: 0.6484126984126984\n",
      "Step: 105, Loss: 1.1515341997146606, Accuracy: 0.6501572327044025\n",
      "Step: 106, Loss: 1.512363076210022, Accuracy: 0.647196261682243\n",
      "Step: 107, Loss: 1.262323021888733, Accuracy: 0.6473765432098766\n",
      "Step: 108, Loss: 1.3158100843429565, Accuracy: 0.6467889908256881\n",
      "Step: 109, Loss: 1.213483452796936, Accuracy: 0.646969696969697\n",
      "Step: 110, Loss: 1.2147969007492065, Accuracy: 0.6471471471471472\n",
      "Step: 111, Loss: 1.2751413583755493, Accuracy: 0.6473214285714286\n",
      "Step: 112, Loss: 1.4524482488632202, Accuracy: 0.6452802359882006\n",
      "Step: 113, Loss: 1.2951568365097046, Accuracy: 0.6447368421052632\n",
      "Step: 114, Loss: 1.1872237920761108, Accuracy: 0.6456521739130435\n",
      "Step: 115, Loss: 1.1296535730361938, Accuracy: 0.646551724137931\n",
      "Step: 116, Loss: 1.3407659530639648, Accuracy: 0.6452991452991453\n",
      "Step: 117, Loss: 1.1589370965957642, Accuracy: 0.6461864406779662\n",
      "Step: 118, Loss: 1.212430477142334, Accuracy: 0.646358543417367\n",
      "Step: 119, Loss: 1.2055121660232544, Accuracy: 0.6465277777777778\n",
      "Step: 120, Loss: 1.2906705141067505, Accuracy: 0.6466942148760331\n",
      "Step: 121, Loss: 1.3321936130523682, Accuracy: 0.6461748633879781\n",
      "Step: 122, Loss: 1.1632699966430664, Accuracy: 0.6470189701897019\n",
      "Step: 123, Loss: 1.1959035396575928, Accuracy: 0.6471774193548387\n",
      "Step: 124, Loss: 1.3712663650512695, Accuracy: 0.646\n",
      "Step: 125, Loss: 1.3163728713989258, Accuracy: 0.6455026455026455\n",
      "Step: 126, Loss: 1.176894187927246, Accuracy: 0.6463254593175853\n",
      "Step: 127, Loss: 1.0685487985610962, Accuracy: 0.6477864583333334\n",
      "Step: 128, Loss: 1.1922427415847778, Accuracy: 0.648578811369509\n",
      "Step: 129, Loss: 1.1739546060562134, Accuracy: 0.6493589743589744\n",
      "Step: 130, Loss: 1.3085644245147705, Accuracy: 0.648854961832061\n",
      "Step: 131, Loss: 1.3165534734725952, Accuracy: 0.6483585858585859\n",
      "Step: 132, Loss: 1.2357951402664185, Accuracy: 0.6484962406015038\n",
      "Step: 133, Loss: 1.2938767671585083, Accuracy: 0.6486318407960199\n",
      "Step: 134, Loss: 1.26956045627594, Accuracy: 0.6487654320987655\n",
      "Step: 135, Loss: 1.5835576057434082, Accuracy: 0.6458333333333334\n",
      "Step: 136, Loss: 1.104336142539978, Accuracy: 0.6472019464720195\n",
      "Step: 137, Loss: 1.3599376678466797, Accuracy: 0.6467391304347826\n",
      "Step: 138, Loss: 1.1016969680786133, Accuracy: 0.6474820143884892\n",
      "Step: 139, Loss: 1.2435907125473022, Accuracy: 0.6476190476190476\n",
      "Step: 140, Loss: 1.1399668455123901, Accuracy: 0.6483451536643026\n",
      "Step: 141, Loss: 1.4414299726486206, Accuracy: 0.6467136150234741\n",
      "Step: 142, Loss: 1.2977641820907593, Accuracy: 0.6462703962703963\n",
      "Step: 143, Loss: 1.4167486429214478, Accuracy: 0.6452546296296297\n",
      "Step: 144, Loss: 1.0739868879318237, Accuracy: 0.646551724137931\n",
      "Step: 145, Loss: 1.133147120475769, Accuracy: 0.6472602739726028\n",
      "Step: 146, Loss: 1.1236484050750732, Accuracy: 0.6485260770975056\n",
      "Step: 147, Loss: 1.3653855323791504, Accuracy: 0.6480855855855856\n",
      "Step: 148, Loss: 1.3149126768112183, Accuracy: 0.6470917225950783\n",
      "Step: 149, Loss: 1.636175274848938, Accuracy: 0.6444444444444445\n",
      "Step: 150, Loss: 1.41851806640625, Accuracy: 0.6434878587196468\n",
      "Step: 151, Loss: 1.2603176832199097, Accuracy: 0.6430921052631579\n",
      "Step: 152, Loss: 1.5408552885055542, Accuracy: 0.6410675381263616\n",
      "Step: 153, Loss: 1.0681941509246826, Accuracy: 0.6423160173160173\n",
      "Step: 154, Loss: 1.2333711385726929, Accuracy: 0.6424731182795699\n",
      "Step: 155, Loss: 1.264541745185852, Accuracy: 0.6420940170940171\n",
      "Step: 156, Loss: 1.239742398262024, Accuracy: 0.6422505307855626\n",
      "Step: 157, Loss: 1.4840083122253418, Accuracy: 0.6408227848101266\n",
      "Step: 158, Loss: 1.3802576065063477, Accuracy: 0.639937106918239\n",
      "Step: 159, Loss: 1.4974652528762817, Accuracy: 0.6385416666666667\n",
      "Step: 160, Loss: 1.2472164630889893, Accuracy: 0.6381987577639752\n",
      "Step: 161, Loss: 1.303464651107788, Accuracy: 0.6378600823045267\n",
      "Step: 162, Loss: 1.1842936277389526, Accuracy: 0.6380368098159509\n",
      "Step: 163, Loss: 1.2536591291427612, Accuracy: 0.6382113821138211\n",
      "Step: 164, Loss: 1.2396072149276733, Accuracy: 0.6383838383838384\n",
      "Step: 165, Loss: 1.3177357912063599, Accuracy: 0.6380522088353414\n",
      "Step: 166, Loss: 1.3488436937332153, Accuracy: 0.6377245508982036\n",
      "Step: 167, Loss: 1.0326563119888306, Accuracy: 0.6393849206349206\n",
      "Step: 168, Loss: 1.2186788320541382, Accuracy: 0.6395463510848126\n",
      "Step: 169, Loss: 1.37669038772583, Accuracy: 0.6387254901960784\n",
      "Step: 170, Loss: 1.0284174680709839, Accuracy: 0.6403508771929824\n",
      "Step: 171, Loss: 1.1891789436340332, Accuracy: 0.6405038759689923\n",
      "Step: 172, Loss: 1.4329547882080078, Accuracy: 0.6392100192678227\n",
      "Step: 173, Loss: 1.231997013092041, Accuracy: 0.639367816091954\n",
      "Step: 174, Loss: 1.154150366783142, Accuracy: 0.64\n",
      "Step: 175, Loss: 1.2163218259811401, Accuracy: 0.6401515151515151\n",
      "Step: 176, Loss: 1.0041850805282593, Accuracy: 0.641713747645951\n",
      "Step: 177, Loss: 1.2561089992523193, Accuracy: 0.6418539325842697\n",
      "Step: 178, Loss: 1.1858714818954468, Accuracy: 0.6424581005586593\n",
      "Step: 179, Loss: 1.1490209102630615, Accuracy: 0.6430555555555556\n",
      "Step: 180, Loss: 1.4464492797851562, Accuracy: 0.641804788213628\n",
      "Step: 181, Loss: 1.250300407409668, Accuracy: 0.6414835164835165\n",
      "Step: 182, Loss: 1.762459397315979, Accuracy: 0.6384335154826958\n",
      "Step: 183, Loss: 1.1596662998199463, Accuracy: 0.6390398550724637\n",
      "Step: 184, Loss: 1.5460920333862305, Accuracy: 0.6373873873873874\n",
      "Step: 185, Loss: 1.377110481262207, Accuracy: 0.6366487455197133\n",
      "Step: 186, Loss: 1.1524606943130493, Accuracy: 0.6372549019607843\n",
      "Step: 187, Loss: 1.090506672859192, Accuracy: 0.6382978723404256\n",
      "Step: 188, Loss: 1.0193852186203003, Accuracy: 0.6397707231040565\n",
      "Step: 189, Loss: 1.2745654582977295, Accuracy: 0.6399122807017544\n",
      "Step: 190, Loss: 1.1153658628463745, Accuracy: 0.6404886561954625\n",
      "Step: 191, Loss: 1.6089801788330078, Accuracy: 0.6384548611111112\n",
      "Step: 192, Loss: 1.2243717908859253, Accuracy: 0.6386010362694301\n",
      "Step: 193, Loss: 1.134835124015808, Accuracy: 0.6396048109965635\n",
      "Step: 194, Loss: 0.9186110496520996, Accuracy: 0.6414529914529915\n",
      "Step: 195, Loss: 1.142225980758667, Accuracy: 0.6420068027210885\n",
      "Step: 196, Loss: 1.165103554725647, Accuracy: 0.6425549915397631\n",
      "Step: 197, Loss: 1.0216399431228638, Accuracy: 0.6439393939393939\n",
      "Step: 198, Loss: 1.2318657636642456, Accuracy: 0.6440536013400335\n",
      "Step: 199, Loss: 1.1300987005233765, Accuracy: 0.6445833333333333\n",
      "Step: 200, Loss: 1.4897304773330688, Accuracy: 0.6434494195688225\n",
      "Step: 201, Loss: 1.5591750144958496, Accuracy: 0.641914191419142\n",
      "Step: 202, Loss: 1.3226327896118164, Accuracy: 0.6416256157635468\n",
      "Step: 203, Loss: 1.392921805381775, Accuracy: 0.6409313725490197\n",
      "Step: 204, Loss: 1.3388034105300903, Accuracy: 0.640650406504065\n",
      "Step: 205, Loss: 1.234897494316101, Accuracy: 0.6407766990291263\n",
      "Step: 206, Loss: 1.2769759893417358, Accuracy: 0.6409017713365539\n",
      "Step: 207, Loss: 1.2936989068984985, Accuracy: 0.640625\n",
      "Step: 208, Loss: 1.1331101655960083, Accuracy: 0.6411483253588517\n",
      "Step: 209, Loss: 1.2381460666656494, Accuracy: 0.6412698412698413\n",
      "Step: 210, Loss: 1.1817818880081177, Accuracy: 0.641785150078989\n",
      "Step: 211, Loss: 1.420575499534607, Accuracy: 0.6407232704402516\n",
      "Step: 212, Loss: 1.3207945823669434, Accuracy: 0.6404538341158059\n",
      "Step: 213, Loss: 1.390202522277832, Accuracy: 0.639797507788162\n",
      "Step: 214, Loss: 1.2557719945907593, Accuracy: 0.639922480620155\n",
      "Step: 215, Loss: 1.260984182357788, Accuracy: 0.6400462962962963\n",
      "Step: 216, Loss: 1.0801934003829956, Accuracy: 0.640937019969278\n",
      "Step: 217, Loss: 1.2293645143508911, Accuracy: 0.6410550458715596\n",
      "Step: 218, Loss: 1.210317611694336, Accuracy: 0.64117199391172\n",
      "Step: 219, Loss: 1.2397085428237915, Accuracy: 0.6412878787878787\n",
      "Step: 220, Loss: 1.1706055402755737, Accuracy: 0.6417797888386124\n",
      "Step: 221, Loss: 0.9946005344390869, Accuracy: 0.6430180180180181\n",
      "Step: 222, Loss: 1.3552194833755493, Accuracy: 0.6423766816143498\n",
      "Step: 223, Loss: 1.3147741556167603, Accuracy: 0.6421130952380952\n",
      "Step: 224, Loss: 1.245651364326477, Accuracy: 0.6422222222222222\n",
      "Step: 225, Loss: 1.214708685874939, Accuracy: 0.6426991150442478\n",
      "Step: 226, Loss: 1.1150583028793335, Accuracy: 0.6431718061674009\n",
      "Step: 227, Loss: 1.0928493738174438, Accuracy: 0.6440058479532164\n",
      "Step: 228, Loss: 1.223289132118225, Accuracy: 0.6441048034934498\n",
      "Step: 229, Loss: 1.3049288988113403, Accuracy: 0.643840579710145\n",
      "Step: 230, Loss: 1.075669765472412, Accuracy: 0.645021645021645\n",
      "Step: 231, Loss: 1.259024739265442, Accuracy: 0.6451149425287356\n",
      "Step: 232, Loss: 1.207059621810913, Accuracy: 0.6452074391988555\n",
      "Step: 233, Loss: 1.2329117059707642, Accuracy: 0.6452991452991453\n",
      "Step: 234, Loss: 1.485428810119629, Accuracy: 0.6443262411347518\n",
      "Step: 235, Loss: 1.190104365348816, Accuracy: 0.6444209039548022\n",
      "Step: 236, Loss: 1.2758400440216064, Accuracy: 0.6441631504922645\n",
      "Step: 237, Loss: 1.1721522808074951, Accuracy: 0.6442577030812325\n",
      "Step: 238, Loss: 1.1741480827331543, Accuracy: 0.6447001394700139\n",
      "Step: 239, Loss: 1.0020618438720703, Accuracy: 0.6458333333333334\n",
      "Step: 240, Loss: 1.308510661125183, Accuracy: 0.6455739972337483\n",
      "Step: 241, Loss: 1.307074785232544, Accuracy: 0.6456611570247934\n",
      "Step: 242, Loss: 1.2606202363967896, Accuracy: 0.6454046639231824\n",
      "Step: 243, Loss: 1.2883745431900024, Accuracy: 0.6454918032786885\n",
      "Step: 244, Loss: 1.162157654762268, Accuracy: 0.6459183673469387\n",
      "Step: 245, Loss: 1.376546025276184, Accuracy: 0.6453252032520326\n",
      "Step: 246, Loss: 1.0957365036010742, Accuracy: 0.6460863697705803\n",
      "Step: 247, Loss: 1.2877687215805054, Accuracy: 0.6458333333333334\n",
      "Step: 248, Loss: 1.2877200841903687, Accuracy: 0.6455823293172691\n",
      "Step: 249, Loss: 1.349896788597107, Accuracy: 0.6453333333333333\n",
      "Step: 250, Loss: 1.288488507270813, Accuracy: 0.6450863213811421\n",
      "Step: 251, Loss: 1.3838038444519043, Accuracy: 0.644510582010582\n",
      "Step: 252, Loss: 1.1831022500991821, Accuracy: 0.6445981554677207\n",
      "Step: 253, Loss: 1.4483442306518555, Accuracy: 0.6437007874015748\n",
      "Step: 254, Loss: 1.3728243112564087, Accuracy: 0.6431372549019608\n",
      "Step: 255, Loss: 1.230876088142395, Accuracy: 0.6432291666666666\n",
      "Step: 256, Loss: 1.232592225074768, Accuracy: 0.6433203631647212\n",
      "Step: 257, Loss: 1.2738804817199707, Accuracy: 0.6434108527131783\n",
      "Step: 258, Loss: 1.2176960706710815, Accuracy: 0.6435006435006435\n",
      "Step: 259, Loss: 1.150701880455017, Accuracy: 0.6439102564102565\n",
      "Step: 260, Loss: 1.4065557718276978, Accuracy: 0.6433588761174968\n",
      "Step: 261, Loss: 1.2372509241104126, Accuracy: 0.6434478371501272\n",
      "Step: 262, Loss: 1.247284173965454, Accuracy: 0.6435361216730038\n",
      "Step: 263, Loss: 1.1582986116409302, Accuracy: 0.6439393939393939\n",
      "Step: 264, Loss: 1.1204921007156372, Accuracy: 0.6446540880503144\n",
      "Step: 265, Loss: 1.2323263883590698, Accuracy: 0.6447368421052632\n",
      "Step: 266, Loss: 1.0516647100448608, Accuracy: 0.6454431960049938\n",
      "Step: 267, Loss: 1.3852909803390503, Accuracy: 0.6449004975124378\n",
      "Step: 268, Loss: 1.4102071523666382, Accuracy: 0.644361833952912\n",
      "Step: 269, Loss: 1.2304023504257202, Accuracy: 0.6444444444444445\n",
      "Step: 270, Loss: 1.2039613723754883, Accuracy: 0.6448339483394834\n",
      "Step: 271, Loss: 1.168129563331604, Accuracy: 0.6452205882352942\n",
      "Step: 272, Loss: 1.185660481452942, Accuracy: 0.6452991452991453\n",
      "Step: 273, Loss: 0.9879346489906311, Accuracy: 0.6462895377128953\n",
      "Step: 274, Loss: 1.227303385734558, Accuracy: 0.6463636363636364\n",
      "Step: 275, Loss: 1.4091267585754395, Accuracy: 0.6458333333333334\n",
      "Step: 276, Loss: 1.1311572790145874, Accuracy: 0.6465102286401926\n",
      "Step: 277, Loss: 1.131428599357605, Accuracy: 0.6471822541966427\n",
      "Step: 278, Loss: 1.0821269750595093, Accuracy: 0.6478494623655914\n",
      "Step: 279, Loss: 1.157923698425293, Accuracy: 0.6482142857142857\n",
      "Step: 280, Loss: 1.3199807405471802, Accuracy: 0.6479833926453143\n",
      "Step: 281, Loss: 1.317187786102295, Accuracy: 0.6474586288416075\n",
      "Step: 282, Loss: 1.3099803924560547, Accuracy: 0.6472320376914017\n",
      "Step: 283, Loss: 1.2269078493118286, Accuracy: 0.6473004694835681\n",
      "Step: 284, Loss: 1.2191609144210815, Accuracy: 0.6473684210526316\n",
      "Step: 285, Loss: 1.3426700830459595, Accuracy: 0.6468531468531469\n",
      "Step: 286, Loss: 1.2182704210281372, Accuracy: 0.6469221835075494\n",
      "Step: 287, Loss: 1.3249744176864624, Accuracy: 0.6467013888888888\n",
      "Step: 288, Loss: 1.3140944242477417, Accuracy: 0.6464821222606689\n",
      "Step: 289, Loss: 1.3613266944885254, Accuracy: 0.6459770114942529\n",
      "Step: 290, Loss: 1.1600173711776733, Accuracy: 0.6463344788087056\n",
      "Step: 291, Loss: 1.331729531288147, Accuracy: 0.6461187214611872\n",
      "Step: 292, Loss: 1.3076188564300537, Accuracy: 0.646188850967008\n",
      "Step: 293, Loss: 1.2508678436279297, Accuracy: 0.6462585034013606\n",
      "Step: 294, Loss: 1.0393531322479248, Accuracy: 0.6471751412429378\n",
      "Step: 295, Loss: 1.1365208625793457, Accuracy: 0.6475225225225225\n",
      "Step: 296, Loss: 1.1719353199005127, Accuracy: 0.6478675645342312\n",
      "Step: 297, Loss: 1.343436360359192, Accuracy: 0.6473713646532439\n",
      "Step: 298, Loss: 1.0147346258163452, Accuracy: 0.6482720178372352\n",
      "Step: 299, Loss: 1.1874349117279053, Accuracy: 0.6483333333333333\n",
      "Step: 300, Loss: 1.1100419759750366, Accuracy: 0.6489479512735327\n",
      "Step: 301, Loss: 0.9960389137268066, Accuracy: 0.6501103752759382\n",
      "Step: 302, Loss: 1.6108568906784058, Accuracy: 0.6487898789878987\n",
      "Step: 303, Loss: 1.2570518255233765, Accuracy: 0.6488486842105263\n",
      "Step: 304, Loss: 1.002987027168274, Accuracy: 0.6497267759562841\n",
      "Step: 305, Loss: 1.232542872428894, Accuracy: 0.6497821350762527\n",
      "Step: 306, Loss: 1.2341395616531372, Accuracy: 0.6498371335504886\n",
      "Step: 307, Loss: 1.2693235874176025, Accuracy: 0.6498917748917749\n",
      "Step: 308, Loss: 1.3881397247314453, Accuracy: 0.6496763754045307\n",
      "Step: 309, Loss: 1.1584751605987549, Accuracy: 0.65\n",
      "Step: 310, Loss: 1.3964203596115112, Accuracy: 0.6495176848874598\n",
      "Step: 311, Loss: 1.2510803937911987, Accuracy: 0.6495726495726496\n",
      "Step: 312, Loss: 1.4457087516784668, Accuracy: 0.648828541001065\n",
      "Step: 313, Loss: 1.3150354623794556, Accuracy: 0.648619957537155\n",
      "Step: 314, Loss: 1.2757450342178345, Accuracy: 0.6486772486772486\n",
      "Step: 315, Loss: 1.2310571670532227, Accuracy: 0.6487341772151899\n",
      "Step: 316, Loss: 1.1433030366897583, Accuracy: 0.6490536277602523\n",
      "Step: 317, Loss: 1.0479801893234253, Accuracy: 0.6496331236897275\n",
      "Step: 318, Loss: 1.3007934093475342, Accuracy: 0.6496865203761756\n",
      "Step: 319, Loss: 1.189502239227295, Accuracy: 0.6497395833333334\n",
      "Step: 320, Loss: 1.2417304515838623, Accuracy: 0.6497923156801662\n",
      "Step: 321, Loss: 1.236906886100769, Accuracy: 0.6498447204968945\n",
      "Step: 322, Loss: 1.2347338199615479, Accuracy: 0.6498968008255934\n",
      "Step: 323, Loss: 1.2176951169967651, Accuracy: 0.6499485596707819\n",
      "Step: 324, Loss: 1.035569429397583, Accuracy: 0.6507692307692308\n",
      "Step: 325, Loss: 1.394957423210144, Accuracy: 0.6503067484662577\n",
      "Step: 326, Loss: 1.2614167928695679, Accuracy: 0.6501019367991845\n",
      "Step: 327, Loss: 1.3438736200332642, Accuracy: 0.6496443089430894\n",
      "Step: 328, Loss: 1.3528122901916504, Accuracy: 0.6491894630192503\n",
      "Step: 329, Loss: 1.297365665435791, Accuracy: 0.648989898989899\n",
      "Step: 330, Loss: 1.1982074975967407, Accuracy: 0.6492950654582075\n",
      "Step: 331, Loss: 1.1448289155960083, Accuracy: 0.6498493975903614\n",
      "Step: 332, Loss: 1.2179248332977295, Accuracy: 0.6498998998998999\n",
      "Step: 333, Loss: 1.303935170173645, Accuracy: 0.6497005988023952\n",
      "Step: 334, Loss: 1.0730997323989868, Accuracy: 0.6502487562189054\n",
      "Step: 335, Loss: 1.3212534189224243, Accuracy: 0.6500496031746031\n",
      "Step: 336, Loss: 1.1483328342437744, Accuracy: 0.6503461918892186\n",
      "Step: 337, Loss: 1.1335175037384033, Accuracy: 0.6506410256410257\n",
      "Step: 338, Loss: 1.3108397722244263, Accuracy: 0.6504424778761062\n",
      "Step: 339, Loss: 1.2978051900863647, Accuracy: 0.6502450980392157\n",
      "Step: 340, Loss: 1.159981369972229, Accuracy: 0.6505376344086021\n",
      "Step: 341, Loss: 1.178697109222412, Accuracy: 0.6508284600389863\n",
      "Step: 342, Loss: 1.4303697347640991, Accuracy: 0.6503887269193391\n",
      "Step: 343, Loss: 1.6246851682662964, Accuracy: 0.6492248062015504\n",
      "Step: 344, Loss: 1.1141221523284912, Accuracy: 0.6497584541062802\n",
      "Step: 345, Loss: 1.246261715888977, Accuracy: 0.6498073217726397\n",
      "Step: 346, Loss: 1.4817571640014648, Accuracy: 0.649135446685879\n",
      "Step: 347, Loss: 1.3200697898864746, Accuracy: 0.6489463601532567\n",
      "Step: 348, Loss: 1.0583927631378174, Accuracy: 0.6494746895893028\n",
      "Step: 349, Loss: 1.242412805557251, Accuracy: 0.6495238095238095\n",
      "Step: 350, Loss: 1.2130204439163208, Accuracy: 0.6495726495726496\n",
      "Step: 351, Loss: 1.0065728425979614, Accuracy: 0.6503314393939394\n",
      "Step: 352, Loss: 1.4804383516311646, Accuracy: 0.6496694995278565\n",
      "Step: 353, Loss: 1.313015103340149, Accuracy: 0.649482109227872\n",
      "Step: 354, Loss: 1.0984641313552856, Accuracy: 0.65\n",
      "Step: 355, Loss: 1.389872670173645, Accuracy: 0.6495786516853933\n",
      "Step: 356, Loss: 1.2272634506225586, Accuracy: 0.6498599439775911\n",
      "Step: 357, Loss: 1.1905012130737305, Accuracy: 0.6501396648044693\n",
      "Step: 358, Loss: 1.2388861179351807, Accuracy: 0.6501857010213556\n",
      "Step: 359, Loss: 1.1110000610351562, Accuracy: 0.6506944444444445\n",
      "Step: 360, Loss: 1.0190237760543823, Accuracy: 0.6514312096029548\n",
      "Step: 361, Loss: 1.1480250358581543, Accuracy: 0.6517034990791897\n",
      "Step: 362, Loss: 1.2354649305343628, Accuracy: 0.6517447199265382\n",
      "Step: 363, Loss: 1.070625901222229, Accuracy: 0.6522435897435898\n",
      "Step: 364, Loss: 1.3176482915878296, Accuracy: 0.6520547945205479\n",
      "Step: 365, Loss: 1.2432416677474976, Accuracy: 0.6520947176684881\n",
      "Step: 366, Loss: 1.1335543394088745, Accuracy: 0.6525885558583107\n",
      "Step: 367, Loss: 1.3318761587142944, Accuracy: 0.6524003623188406\n",
      "Step: 368, Loss: 1.1863285303115845, Accuracy: 0.6526648599819331\n",
      "Step: 369, Loss: 1.2360342741012573, Accuracy: 0.6527027027027027\n",
      "Step: 370, Loss: 1.2532299757003784, Accuracy: 0.6527403414195867\n",
      "Step: 371, Loss: 1.2782405614852905, Accuracy: 0.6527777777777778\n",
      "Step: 372, Loss: 1.0679763555526733, Accuracy: 0.6532618409294012\n",
      "Step: 373, Loss: 1.0763078927993774, Accuracy: 0.6537433155080213\n",
      "Step: 374, Loss: 1.3547563552856445, Accuracy: 0.6533333333333333\n",
      "Step: 375, Loss: 1.0741089582443237, Accuracy: 0.6538120567375887\n",
      "Step: 376, Loss: 1.3156529664993286, Accuracy: 0.6536251105216623\n",
      "Step: 377, Loss: 1.3557530641555786, Accuracy: 0.6532186948853616\n",
      "Step: 378, Loss: 1.1803452968597412, Accuracy: 0.6534740545294635\n",
      "Step: 379, Loss: 1.3471990823745728, Accuracy: 0.6530701754385965\n",
      "Step: 380, Loss: 1.2759190797805786, Accuracy: 0.6531058617672791\n",
      "Step: 381, Loss: 1.307421088218689, Accuracy: 0.6527050610820244\n",
      "Step: 382, Loss: 1.2347866296768188, Accuracy: 0.6529590948651001\n",
      "Step: 383, Loss: 1.3369673490524292, Accuracy: 0.6527777777777778\n",
      "Step: 384, Loss: 1.2126424312591553, Accuracy: 0.6528138528138528\n",
      "Step: 385, Loss: 1.2837344408035278, Accuracy: 0.6526338514680483\n",
      "Step: 386, Loss: 1.221572756767273, Accuracy: 0.6526701119724375\n",
      "Step: 387, Loss: 1.1909672021865845, Accuracy: 0.6529209621993127\n",
      "Step: 388, Loss: 1.303119421005249, Accuracy: 0.6527420736932305\n",
      "Step: 389, Loss: 1.0847744941711426, Accuracy: 0.6532051282051282\n",
      "Step: 390, Loss: 1.130581259727478, Accuracy: 0.6534526854219949\n",
      "Step: 391, Loss: 1.019452452659607, Accuracy: 0.654124149659864\n",
      "Step: 392, Loss: 1.1945551633834839, Accuracy: 0.654156064461408\n",
      "Step: 393, Loss: 1.307961106300354, Accuracy: 0.6539763113367174\n",
      "Step: 394, Loss: 1.2083983421325684, Accuracy: 0.6540084388185654\n",
      "Step: 395, Loss: 1.367813229560852, Accuracy: 0.6536195286195287\n",
      "Step: 396, Loss: 1.3493837118148804, Accuracy: 0.6534424853064652\n",
      "Step: 397, Loss: 1.1764132976531982, Accuracy: 0.6536850921273032\n",
      "Step: 398, Loss: 1.0329536199569702, Accuracy: 0.654344193817878\n",
      "Step: 399, Loss: 1.3769127130508423, Accuracy: 0.6539583333333333\n",
      "Step: 400, Loss: 1.2348872423171997, Accuracy: 0.6539900249376559\n",
      "Step: 401, Loss: 1.4048606157302856, Accuracy: 0.6536069651741293\n",
      "Step: 402, Loss: 1.3988447189331055, Accuracy: 0.6532258064516129\n",
      "Step: 403, Loss: 1.1052417755126953, Accuracy: 0.6534653465346535\n",
      "Step: 404, Loss: 1.5656903982162476, Accuracy: 0.6526748971193416\n",
      "Step: 405, Loss: 1.2190436124801636, Accuracy: 0.6527093596059114\n",
      "Step: 406, Loss: 1.235315203666687, Accuracy: 0.6527436527436528\n",
      "Step: 407, Loss: 1.112963080406189, Accuracy: 0.6531862745098039\n",
      "Step: 408, Loss: 1.290100336074829, Accuracy: 0.6530154849225754\n",
      "Step: 409, Loss: 1.1242583990097046, Accuracy: 0.6532520325203252\n",
      "Step: 410, Loss: 1.191433310508728, Accuracy: 0.6532846715328468\n",
      "Step: 411, Loss: 1.3913630247116089, Accuracy: 0.6529126213592233\n",
      "Step: 412, Loss: 1.244210124015808, Accuracy: 0.6529459241323649\n",
      "Step: 413, Loss: 1.2321048974990845, Accuracy: 0.6531803542673108\n",
      "Step: 414, Loss: 1.2968660593032837, Accuracy: 0.653012048192771\n",
      "Step: 415, Loss: 1.170024037361145, Accuracy: 0.6532451923076923\n",
      "Step: 416, Loss: 1.1568859815597534, Accuracy: 0.6534772182254197\n",
      "Step: 417, Loss: 0.9991353154182434, Accuracy: 0.6541068580542265\n",
      "Step: 418, Loss: 1.276758074760437, Accuracy: 0.6541368337311058\n",
      "Step: 419, Loss: 1.29275381565094, Accuracy: 0.653968253968254\n",
      "Step: 420, Loss: 1.4705168008804321, Accuracy: 0.6534045922406968\n",
      "Step: 421, Loss: 1.1710377931594849, Accuracy: 0.6536334913112164\n",
      "Step: 422, Loss: 1.143680214881897, Accuracy: 0.6538613081166272\n",
      "Step: 423, Loss: 1.146340250968933, Accuracy: 0.6542845911949685\n",
      "Step: 424, Loss: 1.2953728437423706, Accuracy: 0.6541176470588236\n",
      "Step: 425, Loss: 1.3660882711410522, Accuracy: 0.6539514866979655\n",
      "Step: 426, Loss: 1.2273308038711548, Accuracy: 0.6539812646370023\n",
      "Step: 427, Loss: 1.2245053052902222, Accuracy: 0.6540109034267912\n",
      "Step: 428, Loss: 1.1633180379867554, Accuracy: 0.6542346542346542\n",
      "Step: 429, Loss: 1.2582608461380005, Accuracy: 0.6540697674418605\n",
      "Step: 430, Loss: 1.3333078622817993, Accuracy: 0.6539056457849961\n",
      "Step: 431, Loss: 1.2051557302474976, Accuracy: 0.6539351851851852\n",
      "Step: 432, Loss: 1.201391577720642, Accuracy: 0.6539645881447267\n",
      "Step: 433, Loss: 1.2648448944091797, Accuracy: 0.6539938556067588\n",
      "Step: 434, Loss: 1.3624591827392578, Accuracy: 0.653639846743295\n",
      "Step: 435, Loss: 1.0726723670959473, Accuracy: 0.6540519877675841\n",
      "Step: 436, Loss: 1.2371093034744263, Accuracy: 0.6540808543096872\n",
      "Step: 437, Loss: 1.1214708089828491, Accuracy: 0.6542998477929984\n",
      "Step: 438, Loss: 1.5454376935958862, Accuracy: 0.6537585421412301\n",
      "Step: 439, Loss: 1.2808105945587158, Accuracy: 0.6535984848484848\n",
      "Step: 440, Loss: 1.4085043668746948, Accuracy: 0.6532501889644747\n",
      "Step: 441, Loss: 1.1564593315124512, Accuracy: 0.6534690799396682\n",
      "Step: 442, Loss: 1.4410673379898071, Accuracy: 0.6529345372460497\n",
      "Step: 443, Loss: 1.1557950973510742, Accuracy: 0.6531531531531531\n",
      "Step: 444, Loss: 1.204920768737793, Accuracy: 0.6531835205992509\n",
      "Step: 445, Loss: 1.3691434860229492, Accuracy: 0.6528400597907325\n",
      "Step: 446, Loss: 1.2940983772277832, Accuracy: 0.6526845637583892\n",
      "Step: 447, Loss: 1.1589545011520386, Accuracy: 0.6529017857142857\n",
      "Step: 448, Loss: 1.2766143083572388, Accuracy: 0.652746844840386\n",
      "Step: 449, Loss: 1.1589857339859009, Accuracy: 0.652962962962963\n",
      "Step: 450, Loss: 1.1432260274887085, Accuracy: 0.6531781226903178\n",
      "Step: 451, Loss: 1.4659401178359985, Accuracy: 0.6526548672566371\n",
      "Step: 452, Loss: 1.3175780773162842, Accuracy: 0.6525018395879323\n",
      "Step: 453, Loss: 1.1954008340835571, Accuracy: 0.6527165932452276\n",
      "Step: 454, Loss: 1.3487706184387207, Accuracy: 0.6523809523809524\n",
      "Step: 455, Loss: 1.0117000341415405, Accuracy: 0.6529605263157895\n",
      "Step: 456, Loss: 1.2181644439697266, Accuracy: 0.6529905178701677\n",
      "Step: 457, Loss: 1.2702471017837524, Accuracy: 0.6528384279475983\n",
      "Step: 458, Loss: 0.9860183596611023, Accuracy: 0.6535947712418301\n",
      "Step: 459, Loss: 1.3956255912780762, Accuracy: 0.6532608695652173\n",
      "Step: 460, Loss: 1.224104881286621, Accuracy: 0.6532899493853941\n",
      "Step: 461, Loss: 1.0776172876358032, Accuracy: 0.6536796536796536\n",
      "Step: 462, Loss: 1.2482463121414185, Accuracy: 0.6537077033837293\n",
      "Step: 463, Loss: 1.23763906955719, Accuracy: 0.6537356321839081\n",
      "Step: 464, Loss: 1.095295786857605, Accuracy: 0.6541218637992832\n",
      "Step: 465, Loss: 1.2631951570510864, Accuracy: 0.653969957081545\n",
      "Step: 466, Loss: 1.2059789896011353, Accuracy: 0.6539971448965025\n",
      "Step: 467, Loss: 1.5031614303588867, Accuracy: 0.6534900284900285\n",
      "Step: 468, Loss: 1.293969988822937, Accuracy: 0.6533404406538735\n",
      "Step: 469, Loss: 1.1771095991134644, Accuracy: 0.6535460992907801\n",
      "Step: 470, Loss: 1.2302870750427246, Accuracy: 0.6535739561217269\n",
      "Step: 471, Loss: 1.3229180574417114, Accuracy: 0.6534251412429378\n",
      "Step: 472, Loss: 1.2604647874832153, Accuracy: 0.6534531360112755\n",
      "Step: 473, Loss: 1.221953272819519, Accuracy: 0.6534810126582279\n",
      "Step: 474, Loss: 1.1479206085205078, Accuracy: 0.6536842105263158\n",
      "Step: 475, Loss: 1.1987205743789673, Accuracy: 0.6538865546218487\n",
      "Step: 476, Loss: 1.2233844995498657, Accuracy: 0.6539133473095737\n",
      "Step: 477, Loss: 1.1910825967788696, Accuracy: 0.6541143654114365\n",
      "Step: 478, Loss: 1.402077078819275, Accuracy: 0.6537926235212248\n",
      "Step: 479, Loss: 1.2000764608383179, Accuracy: 0.6538194444444444\n",
      "Step: 480, Loss: 1.3497270345687866, Accuracy: 0.6534996534996536\n",
      "Step: 481, Loss: 1.3529000282287598, Accuracy: 0.6531811894882434\n",
      "Step: 482, Loss: 1.16909921169281, Accuracy: 0.6533816425120773\n",
      "Step: 483, Loss: 1.2395185232162476, Accuracy: 0.6534090909090909\n",
      "Step: 484, Loss: 1.162384033203125, Accuracy: 0.6536082474226804\n",
      "Step: 485, Loss: 1.4036186933517456, Accuracy: 0.6532921810699589\n",
      "Step: 486, Loss: 1.3264471292495728, Accuracy: 0.6531485284052019\n",
      "Step: 487, Loss: 1.1873024702072144, Accuracy: 0.6533469945355191\n",
      "Step: 488, Loss: 1.0798780918121338, Accuracy: 0.6537150647580096\n",
      "Step: 489, Loss: 1.4103546142578125, Accuracy: 0.6534013605442177\n",
      "Step: 490, Loss: 1.2104741334915161, Accuracy: 0.653428377460964\n",
      "Step: 491, Loss: 1.3136545419692993, Accuracy: 0.6532859078590786\n",
      "Step: 492, Loss: 1.2160736322402954, Accuracy: 0.6533130493576741\n",
      "Step: 493, Loss: 1.1067500114440918, Accuracy: 0.6536774628879892\n",
      "Step: 494, Loss: 1.203604817390442, Accuracy: 0.6538720538720538\n",
      "Step: 495, Loss: 1.1581732034683228, Accuracy: 0.6540658602150538\n",
      "Step: 496, Loss: 1.220778226852417, Accuracy: 0.6540912139503688\n",
      "Step: 497, Loss: 1.2052615880966187, Accuracy: 0.6541164658634538\n",
      "Step: 498, Loss: 1.1810320615768433, Accuracy: 0.654308617234469\n",
      "Step: 499, Loss: 1.1561989784240723, Accuracy: 0.6545\n",
      "Step: 500, Loss: 1.2207788228988647, Accuracy: 0.6545242847638058\n",
      "Step: 501, Loss: 1.0941898822784424, Accuracy: 0.6548804780876494\n",
      "Step: 502, Loss: 1.0032368898391724, Accuracy: 0.655400927766733\n",
      "Step: 503, Loss: 1.2331292629241943, Accuracy: 0.6554232804232805\n",
      "Step: 504, Loss: 1.197583556175232, Accuracy: 0.6556105610561056\n",
      "Step: 505, Loss: 1.181485652923584, Accuracy: 0.6557971014492754\n",
      "Step: 506, Loss: 1.069664716720581, Accuracy: 0.6561472715318869\n",
      "Step: 507, Loss: 1.2528414726257324, Accuracy: 0.6561679790026247\n",
      "Step: 508, Loss: 1.393813967704773, Accuracy: 0.6558611656843484\n",
      "Step: 509, Loss: 1.4131120443344116, Accuracy: 0.6555555555555556\n",
      "Step: 510, Loss: 1.4632371664047241, Accuracy: 0.6550880626223092\n",
      "Step: 511, Loss: 1.177682876586914, Accuracy: 0.6552734375\n",
      "Step: 512, Loss: 1.4264317750930786, Accuracy: 0.6548083170890189\n",
      "Step: 513, Loss: 1.1454638242721558, Accuracy: 0.6549935149156939\n",
      "Step: 514, Loss: 1.3749772310256958, Accuracy: 0.6546925566343043\n",
      "Step: 515, Loss: 1.210166096687317, Accuracy: 0.6547157622739018\n",
      "Step: 516, Loss: 1.305051326751709, Accuracy: 0.6545776918117344\n",
      "Step: 517, Loss: 1.2435929775238037, Accuracy: 0.6546010296010296\n",
      "Step: 518, Loss: 1.3147894144058228, Accuracy: 0.6544637122671805\n",
      "Step: 519, Loss: 0.9808720946311951, Accuracy: 0.6549679487179487\n",
      "Step: 520, Loss: 1.3118431568145752, Accuracy: 0.6548304542546385\n",
      "Step: 521, Loss: 1.369879126548767, Accuracy: 0.6545338441890166\n",
      "Step: 522, Loss: 1.47999906539917, Accuracy: 0.6540790312300828\n",
      "Step: 523, Loss: 1.3331433534622192, Accuracy: 0.6539440203562341\n",
      "Step: 524, Loss: 1.0843197107315063, Accuracy: 0.6542857142857142\n",
      "Step: 525, Loss: 1.0760096311569214, Accuracy: 0.6546261089987325\n",
      "Step: 526, Loss: 1.2828381061553955, Accuracy: 0.6544908285895004\n",
      "Step: 527, Loss: 1.363967776298523, Accuracy: 0.6543560606060606\n",
      "Step: 528, Loss: 0.9921349883079529, Accuracy: 0.6548519218651544\n",
      "Step: 529, Loss: 1.2068666219711304, Accuracy: 0.654874213836478\n",
      "Step: 530, Loss: 1.5723899602890015, Accuracy: 0.6542686754551161\n",
      "Step: 531, Loss: 1.1444841623306274, Accuracy: 0.6544486215538847\n",
      "Step: 532, Loss: 1.2588446140289307, Accuracy: 0.6544715447154471\n",
      "Step: 533, Loss: 1.23380708694458, Accuracy: 0.6546504369538078\n",
      "Step: 534, Loss: 1.3041847944259644, Accuracy: 0.6545171339563863\n",
      "Step: 535, Loss: 1.2885102033615112, Accuracy: 0.6543843283582089\n",
      "Step: 536, Loss: 1.2343498468399048, Accuracy: 0.654407200496586\n",
      "Step: 537, Loss: 1.2175530195236206, Accuracy: 0.6544299876084263\n",
      "Step: 538, Loss: 1.3384653329849243, Accuracy: 0.6542980828695114\n",
      "Step: 539, Loss: 1.3194350004196167, Accuracy: 0.6541666666666667\n",
      "Step: 540, Loss: 1.2999000549316406, Accuracy: 0.6540357362908195\n",
      "Step: 541, Loss: 1.2226886749267578, Accuracy: 0.6540590405904059\n",
      "Step: 542, Loss: 1.3038824796676636, Accuracy: 0.6539287906691221\n",
      "Step: 543, Loss: 1.2947694063186646, Accuracy: 0.6537990196078431\n",
      "Step: 544, Loss: 1.3781837224960327, Accuracy: 0.6535168195718655\n",
      "Step: 545, Loss: 1.392574429512024, Accuracy: 0.6532356532356532\n",
      "Step: 546, Loss: 1.4333620071411133, Accuracy: 0.6529555149299208\n",
      "Step: 547, Loss: 1.4421480894088745, Accuracy: 0.6525243309002433\n",
      "Step: 548, Loss: 1.449121117591858, Accuracy: 0.6520947176684881\n",
      "Step: 549, Loss: 1.2727196216583252, Accuracy: 0.6521212121212121\n",
      "Step: 550, Loss: 1.1694085597991943, Accuracy: 0.6522988505747126\n",
      "Step: 551, Loss: 1.3492225408554077, Accuracy: 0.6520229468599034\n",
      "Step: 552, Loss: 1.3122578859329224, Accuracy: 0.6518987341772152\n",
      "Step: 553, Loss: 1.397231101989746, Accuracy: 0.6516245487364621\n",
      "Step: 554, Loss: 1.4130617380142212, Accuracy: 0.6513513513513514\n",
      "Step: 555, Loss: 1.3607536554336548, Accuracy: 0.6510791366906474\n",
      "Step: 556, Loss: 1.1728867292404175, Accuracy: 0.6512567324955116\n",
      "Step: 557, Loss: 1.1970386505126953, Accuracy: 0.651284348864994\n",
      "Step: 558, Loss: 1.3171625137329102, Accuracy: 0.6511627906976745\n",
      "Step: 559, Loss: 1.4540294408798218, Accuracy: 0.6508928571428572\n",
      "Step: 560, Loss: 1.1958531141281128, Accuracy: 0.6510695187165776\n",
      "Step: 561, Loss: 1.1277590990066528, Accuracy: 0.6512455516014235\n",
      "Step: 562, Loss: 1.2508875131607056, Accuracy: 0.6511249259917111\n",
      "Step: 563, Loss: 1.3269627094268799, Accuracy: 0.6510047281323877\n",
      "Step: 564, Loss: 1.1397432088851929, Accuracy: 0.6513274336283186\n",
      "Step: 565, Loss: 1.320290207862854, Accuracy: 0.6512073027090695\n",
      "Step: 566, Loss: 1.1972044706344604, Accuracy: 0.6512345679012346\n",
      "Step: 567, Loss: 1.2668622732162476, Accuracy: 0.6512617370892019\n",
      "Step: 568, Loss: 1.0884071588516235, Accuracy: 0.6515817223198594\n",
      "Step: 569, Loss: 1.284792184829712, Accuracy: 0.6514619883040935\n",
      "Step: 570, Loss: 1.491707444190979, Accuracy: 0.6509048453006422\n",
      "Step: 571, Loss: 1.1580561399459839, Accuracy: 0.6510780885780886\n",
      "Step: 572, Loss: 1.3919519186019897, Accuracy: 0.6508144269924374\n",
      "Step: 573, Loss: 1.224839448928833, Accuracy: 0.650842044134727\n",
      "Step: 574, Loss: 1.009353756904602, Accuracy: 0.6513043478260869\n",
      "Step: 575, Loss: 1.216577172279358, Accuracy: 0.6513310185185185\n",
      "Step: 576, Loss: 1.4297324419021606, Accuracy: 0.6510687463893703\n",
      "Step: 577, Loss: 1.0091496706008911, Accuracy: 0.6515282583621684\n",
      "Step: 578, Loss: 1.4143280982971191, Accuracy: 0.6512665515256189\n",
      "Step: 579, Loss: 1.2171813249588013, Accuracy: 0.6512931034482758\n",
      "Step: 580, Loss: 1.3263474702835083, Accuracy: 0.6511761331038439\n",
      "Step: 581, Loss: 1.0744588375091553, Accuracy: 0.6514891179839634\n",
      "Step: 582, Loss: 1.3508135080337524, Accuracy: 0.6513722126929674\n",
      "Step: 583, Loss: 1.26506769657135, Accuracy: 0.651398401826484\n",
      "Step: 584, Loss: 1.1576482057571411, Accuracy: 0.6515669515669515\n",
      "Step: 585, Loss: 1.2659043073654175, Accuracy: 0.6515927189988624\n",
      "Step: 586, Loss: 1.4018253087997437, Accuracy: 0.6513344690516751\n",
      "Step: 587, Loss: 1.1456292867660522, Accuracy: 0.6515022675736961\n",
      "Step: 588, Loss: 1.3335202932357788, Accuracy: 0.6512450481041313\n",
      "Step: 589, Loss: 1.3097394704818726, Accuracy: 0.6511299435028248\n",
      "Step: 590, Loss: 1.109259843826294, Accuracy: 0.6514382402707276\n",
      "Step: 591, Loss: 1.1445976495742798, Accuracy: 0.6516047297297297\n",
      "Step: 592, Loss: 1.1434270143508911, Accuracy: 0.6517706576728499\n",
      "Step: 593, Loss: 1.4124808311462402, Accuracy: 0.6515151515151515\n",
      "Step: 594, Loss: 1.3525062799453735, Accuracy: 0.6512605042016807\n",
      "Step: 595, Loss: 1.1886982917785645, Accuracy: 0.6512863534675615\n",
      "Step: 596, Loss: 1.1567937135696411, Accuracy: 0.6514517029592406\n",
      "Step: 597, Loss: 1.2893532514572144, Accuracy: 0.6514771460423634\n",
      "Step: 598, Loss: 1.1665730476379395, Accuracy: 0.6516416249304396\n",
      "Step: 599, Loss: 1.0080751180648804, Accuracy: 0.6520833333333333\n",
      "Step: 600, Loss: 1.2460323572158813, Accuracy: 0.6521075984470327\n",
      "Step: 601, Loss: 1.1079350709915161, Accuracy: 0.6522702104097453\n",
      "Step: 602, Loss: 1.1673706769943237, Accuracy: 0.6524322830292979\n",
      "Step: 603, Loss: 1.3321375846862793, Accuracy: 0.652317880794702\n",
      "Step: 604, Loss: 1.3890959024429321, Accuracy: 0.6520661157024793\n",
      "Step: 605, Loss: 1.2877942323684692, Accuracy: 0.6519526952695269\n",
      "Step: 606, Loss: 1.048426628112793, Accuracy: 0.6522515101592532\n",
      "Step: 607, Loss: 1.2035373449325562, Accuracy: 0.6524122807017544\n",
      "Step: 608, Loss: 1.1340454816818237, Accuracy: 0.6525725232621784\n",
      "Step: 609, Loss: 1.0901212692260742, Accuracy: 0.6528688524590164\n",
      "Step: 610, Loss: 0.9915854334831238, Accuracy: 0.6533006001091107\n",
      "Step: 611, Loss: 1.1674970388412476, Accuracy: 0.653458605664488\n",
      "Step: 612, Loss: 1.3923840522766113, Accuracy: 0.6532082653616096\n",
      "Step: 613, Loss: 1.2204248905181885, Accuracy: 0.6532301845819761\n",
      "Step: 614, Loss: 1.0516201257705688, Accuracy: 0.6535230352303523\n",
      "Step: 615, Loss: 1.2134445905685425, Accuracy: 0.6535443722943723\n",
      "Step: 616, Loss: 1.258459448814392, Accuracy: 0.6535656401944895\n",
      "Step: 617, Loss: 1.2371482849121094, Accuracy: 0.6535868392664509\n",
      "Step: 618, Loss: 1.4333044290542603, Accuracy: 0.653338718362951\n",
      "Step: 619, Loss: 1.16665518283844, Accuracy: 0.6534946236559139\n",
      "Step: 620, Loss: 1.4434953927993774, Accuracy: 0.653113258185722\n",
      "Step: 621, Loss: 1.3155338764190674, Accuracy: 0.6530010718113612\n",
      "Step: 622, Loss: 1.1298412084579468, Accuracy: 0.6531567683253077\n",
      "Step: 623, Loss: 1.2281652688980103, Accuracy: 0.6531784188034188\n",
      "Step: 624, Loss: 1.402258276939392, Accuracy: 0.6528\n",
      "Step: 625, Loss: 0.9954354763031006, Accuracy: 0.6532215122470714\n",
      "Step: 626, Loss: 1.279819369316101, Accuracy: 0.6529771398192451\n",
      "Step: 627, Loss: 1.3620764017105103, Accuracy: 0.6527335456475584\n",
      "Step: 628, Loss: 1.2458763122558594, Accuracy: 0.6527556968733439\n",
      "Step: 629, Loss: 1.5286248922348022, Accuracy: 0.6522486772486773\n",
      "Step: 630, Loss: 1.24453604221344, Accuracy: 0.6522715266772319\n",
      "Step: 631, Loss: 1.305873990058899, Accuracy: 0.6521624472573839\n",
      "Step: 632, Loss: 1.014318823814392, Accuracy: 0.6525803054239073\n",
      "Step: 633, Loss: 1.1634823083877563, Accuracy: 0.6528654048370137\n",
      "Step: 634, Loss: 1.3158105611801147, Accuracy: 0.6527559055118111\n",
      "Step: 635, Loss: 1.4572954177856445, Accuracy: 0.6523846960167715\n",
      "Step: 636, Loss: 1.2653030157089233, Accuracy: 0.652276295133438\n",
      "Step: 637, Loss: 1.1713203191757202, Accuracy: 0.6524294670846394\n",
      "Step: 638, Loss: 1.1139193773269653, Accuracy: 0.6527125717266562\n",
      "Step: 639, Loss: 1.618830680847168, Accuracy: 0.6520833333333333\n",
      "Step: 640, Loss: 1.2927614450454712, Accuracy: 0.6519760790431617\n",
      "Step: 641, Loss: 1.1654706001281738, Accuracy: 0.652128764278297\n",
      "Step: 642, Loss: 1.4584192037582397, Accuracy: 0.6517625712804562\n",
      "Step: 643, Loss: 1.0810414552688599, Accuracy: 0.652044513457557\n",
      "Step: 644, Loss: 1.2801752090454102, Accuracy: 0.6519379844961241\n",
      "Step: 645, Loss: 1.3944371938705444, Accuracy: 0.6515737874097007\n",
      "Step: 646, Loss: 1.067177653312683, Accuracy: 0.651854714064915\n",
      "Step: 647, Loss: 1.3103933334350586, Accuracy: 0.6517489711934157\n",
      "Step: 648, Loss: 1.0538568496704102, Accuracy: 0.6520287621982537\n",
      "Step: 649, Loss: 1.2687057256698608, Accuracy: 0.6519230769230769\n",
      "Step: 650, Loss: 1.4292408227920532, Accuracy: 0.6515616999487968\n",
      "Step: 651, Loss: 1.156943678855896, Accuracy: 0.6515848670756647\n",
      "Step: 652, Loss: 1.3124189376831055, Accuracy: 0.6514803471158754\n",
      "Step: 653, Loss: 1.3901996612548828, Accuracy: 0.6512487257900101\n",
      "Step: 654, Loss: 1.3767448663711548, Accuracy: 0.6510178117048346\n",
      "Step: 655, Loss: 1.349907398223877, Accuracy: 0.6507876016260162\n",
      "Step: 656, Loss: 1.1619330644607544, Accuracy: 0.6509386098427195\n",
      "Step: 657, Loss: 1.2985481023788452, Accuracy: 0.6508358662613982\n",
      "Step: 658, Loss: 1.2090483903884888, Accuracy: 0.6508598887202832\n",
      "Step: 659, Loss: 1.1699334383010864, Accuracy: 0.651010101010101\n",
      "Step: 660, Loss: 1.3415976762771606, Accuracy: 0.6509077155824509\n",
      "Step: 661, Loss: 1.0763788223266602, Accuracy: 0.651183282980866\n",
      "Step: 662, Loss: 1.3064144849777222, Accuracy: 0.6510809451985923\n",
      "Step: 663, Loss: 1.0709713697433472, Accuracy: 0.651355421686747\n",
      "Step: 664, Loss: 1.3794599771499634, Accuracy: 0.6511278195488722\n",
      "Step: 665, Loss: 1.4999452829360962, Accuracy: 0.6506506506506506\n",
      "Step: 666, Loss: 1.2444463968276978, Accuracy: 0.6506746626686657\n",
      "Step: 667, Loss: 1.1663285493850708, Accuracy: 0.6508233532934131\n",
      "Step: 668, Loss: 1.0936113595962524, Accuracy: 0.651096163428002\n",
      "Step: 669, Loss: 1.2729873657226562, Accuracy: 0.6509950248756219\n",
      "Step: 670, Loss: 1.1385623216629028, Accuracy: 0.6511425732737208\n",
      "Step: 671, Loss: 1.3533281087875366, Accuracy: 0.6510416666666666\n",
      "Step: 672, Loss: 1.4663538932800293, Accuracy: 0.6506934125804854\n",
      "Step: 673, Loss: 1.0614562034606934, Accuracy: 0.6509643916913946\n",
      "Step: 674, Loss: 0.9489603042602539, Accuracy: 0.6514814814814814\n",
      "Step: 675, Loss: 1.354019045829773, Accuracy: 0.6512573964497042\n",
      "Step: 676, Loss: 1.4157031774520874, Accuracy: 0.6510339734121122\n",
      "Step: 677, Loss: 1.461395263671875, Accuracy: 0.6506882989183874\n",
      "Step: 678, Loss: 1.14080810546875, Accuracy: 0.6508345606283751\n",
      "Step: 679, Loss: 1.1923907995224, Accuracy: 0.6509803921568628\n",
      "Step: 680, Loss: 1.2691895961761475, Accuracy: 0.6508810572687225\n",
      "Step: 681, Loss: 1.2662795782089233, Accuracy: 0.6507820136852395\n",
      "Step: 682, Loss: 1.1201802492141724, Accuracy: 0.6510492923377257\n",
      "Step: 683, Loss: 1.2297117710113525, Accuracy: 0.6510721247563352\n",
      "Step: 684, Loss: 1.119631052017212, Accuracy: 0.6512165450121654\n",
      "Step: 685, Loss: 1.1516820192337036, Accuracy: 0.6513605442176871\n",
      "Step: 686, Loss: 1.217626929283142, Accuracy: 0.6513828238719068\n",
      "Step: 687, Loss: 1.1771761178970337, Accuracy: 0.6515261627906976\n",
      "Step: 688, Loss: 1.2288494110107422, Accuracy: 0.651548137397194\n",
      "Step: 689, Loss: 1.3594814538955688, Accuracy: 0.6514492753623189\n",
      "Step: 690, Loss: 1.3583074808120728, Accuracy: 0.6512301013024602\n",
      "Step: 691, Loss: 1.1597596406936646, Accuracy: 0.6513728323699421\n",
      "Step: 692, Loss: 1.1155434846878052, Accuracy: 0.6516354016354017\n",
      "Step: 693, Loss: 1.2474334239959717, Accuracy: 0.651657060518732\n",
      "Step: 694, Loss: 1.3230446577072144, Accuracy: 0.6515587529976019\n",
      "Step: 695, Loss: 1.266218900680542, Accuracy: 0.6515804597701149\n",
      "Step: 696, Loss: 1.1400351524353027, Accuracy: 0.6517216642754663\n",
      "Step: 697, Loss: 1.4493368864059448, Accuracy: 0.6513849092645654\n",
      "Step: 698, Loss: 1.2065035104751587, Accuracy: 0.6514067715784454\n",
      "Step: 699, Loss: 1.374079704284668, Accuracy: 0.6511904761904762\n",
      "Step: 700, Loss: 1.2845624685287476, Accuracy: 0.651093675701379\n",
      "Step: 701, Loss: 1.2427440881729126, Accuracy: 0.6511158594491928\n",
      "Step: 702, Loss: 1.0871919393539429, Accuracy: 0.6513750592697961\n",
      "Step: 703, Loss: 1.106889247894287, Accuracy: 0.6516335227272727\n",
      "Step: 704, Loss: 1.2015821933746338, Accuracy: 0.6517730496453901\n",
      "Step: 705, Loss: 1.0294365882873535, Accuracy: 0.6520302171860245\n",
      "Step: 706, Loss: 1.4040614366531372, Accuracy: 0.6518151815181518\n",
      "Step: 707, Loss: 1.2503055334091187, Accuracy: 0.6518361581920904\n",
      "Step: 708, Loss: 1.3232930898666382, Accuracy: 0.6517395392571698\n",
      "Step: 709, Loss: 1.2123724222183228, Accuracy: 0.6517605633802817\n",
      "Step: 710, Loss: 1.2692373991012573, Accuracy: 0.6516643225503985\n",
      "Step: 711, Loss: 1.4975768327713013, Accuracy: 0.6513342696629213\n",
      "Step: 712, Loss: 0.9849345088005066, Accuracy: 0.6517064048620851\n",
      "Step: 713, Loss: 1.197834849357605, Accuracy: 0.651844070961718\n",
      "Step: 714, Loss: 1.3240749835968018, Accuracy: 0.6517482517482518\n",
      "Step: 715, Loss: 1.2083160877227783, Accuracy: 0.6518854748603352\n",
      "Step: 716, Loss: 1.2611433267593384, Accuracy: 0.6517898651789865\n",
      "Step: 717, Loss: 1.4082263708114624, Accuracy: 0.6515784586815228\n",
      "Step: 718, Loss: 1.3580098152160645, Accuracy: 0.6513676402410755\n",
      "Step: 719, Loss: 1.272466778755188, Accuracy: 0.6512731481481482\n",
      "Step: 720, Loss: 1.0867682695388794, Accuracy: 0.6515256588072122\n",
      "Step: 721, Loss: 1.1569477319717407, Accuracy: 0.6516620498614959\n",
      "Step: 722, Loss: 1.2901802062988281, Accuracy: 0.6515675426463808\n",
      "Step: 723, Loss: 1.1989103555679321, Accuracy: 0.6517034990791897\n",
      "Step: 724, Loss: 1.0845516920089722, Accuracy: 0.6519540229885058\n",
      "Step: 725, Loss: 1.1223039627075195, Accuracy: 0.6522038567493113\n",
      "Step: 726, Loss: 1.252559781074524, Accuracy: 0.6522237505731316\n",
      "Step: 727, Loss: 1.1350258588790894, Accuracy: 0.6523580586080586\n",
      "Step: 728, Loss: 1.2384363412857056, Accuracy: 0.6523776863283036\n",
      "Step: 729, Loss: 1.400333046913147, Accuracy: 0.6521689497716895\n",
      "Step: 730, Loss: 1.2981096506118774, Accuracy: 0.6520747834017327\n",
      "Step: 731, Loss: 1.0175679922103882, Accuracy: 0.652436247723133\n",
      "Step: 732, Loss: 1.2512456178665161, Accuracy: 0.652455661664393\n",
      "Step: 733, Loss: 1.2521367073059082, Accuracy: 0.6524750227066304\n",
      "Step: 734, Loss: 1.3713430166244507, Accuracy: 0.6522675736961451\n",
      "Step: 735, Loss: 1.115477204322815, Accuracy: 0.6525135869565217\n",
      "Step: 736, Loss: 1.4205665588378906, Accuracy: 0.6521935775667119\n",
      "Step: 737, Loss: 1.1752568483352661, Accuracy: 0.6523261065943993\n",
      "Step: 738, Loss: 1.4308291673660278, Accuracy: 0.6521199819576003\n",
      "Step: 739, Loss: 1.122642159461975, Accuracy: 0.6522522522522523\n",
      "Step: 740, Loss: 1.1096019744873047, Accuracy: 0.6524966261808367\n",
      "Step: 741, Loss: 1.2009259462356567, Accuracy: 0.6526280323450134\n",
      "Step: 742, Loss: 1.239761233329773, Accuracy: 0.6526469268730373\n",
      "Step: 743, Loss: 1.2443023920059204, Accuracy: 0.652665770609319\n",
      "Step: 744, Loss: 1.4814778566360474, Accuracy: 0.6523489932885906\n",
      "Step: 745, Loss: 1.298399567604065, Accuracy: 0.6522564789991063\n",
      "Step: 746, Loss: 1.5168944597244263, Accuracy: 0.6519410977242303\n",
      "Step: 747, Loss: 1.1467385292053223, Accuracy: 0.652072192513369\n",
      "Step: 748, Loss: 1.193785309791565, Accuracy: 0.6522029372496663\n",
      "Step: 749, Loss: 1.0944157838821411, Accuracy: 0.6523333333333333\n",
      "Step: 750, Loss: 1.3063603639602661, Accuracy: 0.6522414558366623\n",
      "Step: 751, Loss: 1.1612180471420288, Accuracy: 0.6524822695035462\n",
      "Step: 752, Loss: 1.0982110500335693, Accuracy: 0.6527224435590969\n",
      "Step: 753, Loss: 1.1108781099319458, Accuracy: 0.6529619805481874\n",
      "Step: 754, Loss: 1.3228670358657837, Accuracy: 0.6528697571743929\n",
      "Step: 755, Loss: 1.3492846488952637, Accuracy: 0.6526675485008818\n",
      "Step: 756, Loss: 1.4283150434494019, Accuracy: 0.6524658740642889\n",
      "Step: 757, Loss: 1.1407010555267334, Accuracy: 0.6527044854881267\n",
      "Step: 758, Loss: 1.0714200735092163, Accuracy: 0.6529424681598595\n",
      "Step: 759, Loss: 1.274859070777893, Accuracy: 0.6528508771929824\n",
      "Step: 760, Loss: 1.2523924112319946, Accuracy: 0.6528690319754709\n",
      "Step: 761, Loss: 1.4683889150619507, Accuracy: 0.6525590551181102\n",
      "Step: 762, Loss: 1.3068987131118774, Accuracy: 0.6524683267802533\n",
      "Step: 763, Loss: 1.29793119430542, Accuracy: 0.6523778359511344\n",
      "Step: 764, Loss: 1.2158576250076294, Accuracy: 0.65239651416122\n",
      "Step: 765, Loss: 1.2595773935317993, Accuracy: 0.6524151436031331\n",
      "Step: 766, Loss: 1.486228585243225, Accuracy: 0.6521077792264233\n",
      "Step: 767, Loss: 1.0871354341506958, Accuracy: 0.65234375\n",
      "Step: 768, Loss: 1.0912007093429565, Accuracy: 0.652579107065453\n",
      "Step: 769, Loss: 1.2333683967590332, Accuracy: 0.6525974025974026\n",
      "Step: 770, Loss: 1.5400446653366089, Accuracy: 0.6521833117163857\n",
      "Step: 771, Loss: 1.196696400642395, Accuracy: 0.6522020725388601\n",
      "Step: 772, Loss: 1.302895188331604, Accuracy: 0.6522207848210435\n",
      "Step: 773, Loss: 1.1199573278427124, Accuracy: 0.6523471145564169\n",
      "Step: 774, Loss: 1.0722310543060303, Accuracy: 0.6525806451612903\n",
      "Step: 775, Loss: 1.2969813346862793, Accuracy: 0.6524914089347079\n",
      "Step: 776, Loss: 1.2334030866622925, Accuracy: 0.6525096525096525\n",
      "Step: 777, Loss: 1.0173993110656738, Accuracy: 0.6528491859468724\n",
      "Step: 778, Loss: 1.3973196744918823, Accuracy: 0.65265297389816\n",
      "Step: 779, Loss: 1.2776082754135132, Accuracy: 0.6525641025641026\n",
      "Step: 780, Loss: 1.3885207176208496, Accuracy: 0.6523687580025608\n",
      "Step: 781, Loss: 1.2383971214294434, Accuracy: 0.652387041773231\n",
      "Step: 782, Loss: 1.149715781211853, Accuracy: 0.6526181353767561\n",
      "Step: 783, Loss: 1.1932563781738281, Accuracy: 0.6526360544217688\n",
      "Step: 784, Loss: 1.2531417608261108, Accuracy: 0.6526539278131634\n",
      "Step: 785, Loss: 1.2060703039169312, Accuracy: 0.6526717557251909\n",
      "Step: 786, Loss: 1.3299016952514648, Accuracy: 0.652583650995341\n",
      "Step: 787, Loss: 1.2616642713546753, Accuracy: 0.6526015228426396\n",
      "Step: 788, Loss: 1.3501487970352173, Accuracy: 0.6525137304604985\n",
      "Step: 789, Loss: 1.2729836702346802, Accuracy: 0.6524261603375527\n",
      "Step: 790, Loss: 1.360844612121582, Accuracy: 0.6522334597555837\n",
      "Step: 791, Loss: 1.3324222564697266, Accuracy: 0.6521464646464646\n",
      "Step: 792, Loss: 1.2441943883895874, Accuracy: 0.6522698612862547\n",
      "Step: 793, Loss: 1.3719369173049927, Accuracy: 0.6521830394626364\n",
      "Step: 794, Loss: 1.0664206743240356, Accuracy: 0.6524109014675052\n",
      "Step: 795, Loss: 1.4297958612442017, Accuracy: 0.6522194304857621\n",
      "Step: 796, Loss: 1.161679744720459, Accuracy: 0.6523421162693434\n",
      "Step: 797, Loss: 1.2662146091461182, Accuracy: 0.652360066833751\n",
      "Step: 798, Loss: 1.3524583578109741, Accuracy: 0.6522736754276178\n",
      "Step: 799, Loss: 1.2653223276138306, Accuracy: 0.6522916666666667\n",
      "Step: 800, Loss: 1.4889006614685059, Accuracy: 0.6518934665002081\n",
      "Step: 801, Loss: 1.207423448562622, Accuracy: 0.6519118869492935\n",
      "Step: 802, Loss: 1.3046759366989136, Accuracy: 0.6518264840182648\n",
      "Step: 803, Loss: 1.1169193983078003, Accuracy: 0.6520522388059702\n",
      "Step: 804, Loss: 1.186797022819519, Accuracy: 0.6521739130434783\n",
      "Step: 805, Loss: 1.313042163848877, Accuracy: 0.6520885028949545\n",
      "Step: 806, Loss: 1.2501485347747803, Accuracy: 0.6521065675340768\n",
      "Step: 807, Loss: 1.1979981660842896, Accuracy: 0.6521245874587459\n",
      "Step: 808, Loss: 1.4351930618286133, Accuracy: 0.6519365471775855\n",
      "Step: 809, Loss: 1.2519426345825195, Accuracy: 0.651954732510288\n",
      "Step: 810, Loss: 1.396882176399231, Accuracy: 0.6517673653925196\n",
      "Step: 811, Loss: 1.1609853506088257, Accuracy: 0.6518883415435139\n",
      "Step: 812, Loss: 1.0943018198013306, Accuracy: 0.6520090200902009\n",
      "Step: 813, Loss: 1.1744797229766846, Accuracy: 0.6521294021294022\n",
      "Step: 814, Loss: 1.157744288444519, Accuracy: 0.6522494887525563\n",
      "Step: 815, Loss: 1.23395836353302, Accuracy: 0.6522671568627451\n",
      "Step: 816, Loss: 1.2279491424560547, Accuracy: 0.6522847817217462\n",
      "Step: 817, Loss: 1.2041749954223633, Accuracy: 0.6523023634881826\n",
      "Step: 818, Loss: 1.4118932485580444, Accuracy: 0.652014652014652\n",
      "Step: 819, Loss: 1.3230844736099243, Accuracy: 0.6519308943089431\n",
      "Step: 820, Loss: 1.3743311166763306, Accuracy: 0.651745838408445\n",
      "Step: 821, Loss: 1.3268498182296753, Accuracy: 0.6516626115166261\n",
      "Step: 822, Loss: 1.1103941202163696, Accuracy: 0.6518833535844472\n",
      "Step: 823, Loss: 1.139326572418213, Accuracy: 0.652002427184466\n",
      "Step: 824, Loss: 1.1315281391143799, Accuracy: 0.6521212121212121\n",
      "Step: 825, Loss: 1.2605538368225098, Accuracy: 0.6520379338175948\n",
      "Step: 826, Loss: 1.0880481004714966, Accuracy: 0.6522571543732366\n",
      "Step: 827, Loss: 1.1193733215332031, Accuracy: 0.6523752012882448\n",
      "Step: 828, Loss: 1.1989492177963257, Accuracy: 0.6523924406915963\n",
      "Step: 829, Loss: 1.552954077720642, Accuracy: 0.6520080321285141\n",
      "Step: 830, Loss: 1.0837137699127197, Accuracy: 0.6522262334536703\n",
      "Step: 831, Loss: 1.0271764993667603, Accuracy: 0.6525440705128205\n",
      "Step: 832, Loss: 1.4742136001586914, Accuracy: 0.6522609043617447\n",
      "Step: 833, Loss: 1.3439844846725464, Accuracy: 0.6521782573940847\n",
      "Step: 834, Loss: 1.4253405332565308, Accuracy: 0.6518962075848304\n",
      "Step: 835, Loss: 1.163427472114563, Accuracy: 0.6520135566188198\n",
      "Step: 836, Loss: 1.171976923942566, Accuracy: 0.6521306252489049\n",
      "Step: 837, Loss: 1.0980159044265747, Accuracy: 0.6523468575974543\n",
      "Step: 838, Loss: 0.9563248753547668, Accuracy: 0.6527612236789829\n",
      "Step: 839, Loss: 1.1144975423812866, Accuracy: 0.6529761904761905\n",
      "Step: 840, Loss: 1.2491950988769531, Accuracy: 0.6529924692826001\n",
      "Step: 841, Loss: 1.4854387044906616, Accuracy: 0.6527117973079969\n",
      "Step: 842, Loss: 1.076845407485962, Accuracy: 0.6529260577303282\n",
      "Step: 843, Loss: 1.1622697114944458, Accuracy: 0.653041074249605\n",
      "Step: 844, Loss: 1.212187647819519, Accuracy: 0.6530571992110453\n",
      "Step: 845, Loss: 1.4741727113723755, Accuracy: 0.6527777777777778\n",
      "Step: 846, Loss: 1.3249439001083374, Accuracy: 0.6526957890594254\n",
      "Step: 847, Loss: 1.5573757886886597, Accuracy: 0.6523191823899371\n",
      "Step: 848, Loss: 1.2357906103134155, Accuracy: 0.6523360816647036\n",
      "Step: 849, Loss: 1.163909912109375, Accuracy: 0.6524509803921569\n",
      "Step: 850, Loss: 1.4534865617752075, Accuracy: 0.6521739130434783\n",
      "Step: 851, Loss: 1.25517737865448, Accuracy: 0.6521909233176839\n",
      "Step: 852, Loss: 1.235122561454773, Accuracy: 0.6522078937084799\n",
      "Step: 853, Loss: 1.2284215688705444, Accuracy: 0.6523224043715847\n",
      "Step: 854, Loss: 1.366463541984558, Accuracy: 0.6521442495126706\n",
      "Step: 855, Loss: 1.0559982061386108, Accuracy: 0.6523559190031153\n",
      "Step: 856, Loss: 1.1955265998840332, Accuracy: 0.6523726176584986\n",
      "Step: 857, Loss: 1.223647117614746, Accuracy: 0.6523892773892774\n",
      "Step: 858, Loss: 1.3995636701583862, Accuracy: 0.6521148622429181\n",
      "Step: 859, Loss: 1.3175232410430908, Accuracy: 0.6520348837209302\n",
      "Step: 860, Loss: 1.015694499015808, Accuracy: 0.6523422377080914\n",
      "Step: 861, Loss: 1.149533748626709, Accuracy: 0.6524555297757154\n",
      "Step: 862, Loss: 1.6088218688964844, Accuracy: 0.6519891850135188\n",
      "Step: 863, Loss: 1.2118327617645264, Accuracy: 0.6520061728395061\n",
      "Step: 864, Loss: 1.2274580001831055, Accuracy: 0.6520231213872832\n",
      "Step: 865, Loss: 1.0890389680862427, Accuracy: 0.6522324865280985\n",
      "Step: 866, Loss: 1.5639171600341797, Accuracy: 0.6518646674356017\n",
      "Step: 867, Loss: 1.1601264476776123, Accuracy: 0.6519777265745008\n",
      "Step: 868, Loss: 1.379801869392395, Accuracy: 0.6518028385116993\n",
      "Step: 869, Loss: 1.0912833213806152, Accuracy: 0.6520114942528735\n",
      "Step: 870, Loss: 1.076849102973938, Accuracy: 0.6522196708763873\n",
      "Step: 871, Loss: 1.0407253503799438, Accuracy: 0.652427370030581\n",
      "Step: 872, Loss: 1.5320320129394531, Accuracy: 0.6520618556701031\n",
      "Step: 873, Loss: 0.9893910884857178, Accuracy: 0.6523646071700991\n",
      "Step: 874, Loss: 1.313666820526123, Accuracy: 0.6522857142857142\n",
      "Step: 875, Loss: 1.240087866783142, Accuracy: 0.6523021308980214\n",
      "Step: 876, Loss: 1.2571648359298706, Accuracy: 0.6523185100722159\n",
      "Step: 877, Loss: 1.269126057624817, Accuracy: 0.6522399392558846\n",
      "Step: 878, Loss: 1.3725742101669312, Accuracy: 0.6520667425104285\n",
      "Step: 879, Loss: 1.2647323608398438, Accuracy: 0.6519886363636364\n",
      "Step: 880, Loss: 1.2273712158203125, Accuracy: 0.6520052970109724\n",
      "Step: 881, Loss: 1.1343270540237427, Accuracy: 0.6521164021164021\n",
      "Step: 882, Loss: 1.271161675453186, Accuracy: 0.6520385050962627\n",
      "Step: 883, Loss: 1.382962703704834, Accuracy: 0.6518665158371041\n",
      "Step: 884, Loss: 1.213474154472351, Accuracy: 0.6518832391713748\n",
      "Step: 885, Loss: 1.3135600090026855, Accuracy: 0.6518058690744921\n",
      "Step: 886, Loss: 1.2097941637039185, Accuracy: 0.6518226230740323\n",
      "Step: 887, Loss: 1.2464847564697266, Accuracy: 0.6518393393393394\n",
      "Step: 888, Loss: 1.2303522825241089, Accuracy: 0.6518560179977503\n",
      "Step: 889, Loss: 1.4369807243347168, Accuracy: 0.6515917602996255\n",
      "Step: 890, Loss: 1.3040522336959839, Accuracy: 0.6515151515151515\n",
      "Step: 891, Loss: 1.038303017616272, Accuracy: 0.6517189835575485\n",
      "Step: 892, Loss: 1.2269279956817627, Accuracy: 0.6517357222844344\n",
      "Step: 893, Loss: 1.3493191003799438, Accuracy: 0.6516592095451156\n",
      "Step: 894, Loss: 1.4034419059753418, Accuracy: 0.6514897579143389\n",
      "Step: 895, Loss: 1.1067816019058228, Accuracy: 0.6516927083333334\n",
      "Step: 896, Loss: 1.0975669622421265, Accuracy: 0.6518952062430323\n",
      "Step: 897, Loss: 1.283382773399353, Accuracy: 0.651818856718634\n",
      "Step: 898, Loss: 1.1451259851455688, Accuracy: 0.6519280682239526\n",
      "Step: 899, Loss: 1.2755259275436401, Accuracy: 0.6518518518518519\n",
      "Step: 900, Loss: 1.2915600538253784, Accuracy: 0.6517758046614872\n",
      "Step: 901, Loss: 1.1398531198501587, Accuracy: 0.6518847006651884\n",
      "Step: 902, Loss: 1.2519289255142212, Accuracy: 0.6519010705057217\n",
      "Step: 903, Loss: 1.1662323474884033, Accuracy: 0.6520095870206489\n",
      "Step: 904, Loss: 1.1425286531448364, Accuracy: 0.6521178637200736\n",
      "Step: 905, Loss: 1.4616583585739136, Accuracy: 0.6518579838116262\n",
      "Step: 906, Loss: 1.259080410003662, Accuracy: 0.6518743109151047\n",
      "Step: 907, Loss: 1.1735116243362427, Accuracy: 0.6519823788546255\n",
      "Step: 908, Loss: 1.1339707374572754, Accuracy: 0.6520902090209021\n",
      "Step: 909, Loss: 1.1776556968688965, Accuracy: 0.6521062271062271\n",
      "Step: 910, Loss: 1.306599497795105, Accuracy: 0.6520307354555434\n",
      "Step: 911, Loss: 1.2009638547897339, Accuracy: 0.6521381578947368\n",
      "Step: 912, Loss: 1.26030433177948, Accuracy: 0.6521540708287696\n",
      "Step: 913, Loss: 1.4652425050735474, Accuracy: 0.6518964259664478\n",
      "Step: 914, Loss: 1.253881573677063, Accuracy: 0.651912568306011\n",
      "Step: 915, Loss: 1.3273276090621948, Accuracy: 0.6518377001455604\n",
      "Step: 916, Loss: 1.127927303314209, Accuracy: 0.6520356234096693\n",
      "Step: 917, Loss: 1.0835963487625122, Accuracy: 0.6522331154684096\n",
      "Step: 918, Loss: 1.3784490823745728, Accuracy: 0.6520674646354734\n",
      "Step: 919, Loss: 1.3230984210968018, Accuracy: 0.6519021739130435\n",
      "Step: 920, Loss: 0.9838443398475647, Accuracy: 0.6521896489323199\n",
      "Step: 921, Loss: 1.3009166717529297, Accuracy: 0.652114967462039\n",
      "Step: 922, Loss: 1.1812595129013062, Accuracy: 0.6522210184182016\n",
      "Step: 923, Loss: 1.0256661176681519, Accuracy: 0.652507215007215\n",
      "Step: 924, Loss: 1.3519301414489746, Accuracy: 0.6524324324324324\n",
      "Step: 925, Loss: 1.1653757095336914, Accuracy: 0.6525377969762419\n",
      "Step: 926, Loss: 1.0988973379135132, Accuracy: 0.652732829917296\n",
      "Step: 927, Loss: 1.5295308828353882, Accuracy: 0.6523886494252874\n",
      "Step: 928, Loss: 1.2948943376541138, Accuracy: 0.6523143164693218\n",
      "Step: 929, Loss: 1.2426315546035767, Accuracy: 0.6523297491039427\n",
      "Step: 930, Loss: 1.2332061529159546, Accuracy: 0.6523451485857501\n",
      "Step: 931, Loss: 1.3087857961654663, Accuracy: 0.6522711015736766\n",
      "Step: 932, Loss: 0.9414089322090149, Accuracy: 0.6526438013576277\n",
      "Step: 933, Loss: 1.2615087032318115, Accuracy: 0.6525695931477516\n",
      "Step: 934, Loss: 1.256351113319397, Accuracy: 0.6526737967914439\n",
      "Step: 935, Loss: 1.1997030973434448, Accuracy: 0.6526887464387464\n",
      "Step: 936, Loss: 1.2058292627334595, Accuracy: 0.6527036641764496\n",
      "Step: 937, Loss: 1.255503535270691, Accuracy: 0.6527185501066098\n",
      "Step: 938, Loss: 1.1543668508529663, Accuracy: 0.6528221512247071\n",
      "Step: 939, Loss: 1.209488034248352, Accuracy: 0.6528368794326241\n",
      "Step: 940, Loss: 1.3882312774658203, Accuracy: 0.6526744597945449\n",
      "Step: 941, Loss: 1.2490187883377075, Accuracy: 0.6526893135173389\n",
      "Step: 942, Loss: 1.178878903388977, Accuracy: 0.6527925061859314\n",
      "Step: 943, Loss: 1.1925872564315796, Accuracy: 0.6528072033898306\n",
      "Step: 944, Loss: 1.3971060514450073, Accuracy: 0.6526455026455027\n",
      "Step: 945, Loss: 1.1911734342575073, Accuracy: 0.6527484143763214\n",
      "Step: 946, Loss: 1.280377984046936, Accuracy: 0.6526751143963393\n",
      "Step: 947, Loss: 1.1458245515823364, Accuracy: 0.6527777777777778\n",
      "Step: 948, Loss: 1.165793538093567, Accuracy: 0.6527924130663857\n",
      "Step: 949, Loss: 1.2982770204544067, Accuracy: 0.652719298245614\n",
      "Step: 950, Loss: 1.1635240316390991, Accuracy: 0.6528215913073957\n",
      "Step: 951, Loss: 1.151373267173767, Accuracy: 0.6529236694677871\n",
      "Step: 952, Loss: 1.2005499601364136, Accuracy: 0.6529380902413431\n",
      "Step: 953, Loss: 1.020898461341858, Accuracy: 0.653214535290007\n",
      "Step: 954, Loss: 1.2372575998306274, Accuracy: 0.6532286212914485\n",
      "Step: 955, Loss: 1.3042739629745483, Accuracy: 0.6532426778242678\n",
      "Step: 956, Loss: 1.2217483520507812, Accuracy: 0.6532567049808429\n",
      "Step: 957, Loss: 0.9500610828399658, Accuracy: 0.6536186499652052\n",
      "Step: 958, Loss: 1.3544713258743286, Accuracy: 0.6535453597497393\n",
      "Step: 959, Loss: 1.38449227809906, Accuracy: 0.6533854166666667\n",
      "Step: 960, Loss: 1.2343717813491821, Accuracy: 0.6533992369060007\n",
      "Step: 961, Loss: 1.3073559999465942, Accuracy: 0.6533264033264033\n",
      "Step: 962, Loss: 1.1605653762817383, Accuracy: 0.6534267912772586\n",
      "Step: 963, Loss: 1.1612461805343628, Accuracy: 0.6535269709543569\n",
      "Step: 964, Loss: 1.2246638536453247, Accuracy: 0.6535405872193437\n",
      "Step: 965, Loss: 1.210236668586731, Accuracy: 0.6535541752933057\n",
      "Step: 966, Loss: 1.1762722730636597, Accuracy: 0.6536539124439849\n",
      "Step: 967, Loss: 1.250351905822754, Accuracy: 0.6536673553719008\n",
      "Step: 968, Loss: 1.0854743719100952, Accuracy: 0.6538527691778466\n",
      "Step: 969, Loss: 1.2360495328903198, Accuracy: 0.6538659793814433\n",
      "Step: 970, Loss: 1.384063720703125, Accuracy: 0.653707518022657\n",
      "Step: 971, Loss: 1.28952956199646, Accuracy: 0.6536351165980796\n",
      "Step: 972, Loss: 1.2994978427886963, Accuracy: 0.6536485097636177\n",
      "Step: 973, Loss: 1.293383002281189, Accuracy: 0.6535763175906913\n",
      "Step: 974, Loss: 1.2664908170700073, Accuracy: 0.6535897435897436\n",
      "Step: 975, Loss: 1.4404312372207642, Accuracy: 0.6533469945355191\n",
      "Step: 976, Loss: 1.40684175491333, Accuracy: 0.6531900375298533\n",
      "Step: 977, Loss: 1.1628303527832031, Accuracy: 0.6532890252215405\n",
      "Step: 978, Loss: 1.0589045286178589, Accuracy: 0.6534729315628192\n",
      "Step: 979, Loss: 1.195401906967163, Accuracy: 0.6535714285714286\n",
      "Step: 980, Loss: 1.5698343515396118, Accuracy: 0.6532449881073734\n",
      "Step: 981, Loss: 1.565304160118103, Accuracy: 0.6528343516632722\n",
      "Step: 982, Loss: 1.2817354202270508, Accuracy: 0.6527636486944727\n",
      "Step: 983, Loss: 1.1829733848571777, Accuracy: 0.6528624661246613\n",
      "Step: 984, Loss: 1.0172144174575806, Accuracy: 0.6531302876480541\n",
      "Step: 985, Loss: 1.4346612691879272, Accuracy: 0.6528904665314401\n",
      "Step: 986, Loss: 1.0355470180511475, Accuracy: 0.6531577169875042\n",
      "Step: 987, Loss: 1.1046019792556763, Accuracy: 0.65334008097166\n",
      "Step: 988, Loss: 1.3609967231750488, Accuracy: 0.6531850353892821\n",
      "Step: 989, Loss: 1.2319713830947876, Accuracy: 0.6531986531986532\n",
      "Step: 990, Loss: 1.178773045539856, Accuracy: 0.6532963336696939\n",
      "Step: 991, Loss: 1.1591871976852417, Accuracy: 0.6533938172043011\n",
      "Step: 992, Loss: 1.3185207843780518, Accuracy: 0.6533232628398792\n",
      "Step: 993, Loss: 1.3505305051803589, Accuracy: 0.653252850435949\n",
      "Step: 994, Loss: 1.4261808395385742, Accuracy: 0.6530988274706868\n",
      "Step: 995, Loss: 1.2535549402236938, Accuracy: 0.6531124497991968\n",
      "Step: 996, Loss: 1.2732508182525635, Accuracy: 0.6530424607154798\n",
      "Step: 997, Loss: 1.2520614862442017, Accuracy: 0.6530561122244489\n",
      "Step: 998, Loss: 1.2762176990509033, Accuracy: 0.6530697364030698\n",
      "Step: 999, Loss: 1.379770278930664, Accuracy: 0.6529166666666667\n",
      "Step: 1000, Loss: 1.3653322458267212, Accuracy: 0.6528471528471529\n",
      "Step: 1001, Loss: 1.0274097919464111, Accuracy: 0.6531104457751165\n",
      "Step: 1002, Loss: 1.0954638719558716, Accuracy: 0.6533732136922565\n",
      "Step: 1003, Loss: 1.0670403242111206, Accuracy: 0.6535524568393094\n",
      "Step: 1004, Loss: 1.2775242328643799, Accuracy: 0.6534825870646767\n",
      "Step: 1005, Loss: 1.2795928716659546, Accuracy: 0.6534128561961564\n",
      "Step: 1006, Loss: 1.4200903177261353, Accuracy: 0.6532605097649785\n",
      "Step: 1007, Loss: 1.3167160749435425, Accuracy: 0.6531911375661376\n",
      "Step: 1008, Loss: 1.195583462715149, Accuracy: 0.653204492897258\n",
      "Step: 1009, Loss: 1.3897615671157837, Accuracy: 0.653052805280528\n",
      "Step: 1010, Loss: 1.058977723121643, Accuracy: 0.6532311242993736\n",
      "Step: 1011, Loss: 1.2648292779922485, Accuracy: 0.6532444005270093\n",
      "Step: 1012, Loss: 1.2782853841781616, Accuracy: 0.6531753866403422\n",
      "Step: 1013, Loss: 1.305607795715332, Accuracy: 0.6531065088757396\n",
      "Step: 1014, Loss: 0.9945154786109924, Accuracy: 0.6533661740558292\n",
      "Step: 1015, Loss: 1.0401800870895386, Accuracy: 0.6535433070866141\n",
      "Step: 1016, Loss: 1.301762342453003, Accuracy: 0.6534742707309079\n",
      "Step: 1017, Loss: 1.2486398220062256, Accuracy: 0.6534872298624754\n",
      "Step: 1018, Loss: 1.1023398637771606, Accuracy: 0.65366372260386\n",
      "Step: 1019, Loss: 1.0467604398727417, Accuracy: 0.653921568627451\n",
      "Step: 1020, Loss: 1.5204054117202759, Accuracy: 0.653607574273588\n",
      "Step: 1021, Loss: 1.3519543409347534, Accuracy: 0.6535388127853882\n",
      "Step: 1022, Loss: 1.175414800643921, Accuracy: 0.6536331052460085\n",
      "Step: 1023, Loss: 1.2649630308151245, Accuracy: 0.653564453125\n",
      "Step: 1024, Loss: 1.2193825244903564, Accuracy: 0.6535772357723577\n",
      "Step: 1025, Loss: 1.4660558700561523, Accuracy: 0.6533463287849253\n",
      "Step: 1026, Loss: 1.0606411695480347, Accuracy: 0.6535215839013307\n",
      "Step: 1027, Loss: 1.3515352010726929, Accuracy: 0.6534533073929961\n",
      "Step: 1028, Loss: 1.2419461011886597, Accuracy: 0.6534661483641075\n",
      "Step: 1029, Loss: 1.3201159238815308, Accuracy: 0.6533980582524271\n",
      "Step: 1030, Loss: 1.1610586643218994, Accuracy: 0.6534917555771096\n",
      "Step: 1031, Loss: 1.3754563331604004, Accuracy: 0.653343023255814\n",
      "Step: 1032, Loss: 1.0549274682998657, Accuracy: 0.6535979348176831\n",
      "Step: 1033, Loss: 1.162778377532959, Accuracy: 0.6536911669890393\n",
      "Step: 1034, Loss: 1.2755271196365356, Accuracy: 0.6537037037037037\n",
      "Step: 1035, Loss: 1.2523635625839233, Accuracy: 0.6537162162162162\n",
      "Step: 1036, Loss: 1.3965548276901245, Accuracy: 0.6535679845708775\n",
      "Step: 1037, Loss: 1.348716139793396, Accuracy: 0.6534200385356455\n",
      "Step: 1038, Loss: 1.326287865638733, Accuracy: 0.6533525826114854\n",
      "Step: 1039, Loss: 1.229441523551941, Accuracy: 0.6533653846153846\n",
      "Step: 1040, Loss: 1.1195331811904907, Accuracy: 0.6535382644892731\n",
      "Step: 1041, Loss: 1.1890867948532104, Accuracy: 0.6536308381317978\n",
      "Step: 1042, Loss: 1.141359567642212, Accuracy: 0.6538031319910514\n",
      "Step: 1043, Loss: 1.2363146543502808, Accuracy: 0.6538154533844189\n",
      "Step: 1044, Loss: 1.1923962831497192, Accuracy: 0.6539872408293461\n",
      "Step: 1045, Loss: 1.188882827758789, Accuracy: 0.6539993626513703\n",
      "Step: 1046, Loss: 1.3133918046951294, Accuracy: 0.6539318688315823\n",
      "Step: 1047, Loss: 1.2761356830596924, Accuracy: 0.6538645038167938\n",
      "Step: 1048, Loss: 1.22628653049469, Accuracy: 0.6538767079758501\n",
      "Step: 1049, Loss: 1.0635627508163452, Accuracy: 0.6540476190476191\n",
      "Step: 1050, Loss: 1.0625015497207642, Accuracy: 0.6542182048842372\n",
      "Step: 1051, Loss: 1.324196457862854, Accuracy: 0.6541508238276299\n",
      "Step: 1052, Loss: 1.290640950202942, Accuracy: 0.6540835707502374\n",
      "Step: 1053, Loss: 1.4273802042007446, Accuracy: 0.6539373814041746\n",
      "Step: 1054, Loss: 1.308508276939392, Accuracy: 0.653870458135861\n",
      "Step: 1055, Loss: 1.1237359046936035, Accuracy: 0.65396148989899\n",
      "Step: 1056, Loss: 1.4621890783309937, Accuracy: 0.6537369914853358\n",
      "Step: 1057, Loss: 1.2754813432693481, Accuracy: 0.6537492123503466\n",
      "Step: 1058, Loss: 1.3113466501235962, Accuracy: 0.6536827195467422\n",
      "Step: 1059, Loss: 1.3336869478225708, Accuracy: 0.6536163522012579\n",
      "Step: 1060, Loss: 1.3058193922042847, Accuracy: 0.6535501099591581\n",
      "Step: 1061, Loss: 1.1695252656936646, Accuracy: 0.6536409290646579\n",
      "Step: 1062, Loss: 1.2700127363204956, Accuracy: 0.6536531828159298\n",
      "Step: 1063, Loss: 1.373002052307129, Accuracy: 0.6535087719298246\n",
      "Step: 1064, Loss: 1.1442424058914185, Accuracy: 0.6535993740219093\n",
      "Step: 1065, Loss: 1.273303508758545, Accuracy: 0.6535334584115072\n",
      "Step: 1066, Loss: 1.4453314542770386, Accuracy: 0.6533114651671352\n",
      "Step: 1067, Loss: 1.3909183740615845, Accuracy: 0.6531679151061174\n",
      "Step: 1068, Loss: 1.3154271841049194, Accuracy: 0.6531025880885563\n",
      "Step: 1069, Loss: 1.22383713722229, Accuracy: 0.6531152647975078\n",
      "Step: 1070, Loss: 1.2480623722076416, Accuracy: 0.6531279178338002\n",
      "Step: 1071, Loss: 1.2937887907028198, Accuracy: 0.6530628109452736\n",
      "Step: 1072, Loss: 1.3631242513656616, Accuracy: 0.6529978254116185\n",
      "Step: 1073, Loss: 1.364172339439392, Accuracy: 0.6528553693358162\n",
      "Step: 1074, Loss: 1.3197966814041138, Accuracy: 0.6527906976744186\n",
      "Step: 1075, Loss: 1.185413122177124, Accuracy: 0.6528810408921933\n",
      "Step: 1076, Loss: 1.3173974752426147, Accuracy: 0.6528164654905602\n",
      "Step: 1077, Loss: 1.0930272340774536, Accuracy: 0.6529839208410637\n",
      "Step: 1078, Loss: 1.2298798561096191, Accuracy: 0.6529966017917825\n",
      "Step: 1079, Loss: 1.2783608436584473, Accuracy: 0.6529320987654321\n",
      "Step: 1080, Loss: 1.460178256034851, Accuracy: 0.652713536848597\n",
      "Step: 1081, Loss: 1.1285325288772583, Accuracy: 0.6528034504004929\n",
      "Step: 1082, Loss: 1.0752185583114624, Accuracy: 0.6529701446598953\n",
      "Step: 1083, Loss: 1.221535563468933, Accuracy: 0.6529827798277983\n",
      "Step: 1084, Loss: 1.6136198043823242, Accuracy: 0.6526113671274961\n",
      "Step: 1085, Loss: 1.1889760494232178, Accuracy: 0.6526243093922652\n",
      "Step: 1086, Loss: 1.0808838605880737, Accuracy: 0.6527138914443422\n",
      "Step: 1087, Loss: 1.1272557973861694, Accuracy: 0.6528033088235294\n",
      "Step: 1088, Loss: 1.2494399547576904, Accuracy: 0.6528160391796756\n",
      "Step: 1089, Loss: 1.0784133672714233, Accuracy: 0.6529816513761468\n",
      "Step: 1090, Loss: 1.1473466157913208, Accuracy: 0.653070577451879\n",
      "Step: 1091, Loss: 1.280602216720581, Accuracy: 0.6530830280830281\n",
      "Step: 1092, Loss: 1.2612335681915283, Accuracy: 0.6530954559316865\n",
      "Step: 1093, Loss: 1.1231637001037598, Accuracy: 0.6532602071907374\n",
      "Step: 1094, Loss: 1.283331274986267, Accuracy: 0.6532724505327245\n",
      "Step: 1095, Loss: 1.2276406288146973, Accuracy: 0.6532846715328468\n",
      "Step: 1096, Loss: 1.2133291959762573, Accuracy: 0.653296870252203\n",
      "Step: 1097, Loss: 1.1835907697677612, Accuracy: 0.6533090467516697\n",
      "Step: 1098, Loss: 1.2380390167236328, Accuracy: 0.6533212010919017\n",
      "Step: 1099, Loss: 1.3180015087127686, Accuracy: 0.6532575757575757\n",
      "Step: 1100, Loss: 1.2083104848861694, Accuracy: 0.6532697547683923\n",
      "Step: 1101, Loss: 1.476709008216858, Accuracy: 0.6530550514216575\n",
      "Step: 1102, Loss: 0.9894809126853943, Accuracy: 0.6532940465397401\n",
      "Step: 1103, Loss: 1.416219711303711, Accuracy: 0.6530797101449275\n",
      "Step: 1104, Loss: 1.1604098081588745, Accuracy: 0.6531674208144796\n",
      "Step: 1105, Loss: 1.384719729423523, Accuracy: 0.6530289330922242\n",
      "Step: 1106, Loss: 1.1311217546463013, Accuracy: 0.6531165311653117\n",
      "Step: 1107, Loss: 1.3633313179016113, Accuracy: 0.6529783393501805\n",
      "Step: 1108, Loss: 1.2499603033065796, Accuracy: 0.6529906822963631\n",
      "Step: 1109, Loss: 1.242428183555603, Accuracy: 0.653003003003003\n",
      "Step: 1110, Loss: 1.3275595903396606, Accuracy: 0.6529402940294029\n",
      "Step: 1111, Loss: 1.1514160633087158, Accuracy: 0.6530275779376499\n",
      "Step: 1112, Loss: 1.3807920217514038, Accuracy: 0.652890086852351\n",
      "Step: 1113, Loss: 1.322198510169983, Accuracy: 0.6528276481149012\n",
      "Step: 1114, Loss: 1.0338786840438843, Accuracy: 0.6530642750373692\n",
      "Step: 1115, Loss: 1.2777715921401978, Accuracy: 0.6530017921146953\n",
      "Step: 1116, Loss: 1.3128998279571533, Accuracy: 0.6529394210683378\n",
      "Step: 1117, Loss: 0.989648163318634, Accuracy: 0.653175313059034\n",
      "Step: 1118, Loss: 1.3954353332519531, Accuracy: 0.6530384271671135\n",
      "Step: 1119, Loss: 1.4817496538162231, Accuracy: 0.6528273809523809\n",
      "Step: 1120, Loss: 1.4931392669677734, Accuracy: 0.6526167112696997\n",
      "Step: 1121, Loss: 1.3433057069778442, Accuracy: 0.6524806892453952\n",
      "Step: 1122, Loss: 1.5429311990737915, Accuracy: 0.6521964974769962\n",
      "Step: 1123, Loss: 1.202368974685669, Accuracy: 0.6522835112692764\n",
      "Step: 1124, Loss: 1.388680338859558, Accuracy: 0.6521481481481481\n",
      "Step: 1125, Loss: 1.2816205024719238, Accuracy: 0.6521610420367081\n",
      "Step: 1126, Loss: 1.1584821939468384, Accuracy: 0.6522478556640048\n",
      "Step: 1127, Loss: 1.1869195699691772, Accuracy: 0.6524083924349882\n",
      "Step: 1128, Loss: 1.3757749795913696, Accuracy: 0.6522733982875701\n",
      "Step: 1129, Loss: 1.2017377614974976, Accuracy: 0.6522861356932154\n",
      "Step: 1130, Loss: 1.336562991142273, Accuracy: 0.6522251694665487\n",
      "Step: 1131, Loss: 1.0531878471374512, Accuracy: 0.6523851590106007\n",
      "Step: 1132, Loss: 1.2048128843307495, Accuracy: 0.6524713150926743\n",
      "Step: 1133, Loss: 1.4261717796325684, Accuracy: 0.6522633744855967\n",
      "Step: 1134, Loss: 1.214417576789856, Accuracy: 0.6522760646108664\n",
      "Step: 1135, Loss: 1.0594878196716309, Accuracy: 0.6525088028169014\n",
      "Step: 1136, Loss: 1.4353159666061401, Accuracy: 0.6523013778950454\n",
      "Step: 1137, Loss: 1.089942455291748, Accuracy: 0.6524604569420035\n",
      "Step: 1138, Loss: 1.2281795740127563, Accuracy: 0.6524729294702956\n",
      "Step: 1139, Loss: 1.2127301692962646, Accuracy: 0.6524853801169591\n",
      "Step: 1140, Loss: 1.3967989683151245, Accuracy: 0.6522787028921998\n",
      "Step: 1141, Loss: 1.1660361289978027, Accuracy: 0.6523642732049036\n",
      "Step: 1142, Loss: 1.2626014947891235, Accuracy: 0.6523038786818315\n",
      "Step: 1143, Loss: 1.0881601572036743, Accuracy: 0.6524621212121212\n",
      "Step: 1144, Loss: 1.1338406801223755, Accuracy: 0.6526200873362445\n",
      "Step: 1145, Loss: 1.4517178535461426, Accuracy: 0.652414194299011\n",
      "Step: 1146, Loss: 1.1620193719863892, Accuracy: 0.6524266201685557\n",
      "Step: 1147, Loss: 1.2934536933898926, Accuracy: 0.6523664343786295\n",
      "Step: 1148, Loss: 1.1750749349594116, Accuracy: 0.6524514070205977\n",
      "Step: 1149, Loss: 1.1206822395324707, Accuracy: 0.652536231884058\n",
      "Step: 1150, Loss: 1.156358242034912, Accuracy: 0.6526209093541848\n",
      "Step: 1151, Loss: 1.2849973440170288, Accuracy: 0.6525607638888888\n",
      "Step: 1152, Loss: 1.1944209337234497, Accuracy: 0.6526452732003469\n",
      "Step: 1153, Loss: 1.1610422134399414, Accuracy: 0.6527296360485269\n",
      "Step: 1154, Loss: 1.212343454360962, Accuracy: 0.6527417027417027\n",
      "Step: 1155, Loss: 1.3507121801376343, Accuracy: 0.652681660899654\n",
      "Step: 1156, Loss: 1.102138638496399, Accuracy: 0.6528377989052147\n",
      "Step: 1157, Loss: 1.2342945337295532, Accuracy: 0.6528497409326425\n",
      "Step: 1158, Loss: 1.111677646636963, Accuracy: 0.6530054644808743\n",
      "Step: 1159, Loss: 1.3382593393325806, Accuracy: 0.6529454022988506\n",
      "Step: 1160, Loss: 1.3640385866165161, Accuracy: 0.6528136663795578\n",
      "Step: 1161, Loss: 1.473476529121399, Accuracy: 0.6526104417670683\n",
      "Step: 1162, Loss: 1.298876404762268, Accuracy: 0.6525508741759817\n",
      "Step: 1163, Loss: 1.0752216577529907, Accuracy: 0.6527061855670103\n",
      "Step: 1164, Loss: 1.2404314279556274, Accuracy: 0.6527181688125894\n",
      "Step: 1165, Loss: 1.3023654222488403, Accuracy: 0.6527301315037164\n",
      "Step: 1166, Loss: 1.1475504636764526, Accuracy: 0.652813481862325\n",
      "Step: 1167, Loss: 1.398598074913025, Accuracy: 0.6526826484018264\n",
      "Step: 1168, Loss: 1.3328577280044556, Accuracy: 0.6525520387795837\n",
      "Step: 1169, Loss: 1.2074179649353027, Accuracy: 0.6525641025641026\n",
      "Step: 1170, Loss: 0.9802106022834778, Accuracy: 0.6527896384856248\n",
      "Step: 1171, Loss: 1.3087358474731445, Accuracy: 0.652801478953356\n",
      "Step: 1172, Loss: 1.354758381843567, Accuracy: 0.6526712134129014\n",
      "Step: 1173, Loss: 1.1792656183242798, Accuracy: 0.6527541169789892\n",
      "Step: 1174, Loss: 1.1635421514511108, Accuracy: 0.6528368794326241\n",
      "Step: 1175, Loss: 1.1579784154891968, Accuracy: 0.6529195011337868\n",
      "Step: 1176, Loss: 1.3344289064407349, Accuracy: 0.6528603794958935\n",
      "Step: 1177, Loss: 1.187886118888855, Accuracy: 0.6529428409734013\n",
      "Step: 1178, Loss: 1.1165541410446167, Accuracy: 0.6530958439355385\n",
      "Step: 1179, Loss: 1.1346473693847656, Accuracy: 0.6532485875706214\n",
      "Step: 1180, Loss: 1.2137538194656372, Accuracy: 0.6532599491955969\n",
      "Step: 1181, Loss: 1.6255978345870972, Accuracy: 0.6529187817258884\n",
      "Step: 1182, Loss: 1.4045590162277222, Accuracy: 0.6527190757959989\n",
      "Step: 1183, Loss: 1.2331339120864868, Accuracy: 0.6527308558558559\n",
      "Step: 1184, Loss: 1.4720886945724487, Accuracy: 0.6525316455696203\n",
      "Step: 1185, Loss: 1.1572617292404175, Accuracy: 0.6526138279932546\n",
      "Step: 1186, Loss: 1.500925064086914, Accuracy: 0.652344846953103\n",
      "Step: 1187, Loss: 1.4385485649108887, Accuracy: 0.6521464646464646\n",
      "Step: 1188, Loss: 1.3402405977249146, Accuracy: 0.6520885898514157\n",
      "Step: 1189, Loss: 1.179234504699707, Accuracy: 0.652170868347339\n",
      "Step: 1190, Loss: 1.1544028520584106, Accuracy: 0.6522530086761825\n",
      "Step: 1191, Loss: 0.9975836277008057, Accuracy: 0.6524748322147651\n",
      "Step: 1192, Loss: 1.5759574174880981, Accuracy: 0.6521374685666387\n",
      "Step: 1193, Loss: 1.4185495376586914, Accuracy: 0.6519402568397543\n",
      "Step: 1194, Loss: 1.0934749841690063, Accuracy: 0.652092050209205\n",
      "Step: 1195, Loss: 1.3400405645370483, Accuracy: 0.6519648829431438\n",
      "Step: 1196, Loss: 1.1172837018966675, Accuracy: 0.652046783625731\n",
      "Step: 1197, Loss: 1.0873063802719116, Accuracy: 0.6521981079577073\n",
      "Step: 1198, Loss: 1.169333815574646, Accuracy: 0.6522796775090353\n",
      "Step: 1199, Loss: 1.3497284650802612, Accuracy: 0.6521527777777778\n",
      "Step: 1200, Loss: 1.3010841608047485, Accuracy: 0.6520954759922287\n",
      "Step: 1201, Loss: 1.2417997121810913, Accuracy: 0.6521075984470327\n",
      "Step: 1202, Loss: 1.0725706815719604, Accuracy: 0.6522582432806872\n",
      "Step: 1203, Loss: 1.1249618530273438, Accuracy: 0.6523394241417497\n",
      "Step: 1204, Loss: 1.1205315589904785, Accuracy: 0.6524204702627939\n",
      "Step: 1205, Loss: 1.4137142896652222, Accuracy: 0.652294085129906\n",
      "Step: 1206, Loss: 1.1125937700271606, Accuracy: 0.6524440762220381\n",
      "Step: 1207, Loss: 1.1480177640914917, Accuracy: 0.6525248344370861\n",
      "Step: 1208, Loss: 1.0339421033859253, Accuracy: 0.6527433140336366\n",
      "Step: 1209, Loss: 1.2909661531448364, Accuracy: 0.6527548209366392\n",
      "Step: 1210, Loss: 1.1376515626907349, Accuracy: 0.6528351224883017\n",
      "Step: 1211, Loss: 1.0922060012817383, Accuracy: 0.6529840484048405\n",
      "Step: 1212, Loss: 1.162920355796814, Accuracy: 0.6529953283869195\n",
      "Step: 1213, Loss: 1.1524285078048706, Accuracy: 0.6530752333882482\n",
      "Step: 1214, Loss: 1.3709096908569336, Accuracy: 0.6529492455418381\n",
      "Step: 1215, Loss: 1.3624454736709595, Accuracy: 0.6528234649122807\n",
      "Step: 1216, Loss: 1.2335741519927979, Accuracy: 0.652903314160504\n",
      "Step: 1217, Loss: 1.1797749996185303, Accuracy: 0.6529830322933772\n",
      "Step: 1218, Loss: 1.2385339736938477, Accuracy: 0.6529942575881871\n",
      "Step: 1219, Loss: 1.1016477346420288, Accuracy: 0.6530737704918033\n",
      "Step: 1220, Loss: 1.368789792060852, Accuracy: 0.653016653016653\n",
      "Step: 1221, Loss: 1.2342346906661987, Accuracy: 0.6530278232405892\n",
      "Step: 1222, Loss: 1.180578589439392, Accuracy: 0.6531071136549469\n",
      "Step: 1223, Loss: 1.5167793035507202, Accuracy: 0.6528458605664488\n",
      "Step: 1224, Loss: 1.260443091392517, Accuracy: 0.6528571428571428\n",
      "Step: 1225, Loss: 1.2426320314407349, Accuracy: 0.652868406742795\n",
      "Step: 1226, Loss: 1.4218639135360718, Accuracy: 0.6527438196142352\n",
      "Step: 1227, Loss: 1.2462224960327148, Accuracy: 0.6527551574375678\n",
      "Step: 1228, Loss: 1.3027511835098267, Accuracy: 0.6526986710062381\n",
      "Step: 1229, Loss: 1.3454822301864624, Accuracy: 0.6526422764227642\n",
      "Step: 1230, Loss: 1.2034846544265747, Accuracy: 0.6527213647441105\n",
      "Step: 1231, Loss: 1.2447422742843628, Accuracy: 0.652732683982684\n",
      "Step: 1232, Loss: 1.1223986148834229, Accuracy: 0.6528791565287916\n",
      "Step: 1233, Loss: 1.263397455215454, Accuracy: 0.6528227984873042\n",
      "Step: 1234, Loss: 1.018865942955017, Accuracy: 0.6529689608636977\n",
      "Step: 1235, Loss: 1.0979903936386108, Accuracy: 0.6531148867313916\n",
      "Step: 1236, Loss: 1.2374693155288696, Accuracy: 0.6531258420910806\n",
      "Step: 1237, Loss: 1.0976976156234741, Accuracy: 0.6532040926225094\n",
      "Step: 1238, Loss: 1.2793372869491577, Accuracy: 0.6532149582997041\n",
      "Step: 1239, Loss: 1.2601046562194824, Accuracy: 0.6531586021505377\n",
      "Step: 1240, Loss: 1.1159917116165161, Accuracy: 0.6532366371206016\n",
      "Step: 1241, Loss: 1.2356966733932495, Accuracy: 0.6532474503488996\n",
      "Step: 1242, Loss: 1.1154719591140747, Accuracy: 0.6533923303834809\n",
      "Step: 1243, Loss: 1.53248929977417, Accuracy: 0.6531350482315113\n",
      "Step: 1244, Loss: 1.3700261116027832, Accuracy: 0.653012048192771\n",
      "Step: 1245, Loss: 1.3016412258148193, Accuracy: 0.6530230069555912\n",
      "Step: 1246, Loss: 1.2697023153305054, Accuracy: 0.6529671210906175\n",
      "Step: 1247, Loss: 1.1295783519744873, Accuracy: 0.6530448717948718\n",
      "Step: 1248, Loss: 1.2601767778396606, Accuracy: 0.6529890579129971\n",
      "Step: 1249, Loss: 1.1189286708831787, Accuracy: 0.6531333333333333\n",
      "Step: 1250, Loss: 1.425960898399353, Accuracy: 0.6529443112176925\n",
      "Step: 1251, Loss: 1.258353590965271, Accuracy: 0.6529552715654952\n",
      "Step: 1252, Loss: 1.3320777416229248, Accuracy: 0.6528997073689811\n",
      "Step: 1253, Loss: 1.1401972770690918, Accuracy: 0.6529771398192451\n",
      "Step: 1254, Loss: 1.1137319803237915, Accuracy: 0.653120849933599\n",
      "Step: 1255, Loss: 1.4109593629837036, Accuracy: 0.6529325902335457\n",
      "Step: 1256, Loss: 1.1274341344833374, Accuracy: 0.6530761071333864\n",
      "Step: 1257, Loss: 1.1736416816711426, Accuracy: 0.6531531531531531\n",
      "Step: 1258, Loss: 1.1678930521011353, Accuracy: 0.6532300767805136\n",
      "Step: 1259, Loss: 1.1725707054138184, Accuracy: 0.6533068783068783\n",
      "Step: 1260, Loss: 1.1180394887924194, Accuracy: 0.6533835580227333\n",
      "Step: 1261, Loss: 1.3276222944259644, Accuracy: 0.6533280507131537\n",
      "Step: 1262, Loss: 1.3615092039108276, Accuracy: 0.6532066508313539\n",
      "Step: 1263, Loss: 1.091776967048645, Accuracy: 0.6533491561181435\n",
      "Step: 1264, Loss: 1.3375681638717651, Accuracy: 0.6532938076416337\n",
      "Step: 1265, Loss: 1.206657886505127, Accuracy: 0.6533043707214323\n",
      "Step: 1266, Loss: 0.9935817718505859, Accuracy: 0.6535122336227308\n",
      "Step: 1267, Loss: 1.3860689401626587, Accuracy: 0.6533911671924291\n",
      "Step: 1268, Loss: 1.1050949096679688, Accuracy: 0.6535329655897032\n",
      "Step: 1269, Loss: 1.3626493215560913, Accuracy: 0.6534120734908136\n",
      "Step: 1270, Loss: 1.2398501634597778, Accuracy: 0.6534225019669552\n",
      "Step: 1271, Loss: 1.3538918495178223, Accuracy: 0.6533018867924528\n",
      "Step: 1272, Loss: 1.1680775880813599, Accuracy: 0.6533778476040848\n",
      "Step: 1273, Loss: 1.25802481174469, Accuracy: 0.6533882783882784\n",
      "Step: 1274, Loss: 1.3478680849075317, Accuracy: 0.6533333333333333\n",
      "Step: 1275, Loss: 1.3252946138381958, Accuracy: 0.6532784743991641\n",
      "Step: 1276, Loss: 1.5087661743164062, Accuracy: 0.6530279300443749\n",
      "Step: 1277, Loss: 1.207184076309204, Accuracy: 0.6531038080333855\n",
      "Step: 1278, Loss: 1.3648165464401245, Accuracy: 0.6529841021631483\n",
      "Step: 1279, Loss: 1.2698227167129517, Accuracy: 0.6529947916666666\n",
      "Step: 1280, Loss: 1.3156508207321167, Accuracy: 0.6529404111371324\n",
      "Step: 1281, Loss: 1.0988513231277466, Accuracy: 0.6530811232449298\n",
      "Step: 1282, Loss: 1.1162718534469604, Accuracy: 0.6531566640685893\n",
      "Step: 1283, Loss: 1.3373684883117676, Accuracy: 0.6530373831775701\n",
      "Step: 1284, Loss: 1.0352979898452759, Accuracy: 0.653242542153048\n",
      "Step: 1285, Loss: 1.2427880764007568, Accuracy: 0.6532529808190772\n",
      "Step: 1286, Loss: 1.2696987390518188, Accuracy: 0.6532634032634033\n",
      "Step: 1287, Loss: 1.4210700988769531, Accuracy: 0.6530797101449275\n",
      "Step: 1288, Loss: 1.4420338869094849, Accuracy: 0.6528963020429274\n",
      "Step: 1289, Loss: 1.0880705118179321, Accuracy: 0.6530361757105944\n",
      "Step: 1290, Loss: 1.225511074066162, Accuracy: 0.6530467337980893\n",
      "Step: 1291, Loss: 1.4599108695983887, Accuracy: 0.6528637770897833\n",
      "Step: 1292, Loss: 1.089134693145752, Accuracy: 0.6530033513792215\n",
      "Step: 1293, Loss: 1.2224332094192505, Accuracy: 0.6530139103554868\n",
      "Step: 1294, Loss: 1.2586479187011719, Accuracy: 0.652960102960103\n",
      "Step: 1295, Loss: 1.0941846370697021, Accuracy: 0.653099279835391\n",
      "Step: 1296, Loss: 1.3495279550552368, Accuracy: 0.6530454895913647\n",
      "Step: 1297, Loss: 1.3195117712020874, Accuracy: 0.6529917822290704\n",
      "Step: 1298, Loss: 1.2818961143493652, Accuracy: 0.6529381575570952\n",
      "Step: 1299, Loss: 1.0566750764846802, Accuracy: 0.6530769230769231\n",
      "Step: 1300, Loss: 1.09433114528656, Accuracy: 0.6532154752754291\n",
      "Step: 1301, Loss: 1.4485441446304321, Accuracy: 0.6530337941628265\n",
      "Step: 1302, Loss: 1.0935062170028687, Accuracy: 0.6531082118188795\n",
      "Step: 1303, Loss: 1.075713038444519, Accuracy: 0.6532464212678937\n",
      "Step: 1304, Loss: 1.287044644355774, Accuracy: 0.6531928480204342\n",
      "Step: 1305, Loss: 1.2456119060516357, Accuracy: 0.6532031648800408\n",
      "Step: 1306, Loss: 1.218347191810608, Accuracy: 0.6532134659525631\n",
      "Step: 1307, Loss: 1.2334216833114624, Accuracy: 0.65322375127421\n",
      "Step: 1308, Loss: 1.280563235282898, Accuracy: 0.653170359052712\n",
      "Step: 1309, Loss: 1.1072444915771484, Accuracy: 0.6533715012722646\n",
      "Step: 1310, Loss: 1.177777886390686, Accuracy: 0.6534452072209509\n",
      "Step: 1311, Loss: 1.4617177248001099, Accuracy: 0.6532647357723578\n",
      "Step: 1312, Loss: 1.6086840629577637, Accuracy: 0.652957603452653\n",
      "Step: 1313, Loss: 0.9933919310569763, Accuracy: 0.6532217148655505\n",
      "Step: 1314, Loss: 1.1634553670883179, Accuracy: 0.6532953105196451\n",
      "Step: 1315, Loss: 1.0793622732162476, Accuracy: 0.6534321175278622\n",
      "Step: 1316, Loss: 1.0849040746688843, Accuracy: 0.6535687167805618\n",
      "Step: 1317, Loss: 1.0943597555160522, Accuracy: 0.6537051087506323\n",
      "Step: 1318, Loss: 1.3149816989898682, Accuracy: 0.6536517563810967\n",
      "Step: 1319, Loss: 1.2366154193878174, Accuracy: 0.6536616161616161\n",
      "Step: 1320, Loss: 1.2714923620224, Accuracy: 0.653671461014383\n",
      "Step: 1321, Loss: 1.1160249710083008, Accuracy: 0.6537443267776096\n",
      "Step: 1322, Loss: 1.1682592630386353, Accuracy: 0.653817082388511\n",
      "Step: 1323, Loss: 1.3646734952926636, Accuracy: 0.6537638469284995\n",
      "Step: 1324, Loss: 1.3595390319824219, Accuracy: 0.6537106918238994\n",
      "Step: 1325, Loss: 1.1102946996688843, Accuracy: 0.6538461538461539\n",
      "Step: 1326, Loss: 1.1943178176879883, Accuracy: 0.6539186134137152\n",
      "Step: 1327, Loss: 1.4536099433898926, Accuracy: 0.6537399598393574\n",
      "Step: 1328, Loss: 1.226216197013855, Accuracy: 0.6537496864810635\n",
      "Step: 1329, Loss: 1.272897720336914, Accuracy: 0.6537593984962407\n",
      "Step: 1330, Loss: 1.0314626693725586, Accuracy: 0.6539569246180816\n",
      "Step: 1331, Loss: 1.0803042650222778, Accuracy: 0.6540915915915916\n",
      "Step: 1332, Loss: 1.2567821741104126, Accuracy: 0.6540385096274068\n",
      "Step: 1333, Loss: 1.0974061489105225, Accuracy: 0.6541729135432284\n",
      "Step: 1334, Loss: 1.2459564208984375, Accuracy: 0.6541198501872659\n",
      "Step: 1335, Loss: 1.265128254890442, Accuracy: 0.654129241516966\n",
      "Step: 1336, Loss: 1.3774242401123047, Accuracy: 0.6540139616055847\n",
      "Step: 1337, Loss: 1.271782636642456, Accuracy: 0.6539611360239163\n",
      "Step: 1338, Loss: 1.0093482732772827, Accuracy: 0.6541573313417973\n",
      "Step: 1339, Loss: 1.2556992769241333, Accuracy: 0.6541666666666667\n",
      "Step: 1340, Loss: 1.405288815498352, Accuracy: 0.6540517027094208\n",
      "Step: 1341, Loss: 1.356324315071106, Accuracy: 0.6539369100844511\n",
      "Step: 1342, Loss: 1.0102256536483765, Accuracy: 0.654132539091586\n",
      "Step: 1343, Loss: 1.1529051065444946, Accuracy: 0.6542038690476191\n",
      "Step: 1344, Loss: 1.2640079259872437, Accuracy: 0.6541511771995043\n",
      "Step: 1345, Loss: 1.4154926538467407, Accuracy: 0.6540366518078257\n",
      "Step: 1346, Loss: 1.1248396635055542, Accuracy: 0.6541697599604058\n",
      "Step: 1347, Loss: 1.4847239255905151, Accuracy: 0.6539317507418397\n",
      "Step: 1348, Loss: 1.1269922256469727, Accuracy: 0.6540029651593773\n",
      "Step: 1349, Loss: 1.1995011568069458, Accuracy: 0.6540123456790123\n",
      "Step: 1350, Loss: 1.2261399030685425, Accuracy: 0.6540217123118678\n",
      "Step: 1351, Loss: 1.290408730506897, Accuracy: 0.6539694280078896\n",
      "Step: 1352, Loss: 1.2380317449569702, Accuracy: 0.6539788125153979\n",
      "Step: 1353, Loss: 1.317440152168274, Accuracy: 0.6539266371245692\n",
      "Step: 1354, Loss: 0.9620437026023865, Accuracy: 0.6541820418204182\n",
      "Step: 1355, Loss: 1.1515612602233887, Accuracy: 0.6542527040314651\n",
      "Step: 1356, Loss: 1.2787038087844849, Accuracy: 0.6542004421518055\n",
      "Step: 1357, Loss: 1.2029855251312256, Accuracy: 0.6542096219931272\n",
      "Step: 1358, Loss: 1.2443703413009644, Accuracy: 0.6542187883247486\n",
      "Step: 1359, Loss: 1.1767464876174927, Accuracy: 0.6542892156862745\n",
      "Step: 1360, Loss: 1.1696304082870483, Accuracy: 0.6543595395542493\n",
      "Step: 1361, Loss: 1.3129485845565796, Accuracy: 0.654307391091532\n",
      "Step: 1362, Loss: 0.9888685345649719, Accuracy: 0.6544998777207142\n",
      "Step: 1363, Loss: 1.4686843156814575, Accuracy: 0.6543255131964809\n",
      "Step: 1364, Loss: 1.049515724182129, Accuracy: 0.6545177045177045\n",
      "Step: 1365, Loss: 1.1899714469909668, Accuracy: 0.654526598340654\n",
      "Step: 1366, Loss: 1.1514662504196167, Accuracy: 0.6545964398927091\n",
      "Step: 1367, Loss: 1.5243240594863892, Accuracy: 0.6543615984405458\n",
      "Step: 1368, Loss: 1.2210668325424194, Accuracy: 0.6543705868030192\n",
      "Step: 1369, Loss: 1.1884108781814575, Accuracy: 0.6544403892944038\n",
      "Step: 1370, Loss: 1.3649133443832397, Accuracy: 0.6543277413080476\n",
      "Step: 1371, Loss: 1.4013739824295044, Accuracy: 0.654215257531584\n",
      "Step: 1372, Loss: 1.5146499872207642, Accuracy: 0.6539815489196407\n",
      "Step: 1373, Loss: 1.0960379838943481, Accuracy: 0.6541120815138283\n",
      "Step: 1374, Loss: 1.0907392501831055, Accuracy: 0.6542424242424243\n",
      "Step: 1375, Loss: 1.4270009994506836, Accuracy: 0.6540697674418605\n",
      "Step: 1376, Loss: 1.2612966299057007, Accuracy: 0.6540789155168241\n",
      "Step: 1377, Loss: 1.1769318580627441, Accuracy: 0.6541485244315433\n",
      "Step: 1378, Loss: 1.1545718908309937, Accuracy: 0.6542180323906213\n",
      "Step: 1379, Loss: 1.2370442152023315, Accuracy: 0.6542270531400967\n",
      "Step: 1380, Loss: 1.133724570274353, Accuracy: 0.6542964035722906\n",
      "Step: 1381, Loss: 1.2926312685012817, Accuracy: 0.6542450554751568\n",
      "Step: 1382, Loss: 1.0269736051559448, Accuracy: 0.6544348035671246\n",
      "Step: 1383, Loss: 1.0302515029907227, Accuracy: 0.6546242774566474\n",
      "Step: 1384, Loss: 1.1520651578903198, Accuracy: 0.6546931407942238\n",
      "Step: 1385, Loss: 1.2353841066360474, Accuracy: 0.6547017797017797\n",
      "Step: 1386, Loss: 1.4611886739730835, Accuracy: 0.6545301610189859\n",
      "Step: 1387, Loss: 1.1024314165115356, Accuracy: 0.6546589817483189\n",
      "Step: 1388, Loss: 0.9302975535392761, Accuracy: 0.6549076073914087\n",
      "Step: 1389, Loss: 1.2510650157928467, Accuracy: 0.6548561151079136\n",
      "Step: 1390, Loss: 1.3881067037582397, Accuracy: 0.654744787922358\n",
      "Step: 1391, Loss: 1.0844172239303589, Accuracy: 0.6548730842911877\n",
      "Step: 1392, Loss: 1.2221132516860962, Accuracy: 0.6548815506101938\n",
      "Step: 1393, Loss: 1.379507064819336, Accuracy: 0.6547704447632712\n",
      "Step: 1394, Loss: 1.3330341577529907, Accuracy: 0.6546594982078853\n",
      "Step: 1395, Loss: 1.148591160774231, Accuracy: 0.6547277936962751\n",
      "Step: 1396, Loss: 1.1980832815170288, Accuracy: 0.6547959914101646\n",
      "Step: 1397, Loss: 1.237030267715454, Accuracy: 0.6548044825941821\n",
      "Step: 1398, Loss: 1.392738938331604, Accuracy: 0.6546938289254229\n",
      "Step: 1399, Loss: 1.1734192371368408, Accuracy: 0.6547619047619048\n",
      "Step: 1400, Loss: 1.1405221223831177, Accuracy: 0.6548298834166072\n",
      "Step: 1401, Loss: 1.3065922260284424, Accuracy: 0.6547788873038516\n",
      "Step: 1402, Loss: 1.2196422815322876, Accuracy: 0.6547873604181516\n",
      "Step: 1403, Loss: 1.2124234437942505, Accuracy: 0.654855175688509\n",
      "Step: 1404, Loss: 1.2776321172714233, Accuracy: 0.6548042704626335\n",
      "Step: 1405, Loss: 1.2830183506011963, Accuracy: 0.6547534376481745\n",
      "Step: 1406, Loss: 1.1374925374984741, Accuracy: 0.6548211324330727\n",
      "Step: 1407, Loss: 1.2430038452148438, Accuracy: 0.6548295454545454\n",
      "Step: 1408, Loss: 1.3244603872299194, Accuracy: 0.6547196593328601\n",
      "Step: 1409, Loss: 1.1725646257400513, Accuracy: 0.6547872340425532\n",
      "Step: 1410, Loss: 1.3891183137893677, Accuracy: 0.654677533664068\n",
      "Step: 1411, Loss: 1.3711875677108765, Accuracy: 0.6545679886685553\n",
      "Step: 1412, Loss: 1.0731955766677856, Accuracy: 0.6546945034206181\n",
      "Step: 1413, Loss: 1.2875036001205444, Accuracy: 0.6547029702970297\n",
      "Step: 1414, Loss: 1.262050747871399, Accuracy: 0.6546525323910483\n",
      "Step: 1415, Loss: 1.1246665716171265, Accuracy: 0.654719868173258\n",
      "Step: 1416, Loss: 1.2543888092041016, Accuracy: 0.6547282992237121\n",
      "Step: 1417, Loss: 1.1288071870803833, Accuracy: 0.6547954866008463\n",
      "Step: 1418, Loss: 1.4020768404006958, Accuracy: 0.6546863988724454\n",
      "Step: 1419, Loss: 1.2647902965545654, Accuracy: 0.6546948356807512\n",
      "Step: 1420, Loss: 1.3482438325881958, Accuracy: 0.6545859723199625\n",
      "Step: 1421, Loss: 1.4009686708450317, Accuracy: 0.6544772620721988\n",
      "Step: 1422, Loss: 1.3497315645217896, Accuracy: 0.6543687046146639\n",
      "Step: 1423, Loss: 1.190177083015442, Accuracy: 0.654435861423221\n",
      "Step: 1424, Loss: 1.4185943603515625, Accuracy: 0.654327485380117\n",
      "Step: 1425, Loss: 1.1438640356063843, Accuracy: 0.65445301542777\n",
      "Step: 1426, Loss: 1.4063669443130493, Accuracy: 0.6541695865451997\n",
      "Epoch: 2, Val_Accuracy: 0.24548286604361372\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cb9758b582a410d8763c3c7b77789e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1427 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0, Loss: 1.4626669883728027, Accuracy: 0.4166666666666667\n",
      "Step: 1, Loss: 1.1571117639541626, Accuracy: 0.5833333333333334\n",
      "Step: 2, Loss: 1.3392633199691772, Accuracy: 0.5833333333333334\n",
      "Step: 3, Loss: 1.2823463678359985, Accuracy: 0.5833333333333334\n",
      "Step: 4, Loss: 1.2998541593551636, Accuracy: 0.5833333333333334\n",
      "Step: 5, Loss: 1.0719369649887085, Accuracy: 0.625\n",
      "Step: 6, Loss: 1.1058017015457153, Accuracy: 0.6547619047619048\n",
      "Step: 7, Loss: 1.2479246854782104, Accuracy: 0.65625\n",
      "Step: 8, Loss: 1.1681421995162964, Accuracy: 0.6574074074074074\n",
      "Step: 9, Loss: 1.2543463706970215, Accuracy: 0.6583333333333333\n",
      "Step: 10, Loss: 1.1273103952407837, Accuracy: 0.6666666666666666\n",
      "Step: 11, Loss: 1.386893391609192, Accuracy: 0.6527777777777778\n",
      "Step: 12, Loss: 1.130466103553772, Accuracy: 0.6602564102564102\n",
      "Step: 13, Loss: 1.295440912246704, Accuracy: 0.6547619047619048\n",
      "Step: 14, Loss: 1.171133041381836, Accuracy: 0.6611111111111111\n",
      "Step: 15, Loss: 1.0983192920684814, Accuracy: 0.671875\n",
      "Step: 16, Loss: 1.1593096256256104, Accuracy: 0.6764705882352942\n",
      "Step: 17, Loss: 1.1504294872283936, Accuracy: 0.6805555555555556\n",
      "Step: 18, Loss: 1.4123026132583618, Accuracy: 0.6666666666666666\n",
      "Step: 19, Loss: 1.2669389247894287, Accuracy: 0.6666666666666666\n",
      "Step: 20, Loss: 1.0895441770553589, Accuracy: 0.6746031746031746\n",
      "Step: 21, Loss: 1.2492094039916992, Accuracy: 0.6742424242424242\n",
      "Step: 22, Loss: 1.0777018070220947, Accuracy: 0.6811594202898551\n",
      "Step: 23, Loss: 1.2616132497787476, Accuracy: 0.6805555555555556\n",
      "Step: 24, Loss: 1.1644753217697144, Accuracy: 0.6833333333333333\n",
      "Step: 25, Loss: 1.341884970664978, Accuracy: 0.6794871794871795\n",
      "Step: 26, Loss: 1.3489183187484741, Accuracy: 0.6728395061728395\n",
      "Step: 27, Loss: 0.9575292468070984, Accuracy: 0.6845238095238095\n",
      "Step: 28, Loss: 1.2027486562728882, Accuracy: 0.6839080459770115\n",
      "Step: 29, Loss: 1.2340010404586792, Accuracy: 0.6833333333333333\n",
      "Step: 30, Loss: 1.1539860963821411, Accuracy: 0.6854838709677419\n",
      "Step: 31, Loss: 1.203639268875122, Accuracy: 0.6848958333333334\n",
      "Step: 32, Loss: 1.1002346277236938, Accuracy: 0.6893939393939394\n",
      "Step: 33, Loss: 1.1323643922805786, Accuracy: 0.6911764705882353\n",
      "Step: 34, Loss: 1.0463716983795166, Accuracy: 0.6976190476190476\n",
      "Step: 35, Loss: 1.2531747817993164, Accuracy: 0.6967592592592593\n",
      "Step: 36, Loss: 1.205668568611145, Accuracy: 0.6959459459459459\n",
      "Step: 37, Loss: 1.1411762237548828, Accuracy: 0.6973684210526315\n",
      "Step: 38, Loss: 1.5454192161560059, Accuracy: 0.688034188034188\n",
      "Step: 39, Loss: 1.155982255935669, Accuracy: 0.6895833333333333\n",
      "Step: 40, Loss: 1.2127960920333862, Accuracy: 0.6890243902439024\n",
      "Step: 41, Loss: 1.228342056274414, Accuracy: 0.6904761904761905\n",
      "Step: 42, Loss: 1.318280577659607, Accuracy: 0.687984496124031\n",
      "Step: 43, Loss: 1.2734293937683105, Accuracy: 0.6856060606060606\n",
      "Step: 44, Loss: 1.1869171857833862, Accuracy: 0.6851851851851852\n",
      "Step: 45, Loss: 1.4775753021240234, Accuracy: 0.6793478260869565\n",
      "Step: 46, Loss: 1.1608484983444214, Accuracy: 0.6808510638297872\n",
      "Step: 47, Loss: 1.1581861972808838, Accuracy: 0.6822916666666666\n",
      "Step: 48, Loss: 1.0832825899124146, Accuracy: 0.685374149659864\n",
      "Step: 49, Loss: 1.1053179502487183, Accuracy: 0.6866666666666666\n",
      "Step: 50, Loss: 1.1422157287597656, Accuracy: 0.6879084967320261\n",
      "Step: 51, Loss: 1.1069183349609375, Accuracy: 0.6891025641025641\n",
      "Step: 52, Loss: 1.1483110189437866, Accuracy: 0.690251572327044\n",
      "Step: 53, Loss: 1.2026194334030151, Accuracy: 0.691358024691358\n",
      "Step: 54, Loss: 1.419286847114563, Accuracy: 0.6878787878787879\n",
      "Step: 55, Loss: 1.1748528480529785, Accuracy: 0.6875\n",
      "Step: 56, Loss: 1.1478042602539062, Accuracy: 0.6885964912280702\n",
      "Step: 57, Loss: 1.2865577936172485, Accuracy: 0.6882183908045977\n",
      "Step: 58, Loss: 1.1433225870132446, Accuracy: 0.6892655367231638\n",
      "Step: 59, Loss: 1.2396169900894165, Accuracy: 0.6888888888888889\n",
      "Step: 60, Loss: 1.327530860900879, Accuracy: 0.6871584699453552\n",
      "Step: 61, Loss: 1.106772780418396, Accuracy: 0.6895161290322581\n",
      "Step: 62, Loss: 1.0954612493515015, Accuracy: 0.6917989417989417\n",
      "Step: 63, Loss: 1.077929139137268, Accuracy: 0.6940104166666666\n",
      "Step: 64, Loss: 1.2820188999176025, Accuracy: 0.6935897435897436\n",
      "Step: 65, Loss: 1.1529113054275513, Accuracy: 0.6944444444444444\n",
      "Step: 66, Loss: 1.2381623983383179, Accuracy: 0.6940298507462687\n",
      "Step: 67, Loss: 1.2418965101242065, Accuracy: 0.6936274509803921\n",
      "Step: 68, Loss: 1.548363208770752, Accuracy: 0.6884057971014492\n",
      "Step: 69, Loss: 0.9800754189491272, Accuracy: 0.6916666666666667\n",
      "Step: 70, Loss: 1.1978763341903687, Accuracy: 0.6924882629107981\n",
      "Step: 71, Loss: 1.1435425281524658, Accuracy: 0.6932870370370371\n",
      "Step: 72, Loss: 1.2370580434799194, Accuracy: 0.6929223744292238\n",
      "Step: 73, Loss: 1.1071679592132568, Accuracy: 0.6948198198198198\n",
      "Step: 74, Loss: 1.1151437759399414, Accuracy: 0.6955555555555556\n",
      "Step: 75, Loss: 1.3894213438034058, Accuracy: 0.6929824561403509\n",
      "Step: 76, Loss: 1.2103675603866577, Accuracy: 0.6937229437229437\n",
      "Step: 77, Loss: 1.2308732271194458, Accuracy: 0.6933760683760684\n",
      "Step: 78, Loss: 1.5101279020309448, Accuracy: 0.6888185654008439\n",
      "Step: 79, Loss: 1.1983674764633179, Accuracy: 0.6885416666666667\n",
      "Step: 80, Loss: 1.138805866241455, Accuracy: 0.6893004115226338\n",
      "Step: 81, Loss: 1.3678370714187622, Accuracy: 0.6869918699186992\n",
      "Step: 82, Loss: 1.3349814414978027, Accuracy: 0.6857429718875502\n",
      "Step: 83, Loss: 1.3256186246871948, Accuracy: 0.6845238095238095\n",
      "Step: 84, Loss: 1.5182384252548218, Accuracy: 0.6803921568627451\n",
      "Step: 85, Loss: 1.1538491249084473, Accuracy: 0.6812015503875969\n",
      "Step: 86, Loss: 1.204482078552246, Accuracy: 0.6810344827586207\n",
      "Step: 87, Loss: 1.2004177570343018, Accuracy: 0.6808712121212122\n",
      "Step: 88, Loss: 1.2801636457443237, Accuracy: 0.6797752808988764\n",
      "Step: 89, Loss: 1.3177669048309326, Accuracy: 0.6787037037037037\n",
      "Step: 90, Loss: 1.2043309211730957, Accuracy: 0.6785714285714286\n",
      "Step: 91, Loss: 1.5183850526809692, Accuracy: 0.6757246376811594\n",
      "Step: 92, Loss: 1.4697884321212769, Accuracy: 0.6729390681003584\n",
      "Step: 93, Loss: 1.3088936805725098, Accuracy: 0.6719858156028369\n",
      "Step: 94, Loss: 1.2938600778579712, Accuracy: 0.6710526315789473\n",
      "Step: 95, Loss: 1.2841717004776, Accuracy: 0.6710069444444444\n",
      "Step: 96, Loss: 1.017505168914795, Accuracy: 0.6735395189003437\n",
      "Step: 97, Loss: 1.1741598844528198, Accuracy: 0.673469387755102\n",
      "Step: 98, Loss: 1.4060049057006836, Accuracy: 0.6717171717171717\n",
      "Step: 99, Loss: 1.3466957807540894, Accuracy: 0.67\n",
      "Step: 100, Loss: 1.4553923606872559, Accuracy: 0.6674917491749175\n",
      "Step: 101, Loss: 1.3238732814788818, Accuracy: 0.6666666666666666\n",
      "Step: 102, Loss: 1.2444474697113037, Accuracy: 0.6666666666666666\n",
      "Step: 103, Loss: 1.3566393852233887, Accuracy: 0.6650641025641025\n",
      "Step: 104, Loss: 1.1440796852111816, Accuracy: 0.6658730158730158\n",
      "Step: 105, Loss: 1.0770190954208374, Accuracy: 0.6674528301886793\n",
      "Step: 106, Loss: 1.2460205554962158, Accuracy: 0.6674454828660437\n",
      "Step: 107, Loss: 1.3662065267562866, Accuracy: 0.6666666666666666\n",
      "Step: 108, Loss: 1.32608163356781, Accuracy: 0.6659021406727829\n",
      "Step: 109, Loss: 1.1081843376159668, Accuracy: 0.6674242424242425\n",
      "Step: 110, Loss: 1.2877691984176636, Accuracy: 0.6666666666666666\n",
      "Step: 111, Loss: 1.2953180074691772, Accuracy: 0.6659226190476191\n",
      "Step: 112, Loss: 1.021641731262207, Accuracy: 0.668141592920354\n",
      "Step: 113, Loss: 1.1175140142440796, Accuracy: 0.668859649122807\n",
      "Step: 114, Loss: 1.2409130334854126, Accuracy: 0.6688405797101449\n",
      "Step: 115, Loss: 1.318198323249817, Accuracy: 0.6681034482758621\n",
      "Step: 116, Loss: 1.3614240884780884, Accuracy: 0.6666666666666666\n",
      "Step: 117, Loss: 1.578024983406067, Accuracy: 0.6638418079096046\n",
      "Step: 118, Loss: 1.1470478773117065, Accuracy: 0.6645658263305322\n",
      "Step: 119, Loss: 1.1649397611618042, Accuracy: 0.6652777777777777\n",
      "Step: 120, Loss: 1.1430238485336304, Accuracy: 0.6659779614325069\n",
      "Step: 121, Loss: 1.4890828132629395, Accuracy: 0.6639344262295082\n",
      "Step: 122, Loss: 1.2633723020553589, Accuracy: 0.6639566395663956\n",
      "Step: 123, Loss: 1.101012110710144, Accuracy: 0.6653225806451613\n",
      "Step: 124, Loss: 1.1028962135314941, Accuracy: 0.6666666666666666\n",
      "Step: 125, Loss: 1.2287238836288452, Accuracy: 0.6666666666666666\n",
      "Step: 126, Loss: 1.387593388557434, Accuracy: 0.6653543307086615\n",
      "Step: 127, Loss: 1.1977028846740723, Accuracy: 0.6653645833333334\n",
      "Step: 128, Loss: 1.1282938718795776, Accuracy: 0.6666666666666666\n",
      "Step: 129, Loss: 1.003015398979187, Accuracy: 0.6685897435897435\n",
      "Step: 130, Loss: 1.0731576681137085, Accuracy: 0.6698473282442748\n",
      "Step: 131, Loss: 1.2817937135696411, Accuracy: 0.6691919191919192\n",
      "Step: 132, Loss: 1.1942113637924194, Accuracy: 0.6697994987468672\n",
      "Step: 133, Loss: 1.3488430976867676, Accuracy: 0.6691542288557214\n",
      "Step: 134, Loss: 1.2604960203170776, Accuracy: 0.6691358024691358\n",
      "Step: 135, Loss: 1.215816855430603, Accuracy: 0.6691176470588235\n",
      "Step: 136, Loss: 1.3812074661254883, Accuracy: 0.6678832116788321\n",
      "Step: 137, Loss: 1.1149812936782837, Accuracy: 0.6690821256038647\n",
      "Step: 138, Loss: 1.1452289819717407, Accuracy: 0.669664268585132\n",
      "Step: 139, Loss: 1.1736990213394165, Accuracy: 0.6702380952380952\n",
      "Step: 140, Loss: 1.074314832687378, Accuracy: 0.6713947990543735\n",
      "Step: 141, Loss: 1.3001229763031006, Accuracy: 0.670774647887324\n",
      "Step: 142, Loss: 1.335858702659607, Accuracy: 0.6701631701631702\n",
      "Step: 143, Loss: 1.3273658752441406, Accuracy: 0.6695601851851852\n",
      "Step: 144, Loss: 1.2256110906600952, Accuracy: 0.6695402298850575\n",
      "Step: 145, Loss: 1.3247441053390503, Accuracy: 0.66837899543379\n",
      "Step: 146, Loss: 1.1176055669784546, Accuracy: 0.6689342403628118\n",
      "Step: 147, Loss: 1.0635812282562256, Accuracy: 0.670045045045045\n",
      "Step: 148, Loss: 1.2363358736038208, Accuracy: 0.6700223713646533\n",
      "Step: 149, Loss: 1.198477864265442, Accuracy: 0.67\n",
      "Step: 150, Loss: 1.3093208074569702, Accuracy: 0.6694260485651214\n",
      "Step: 151, Loss: 1.1852327585220337, Accuracy: 0.6699561403508771\n",
      "Step: 152, Loss: 1.3353732824325562, Accuracy: 0.6693899782135077\n",
      "Step: 153, Loss: 1.1189030408859253, Accuracy: 0.6699134199134199\n",
      "Step: 154, Loss: 1.2311418056488037, Accuracy: 0.6698924731182796\n",
      "Step: 155, Loss: 1.2859925031661987, Accuracy: 0.6693376068376068\n",
      "Step: 156, Loss: 1.2579656839370728, Accuracy: 0.6687898089171974\n",
      "Step: 157, Loss: 1.0661596059799194, Accuracy: 0.669831223628692\n",
      "Step: 158, Loss: 1.2708905935287476, Accuracy: 0.6692872117400419\n",
      "Step: 159, Loss: 1.1341407299041748, Accuracy: 0.6697916666666667\n",
      "Step: 160, Loss: 1.3881406784057617, Accuracy: 0.6682194616977226\n",
      "Step: 161, Loss: 1.070259928703308, Accuracy: 0.6692386831275721\n",
      "Step: 162, Loss: 1.352401852607727, Accuracy: 0.668200408997955\n",
      "Step: 163, Loss: 1.435482382774353, Accuracy: 0.6671747967479674\n",
      "Step: 164, Loss: 1.3477602005004883, Accuracy: 0.6666666666666666\n",
      "Step: 165, Loss: 1.1747093200683594, Accuracy: 0.6666666666666666\n",
      "Step: 166, Loss: 1.3970541954040527, Accuracy: 0.6656686626746507\n",
      "Step: 167, Loss: 1.2121397256851196, Accuracy: 0.6656746031746031\n",
      "Step: 168, Loss: 1.242300271987915, Accuracy: 0.665680473372781\n",
      "Step: 169, Loss: 1.25676691532135, Accuracy: 0.6656862745098039\n",
      "Step: 170, Loss: 1.2518807649612427, Accuracy: 0.665692007797271\n",
      "Step: 171, Loss: 1.140432596206665, Accuracy: 0.6661821705426356\n",
      "Step: 172, Loss: 1.0830775499343872, Accuracy: 0.6671483622350675\n",
      "Step: 173, Loss: 1.2504407167434692, Accuracy: 0.6666666666666666\n",
      "Step: 174, Loss: 1.1298784017562866, Accuracy: 0.6671428571428571\n",
      "Step: 175, Loss: 1.2214633226394653, Accuracy: 0.6671401515151515\n",
      "Step: 176, Loss: 1.1275850534439087, Accuracy: 0.6680790960451978\n",
      "Step: 177, Loss: 1.3004462718963623, Accuracy: 0.6676029962546817\n",
      "Step: 178, Loss: 1.0669435262680054, Accuracy: 0.6685288640595903\n",
      "Step: 179, Loss: 1.4156070947647095, Accuracy: 0.6675925925925926\n",
      "Step: 180, Loss: 1.1961990594863892, Accuracy: 0.66804788213628\n",
      "Step: 181, Loss: 1.0917582511901855, Accuracy: 0.6684981684981685\n",
      "Step: 182, Loss: 1.1750982999801636, Accuracy: 0.668943533697632\n",
      "Step: 183, Loss: 1.1036055088043213, Accuracy: 0.6698369565217391\n",
      "Step: 184, Loss: 1.211151361465454, Accuracy: 0.6698198198198199\n",
      "Step: 185, Loss: 1.1405941247940063, Accuracy: 0.6702508960573477\n",
      "Step: 186, Loss: 1.1749783754348755, Accuracy: 0.6706773618538324\n",
      "Step: 187, Loss: 1.2167855501174927, Accuracy: 0.6706560283687943\n",
      "Step: 188, Loss: 1.3746670484542847, Accuracy: 0.6697530864197531\n",
      "Step: 189, Loss: 1.1143802404403687, Accuracy: 0.6701754385964912\n",
      "Step: 190, Loss: 1.3035691976547241, Accuracy: 0.6701570680628273\n",
      "Step: 191, Loss: 1.203134298324585, Accuracy: 0.6705729166666666\n",
      "Step: 192, Loss: 1.0040024518966675, Accuracy: 0.6718480138169257\n",
      "Step: 193, Loss: 1.273003101348877, Accuracy: 0.6718213058419243\n",
      "Step: 194, Loss: 1.4159425497055054, Accuracy: 0.6709401709401709\n",
      "Step: 195, Loss: 1.2835137844085693, Accuracy: 0.6709183673469388\n",
      "Step: 196, Loss: 1.0800992250442505, Accuracy: 0.6717428087986463\n",
      "Step: 197, Loss: 1.2890385389328003, Accuracy: 0.6717171717171717\n",
      "Step: 198, Loss: 1.200203776359558, Accuracy: 0.6721105527638191\n",
      "Step: 199, Loss: 1.130711555480957, Accuracy: 0.6725\n",
      "Step: 200, Loss: 1.1524959802627563, Accuracy: 0.6728855721393034\n",
      "Step: 201, Loss: 1.1459684371948242, Accuracy: 0.6732673267326733\n",
      "Step: 202, Loss: 1.2355388402938843, Accuracy: 0.6732348111658456\n",
      "Step: 203, Loss: 1.078163743019104, Accuracy: 0.6740196078431373\n",
      "Step: 204, Loss: 1.233178734779358, Accuracy: 0.6739837398373983\n",
      "Step: 205, Loss: 1.1599156856536865, Accuracy: 0.6743527508090615\n",
      "Step: 206, Loss: 1.2630902528762817, Accuracy: 0.6739130434782609\n",
      "Step: 207, Loss: 1.2209798097610474, Accuracy: 0.6738782051282052\n",
      "Step: 208, Loss: 1.382120132446289, Accuracy: 0.6730462519936204\n",
      "Step: 209, Loss: 1.1727079153060913, Accuracy: 0.6734126984126985\n",
      "Step: 210, Loss: 1.312381625175476, Accuracy: 0.6729857819905213\n",
      "Step: 211, Loss: 1.1872020959854126, Accuracy: 0.6729559748427673\n",
      "Step: 212, Loss: 1.2855061292648315, Accuracy: 0.6725352112676056\n",
      "Step: 213, Loss: 1.2289761304855347, Accuracy: 0.6725077881619937\n",
      "Step: 214, Loss: 1.1957931518554688, Accuracy: 0.6728682170542636\n",
      "Step: 215, Loss: 1.5396183729171753, Accuracy: 0.6716820987654321\n",
      "Step: 216, Loss: 1.1187820434570312, Accuracy: 0.6724270353302612\n",
      "Step: 217, Loss: 1.0699070692062378, Accuracy: 0.6731651376146789\n",
      "Step: 218, Loss: 1.1980265378952026, Accuracy: 0.6735159817351598\n",
      "Step: 219, Loss: 1.389391303062439, Accuracy: 0.6727272727272727\n",
      "Step: 220, Loss: 1.1848187446594238, Accuracy: 0.6730769230769231\n",
      "Step: 221, Loss: 1.3117915391921997, Accuracy: 0.6722972972972973\n",
      "Step: 222, Loss: 1.221673607826233, Accuracy: 0.672272047832586\n",
      "Step: 223, Loss: 1.317318320274353, Accuracy: 0.671875\n",
      "Step: 224, Loss: 1.0878154039382935, Accuracy: 0.6725925925925926\n",
      "Step: 225, Loss: 1.0300129652023315, Accuracy: 0.6733038348082596\n",
      "Step: 226, Loss: 1.3884371519088745, Accuracy: 0.6725403817914831\n",
      "Step: 227, Loss: 1.091753602027893, Accuracy: 0.6732456140350878\n",
      "Step: 228, Loss: 1.2777951955795288, Accuracy: 0.6728529839883551\n",
      "Step: 229, Loss: 1.4567450284957886, Accuracy: 0.6717391304347826\n",
      "Step: 230, Loss: 1.2130526304244995, Accuracy: 0.6717171717171717\n",
      "Step: 231, Loss: 1.273443579673767, Accuracy: 0.6713362068965517\n",
      "Step: 232, Loss: 1.469507098197937, Accuracy: 0.6702432045779685\n",
      "Step: 233, Loss: 1.1002681255340576, Accuracy: 0.6709401709401709\n",
      "Step: 234, Loss: 1.1698797941207886, Accuracy: 0.6709219858156028\n",
      "Step: 235, Loss: 0.9160065650939941, Accuracy: 0.672316384180791\n",
      "Step: 236, Loss: 1.3701695203781128, Accuracy: 0.6715893108298172\n",
      "Step: 237, Loss: 1.2305322885513306, Accuracy: 0.6715686274509803\n",
      "Step: 238, Loss: 1.1638127565383911, Accuracy: 0.6718967921896792\n",
      "Step: 239, Loss: 1.1949211359024048, Accuracy: 0.671875\n",
      "Step: 240, Loss: 1.3394488096237183, Accuracy: 0.6715076071922544\n",
      "Step: 241, Loss: 1.2457343339920044, Accuracy: 0.6714876033057852\n",
      "Step: 242, Loss: 1.2386032342910767, Accuracy: 0.6714677640603567\n",
      "Step: 243, Loss: 1.2592395544052124, Accuracy: 0.671448087431694\n",
      "Step: 244, Loss: 1.4188324213027954, Accuracy: 0.6704081632653062\n",
      "Step: 245, Loss: 1.208789587020874, Accuracy: 0.6703929539295393\n",
      "Step: 246, Loss: 1.1098862886428833, Accuracy: 0.6710526315789473\n",
      "Step: 247, Loss: 1.4045099020004272, Accuracy: 0.6703629032258065\n",
      "Step: 248, Loss: 1.1773959398269653, Accuracy: 0.6706827309236948\n",
      "Step: 249, Loss: 1.2169913053512573, Accuracy: 0.6706666666666666\n",
      "Step: 250, Loss: 1.2298840284347534, Accuracy: 0.6706507304116865\n",
      "Step: 251, Loss: 1.358831524848938, Accuracy: 0.6703042328042328\n",
      "Step: 252, Loss: 1.3658385276794434, Accuracy: 0.6699604743083004\n",
      "Step: 253, Loss: 1.2155345678329468, Accuracy: 0.6699475065616798\n",
      "Step: 254, Loss: 1.2315813302993774, Accuracy: 0.6699346405228758\n",
      "Step: 255, Loss: 1.0520964860916138, Accuracy: 0.6708984375\n",
      "Step: 256, Loss: 1.2030583620071411, Accuracy: 0.670881971465629\n",
      "Step: 257, Loss: 1.1815571784973145, Accuracy: 0.6708656330749354\n",
      "Step: 258, Loss: 1.044122338294983, Accuracy: 0.6718146718146718\n",
      "Step: 259, Loss: 1.2213343381881714, Accuracy: 0.6721153846153847\n",
      "Step: 260, Loss: 1.246224045753479, Accuracy: 0.6720945083014048\n",
      "Step: 261, Loss: 1.2646183967590332, Accuracy: 0.6717557251908397\n",
      "Step: 262, Loss: 1.1378743648529053, Accuracy: 0.6720532319391636\n",
      "Step: 263, Loss: 1.2017306089401245, Accuracy: 0.6720328282828283\n",
      "Step: 264, Loss: 1.2708522081375122, Accuracy: 0.6720125786163522\n",
      "Step: 265, Loss: 1.1172372102737427, Accuracy: 0.6723057644110275\n",
      "Step: 266, Loss: 1.1488600969314575, Accuracy: 0.6725967540574282\n",
      "Step: 267, Loss: 1.3762050867080688, Accuracy: 0.6719527363184079\n",
      "Step: 268, Loss: 1.3627510070800781, Accuracy: 0.6713135068153655\n",
      "Step: 269, Loss: 1.1308993101119995, Accuracy: 0.6719135802469136\n",
      "Step: 270, Loss: 1.2388964891433716, Accuracy: 0.6718942189421894\n",
      "Step: 271, Loss: 1.1816679239273071, Accuracy: 0.6721813725490197\n",
      "Step: 272, Loss: 1.1127229928970337, Accuracy: 0.6727716727716728\n",
      "Step: 273, Loss: 1.3141539096832275, Accuracy: 0.6724452554744526\n",
      "Step: 274, Loss: 1.312438726425171, Accuracy: 0.6721212121212121\n",
      "Step: 275, Loss: 1.3107097148895264, Accuracy: 0.6717995169082126\n",
      "Step: 276, Loss: 1.3830376863479614, Accuracy: 0.6711793020457281\n",
      "Step: 277, Loss: 1.0133031606674194, Accuracy: 0.6720623501199041\n",
      "Step: 278, Loss: 1.198280692100525, Accuracy: 0.6720430107526881\n",
      "Step: 279, Loss: 1.3903111219406128, Accuracy: 0.6714285714285714\n",
      "Step: 280, Loss: 1.313084363937378, Accuracy: 0.6711150652431791\n",
      "Step: 281, Loss: 1.3487869501113892, Accuracy: 0.6708037825059102\n",
      "Step: 282, Loss: 1.2281746864318848, Accuracy: 0.6707891637220259\n",
      "Step: 283, Loss: 1.2015119791030884, Accuracy: 0.670774647887324\n",
      "Step: 284, Loss: 1.3236387968063354, Accuracy: 0.67046783625731\n",
      "Step: 285, Loss: 1.255804419517517, Accuracy: 0.6704545454545454\n",
      "Step: 286, Loss: 1.225913405418396, Accuracy: 0.6704413472706156\n",
      "Step: 287, Loss: 1.2381213903427124, Accuracy: 0.6704282407407407\n",
      "Step: 288, Loss: 1.2586545944213867, Accuracy: 0.6704152249134948\n",
      "Step: 289, Loss: 1.068097472190857, Accuracy: 0.6709770114942529\n",
      "Step: 290, Loss: 1.142539381980896, Accuracy: 0.6712485681557846\n",
      "Step: 291, Loss: 1.3546079397201538, Accuracy: 0.6709474885844748\n",
      "Step: 292, Loss: 1.3843168020248413, Accuracy: 0.6703640500568828\n",
      "Step: 293, Loss: 1.1882389783859253, Accuracy: 0.6706349206349206\n",
      "Step: 294, Loss: 1.0939091444015503, Accuracy: 0.6711864406779661\n",
      "Step: 295, Loss: 1.2968624830245972, Accuracy: 0.6708896396396397\n",
      "Step: 296, Loss: 1.366796851158142, Accuracy: 0.670314253647587\n",
      "Step: 297, Loss: 1.072275161743164, Accuracy: 0.6708612975391499\n",
      "Step: 298, Loss: 1.2755002975463867, Accuracy: 0.6705685618729097\n",
      "Step: 299, Loss: 1.321677803993225, Accuracy: 0.6702777777777778\n",
      "Step: 300, Loss: 1.2329994440078735, Accuracy: 0.670265780730897\n",
      "Step: 301, Loss: 1.245581865310669, Accuracy: 0.6702538631346578\n",
      "Step: 302, Loss: 1.235547661781311, Accuracy: 0.6702420242024203\n",
      "Step: 303, Loss: 1.3486214876174927, Accuracy: 0.6696820175438597\n",
      "Step: 304, Loss: 1.3678823709487915, Accuracy: 0.6693989071038251\n",
      "Step: 305, Loss: 1.2916821241378784, Accuracy: 0.6693899782135077\n",
      "Step: 306, Loss: 1.2224096059799194, Accuracy: 0.6696525515743756\n",
      "Step: 307, Loss: 1.3099136352539062, Accuracy: 0.6693722943722944\n",
      "Step: 308, Loss: 1.1347354650497437, Accuracy: 0.6699029126213593\n",
      "Step: 309, Loss: 1.3081200122833252, Accuracy: 0.6696236559139785\n",
      "Step: 310, Loss: 1.180033802986145, Accuracy: 0.669882100750268\n",
      "Step: 311, Loss: 1.1739314794540405, Accuracy: 0.6701388888888888\n",
      "Step: 312, Loss: 1.1700663566589355, Accuracy: 0.6703940362087327\n",
      "Step: 313, Loss: 1.0391815900802612, Accuracy: 0.6711783439490446\n",
      "Step: 314, Loss: 1.230351448059082, Accuracy: 0.6711640211640212\n",
      "Step: 315, Loss: 1.2646241188049316, Accuracy: 0.6708860759493671\n",
      "Step: 316, Loss: 0.9900720715522766, Accuracy: 0.6716614090431126\n",
      "Step: 317, Loss: 1.3039900064468384, Accuracy: 0.6713836477987422\n",
      "Step: 318, Loss: 1.1648809909820557, Accuracy: 0.6716300940438872\n",
      "Step: 319, Loss: 0.9964404106140137, Accuracy: 0.6723958333333333\n",
      "Step: 320, Loss: 1.1664711236953735, Accuracy: 0.6726375908618899\n",
      "Step: 321, Loss: 1.2259629964828491, Accuracy: 0.6726190476190477\n",
      "Step: 322, Loss: 1.1462347507476807, Accuracy: 0.672858617131063\n",
      "Step: 323, Loss: 1.302674412727356, Accuracy: 0.6725823045267489\n",
      "Step: 324, Loss: 0.9980473518371582, Accuracy: 0.6733333333333333\n",
      "Step: 325, Loss: 1.0279290676116943, Accuracy: 0.674079754601227\n",
      "Step: 326, Loss: 1.280684471130371, Accuracy: 0.6738022426095821\n",
      "Step: 327, Loss: 1.0005583763122559, Accuracy: 0.6745426829268293\n",
      "Step: 328, Loss: 1.1721619367599487, Accuracy: 0.6747720364741642\n",
      "Step: 329, Loss: 1.3894046545028687, Accuracy: 0.6742424242424242\n",
      "Step: 330, Loss: 1.1519408226013184, Accuracy: 0.6744712990936556\n",
      "Step: 331, Loss: 1.1815439462661743, Accuracy: 0.6744477911646586\n",
      "Step: 332, Loss: 1.2109851837158203, Accuracy: 0.6746746746746747\n",
      "Step: 333, Loss: 1.1373611688613892, Accuracy: 0.6749001996007984\n",
      "Step: 334, Loss: 1.384839415550232, Accuracy: 0.6743781094527364\n",
      "Step: 335, Loss: 1.324829339981079, Accuracy: 0.6741071428571429\n",
      "Step: 336, Loss: 1.1485193967819214, Accuracy: 0.6743323442136498\n",
      "Step: 337, Loss: 1.0212045907974243, Accuracy: 0.6750493096646942\n",
      "Step: 338, Loss: 1.2719918489456177, Accuracy: 0.6747787610619469\n",
      "Step: 339, Loss: 1.058073878288269, Accuracy: 0.6752450980392157\n",
      "Step: 340, Loss: 1.1471418142318726, Accuracy: 0.6754643206256109\n",
      "Step: 341, Loss: 1.1226261854171753, Accuracy: 0.675682261208577\n",
      "Step: 342, Loss: 1.234298586845398, Accuracy: 0.6756559766763849\n",
      "Step: 343, Loss: 1.2423409223556519, Accuracy: 0.6756298449612403\n",
      "Step: 344, Loss: 1.1472806930541992, Accuracy: 0.6758454106280193\n",
      "Step: 345, Loss: 1.19780433177948, Accuracy: 0.6758188824662813\n",
      "Step: 346, Loss: 1.2441437244415283, Accuracy: 0.6757925072046109\n",
      "Step: 347, Loss: 1.2319097518920898, Accuracy: 0.6757662835249042\n",
      "Step: 348, Loss: 1.5353590250015259, Accuracy: 0.6747851002865329\n",
      "Step: 349, Loss: 1.1681233644485474, Accuracy: 0.675\n",
      "Step: 350, Loss: 1.286311149597168, Accuracy: 0.6747388414055081\n",
      "Step: 351, Loss: 1.186018943786621, Accuracy: 0.6749526515151515\n",
      "Step: 352, Loss: 1.0854015350341797, Accuracy: 0.6754013220018886\n",
      "Step: 353, Loss: 1.1458152532577515, Accuracy: 0.6756120527306968\n",
      "Step: 354, Loss: 1.323851466178894, Accuracy: 0.6753521126760563\n",
      "Step: 355, Loss: 1.1351298093795776, Accuracy: 0.675561797752809\n",
      "Step: 356, Loss: 1.2618861198425293, Accuracy: 0.6755368814192344\n",
      "Step: 357, Loss: 1.138283610343933, Accuracy: 0.6757448789571695\n",
      "Step: 358, Loss: 1.150286316871643, Accuracy: 0.6759517177344475\n",
      "Step: 359, Loss: 1.5561777353286743, Accuracy: 0.675\n",
      "Step: 360, Loss: 1.190062403678894, Accuracy: 0.6749769159741459\n",
      "Step: 361, Loss: 1.1304972171783447, Accuracy: 0.6751841620626151\n",
      "Step: 362, Loss: 1.394891381263733, Accuracy: 0.6747015610651974\n",
      "Step: 363, Loss: 1.1630043983459473, Accuracy: 0.674908424908425\n",
      "Step: 364, Loss: 1.0803321599960327, Accuracy: 0.6753424657534246\n",
      "Step: 365, Loss: 1.1995513439178467, Accuracy: 0.6753187613843351\n",
      "Step: 366, Loss: 1.247902512550354, Accuracy: 0.6752951861943688\n",
      "Step: 367, Loss: 1.2386201620101929, Accuracy: 0.6752717391304348\n",
      "Step: 368, Loss: 1.158826231956482, Accuracy: 0.6754742547425474\n",
      "Step: 369, Loss: 1.2642103433609009, Accuracy: 0.6754504504504505\n",
      "Step: 370, Loss: 1.2269445657730103, Accuracy: 0.6754267744833783\n",
      "Step: 371, Loss: 1.2293473482131958, Accuracy: 0.6754032258064516\n",
      "Step: 372, Loss: 1.057308554649353, Accuracy: 0.6758266309204647\n",
      "Step: 373, Loss: 1.1728399991989136, Accuracy: 0.6760249554367201\n",
      "Step: 374, Loss: 1.2257581949234009, Accuracy: 0.676\n",
      "Step: 375, Loss: 1.1070680618286133, Accuracy: 0.6764184397163121\n",
      "Step: 376, Loss: 1.0334722995758057, Accuracy: 0.6770557029177718\n",
      "Step: 377, Loss: 1.0519356727600098, Accuracy: 0.6774691358024691\n",
      "Step: 378, Loss: 1.405417799949646, Accuracy: 0.6770008795074758\n",
      "Step: 379, Loss: 1.1639106273651123, Accuracy: 0.6771929824561403\n",
      "Step: 380, Loss: 1.1529828310012817, Accuracy: 0.6776027996500438\n",
      "Step: 381, Loss: 1.2827931642532349, Accuracy: 0.6773560209424084\n",
      "Step: 382, Loss: 1.3817381858825684, Accuracy: 0.6768929503916449\n",
      "Step: 383, Loss: 1.061507225036621, Accuracy: 0.6773003472222222\n",
      "Step: 384, Loss: 1.2249228954315186, Accuracy: 0.6772727272727272\n",
      "Step: 385, Loss: 1.2939532995224, Accuracy: 0.6770293609671848\n",
      "Step: 386, Loss: 1.3431819677352905, Accuracy: 0.6767872523686477\n",
      "Step: 387, Loss: 1.3388501405715942, Accuracy: 0.676331615120275\n",
      "Step: 388, Loss: 1.305799126625061, Accuracy: 0.6763067694944301\n",
      "Step: 389, Loss: 1.1993134021759033, Accuracy: 0.6762820512820513\n",
      "Step: 390, Loss: 1.2240115404129028, Accuracy: 0.6762574595055414\n",
      "Step: 391, Loss: 0.9988182187080383, Accuracy: 0.6768707482993197\n",
      "Step: 392, Loss: 1.3197633028030396, Accuracy: 0.6766327396098388\n",
      "Step: 393, Loss: 1.239874005317688, Accuracy: 0.6766074450084603\n",
      "Step: 394, Loss: 1.3487039804458618, Accuracy: 0.6761603375527426\n",
      "Step: 395, Loss: 1.2199013233184814, Accuracy: 0.6761363636363636\n",
      "Step: 396, Loss: 1.259154200553894, Accuracy: 0.6759026028547439\n",
      "Step: 397, Loss: 1.2869980335235596, Accuracy: 0.6756700167504187\n",
      "Step: 398, Loss: 1.2329449653625488, Accuracy: 0.6756474519632414\n",
      "Step: 399, Loss: 1.2856489419937134, Accuracy: 0.6754166666666667\n",
      "Step: 400, Loss: 1.0950584411621094, Accuracy: 0.6758104738154613\n",
      "Step: 401, Loss: 1.373691201210022, Accuracy: 0.6753731343283582\n",
      "Step: 402, Loss: 1.2797956466674805, Accuracy: 0.6751447477253929\n",
      "Step: 403, Loss: 1.2258391380310059, Accuracy: 0.6751237623762376\n",
      "Step: 404, Loss: 1.4977113008499146, Accuracy: 0.6742798353909465\n",
      "Step: 405, Loss: 1.0775564908981323, Accuracy: 0.6746715927750411\n",
      "Step: 406, Loss: 1.3225497007369995, Accuracy: 0.6744471744471745\n",
      "Step: 407, Loss: 1.1114389896392822, Accuracy: 0.6748366013071896\n",
      "Step: 408, Loss: 1.2197965383529663, Accuracy: 0.6748166259168704\n",
      "Step: 409, Loss: 1.0779147148132324, Accuracy: 0.6752032520325203\n",
      "Step: 410, Loss: 1.423636555671692, Accuracy: 0.6745742092457421\n",
      "Step: 411, Loss: 1.3658785820007324, Accuracy: 0.6741504854368932\n",
      "Step: 412, Loss: 1.2702494859695435, Accuracy: 0.6739305891848265\n",
      "Step: 413, Loss: 1.272844672203064, Accuracy: 0.6737117552334944\n",
      "Step: 414, Loss: 1.146156668663025, Accuracy: 0.6738955823293172\n",
      "Step: 415, Loss: 1.066916584968567, Accuracy: 0.6742788461538461\n",
      "Step: 416, Loss: 1.2668368816375732, Accuracy: 0.6740607513988809\n",
      "Step: 417, Loss: 1.1637018918991089, Accuracy: 0.6742424242424242\n",
      "Step: 418, Loss: 1.2694374322891235, Accuracy: 0.6740254574383453\n",
      "Step: 419, Loss: 1.452683448791504, Accuracy: 0.6736111111111112\n",
      "Step: 420, Loss: 1.2148042917251587, Accuracy: 0.6735946159936659\n",
      "Step: 421, Loss: 1.2100309133529663, Accuracy: 0.6737756714060031\n",
      "Step: 422, Loss: 1.1869022846221924, Accuracy: 0.6739558707643813\n",
      "Step: 423, Loss: 1.1123793125152588, Accuracy: 0.6741352201257862\n",
      "Step: 424, Loss: 1.1048246622085571, Accuracy: 0.6745098039215687\n",
      "Step: 425, Loss: 1.1458643674850464, Accuracy: 0.6746870109546166\n",
      "Step: 426, Loss: 1.3406537771224976, Accuracy: 0.6744730679156908\n",
      "Step: 427, Loss: 1.257851004600525, Accuracy: 0.6742601246105919\n",
      "Step: 428, Loss: 1.5300397872924805, Accuracy: 0.6734654234654235\n",
      "Step: 429, Loss: 1.305975079536438, Accuracy: 0.6732558139534883\n",
      "Step: 430, Loss: 1.0912485122680664, Accuracy: 0.6734338747099768\n",
      "Step: 431, Loss: 1.2588998079299927, Accuracy: 0.6734182098765432\n",
      "Step: 432, Loss: 1.2050971984863281, Accuracy: 0.6735950731331793\n",
      "Step: 433, Loss: 1.2198388576507568, Accuracy: 0.67357910906298\n",
      "Step: 434, Loss: 1.2905532121658325, Accuracy: 0.6733716475095786\n",
      "Step: 435, Loss: 1.3176634311676025, Accuracy: 0.6731651376146789\n",
      "Step: 436, Loss: 1.2691570520401, Accuracy: 0.6729595728451564\n",
      "Step: 437, Loss: 1.2529504299163818, Accuracy: 0.672945205479452\n",
      "Step: 438, Loss: 1.218589186668396, Accuracy: 0.6729309035687168\n",
      "Step: 439, Loss: 1.0650588274002075, Accuracy: 0.6732954545454546\n",
      "Step: 440, Loss: 1.1570085287094116, Accuracy: 0.673469387755102\n",
      "Step: 441, Loss: 1.2899959087371826, Accuracy: 0.673265460030166\n",
      "Step: 442, Loss: 1.6147657632827759, Accuracy: 0.6723100075244545\n",
      "Step: 443, Loss: 1.1040393114089966, Accuracy: 0.672484984984985\n",
      "Step: 444, Loss: 1.242109775543213, Accuracy: 0.6724719101123595\n",
      "Step: 445, Loss: 1.159674048423767, Accuracy: 0.67245889387145\n",
      "Step: 446, Loss: 1.0739736557006836, Accuracy: 0.6728187919463087\n",
      "Step: 447, Loss: 1.292981505393982, Accuracy: 0.6728050595238095\n",
      "Step: 448, Loss: 1.1501668691635132, Accuracy: 0.6729769858945805\n",
      "Step: 449, Loss: 1.3497986793518066, Accuracy: 0.6727777777777778\n",
      "Step: 450, Loss: 1.200518250465393, Accuracy: 0.6727642276422764\n",
      "Step: 451, Loss: 1.2009230852127075, Accuracy: 0.6727507374631269\n",
      "Step: 452, Loss: 1.299436330795288, Accuracy: 0.6725533480500367\n",
      "Step: 453, Loss: 1.4171028137207031, Accuracy: 0.672173274596182\n",
      "Step: 454, Loss: 1.218403697013855, Accuracy: 0.6721611721611722\n",
      "Step: 455, Loss: 1.2072399854660034, Accuracy: 0.6721491228070176\n",
      "Step: 456, Loss: 1.301384687423706, Accuracy: 0.6719547775346463\n",
      "Step: 457, Loss: 1.1748279333114624, Accuracy: 0.6719432314410481\n",
      "Step: 458, Loss: 1.099856972694397, Accuracy: 0.6722948438634713\n",
      "Step: 459, Loss: 1.0473026037216187, Accuracy: 0.6728260869565217\n",
      "Step: 460, Loss: 1.1430782079696655, Accuracy: 0.6729934924078091\n",
      "Step: 461, Loss: 1.1206812858581543, Accuracy: 0.6733405483405484\n",
      "Step: 462, Loss: 1.3095717430114746, Accuracy: 0.6731461483081354\n",
      "Step: 463, Loss: 1.0127214193344116, Accuracy: 0.6736709770114943\n",
      "Step: 464, Loss: 1.397924542427063, Accuracy: 0.6732974910394265\n",
      "Step: 465, Loss: 1.2053521871566772, Accuracy: 0.6732832618025751\n",
      "Step: 466, Loss: 1.3772401809692383, Accuracy: 0.6730906495360457\n",
      "Step: 467, Loss: 1.1514943838119507, Accuracy: 0.6732549857549858\n",
      "Step: 468, Loss: 1.1706485748291016, Accuracy: 0.6732409381663113\n",
      "Step: 469, Loss: 1.1393362283706665, Accuracy: 0.673581560283688\n",
      "Step: 470, Loss: 1.3239505290985107, Accuracy: 0.6733899504600142\n",
      "Step: 471, Loss: 1.0970513820648193, Accuracy: 0.673728813559322\n",
      "Step: 472, Loss: 1.2101762294769287, Accuracy: 0.6737138830162086\n",
      "Step: 473, Loss: 1.2729979753494263, Accuracy: 0.6735232067510548\n",
      "Step: 474, Loss: 1.1501001119613647, Accuracy: 0.6736842105263158\n",
      "Step: 475, Loss: 1.0931600332260132, Accuracy: 0.6740196078431373\n",
      "Step: 476, Loss: 1.205849289894104, Accuracy: 0.6740041928721174\n",
      "Step: 477, Loss: 1.2427030801773071, Accuracy: 0.6739888423988842\n",
      "Step: 478, Loss: 1.3898900747299194, Accuracy: 0.673625608907446\n",
      "Step: 479, Loss: 1.2717584371566772, Accuracy: 0.6734375\n",
      "Step: 480, Loss: 1.2892042398452759, Accuracy: 0.6732501732501732\n",
      "Step: 481, Loss: 1.2450947761535645, Accuracy: 0.6732365145228216\n",
      "Step: 482, Loss: 1.133779525756836, Accuracy: 0.6733954451345756\n",
      "Step: 483, Loss: 1.1595215797424316, Accuracy: 0.6733815426997245\n",
      "Step: 484, Loss: 1.103986144065857, Accuracy: 0.6737113402061856\n",
      "Step: 485, Loss: 1.158211350440979, Accuracy: 0.6738683127572016\n",
      "Step: 486, Loss: 1.1976789236068726, Accuracy: 0.6740246406570842\n",
      "Step: 487, Loss: 1.0705941915512085, Accuracy: 0.6745218579234973\n",
      "Step: 488, Loss: 1.2211076021194458, Accuracy: 0.674505794137696\n",
      "Step: 489, Loss: 1.312762975692749, Accuracy: 0.6743197278911565\n",
      "Step: 490, Loss: 1.2188388109207153, Accuracy: 0.6743041412084182\n",
      "Step: 491, Loss: 1.3875904083251953, Accuracy: 0.673949864498645\n",
      "Step: 492, Loss: 1.2930939197540283, Accuracy: 0.6737660581473969\n",
      "Step: 493, Loss: 1.1455680131912231, Accuracy: 0.6739203778677463\n",
      "Step: 494, Loss: 1.0351066589355469, Accuracy: 0.6744107744107745\n",
      "Step: 495, Loss: 1.2598408460617065, Accuracy: 0.6743951612903226\n",
      "Step: 496, Loss: 1.387731671333313, Accuracy: 0.6742119382964453\n",
      "Step: 497, Loss: 1.5322651863098145, Accuracy: 0.6735274431057564\n",
      "Step: 498, Loss: 1.402020812034607, Accuracy: 0.6731796927187709\n",
      "Step: 499, Loss: 1.2534931898117065, Accuracy: 0.6731666666666667\n",
      "Step: 500, Loss: 1.140972375869751, Accuracy: 0.6734863606121091\n",
      "Step: 501, Loss: 1.2690410614013672, Accuracy: 0.6733067729083665\n",
      "Step: 502, Loss: 0.9932836890220642, Accuracy: 0.673790589794566\n",
      "Step: 503, Loss: 1.2915570735931396, Accuracy: 0.6736111111111112\n",
      "Step: 504, Loss: 1.1889333724975586, Accuracy: 0.6737623762376238\n",
      "Step: 505, Loss: 1.14537513256073, Accuracy: 0.6739130434782609\n",
      "Step: 506, Loss: 1.168899416923523, Accuracy: 0.6740631163708086\n",
      "Step: 507, Loss: 1.0877104997634888, Accuracy: 0.6743766404199475\n",
      "Step: 508, Loss: 1.162721037864685, Accuracy: 0.6745252128356254\n",
      "Step: 509, Loss: 1.151114821434021, Accuracy: 0.6746732026143791\n",
      "Step: 510, Loss: 1.1984606981277466, Accuracy: 0.6746575342465754\n",
      "Step: 511, Loss: 0.9954364895820618, Accuracy: 0.6751302083333334\n",
      "Step: 512, Loss: 1.2276813983917236, Accuracy: 0.6751137102014295\n",
      "Step: 513, Loss: 1.3113244771957397, Accuracy: 0.6749351491569391\n",
      "Step: 514, Loss: 1.214643955230713, Accuracy: 0.6750809061488673\n",
      "Step: 515, Loss: 1.2174564599990845, Accuracy: 0.6750645994832042\n",
      "Step: 516, Loss: 1.3286842107772827, Accuracy: 0.6748871695680206\n",
      "Step: 517, Loss: 1.217645287513733, Accuracy: 0.6747104247104247\n",
      "Step: 518, Loss: 1.26233971118927, Accuracy: 0.6746949261400128\n",
      "Step: 519, Loss: 1.1522552967071533, Accuracy: 0.6748397435897436\n",
      "Step: 520, Loss: 1.293224573135376, Accuracy: 0.6746641074856046\n",
      "Step: 521, Loss: 1.0776623487472534, Accuracy: 0.6749680715197957\n",
      "Step: 522, Loss: 1.0586037635803223, Accuracy: 0.6752708731676227\n",
      "Step: 523, Loss: 1.0569509267807007, Accuracy: 0.6755725190839694\n",
      "Step: 524, Loss: 1.2246851921081543, Accuracy: 0.6755555555555556\n",
      "Step: 525, Loss: 1.2828764915466309, Accuracy: 0.6753802281368821\n",
      "Step: 526, Loss: 1.0869359970092773, Accuracy: 0.6756799493991145\n",
      "Step: 527, Loss: 1.128705382347107, Accuracy: 0.6758207070707071\n",
      "Step: 528, Loss: 1.023690938949585, Accuracy: 0.6762759924385633\n",
      "Step: 529, Loss: 1.4095150232315063, Accuracy: 0.6759433962264151\n",
      "Step: 530, Loss: 1.2030460834503174, Accuracy: 0.6760828625235404\n",
      "Step: 531, Loss: 1.17900812625885, Accuracy: 0.6762218045112782\n",
      "Step: 532, Loss: 1.1722339391708374, Accuracy: 0.6763602251407129\n",
      "Step: 533, Loss: 1.180687427520752, Accuracy: 0.6764981273408239\n",
      "Step: 534, Loss: 1.2456549406051636, Accuracy: 0.6764797507788162\n",
      "Step: 535, Loss: 1.2230100631713867, Accuracy: 0.6764614427860697\n",
      "Step: 536, Loss: 1.31203031539917, Accuracy: 0.6762880198634389\n",
      "Step: 537, Loss: 1.0503524541854858, Accuracy: 0.6765799256505576\n",
      "Step: 538, Loss: 1.1451984643936157, Accuracy: 0.6767161410018553\n",
      "Step: 539, Loss: 1.2608062028884888, Accuracy: 0.6765432098765433\n",
      "Step: 540, Loss: 1.4034796953201294, Accuracy: 0.6762168823166975\n",
      "Step: 541, Loss: 1.2686134576797485, Accuracy: 0.6760455104551045\n",
      "Step: 542, Loss: 1.1000653505325317, Accuracy: 0.6763351749539595\n",
      "Step: 543, Loss: 1.0260249376296997, Accuracy: 0.6767769607843137\n",
      "Step: 544, Loss: 1.0519826412200928, Accuracy: 0.677217125382263\n",
      "Step: 545, Loss: 1.0733476877212524, Accuracy: 0.6775030525030525\n",
      "Step: 546, Loss: 1.1448758840560913, Accuracy: 0.6776355880560634\n",
      "Step: 547, Loss: 1.1328866481781006, Accuracy: 0.6777676399026764\n",
      "Step: 548, Loss: 1.0390403270721436, Accuracy: 0.6780510018214936\n",
      "Step: 549, Loss: 1.2239493131637573, Accuracy: 0.678030303030303\n",
      "Step: 550, Loss: 1.0889759063720703, Accuracy: 0.6783121597096189\n",
      "Step: 551, Loss: 1.321654200553894, Accuracy: 0.6781400966183575\n",
      "Step: 552, Loss: 1.2561002969741821, Accuracy: 0.6781193490054249\n",
      "Step: 553, Loss: 1.4658232927322388, Accuracy: 0.677647412755716\n",
      "Step: 554, Loss: 1.4153772592544556, Accuracy: 0.6773273273273274\n",
      "Step: 555, Loss: 1.1593660116195679, Accuracy: 0.6774580335731415\n",
      "Step: 556, Loss: 1.3350540399551392, Accuracy: 0.6771394374625972\n",
      "Step: 557, Loss: 1.043973445892334, Accuracy: 0.6775686977299881\n",
      "Step: 558, Loss: 1.2603551149368286, Accuracy: 0.6775491949910555\n",
      "Step: 559, Loss: 1.1632519960403442, Accuracy: 0.6776785714285715\n",
      "Step: 560, Loss: 1.2689073085784912, Accuracy: 0.6775103980986333\n",
      "Step: 561, Loss: 1.1931334733963013, Accuracy: 0.6776393831553974\n",
      "Step: 562, Loss: 1.3219627141952515, Accuracy: 0.6774718768502073\n",
      "Step: 563, Loss: 1.406497597694397, Accuracy: 0.6771572104018913\n",
      "Step: 564, Loss: 1.0465675592422485, Accuracy: 0.6774336283185841\n",
      "Step: 565, Loss: 1.1370190382003784, Accuracy: 0.6775618374558304\n",
      "Step: 566, Loss: 1.0499956607818604, Accuracy: 0.6778365667254557\n",
      "Step: 567, Loss: 1.2690736055374146, Accuracy: 0.6776701877934272\n",
      "Step: 568, Loss: 1.086482048034668, Accuracy: 0.6779437609841827\n",
      "Step: 569, Loss: 1.2886962890625, Accuracy: 0.6777777777777778\n",
      "Step: 570, Loss: 1.162440299987793, Accuracy: 0.6779042615294805\n",
      "Step: 571, Loss: 1.1868876218795776, Accuracy: 0.678030303030303\n",
      "Step: 572, Loss: 1.3035250902175903, Accuracy: 0.6778650378126818\n",
      "Step: 573, Loss: 1.4738441705703735, Accuracy: 0.6774099883855982\n",
      "Step: 574, Loss: 1.3285877704620361, Accuracy: 0.6772463768115942\n",
      "Step: 575, Loss: 1.169146180152893, Accuracy: 0.6773726851851852\n",
      "Step: 576, Loss: 1.4211102724075317, Accuracy: 0.6770652801848642\n",
      "Step: 577, Loss: 1.1927247047424316, Accuracy: 0.6771914648212226\n",
      "Step: 578, Loss: 1.2436774969100952, Accuracy: 0.6771732872769142\n",
      "Step: 579, Loss: 1.0546164512634277, Accuracy: 0.6774425287356322\n",
      "Step: 580, Loss: 1.3765478134155273, Accuracy: 0.6771371199082042\n",
      "Step: 581, Loss: 1.3177708387374878, Accuracy: 0.6769759450171822\n",
      "Step: 582, Loss: 1.2635082006454468, Accuracy: 0.6768153230417381\n",
      "Step: 583, Loss: 1.3163856267929077, Accuracy: 0.6766552511415526\n",
      "Step: 584, Loss: 1.136435627937317, Accuracy: 0.6767806267806268\n",
      "Step: 585, Loss: 1.1092243194580078, Accuracy: 0.676905574516496\n",
      "Step: 586, Loss: 1.1609622240066528, Accuracy: 0.6770300965360591\n",
      "Step: 587, Loss: 1.0626944303512573, Accuracy: 0.6772959183673469\n",
      "Step: 588, Loss: 1.4502862691879272, Accuracy: 0.6768534238822863\n",
      "Step: 589, Loss: 1.203494668006897, Accuracy: 0.6768361581920904\n",
      "Step: 590, Loss: 1.2493525743484497, Accuracy: 0.676818950930626\n",
      "Step: 591, Loss: 1.1524512767791748, Accuracy: 0.6769425675675675\n",
      "Step: 592, Loss: 1.4409704208374023, Accuracy: 0.6765036537380551\n",
      "Step: 593, Loss: 1.1967564821243286, Accuracy: 0.6766273849607183\n",
      "Step: 594, Loss: 1.0845705270767212, Accuracy: 0.676890756302521\n",
      "Step: 595, Loss: 1.218063473701477, Accuracy: 0.6768736017897091\n",
      "Step: 596, Loss: 1.1335554122924805, Accuracy: 0.6769960915689559\n",
      "Step: 597, Loss: 1.0811086893081665, Accuracy: 0.677257525083612\n",
      "Step: 598, Loss: 1.069053053855896, Accuracy: 0.6775180856983862\n",
      "Step: 599, Loss: 1.3311439752578735, Accuracy: 0.6773611111111111\n",
      "Step: 600, Loss: 1.1490756273269653, Accuracy: 0.6774819744869661\n",
      "Step: 601, Loss: 1.3857369422912598, Accuracy: 0.67718715393134\n",
      "Step: 602, Loss: 1.2099043130874634, Accuracy: 0.6771697070204533\n",
      "Step: 603, Loss: 1.544064998626709, Accuracy: 0.6766004415011038\n",
      "Step: 604, Loss: 1.1649669408798218, Accuracy: 0.6767217630853994\n",
      "Step: 605, Loss: 1.2163186073303223, Accuracy: 0.6768426842684269\n",
      "Step: 606, Loss: 1.3114087581634521, Accuracy: 0.6766886326194399\n",
      "Step: 607, Loss: 1.1127959489822388, Accuracy: 0.6769462719298246\n",
      "Step: 608, Loss: 1.3934577703475952, Accuracy: 0.676655719759168\n",
      "Step: 609, Loss: 1.3016972541809082, Accuracy: 0.6765027322404371\n",
      "Step: 610, Loss: 1.0818785429000854, Accuracy: 0.676759410801964\n",
      "Step: 611, Loss: 1.1958354711532593, Accuracy: 0.6768790849673203\n",
      "Step: 612, Loss: 0.9939818978309631, Accuracy: 0.6772702555736814\n",
      "Step: 613, Loss: 1.2775630950927734, Accuracy: 0.6771172638436482\n",
      "Step: 614, Loss: 1.205748438835144, Accuracy: 0.6772357723577236\n",
      "Step: 615, Loss: 1.1330254077911377, Accuracy: 0.6774891774891775\n",
      "Step: 616, Loss: 1.0673489570617676, Accuracy: 0.6777417612101567\n",
      "Step: 617, Loss: 1.1134706735610962, Accuracy: 0.6778586839266451\n",
      "Step: 618, Loss: 1.2208250761032104, Accuracy: 0.6778406031233172\n",
      "Step: 619, Loss: 1.13200843334198, Accuracy: 0.6779569892473118\n",
      "Step: 620, Loss: 1.0740832090377808, Accuracy: 0.6782071926999463\n",
      "Step: 621, Loss: 1.2795642614364624, Accuracy: 0.6781886387995713\n",
      "Step: 622, Loss: 1.2829467058181763, Accuracy: 0.6780363830925629\n",
      "Step: 623, Loss: 1.279533863067627, Accuracy: 0.6778846153846154\n",
      "Step: 624, Loss: 1.4198179244995117, Accuracy: 0.6776\n",
      "Step: 625, Loss: 1.1816368103027344, Accuracy: 0.6777156549520766\n",
      "Step: 626, Loss: 1.0089060068130493, Accuracy: 0.6780967570441254\n",
      "Step: 627, Loss: 1.1799753904342651, Accuracy: 0.6780785562632696\n",
      "Step: 628, Loss: 1.234800100326538, Accuracy: 0.678060413354531\n",
      "Step: 629, Loss: 1.1613571643829346, Accuracy: 0.6781746031746032\n",
      "Step: 630, Loss: 1.295117735862732, Accuracy: 0.6780243000528262\n",
      "Step: 631, Loss: 1.3023465871810913, Accuracy: 0.6778744725738397\n",
      "Step: 632, Loss: 1.3061904907226562, Accuracy: 0.6777251184834123\n",
      "Step: 633, Loss: 0.9906328320503235, Accuracy: 0.6781019978969506\n",
      "Step: 634, Loss: 1.2775996923446655, Accuracy: 0.6780839895013123\n",
      "Step: 635, Loss: 1.2697969675064087, Accuracy: 0.6780660377358491\n",
      "Step: 636, Loss: 1.4632930755615234, Accuracy: 0.6776556776556777\n",
      "Step: 637, Loss: 1.4214277267456055, Accuracy: 0.6773772204806687\n",
      "Step: 638, Loss: 1.1717606782913208, Accuracy: 0.6773604590505999\n",
      "Step: 639, Loss: 1.3428276777267456, Accuracy: 0.6770833333333334\n",
      "Step: 640, Loss: 1.3483885526657104, Accuracy: 0.6768070722828913\n",
      "Step: 641, Loss: 1.2432094812393188, Accuracy: 0.676791277258567\n",
      "Step: 642, Loss: 1.1920815706253052, Accuracy: 0.6767755313634007\n",
      "Step: 643, Loss: 1.241977572441101, Accuracy: 0.67675983436853\n",
      "Step: 644, Loss: 1.2018684148788452, Accuracy: 0.6767441860465117\n",
      "Step: 645, Loss: 1.163845181465149, Accuracy: 0.6767285861713106\n",
      "Step: 646, Loss: 1.1378661394119263, Accuracy: 0.6768418341061309\n",
      "Step: 647, Loss: 1.2780697345733643, Accuracy: 0.6768261316872428\n",
      "Step: 648, Loss: 1.368139624595642, Accuracy: 0.6765536723163842\n",
      "Step: 649, Loss: 1.162544846534729, Accuracy: 0.6766666666666666\n",
      "Step: 650, Loss: 1.4119230508804321, Accuracy: 0.6762672811059908\n",
      "Step: 651, Loss: 1.3395518064498901, Accuracy: 0.6761247443762781\n",
      "Step: 652, Loss: 1.2056165933609009, Accuracy: 0.6762378764675855\n",
      "Step: 653, Loss: 1.3001301288604736, Accuracy: 0.6760958205912334\n",
      "Step: 654, Loss: 1.454259991645813, Accuracy: 0.6756997455470738\n",
      "Step: 655, Loss: 1.346816062927246, Accuracy: 0.6755589430894309\n",
      "Step: 656, Loss: 1.1474285125732422, Accuracy: 0.6756722475900558\n",
      "Step: 657, Loss: 1.12090003490448, Accuracy: 0.6759118541033434\n",
      "Step: 658, Loss: 1.3299440145492554, Accuracy: 0.6756449165402124\n",
      "Step: 659, Loss: 1.2867532968521118, Accuracy: 0.6756313131313131\n",
      "Step: 660, Loss: 1.2786887884140015, Accuracy: 0.6754916792738276\n",
      "Step: 661, Loss: 1.167908787727356, Accuracy: 0.6756042296072508\n",
      "Step: 662, Loss: 1.2654279470443726, Accuracy: 0.6755907491201609\n",
      "Step: 663, Loss: 1.2267924547195435, Accuracy: 0.6755773092369478\n",
      "Step: 664, Loss: 1.469812273979187, Accuracy: 0.675187969924812\n",
      "Step: 665, Loss: 1.312296986579895, Accuracy: 0.6751751751751752\n",
      "Step: 666, Loss: 1.3302333354949951, Accuracy: 0.6750374812593704\n",
      "Step: 667, Loss: 1.247400164604187, Accuracy: 0.6750249500998003\n",
      "Step: 668, Loss: 1.3138059377670288, Accuracy: 0.6748878923766816\n",
      "Step: 669, Loss: 1.0820680856704712, Accuracy: 0.6751243781094527\n",
      "Step: 670, Loss: 1.3972954750061035, Accuracy: 0.674863387978142\n",
      "Step: 671, Loss: 1.3936322927474976, Accuracy: 0.6746031746031746\n",
      "Step: 672, Loss: 1.4043563604354858, Accuracy: 0.6743437345220407\n",
      "Step: 673, Loss: 1.1132283210754395, Accuracy: 0.674455984174085\n",
      "Step: 674, Loss: 1.3937469720840454, Accuracy: 0.6741975308641975\n",
      "Step: 675, Loss: 1.4775700569152832, Accuracy: 0.6738165680473372\n",
      "Step: 676, Loss: 1.545154094696045, Accuracy: 0.673313638601674\n",
      "Step: 677, Loss: 1.0285977125167847, Accuracy: 0.6736725663716814\n",
      "Step: 678, Loss: 1.1698429584503174, Accuracy: 0.6737849779086893\n",
      "Step: 679, Loss: 1.295225977897644, Accuracy: 0.6736519607843138\n",
      "Step: 680, Loss: 1.2606536149978638, Accuracy: 0.6736417033773862\n",
      "Step: 681, Loss: 1.2682783603668213, Accuracy: 0.6736314760508308\n",
      "Step: 682, Loss: 1.2107993364334106, Accuracy: 0.6736212786725232\n",
      "Step: 683, Loss: 1.1663508415222168, Accuracy: 0.6737329434697856\n",
      "Step: 684, Loss: 1.0320743322372437, Accuracy: 0.6740875912408759\n",
      "Step: 685, Loss: 1.288745403289795, Accuracy: 0.6740767735665695\n",
      "Step: 686, Loss: 1.1138838529586792, Accuracy: 0.6741872877244056\n",
      "Step: 687, Loss: 1.2837345600128174, Accuracy: 0.6740552325581395\n",
      "Step: 688, Loss: 1.409745693206787, Accuracy: 0.6738026124818578\n",
      "Step: 689, Loss: 1.240634799003601, Accuracy: 0.673792270531401\n",
      "Step: 690, Loss: 1.6256097555160522, Accuracy: 0.6731789676796913\n",
      "Step: 691, Loss: 1.1762317419052124, Accuracy: 0.6732899807321773\n",
      "Step: 692, Loss: 1.1380208730697632, Accuracy: 0.6734006734006734\n",
      "Step: 693, Loss: 1.579022765159607, Accuracy: 0.6729106628242075\n",
      "Step: 694, Loss: 1.1135443449020386, Accuracy: 0.6731414868105515\n",
      "Step: 695, Loss: 1.3572386503219604, Accuracy: 0.6730124521072797\n",
      "Step: 696, Loss: 1.1281508207321167, Accuracy: 0.6731229076996652\n",
      "Step: 697, Loss: 1.3956435918807983, Accuracy: 0.6728748806112703\n",
      "Step: 698, Loss: 1.1693419218063354, Accuracy: 0.6729852169766333\n",
      "Step: 699, Loss: 1.2407007217407227, Accuracy: 0.6729761904761905\n",
      "Step: 700, Loss: 1.517985463142395, Accuracy: 0.672491678554446\n",
      "Step: 701, Loss: 1.2212392091751099, Accuracy: 0.6724833808167141\n",
      "Step: 702, Loss: 1.1890883445739746, Accuracy: 0.672475106685633\n",
      "Step: 703, Loss: 1.0256339311599731, Accuracy: 0.6728219696969697\n",
      "Step: 704, Loss: 1.3202403783798218, Accuracy: 0.672695035460993\n",
      "Step: 705, Loss: 1.288767695426941, Accuracy: 0.6725684608120869\n",
      "Step: 706, Loss: 0.9882534146308899, Accuracy: 0.6729137199434229\n",
      "Step: 707, Loss: 1.3100210428237915, Accuracy: 0.6727871939736346\n",
      "Step: 708, Loss: 1.3184479475021362, Accuracy: 0.6726610249177245\n",
      "Step: 709, Loss: 1.374124526977539, Accuracy: 0.6724178403755868\n",
      "Step: 710, Loss: 1.3617132902145386, Accuracy: 0.6722925457102672\n",
      "Step: 711, Loss: 1.3754109144210815, Accuracy: 0.6720505617977528\n",
      "Step: 712, Loss: 1.1575804948806763, Accuracy: 0.6721598877980365\n",
      "Step: 713, Loss: 1.4082708358764648, Accuracy: 0.6719187675070029\n",
      "Step: 714, Loss: 1.1244739294052124, Accuracy: 0.672027972027972\n",
      "Step: 715, Loss: 1.2762726545333862, Accuracy: 0.6720204841713222\n",
      "Step: 716, Loss: 1.2042156457901, Accuracy: 0.6720130172013017\n",
      "Step: 717, Loss: 1.235225796699524, Accuracy: 0.6720055710306406\n",
      "Step: 718, Loss: 1.316564679145813, Accuracy: 0.6718822438572091\n",
      "Step: 719, Loss: 1.1359199285507202, Accuracy: 0.6719907407407407\n",
      "Step: 720, Loss: 1.012994647026062, Accuracy: 0.6723300970873787\n",
      "Step: 721, Loss: 1.2302547693252563, Accuracy: 0.6723222530009234\n",
      "Step: 722, Loss: 1.211490511894226, Accuracy: 0.6724296911018903\n",
      "Step: 723, Loss: 1.234405755996704, Accuracy: 0.6724217311233885\n",
      "Step: 724, Loss: 1.2324130535125732, Accuracy: 0.6724137931034483\n",
      "Step: 725, Loss: 1.3003268241882324, Accuracy: 0.6722910927456383\n",
      "Step: 726, Loss: 1.2935287952423096, Accuracy: 0.6721687299403943\n",
      "Step: 727, Loss: 1.2720898389816284, Accuracy: 0.6720467032967034\n",
      "Step: 728, Loss: 1.306503415107727, Accuracy: 0.6719250114311842\n",
      "Step: 729, Loss: 1.3244236707687378, Accuracy: 0.6718036529680366\n",
      "Step: 730, Loss: 1.3244315385818481, Accuracy: 0.6716826265389877\n",
      "Step: 731, Loss: 1.397739052772522, Accuracy: 0.671448087431694\n",
      "Step: 732, Loss: 1.217339038848877, Accuracy: 0.6714415643474306\n",
      "Step: 733, Loss: 1.6036542654037476, Accuracy: 0.6708673932788374\n",
      "Step: 734, Loss: 1.3579583168029785, Accuracy: 0.6707482993197279\n",
      "Step: 735, Loss: 1.0836058855056763, Accuracy: 0.6709692028985508\n",
      "Step: 736, Loss: 1.2631362676620483, Accuracy: 0.6709633649932157\n",
      "Step: 737, Loss: 1.2763537168502808, Accuracy: 0.6709575429087624\n",
      "Step: 738, Loss: 1.316888451576233, Accuracy: 0.6708389715832206\n",
      "Step: 739, Loss: 1.1072732210159302, Accuracy: 0.6710585585585586\n",
      "Step: 740, Loss: 1.404438853263855, Accuracy: 0.6708277103013945\n",
      "Step: 741, Loss: 1.2107068300247192, Accuracy: 0.670822102425876\n",
      "Step: 742, Loss: 1.3224307298660278, Accuracy: 0.670704351727232\n",
      "Step: 743, Loss: 1.2065491676330566, Accuracy: 0.6706989247311828\n",
      "Step: 744, Loss: 1.2046719789505005, Accuracy: 0.6706935123042506\n",
      "Step: 745, Loss: 1.2889866828918457, Accuracy: 0.6706881143878463\n",
      "Step: 746, Loss: 1.0529574155807495, Accuracy: 0.6709058456046408\n",
      "Step: 747, Loss: 1.03077232837677, Accuracy: 0.6712344028520499\n",
      "Step: 748, Loss: 1.4156746864318848, Accuracy: 0.6710057854917668\n",
      "Step: 749, Loss: 1.6351208686828613, Accuracy: 0.6704444444444444\n",
      "Step: 750, Loss: 1.177411675453186, Accuracy: 0.6705503772747448\n",
      "Step: 751, Loss: 1.347199559211731, Accuracy: 0.6703235815602837\n",
      "Step: 752, Loss: 1.5283565521240234, Accuracy: 0.6698760513501549\n",
      "Step: 753, Loss: 1.1377569437026978, Accuracy: 0.6699823165340407\n",
      "Step: 754, Loss: 1.3443862199783325, Accuracy: 0.6698675496688742\n",
      "Step: 755, Loss: 1.3965187072753906, Accuracy: 0.6696428571428571\n",
      "Step: 756, Loss: 1.351997971534729, Accuracy: 0.6695288419198591\n",
      "Step: 757, Loss: 1.165336012840271, Accuracy: 0.6696350043975374\n",
      "Step: 758, Loss: 1.5417836904525757, Accuracy: 0.6691919191919192\n",
      "Step: 759, Loss: 1.4560848474502563, Accuracy: 0.668859649122807\n",
      "Step: 760, Loss: 1.2852202653884888, Accuracy: 0.6687472623740692\n",
      "Step: 761, Loss: 1.2214757204055786, Accuracy: 0.6687445319335084\n",
      "Step: 762, Loss: 1.20463228225708, Accuracy: 0.6687418086500655\n",
      "Step: 763, Loss: 1.173444390296936, Accuracy: 0.668848167539267\n",
      "Step: 764, Loss: 1.2085659503936768, Accuracy: 0.6688453159041394\n",
      "Step: 765, Loss: 1.3512340784072876, Accuracy: 0.668733681462141\n",
      "Step: 766, Loss: 1.3323582410812378, Accuracy: 0.6685136897001304\n",
      "Step: 767, Loss: 1.4485855102539062, Accuracy: 0.6681857638888888\n",
      "Step: 768, Loss: 1.2542632818222046, Accuracy: 0.6681837884698743\n",
      "Step: 769, Loss: 1.4402389526367188, Accuracy: 0.667965367965368\n",
      "Step: 770, Loss: 1.290243148803711, Accuracy: 0.6678555987894509\n",
      "Step: 771, Loss: 1.2242157459259033, Accuracy: 0.6678540587219344\n",
      "Step: 772, Loss: 1.2969582080841064, Accuracy: 0.6677447175506684\n",
      "Step: 773, Loss: 1.3635560274124146, Accuracy: 0.6675279931093885\n",
      "Step: 774, Loss: 1.098494529724121, Accuracy: 0.667741935483871\n",
      "Step: 775, Loss: 1.3060020208358765, Accuracy: 0.6676331615120275\n",
      "Step: 776, Loss: 1.3394221067428589, Accuracy: 0.6674174174174174\n",
      "Step: 777, Loss: 1.0819675922393799, Accuracy: 0.667630676949443\n",
      "Step: 778, Loss: 1.0582209825515747, Accuracy: 0.6679503637141635\n",
      "Step: 779, Loss: 1.2857283353805542, Accuracy: 0.6678418803418803\n",
      "Step: 780, Loss: 0.9576504230499268, Accuracy: 0.6682671788305591\n",
      "Step: 781, Loss: 1.2590118646621704, Accuracy: 0.6682651321398124\n",
      "Step: 782, Loss: 1.0180009603500366, Accuracy: 0.6685823754789272\n",
      "Step: 783, Loss: 1.1182975769042969, Accuracy: 0.6686862244897959\n",
      "Step: 784, Loss: 1.4112428426742554, Accuracy: 0.6684713375796179\n",
      "Step: 785, Loss: 1.245897889137268, Accuracy: 0.6684690415606446\n",
      "Step: 786, Loss: 1.2133021354675293, Accuracy: 0.6684667513765353\n",
      "Step: 787, Loss: 1.0637414455413818, Accuracy: 0.668675972927242\n",
      "Step: 788, Loss: 1.5195083618164062, Accuracy: 0.6682509505703422\n",
      "Step: 789, Loss: 1.182798981666565, Accuracy: 0.6683544303797468\n",
      "Step: 790, Loss: 1.3511276245117188, Accuracy: 0.668141592920354\n",
      "Step: 791, Loss: 1.2205452919006348, Accuracy: 0.6681397306397306\n",
      "Step: 792, Loss: 1.322402000427246, Accuracy: 0.6680327868852459\n",
      "Step: 793, Loss: 1.1737030744552612, Accuracy: 0.6681360201511335\n",
      "Step: 794, Loss: 1.2226893901824951, Accuracy: 0.6681341719077568\n",
      "Step: 795, Loss: 1.0437134504318237, Accuracy: 0.6684463986599665\n",
      "Step: 796, Loss: 1.1525617837905884, Accuracy: 0.6685487243831033\n",
      "Step: 797, Loss: 1.3560733795166016, Accuracy: 0.6683375104427736\n",
      "Step: 798, Loss: 1.0128072500228882, Accuracy: 0.668648310387985\n",
      "Step: 799, Loss: 1.1110731363296509, Accuracy: 0.66875\n",
      "Step: 800, Loss: 1.3584095239639282, Accuracy: 0.6685393258426966\n",
      "Step: 801, Loss: 1.4843991994857788, Accuracy: 0.6682252701579385\n",
      "Step: 802, Loss: 1.1911120414733887, Accuracy: 0.6683271066832711\n",
      "Step: 803, Loss: 1.2081462144851685, Accuracy: 0.6683250414593698\n",
      "Step: 804, Loss: 1.184362530708313, Accuracy: 0.6684265010351967\n",
      "Step: 805, Loss: 1.0596661567687988, Accuracy: 0.668631100082713\n",
      "Step: 806, Loss: 1.0943083763122559, Accuracy: 0.6688351920693928\n",
      "Step: 807, Loss: 1.2272261381149292, Accuracy: 0.6688325082508251\n",
      "Step: 808, Loss: 1.2302570343017578, Accuracy: 0.6688298310671611\n",
      "Step: 809, Loss: 1.3142642974853516, Accuracy: 0.668724279835391\n",
      "Step: 810, Loss: 1.2132221460342407, Accuracy: 0.66872174270448\n",
      "Step: 811, Loss: 1.2046931982040405, Accuracy: 0.6687192118226601\n",
      "Step: 812, Loss: 1.482099175453186, Accuracy: 0.6683066830668307\n",
      "Step: 813, Loss: 1.300126314163208, Accuracy: 0.6682022932022932\n",
      "Step: 814, Loss: 1.2440534830093384, Accuracy: 0.668200408997955\n",
      "Step: 815, Loss: 1.097753643989563, Accuracy: 0.6684027777777778\n",
      "Step: 816, Loss: 1.052141547203064, Accuracy: 0.6687066503467972\n",
      "Step: 817, Loss: 1.0580188035964966, Accuracy: 0.6689079054604727\n",
      "Step: 818, Loss: 1.1839762926101685, Accuracy: 0.6689051689051689\n",
      "Step: 819, Loss: 1.266053318977356, Accuracy: 0.6689024390243903\n",
      "Step: 820, Loss: 1.1605265140533447, Accuracy: 0.6690012180267966\n",
      "Step: 821, Loss: 1.3068779706954956, Accuracy: 0.6688969991889699\n",
      "Step: 822, Loss: 1.1690109968185425, Accuracy: 0.6689955447549615\n",
      "Step: 823, Loss: 1.188137412071228, Accuracy: 0.6689927184466019\n",
      "Step: 824, Loss: 1.1275784969329834, Accuracy: 0.6691919191919192\n",
      "Step: 825, Loss: 1.4108418226242065, Accuracy: 0.6689870863599677\n",
      "Step: 826, Loss: 1.2363080978393555, Accuracy: 0.6689842805320435\n",
      "Step: 827, Loss: 1.1645472049713135, Accuracy: 0.6690821256038647\n",
      "Step: 828, Loss: 1.4725241661071777, Accuracy: 0.6687776437474869\n",
      "Step: 829, Loss: 1.2136831283569336, Accuracy: 0.6687751004016064\n",
      "Step: 830, Loss: 1.2868677377700806, Accuracy: 0.668672282390694\n",
      "Step: 831, Loss: 1.0168977975845337, Accuracy: 0.6689703525641025\n",
      "Step: 832, Loss: 1.260791540145874, Accuracy: 0.668967587034814\n",
      "Step: 833, Loss: 1.3805011510849, Accuracy: 0.6687649880095923\n",
      "Step: 834, Loss: 1.147044062614441, Accuracy: 0.6688622754491018\n",
      "Step: 835, Loss: 1.4337841272354126, Accuracy: 0.6686602870813397\n",
      "Step: 836, Loss: 0.9294182658195496, Accuracy: 0.6690561529271206\n",
      "Step: 837, Loss: 1.2318936586380005, Accuracy: 0.6689538583929993\n",
      "Step: 838, Loss: 1.3195372819900513, Accuracy: 0.6688518077075885\n",
      "Step: 839, Loss: 1.5297107696533203, Accuracy: 0.6684523809523809\n",
      "Step: 840, Loss: 1.0420753955841064, Accuracy: 0.668747522790329\n",
      "Step: 841, Loss: 1.4531527757644653, Accuracy: 0.6685471100554236\n",
      "Step: 842, Loss: 1.2600009441375732, Accuracy: 0.6685448793989719\n",
      "Step: 843, Loss: 1.1353212594985962, Accuracy: 0.6686413902053713\n",
      "Step: 844, Loss: 1.1793686151504517, Accuracy: 0.6687376725838264\n",
      "Step: 845, Loss: 1.2897770404815674, Accuracy: 0.6686367218282112\n",
      "Step: 846, Loss: 1.2537678480148315, Accuracy: 0.6686343959071231\n",
      "Step: 847, Loss: 1.2361150979995728, Accuracy: 0.6686320754716981\n",
      "Step: 848, Loss: 1.2597391605377197, Accuracy: 0.6686297605025521\n",
      "Step: 849, Loss: 1.2224841117858887, Accuracy: 0.6686274509803921\n",
      "Step: 850, Loss: 1.0686458349227905, Accuracy: 0.6688209949079514\n",
      "Step: 851, Loss: 1.239038348197937, Accuracy: 0.6688184663536776\n",
      "Step: 852, Loss: 1.1992331743240356, Accuracy: 0.6688159437280188\n",
      "Step: 853, Loss: 0.996807336807251, Accuracy: 0.6691061670569868\n",
      "Step: 854, Loss: 1.2037774324417114, Accuracy: 0.669103313840156\n",
      "Step: 855, Loss: 1.0589650869369507, Accuracy: 0.6692951713395638\n",
      "Step: 856, Loss: 1.40438711643219, Accuracy: 0.6690976273823415\n",
      "Step: 857, Loss: 1.156409502029419, Accuracy: 0.6691919191919192\n",
      "Step: 858, Loss: 1.0587561130523682, Accuracy: 0.6693830034924331\n",
      "Step: 859, Loss: 1.2965399026870728, Accuracy: 0.6692829457364341\n",
      "Step: 860, Loss: 1.278054118156433, Accuracy: 0.6691831204026326\n",
      "Step: 861, Loss: 1.0072952508926392, Accuracy: 0.6694702242846094\n",
      "Step: 862, Loss: 1.2858728170394897, Accuracy: 0.6693704132869834\n",
      "Step: 863, Loss: 1.2277262210845947, Accuracy: 0.6693672839506173\n",
      "Step: 864, Loss: 1.188895583152771, Accuracy: 0.669364161849711\n",
      "Step: 865, Loss: 1.2251663208007812, Accuracy: 0.6693610469591994\n",
      "Step: 866, Loss: 1.261462688446045, Accuracy: 0.669357939254133\n",
      "Step: 867, Loss: 1.0815823078155518, Accuracy: 0.6695468509984639\n",
      "Step: 868, Loss: 1.178696870803833, Accuracy: 0.6696394322976601\n",
      "Step: 869, Loss: 1.1591662168502808, Accuracy: 0.6697318007662836\n",
      "Step: 870, Loss: 1.0589271783828735, Accuracy: 0.6699196326061998\n",
      "Step: 871, Loss: 1.2529829740524292, Accuracy: 0.6699159021406728\n",
      "Step: 872, Loss: 1.2416718006134033, Accuracy: 0.6699121802214586\n",
      "Step: 873, Loss: 1.5430750846862793, Accuracy: 0.6695270785659801\n",
      "Step: 874, Loss: 1.302543044090271, Accuracy: 0.6695238095238095\n",
      "Step: 875, Loss: 1.5764926671981812, Accuracy: 0.6691400304414004\n",
      "Step: 876, Loss: 1.1505491733551025, Accuracy: 0.66923223109084\n",
      "Step: 877, Loss: 1.2146251201629639, Accuracy: 0.6692293090356871\n",
      "Step: 878, Loss: 1.3340529203414917, Accuracy: 0.6691315889268108\n",
      "Step: 879, Loss: 1.137749195098877, Accuracy: 0.6693181818181818\n",
      "Step: 880, Loss: 1.2299599647521973, Accuracy: 0.6693151721528566\n",
      "Step: 881, Loss: 1.3179553747177124, Accuracy: 0.66921768707483\n",
      "Step: 882, Loss: 1.1450793743133545, Accuracy: 0.6694035485088713\n",
      "Step: 883, Loss: 1.141363501548767, Accuracy: 0.6694947209653092\n",
      "Step: 884, Loss: 1.0478237867355347, Accuracy: 0.6696798493408663\n",
      "Step: 885, Loss: 1.2814127206802368, Accuracy: 0.6695823927765236\n",
      "Step: 886, Loss: 1.0575731992721558, Accuracy: 0.6697670048853814\n",
      "Step: 887, Loss: 1.1708592176437378, Accuracy: 0.6698573573573574\n",
      "Step: 888, Loss: 1.311713457107544, Accuracy: 0.6697600299962505\n",
      "Step: 889, Loss: 1.1655715703964233, Accuracy: 0.6698501872659176\n",
      "Step: 890, Loss: 1.0954188108444214, Accuracy: 0.67003367003367\n",
      "Step: 891, Loss: 1.0760917663574219, Accuracy: 0.6703101644245142\n",
      "Step: 892, Loss: 1.099841833114624, Accuracy: 0.6704927211646137\n",
      "Step: 893, Loss: 1.232974648475647, Accuracy: 0.6704884414615958\n",
      "Step: 894, Loss: 1.0828425884246826, Accuracy: 0.6706703910614525\n",
      "Step: 895, Loss: 1.336621880531311, Accuracy: 0.6705729166666666\n",
      "Step: 896, Loss: 1.1587849855422974, Accuracy: 0.670661464139725\n",
      "Step: 897, Loss: 1.4121880531311035, Accuracy: 0.6704714179658501\n",
      "Step: 898, Loss: 1.3662277460098267, Accuracy: 0.6702817945865777\n",
      "Step: 899, Loss: 1.2970532178878784, Accuracy: 0.6701851851851852\n",
      "Step: 900, Loss: 1.0426547527313232, Accuracy: 0.6703662597114317\n",
      "Step: 901, Loss: 1.092214584350586, Accuracy: 0.6705469327420547\n",
      "Step: 902, Loss: 1.5619322061538696, Accuracy: 0.6701734957548912\n",
      "Step: 903, Loss: 1.4795631170272827, Accuracy: 0.6698930678466076\n",
      "Step: 904, Loss: 1.1474241018295288, Accuracy: 0.6699815837937385\n",
      "Step: 905, Loss: 1.3972300291061401, Accuracy: 0.669793966151582\n",
      "Step: 906, Loss: 1.176087737083435, Accuracy: 0.6698823961778758\n",
      "Step: 907, Loss: 1.2703306674957275, Accuracy: 0.6697870778267254\n",
      "Step: 908, Loss: 1.422144889831543, Accuracy: 0.6696002933626696\n",
      "Step: 909, Loss: 1.2779181003570557, Accuracy: 0.6695970695970695\n",
      "Step: 910, Loss: 1.1823468208312988, Accuracy: 0.6696853274789608\n",
      "Step: 911, Loss: 1.3170710802078247, Accuracy: 0.6695906432748538\n",
      "Step: 912, Loss: 1.1406916379928589, Accuracy: 0.6696787148594378\n",
      "Step: 913, Loss: 1.227290391921997, Accuracy: 0.6696754194018965\n",
      "Step: 914, Loss: 1.357803463935852, Accuracy: 0.6694899817850638\n",
      "Step: 915, Loss: 1.1186472177505493, Accuracy: 0.6695778748180495\n",
      "Step: 916, Loss: 1.2566256523132324, Accuracy: 0.6695747001090513\n",
      "Step: 917, Loss: 1.0756909847259521, Accuracy: 0.6697530864197531\n",
      "Step: 918, Loss: 1.3017418384552002, Accuracy: 0.6696590496916939\n",
      "Step: 919, Loss: 1.3270392417907715, Accuracy: 0.6695652173913044\n",
      "Step: 920, Loss: 1.3066859245300293, Accuracy: 0.6694715888526963\n",
      "Step: 921, Loss: 1.23893141746521, Accuracy: 0.669468546637744\n",
      "Step: 922, Loss: 1.234743595123291, Accuracy: 0.6693752257132539\n",
      "Step: 923, Loss: 1.1303328275680542, Accuracy: 0.6694624819624819\n",
      "Step: 924, Loss: 1.160962462425232, Accuracy: 0.6695495495495496\n",
      "Step: 925, Loss: 1.1038157939910889, Accuracy: 0.6697264218862491\n",
      "Step: 926, Loss: 1.3101180791854858, Accuracy: 0.6696332254584682\n",
      "Step: 927, Loss: 1.3992539644241333, Accuracy: 0.6694504310344828\n",
      "Step: 928, Loss: 1.235865592956543, Accuracy: 0.6694474345174022\n",
      "Step: 929, Loss: 1.166570782661438, Accuracy: 0.6694444444444444\n",
      "Step: 930, Loss: 1.3144960403442383, Accuracy: 0.6693519513068386\n",
      "Step: 931, Loss: 1.3882851600646973, Accuracy: 0.669170243204578\n",
      "Step: 932, Loss: 1.275856614112854, Accuracy: 0.6690782422293676\n",
      "Step: 933, Loss: 1.154532790184021, Accuracy: 0.6691648822269807\n",
      "Step: 934, Loss: 1.2469844818115234, Accuracy: 0.6690730837789661\n",
      "Step: 935, Loss: 1.229956030845642, Accuracy: 0.6690705128205128\n",
      "Step: 936, Loss: 1.1717404127120972, Accuracy: 0.6691568836712913\n",
      "Step: 937, Loss: 1.318677544593811, Accuracy: 0.6690653873489695\n",
      "Step: 938, Loss: 1.395474910736084, Accuracy: 0.6688853390131345\n",
      "Step: 939, Loss: 1.0113649368286133, Accuracy: 0.6691489361702128\n",
      "Step: 940, Loss: 1.3930639028549194, Accuracy: 0.6689691817215728\n",
      "Step: 941, Loss: 1.210344672203064, Accuracy: 0.6690552016985138\n",
      "Step: 942, Loss: 1.0607067346572876, Accuracy: 0.6692294096854012\n",
      "Step: 943, Loss: 1.0696223974227905, Accuracy: 0.6694032485875706\n",
      "Step: 944, Loss: 1.2833647727966309, Accuracy: 0.6693121693121693\n",
      "Step: 945, Loss: 1.2424105405807495, Accuracy: 0.6693093727977449\n",
      "Step: 946, Loss: 1.1032474040985107, Accuracy: 0.6694825765575502\n",
      "Step: 947, Loss: 1.370500087738037, Accuracy: 0.6693037974683544\n",
      "Step: 948, Loss: 1.4904508590698242, Accuracy: 0.669037583421145\n",
      "Step: 949, Loss: 1.2977458238601685, Accuracy: 0.6690350877192982\n",
      "Step: 950, Loss: 0.9996417164802551, Accuracy: 0.6692954784437434\n",
      "Step: 951, Loss: 1.2183140516281128, Accuracy: 0.6692927170868347\n",
      "Step: 952, Loss: 1.1774722337722778, Accuracy: 0.6693774046869535\n",
      "Step: 953, Loss: 1.3119785785675049, Accuracy: 0.6692872117400419\n",
      "Step: 954, Loss: 1.1559282541275024, Accuracy: 0.6693717277486911\n",
      "Step: 955, Loss: 1.3839298486709595, Accuracy: 0.669194560669456\n",
      "Step: 956, Loss: 1.2346583604812622, Accuracy: 0.6691919191919192\n",
      "Step: 957, Loss: 1.116685152053833, Accuracy: 0.6693632567849687\n",
      "Step: 958, Loss: 1.1375843286514282, Accuracy: 0.6694473409801877\n",
      "Step: 959, Loss: 1.2883394956588745, Accuracy: 0.6693576388888889\n",
      "Step: 960, Loss: 1.3960765600204468, Accuracy: 0.6691814082552896\n",
      "Step: 961, Loss: 1.2602654695510864, Accuracy: 0.6691787941787942\n",
      "Step: 962, Loss: 1.3053569793701172, Accuracy: 0.6690896503980616\n",
      "Step: 963, Loss: 1.1398768424987793, Accuracy: 0.6691735822959889\n",
      "Step: 964, Loss: 1.4499715566635132, Accuracy: 0.6689982728842833\n",
      "Step: 965, Loss: 1.2450594902038574, Accuracy: 0.6689958592132506\n",
      "Step: 966, Loss: 1.0661715269088745, Accuracy: 0.6691658048948639\n",
      "Step: 967, Loss: 1.2827192544937134, Accuracy: 0.6690771349862259\n",
      "Step: 968, Loss: 1.2375565767288208, Accuracy: 0.6690746474028207\n",
      "Step: 969, Loss: 1.1974073648452759, Accuracy: 0.6690721649484536\n",
      "Step: 970, Loss: 1.2423689365386963, Accuracy: 0.6690696876072777\n",
      "Step: 971, Loss: 1.2450133562088013, Accuracy: 0.6690672153635117\n",
      "Step: 972, Loss: 1.2260173559188843, Accuracy: 0.6691503939705379\n",
      "Step: 973, Loss: 1.210259199142456, Accuracy: 0.6691478439425051\n",
      "Step: 974, Loss: 1.1678110361099243, Accuracy: 0.6692307692307692\n",
      "Step: 975, Loss: 1.234765648841858, Accuracy: 0.6692281420765027\n",
      "Step: 976, Loss: 1.1657241582870483, Accuracy: 0.669310815421358\n",
      "Step: 977, Loss: 1.4064968824386597, Accuracy: 0.6691376959781867\n",
      "Step: 978, Loss: 1.2194994688034058, Accuracy: 0.6691351719441607\n",
      "Step: 979, Loss: 1.4019109010696411, Accuracy: 0.6689625850340136\n",
      "Step: 980, Loss: 1.3165833950042725, Accuracy: 0.6688752973156643\n",
      "Step: 981, Loss: 1.3397951126098633, Accuracy: 0.6687881873727087\n",
      "Step: 982, Loss: 1.1606056690216064, Accuracy: 0.6688708036622584\n",
      "Step: 983, Loss: 1.1707170009613037, Accuracy: 0.6689532520325203\n",
      "Step: 984, Loss: 1.5735487937927246, Accuracy: 0.6686125211505922\n",
      "Step: 985, Loss: 1.1002119779586792, Accuracy: 0.6687795807978364\n",
      "Step: 986, Loss: 1.437846302986145, Accuracy: 0.6685241472475515\n",
      "Step: 987, Loss: 1.3912662267684937, Accuracy: 0.6682692307692307\n",
      "Step: 988, Loss: 1.2659662961959839, Accuracy: 0.6681833501853724\n",
      "Step: 989, Loss: 1.1724952459335327, Accuracy: 0.6682659932659932\n",
      "Step: 990, Loss: 1.29868483543396, Accuracy: 0.6681802892700975\n",
      "Step: 991, Loss: 1.3939183950424194, Accuracy: 0.668010752688172\n",
      "Step: 992, Loss: 1.2538882493972778, Accuracy: 0.6680093991272239\n",
      "Step: 993, Loss: 1.4653257131576538, Accuracy: 0.6677565392354124\n",
      "Step: 994, Loss: 1.137933373451233, Accuracy: 0.6678391959798995\n",
      "Step: 995, Loss: 1.5025115013122559, Accuracy: 0.6675033467202142\n",
      "Step: 996, Loss: 1.3323358297348022, Accuracy: 0.6674189234369776\n",
      "Step: 997, Loss: 1.1530267000198364, Accuracy: 0.66750167000668\n",
      "Step: 998, Loss: 1.2433229684829712, Accuracy: 0.6675008341675008\n",
      "Step: 999, Loss: 1.128693699836731, Accuracy: 0.6675833333333333\n",
      "Step: 1000, Loss: 1.090720772743225, Accuracy: 0.6676656676656677\n",
      "Step: 1001, Loss: 1.4234042167663574, Accuracy: 0.6674151696606786\n",
      "Step: 1002, Loss: 1.3231399059295654, Accuracy: 0.6673313393153871\n",
      "Step: 1003, Loss: 1.3054972887039185, Accuracy: 0.6672476759628154\n",
      "Step: 1004, Loss: 1.1377904415130615, Accuracy: 0.6673300165837479\n",
      "Step: 1005, Loss: 1.0970169305801392, Accuracy: 0.6674950298210736\n",
      "Step: 1006, Loss: 1.3804987668991089, Accuracy: 0.6673286991062563\n",
      "Step: 1007, Loss: 1.3195701837539673, Accuracy: 0.6672453703703703\n",
      "Step: 1008, Loss: 1.2091126441955566, Accuracy: 0.6672447968285431\n",
      "Step: 1009, Loss: 1.322670340538025, Accuracy: 0.6671617161716171\n",
      "Step: 1010, Loss: 1.2352131605148315, Accuracy: 0.6671612265084075\n",
      "Step: 1011, Loss: 1.215397596359253, Accuracy: 0.6671607378129117\n",
      "Step: 1012, Loss: 1.407604694366455, Accuracy: 0.6669134583744653\n",
      "Step: 1013, Loss: 1.2223286628723145, Accuracy: 0.6669953977646286\n",
      "Step: 1014, Loss: 1.3415685892105103, Accuracy: 0.6669129720853859\n",
      "Step: 1015, Loss: 1.178693413734436, Accuracy: 0.6669127296587927\n",
      "Step: 1016, Loss: 1.6040005683898926, Accuracy: 0.6665027859718126\n",
      "Step: 1017, Loss: 1.1858718395233154, Accuracy: 0.6665848068107401\n",
      "Step: 1018, Loss: 1.2013792991638184, Accuracy: 0.6666666666666666\n",
      "Step: 1019, Loss: 1.2403360605239868, Accuracy: 0.6665849673202614\n",
      "Step: 1020, Loss: 1.0147711038589478, Accuracy: 0.6668299053215802\n",
      "Step: 1021, Loss: 1.2492729425430298, Accuracy: 0.6668297455968689\n",
      "Step: 1022, Loss: 1.294589638710022, Accuracy: 0.6667481264255458\n",
      "Step: 1023, Loss: 1.3848681449890137, Accuracy: 0.6665852864583334\n",
      "Step: 1024, Loss: 1.4400486946105957, Accuracy: 0.6663414634146342\n",
      "Step: 1025, Loss: 1.314466118812561, Accuracy: 0.6662605588044185\n",
      "Step: 1026, Loss: 1.3270810842514038, Accuracy: 0.666179811749432\n",
      "Step: 1027, Loss: 1.0909717082977295, Accuracy: 0.6663424124513618\n",
      "Step: 1028, Loss: 0.955394446849823, Accuracy: 0.6666666666666666\n",
      "Step: 1029, Loss: 1.2840172052383423, Accuracy: 0.6666666666666666\n",
      "Step: 1030, Loss: 1.3096331357955933, Accuracy: 0.6665858389912707\n",
      "Step: 1031, Loss: 1.1340631246566772, Accuracy: 0.6666666666666666\n",
      "Step: 1032, Loss: 1.1075665950775146, Accuracy: 0.6668280090351726\n",
      "Step: 1033, Loss: 1.2672873735427856, Accuracy: 0.6668278529980658\n",
      "Step: 1034, Loss: 1.1847397089004517, Accuracy: 0.6669082125603865\n",
      "Step: 1035, Loss: 1.1277353763580322, Accuracy: 0.666988416988417\n",
      "Step: 1036, Loss: 1.1211167573928833, Accuracy: 0.6670684667309547\n",
      "Step: 1037, Loss: 1.4123040437698364, Accuracy: 0.666907514450867\n",
      "Step: 1038, Loss: 1.3511604070663452, Accuracy: 0.6668270773179339\n",
      "Step: 1039, Loss: 1.0698944330215454, Accuracy: 0.6669871794871794\n",
      "Step: 1040, Loss: 1.3584531545639038, Accuracy: 0.6668267691322446\n",
      "Step: 1041, Loss: 1.295907974243164, Accuracy: 0.666746641074856\n",
      "Step: 1042, Loss: 0.9335677623748779, Accuracy: 0.6670661553211888\n",
      "Step: 1043, Loss: 1.2813059091567993, Accuracy: 0.667065772669221\n",
      "Step: 1044, Loss: 1.1808366775512695, Accuracy: 0.6671451355661882\n",
      "Step: 1045, Loss: 1.3906364440917969, Accuracy: 0.6669853409815168\n",
      "Step: 1046, Loss: 1.2651104927062988, Accuracy: 0.6669054441260746\n",
      "Step: 1047, Loss: 1.1575753688812256, Accuracy: 0.6669847328244275\n",
      "Step: 1048, Loss: 1.0835334062576294, Accuracy: 0.6671433110899269\n",
      "Step: 1049, Loss: 1.2730932235717773, Accuracy: 0.667063492063492\n",
      "Step: 1050, Loss: 1.0326944589614868, Accuracy: 0.6673009831906122\n",
      "Step: 1051, Loss: 1.4698275327682495, Accuracy: 0.6670627376425855\n",
      "Step: 1052, Loss: 1.0820564031600952, Accuracy: 0.6672206394428617\n",
      "Step: 1053, Loss: 1.2745132446289062, Accuracy: 0.6671410499683744\n",
      "Step: 1054, Loss: 1.5162276029586792, Accuracy: 0.6669036334913112\n",
      "Step: 1055, Loss: 1.1886532306671143, Accuracy: 0.6669034090909091\n",
      "Step: 1056, Loss: 1.3779572248458862, Accuracy: 0.6667455061494797\n",
      "Step: 1057, Loss: 1.3735480308532715, Accuracy: 0.6665879017013232\n",
      "Step: 1058, Loss: 1.3299247026443481, Accuracy: 0.6665092854894554\n",
      "Step: 1059, Loss: 1.2216144800186157, Accuracy: 0.6665094339622641\n",
      "Step: 1060, Loss: 1.4125962257385254, Accuracy: 0.6662739553879987\n",
      "Step: 1061, Loss: 1.4295192956924438, Accuracy: 0.6661173885750157\n",
      "Step: 1062, Loss: 1.5001931190490723, Accuracy: 0.6658827218563813\n",
      "Step: 1063, Loss: 1.210135817527771, Accuracy: 0.6658834586466166\n",
      "Step: 1064, Loss: 1.3741167783737183, Accuracy: 0.6657276995305165\n",
      "Step: 1065, Loss: 1.3513332605361938, Accuracy: 0.665650406504065\n",
      "Step: 1066, Loss: 1.5065771341323853, Accuracy: 0.66533895657607\n",
      "Step: 1067, Loss: 1.3134183883666992, Accuracy: 0.6652621722846442\n",
      "Step: 1068, Loss: 1.1552451848983765, Accuracy: 0.6653414405986904\n",
      "Step: 1069, Loss: 1.2345396280288696, Accuracy: 0.6653426791277258\n",
      "Step: 1070, Loss: 1.3056714534759521, Accuracy: 0.6653439153439153\n",
      "Step: 1071, Loss: 0.9910852313041687, Accuracy: 0.6655783582089553\n",
      "Step: 1072, Loss: 1.2307848930358887, Accuracy: 0.6655793724759242\n",
      "Step: 1073, Loss: 1.1614936590194702, Accuracy: 0.6656579764121664\n",
      "Step: 1074, Loss: 1.2346054315567017, Accuracy: 0.6656589147286822\n",
      "Step: 1075, Loss: 1.1712055206298828, Accuracy: 0.6656598513011153\n",
      "Step: 1076, Loss: 1.248370885848999, Accuracy: 0.6656607861343238\n",
      "Step: 1077, Loss: 1.347427248954773, Accuracy: 0.6655071119356833\n",
      "Step: 1078, Loss: 1.1841835975646973, Accuracy: 0.665508186592524\n",
      "Step: 1079, Loss: 1.3903452157974243, Accuracy: 0.6652777777777777\n",
      "Step: 1080, Loss: 1.3871113061904907, Accuracy: 0.6651248843663274\n",
      "Step: 1081, Loss: 1.1705962419509888, Accuracy: 0.6652033271719039\n",
      "Step: 1082, Loss: 1.3536609411239624, Accuracy: 0.6650507848568791\n",
      "Step: 1083, Loss: 1.5196233987808228, Accuracy: 0.6647447724477245\n",
      "Step: 1084, Loss: 1.3081302642822266, Accuracy: 0.6646697388632873\n",
      "Step: 1085, Loss: 1.3880873918533325, Accuracy: 0.6645181092694905\n",
      "Step: 1086, Loss: 1.1049984693527222, Accuracy: 0.6646734130634775\n",
      "Step: 1087, Loss: 1.1564371585845947, Accuracy: 0.664828431372549\n",
      "Step: 1088, Loss: 0.9895436763763428, Accuracy: 0.665136210590756\n",
      "Step: 1089, Loss: 1.3567261695861816, Accuracy: 0.6650611620795107\n",
      "Step: 1090, Loss: 1.3780674934387207, Accuracy: 0.6649098686220593\n",
      "Step: 1091, Loss: 1.5839896202087402, Accuracy: 0.6645299145299145\n",
      "Step: 1092, Loss: 1.0629761219024658, Accuracy: 0.6646843549862763\n",
      "Step: 1093, Loss: 1.1975563764572144, Accuracy: 0.664686166971359\n",
      "Step: 1094, Loss: 0.9688012599945068, Accuracy: 0.6649923896499239\n",
      "Step: 1095, Loss: 1.254408836364746, Accuracy: 0.6649939172749392\n",
      "Step: 1096, Loss: 1.21266508102417, Accuracy: 0.6649954421148587\n",
      "Step: 1097, Loss: 1.2237404584884644, Accuracy: 0.664996964177292\n",
      "Step: 1098, Loss: 1.1304821968078613, Accuracy: 0.6650743099787686\n",
      "Step: 1099, Loss: 1.0428937673568726, Accuracy: 0.6652272727272728\n",
      "Step: 1100, Loss: 1.1474753618240356, Accuracy: 0.6653042688465032\n",
      "Step: 1101, Loss: 1.15182626247406, Accuracy: 0.6653811252268602\n",
      "Step: 1102, Loss: 1.205969214439392, Accuracy: 0.6654578422484134\n",
      "Step: 1103, Loss: 1.207764983177185, Accuracy: 0.6655344202898551\n",
      "Step: 1104, Loss: 1.2648440599441528, Accuracy: 0.6655354449472096\n",
      "Step: 1105, Loss: 1.356709599494934, Accuracy: 0.6654611211573237\n",
      "Step: 1106, Loss: 1.1449813842773438, Accuracy: 0.6655374887082204\n",
      "Step: 1107, Loss: 0.9160134792327881, Accuracy: 0.6658393501805054\n",
      "Step: 1108, Loss: 1.1788678169250488, Accuracy: 0.6659152389540126\n",
      "Step: 1109, Loss: 1.0704618692398071, Accuracy: 0.6660660660660661\n",
      "Step: 1110, Loss: 1.2921940088272095, Accuracy: 0.665991599159916\n",
      "Step: 1111, Loss: 1.2355644702911377, Accuracy: 0.665992206235012\n",
      "Step: 1112, Loss: 0.995185375213623, Accuracy: 0.6662174303683738\n",
      "Step: 1113, Loss: 1.352867603302002, Accuracy: 0.6661430281268701\n",
      "Step: 1114, Loss: 1.2169383764266968, Accuracy: 0.6661434977578475\n",
      "Step: 1115, Loss: 1.2273813486099243, Accuracy: 0.6661439665471923\n",
      "Step: 1116, Loss: 1.222622036933899, Accuracy: 0.666144434497165\n",
      "Step: 1117, Loss: 1.0800269842147827, Accuracy: 0.666293977340489\n",
      "Step: 1118, Loss: 1.0762728452682495, Accuracy: 0.6664432529043789\n",
      "Step: 1119, Loss: 1.401695728302002, Accuracy: 0.6662946428571429\n",
      "Step: 1120, Loss: 1.101001262664795, Accuracy: 0.6664436515016354\n",
      "Step: 1121, Loss: 1.2780386209487915, Accuracy: 0.666369578134284\n",
      "Step: 1122, Loss: 1.2284913063049316, Accuracy: 0.6663698426832888\n",
      "Step: 1123, Loss: 1.1753648519515991, Accuracy: 0.6664442467378411\n",
      "Step: 1124, Loss: 1.2416921854019165, Accuracy: 0.6664444444444444\n",
      "Step: 1125, Loss: 0.9918494820594788, Accuracy: 0.6666666666666666\n",
      "Step: 1126, Loss: 1.3109111785888672, Accuracy: 0.6665187814256137\n",
      "Step: 1127, Loss: 1.1873105764389038, Accuracy: 0.6665189125295509\n",
      "Step: 1128, Loss: 1.1604875326156616, Accuracy: 0.6665928550339534\n",
      "Step: 1129, Loss: 1.2271424531936646, Accuracy: 0.6665929203539823\n",
      "Step: 1130, Loss: 1.4132412672042847, Accuracy: 0.6665193044503389\n",
      "Step: 1131, Loss: 1.2682653665542603, Accuracy: 0.6664458186101295\n",
      "Step: 1132, Loss: 1.0696961879730225, Accuracy: 0.6665931156222419\n",
      "Step: 1133, Loss: 1.2427358627319336, Accuracy: 0.6665931804820694\n",
      "Step: 1134, Loss: 1.1497288942337036, Accuracy: 0.6666666666666666\n",
      "Step: 1135, Loss: 1.2861295938491821, Accuracy: 0.6665933098591549\n",
      "Step: 1136, Loss: 1.1584736108779907, Accuracy: 0.6666666666666666\n",
      "Step: 1137, Loss: 1.1958177089691162, Accuracy: 0.6666666666666666\n",
      "Step: 1138, Loss: 1.3078433275222778, Accuracy: 0.6665935030728709\n",
      "Step: 1139, Loss: 1.3007351160049438, Accuracy: 0.6665204678362573\n",
      "Step: 1140, Loss: 1.2298786640167236, Accuracy: 0.6665205959684487\n",
      "Step: 1141, Loss: 1.2428721189498901, Accuracy: 0.6665207238762405\n",
      "Step: 1142, Loss: 1.3633713722229004, Accuracy: 0.6663750364537766\n",
      "Step: 1143, Loss: 1.2914140224456787, Accuracy: 0.6663024475524476\n",
      "Step: 1144, Loss: 1.123386263847351, Accuracy: 0.6663755458515284\n",
      "Step: 1145, Loss: 1.5916610956192017, Accuracy: 0.6660849331006399\n",
      "Step: 1146, Loss: 1.333727240562439, Accuracy: 0.6659401336820692\n",
      "Step: 1147, Loss: 1.1016559600830078, Accuracy: 0.6660859465737514\n",
      "Step: 1148, Loss: 1.2657544612884521, Accuracy: 0.6660864519872353\n",
      "Step: 1149, Loss: 1.4124048948287964, Accuracy: 0.6659420289855073\n",
      "Step: 1150, Loss: 1.2083494663238525, Accuracy: 0.6660150593686649\n",
      "Step: 1151, Loss: 1.1622035503387451, Accuracy: 0.6660879629629629\n",
      "Step: 1152, Loss: 1.1335772275924683, Accuracy: 0.6661607400982943\n",
      "Step: 1153, Loss: 1.4284944534301758, Accuracy: 0.665944540727903\n",
      "Step: 1154, Loss: 1.1760154962539673, Accuracy: 0.6660173160173161\n",
      "Step: 1155, Loss: 1.0755573511123657, Accuracy: 0.6661620530565168\n",
      "Step: 1156, Loss: 1.3064285516738892, Accuracy: 0.6660904638432729\n",
      "Step: 1157, Loss: 1.4021128416061401, Accuracy: 0.6658750719631549\n",
      "Step: 1158, Loss: 1.0793224573135376, Accuracy: 0.6660195570894449\n",
      "Step: 1159, Loss: 1.198118805885315, Accuracy: 0.6660201149425288\n",
      "Step: 1160, Loss: 1.2648065090179443, Accuracy: 0.6659488946310652\n",
      "Step: 1161, Loss: 1.3505969047546387, Accuracy: 0.6658777969018933\n",
      "Step: 1162, Loss: 1.378320336341858, Accuracy: 0.6658068214388077\n",
      "Step: 1163, Loss: 1.266605257987976, Accuracy: 0.665807560137457\n",
      "Step: 1164, Loss: 1.1318551301956177, Accuracy: 0.6658798283261803\n",
      "Step: 1165, Loss: 1.1672780513763428, Accuracy: 0.6659519725557461\n",
      "Step: 1166, Loss: 1.2190641164779663, Accuracy: 0.6659525849757212\n",
      "Step: 1167, Loss: 1.2384066581726074, Accuracy: 0.665953196347032\n",
      "Step: 1168, Loss: 1.279038667678833, Accuracy: 0.6659538066723696\n",
      "Step: 1169, Loss: 1.2298359870910645, Accuracy: 0.665954415954416\n",
      "Step: 1170, Loss: 1.5056930780410767, Accuracy: 0.6657415314545972\n",
      "Step: 1171, Loss: 1.4037895202636719, Accuracy: 0.6656001137656428\n",
      "Step: 1172, Loss: 1.164318323135376, Accuracy: 0.6656720659278204\n",
      "Step: 1173, Loss: 1.2386173009872437, Accuracy: 0.6656729131175468\n",
      "Step: 1174, Loss: 1.04633629322052, Accuracy: 0.6658865248226951\n",
      "Step: 1175, Loss: 1.137787938117981, Accuracy: 0.6659580498866213\n",
      "Step: 1176, Loss: 1.4632059335708618, Accuracy: 0.6657462475219484\n",
      "Step: 1177, Loss: 1.3282299041748047, Accuracy: 0.6656762874929258\n",
      "Step: 1178, Loss: 1.3363643884658813, Accuracy: 0.6656064461407973\n",
      "Step: 1179, Loss: 1.342694640159607, Accuracy: 0.6655367231638418\n",
      "Step: 1180, Loss: 1.4066410064697266, Accuracy: 0.66539655659046\n",
      "Step: 1181, Loss: 1.2901911735534668, Accuracy: 0.6653271291596164\n",
      "Step: 1182, Loss: 1.1966031789779663, Accuracy: 0.6653282614821077\n",
      "Step: 1183, Loss: 0.9952147006988525, Accuracy: 0.6655405405405406\n",
      "Step: 1184, Loss: 1.1446986198425293, Accuracy: 0.6656118143459916\n",
      "Step: 1185, Loss: 1.243409276008606, Accuracy: 0.6656127037661608\n",
      "Step: 1186, Loss: 1.2824572324752808, Accuracy: 0.6655433866891323\n",
      "Step: 1187, Loss: 1.0704034566879272, Accuracy: 0.6657547699214366\n",
      "Step: 1188, Loss: 1.31887686252594, Accuracy: 0.6656153630501822\n",
      "Step: 1189, Loss: 1.441599726676941, Accuracy: 0.665406162464986\n",
      "Step: 1190, Loss: 1.2658958435058594, Accuracy: 0.665407220822838\n",
      "Step: 1191, Loss: 1.1319446563720703, Accuracy: 0.6654781879194631\n",
      "Step: 1192, Loss: 1.1727153062820435, Accuracy: 0.6655490360435876\n",
      "Step: 1193, Loss: 1.097558856010437, Accuracy: 0.6656895589056393\n",
      "Step: 1194, Loss: 1.2482272386550903, Accuracy: 0.6656903765690376\n",
      "Step: 1195, Loss: 1.3144296407699585, Accuracy: 0.6656215161649944\n",
      "Step: 1196, Loss: 1.3590980768203735, Accuracy: 0.6654831523252576\n",
      "Step: 1197, Loss: 1.241140604019165, Accuracy: 0.6654841402337228\n",
      "Step: 1198, Loss: 1.362198829650879, Accuracy: 0.6654156241312205\n",
      "Step: 1199, Loss: 1.1524869203567505, Accuracy: 0.6654861111111111\n",
      "Step: 1200, Loss: 1.1851675510406494, Accuracy: 0.665556480710519\n",
      "Step: 1201, Loss: 1.4275450706481934, Accuracy: 0.6654187465335551\n",
      "Step: 1202, Loss: 1.2482571601867676, Accuracy: 0.6654197838736492\n",
      "Step: 1203, Loss: 1.2132288217544556, Accuracy: 0.6654900332225914\n",
      "Step: 1204, Loss: 1.2286471128463745, Accuracy: 0.6654218533886583\n",
      "Step: 1205, Loss: 1.4971213340759277, Accuracy: 0.6652155887230514\n",
      "Step: 1206, Loss: 1.303025484085083, Accuracy: 0.6651477492405413\n",
      "Step: 1207, Loss: 1.116843581199646, Accuracy: 0.6652179911699779\n",
      "Step: 1208, Loss: 1.3140424489974976, Accuracy: 0.6651502619244555\n",
      "Step: 1209, Loss: 1.0274333953857422, Accuracy: 0.6652892561983471\n",
      "Step: 1210, Loss: 1.4419232606887817, Accuracy: 0.665083952656207\n",
      "Step: 1211, Loss: 1.2946587800979614, Accuracy: 0.6649477447744775\n",
      "Step: 1212, Loss: 1.1620473861694336, Accuracy: 0.6650178620500138\n",
      "Step: 1213, Loss: 1.1857686042785645, Accuracy: 0.6650192202086765\n",
      "Step: 1214, Loss: 1.104754090309143, Accuracy: 0.6650891632373114\n",
      "Step: 1215, Loss: 1.0664465427398682, Accuracy: 0.6651589912280702\n",
      "Step: 1216, Loss: 1.0830248594284058, Accuracy: 0.6652971788551082\n",
      "Step: 1217, Loss: 1.1049648523330688, Accuracy: 0.6654351395730707\n",
      "Step: 1218, Loss: 1.308778166770935, Accuracy: 0.6653677878042111\n",
      "Step: 1219, Loss: 1.266457438468933, Accuracy: 0.6653688524590164\n",
      "Step: 1220, Loss: 1.4638166427612305, Accuracy: 0.6651651651651652\n",
      "Step: 1221, Loss: 1.1567624807357788, Accuracy: 0.6652345881069285\n",
      "Step: 1222, Loss: 1.2977896928787231, Accuracy: 0.6651676206050695\n",
      "Step: 1223, Loss: 1.3477483987808228, Accuracy: 0.6650326797385621\n",
      "Step: 1224, Loss: 1.2769416570663452, Accuracy: 0.6649659863945578\n",
      "Step: 1225, Loss: 1.4396013021469116, Accuracy: 0.664763458401305\n",
      "Step: 1226, Loss: 1.3262782096862793, Accuracy: 0.6646970931812007\n",
      "Step: 1227, Loss: 1.0399309396743774, Accuracy: 0.6649022801302932\n",
      "Step: 1228, Loss: 1.4590654373168945, Accuracy: 0.6647002983455383\n",
      "Step: 1229, Loss: 1.2584391832351685, Accuracy: 0.6647018970189702\n",
      "Step: 1230, Loss: 1.0619910955429077, Accuracy: 0.6648388843758462\n",
      "Step: 1231, Loss: 1.357388973236084, Accuracy: 0.6647050865800865\n",
      "Step: 1232, Loss: 1.285662293434143, Accuracy: 0.6646390916463909\n",
      "Step: 1233, Loss: 1.3247127532958984, Accuracy: 0.6645732036736899\n",
      "Step: 1234, Loss: 1.18512761592865, Accuracy: 0.6645748987854251\n",
      "Step: 1235, Loss: 1.269375205039978, Accuracy: 0.664576591154261\n",
      "Step: 1236, Loss: 1.1344541311264038, Accuracy: 0.6646456480732956\n",
      "Step: 1237, Loss: 1.1725702285766602, Accuracy: 0.6647145934302638\n",
      "Step: 1238, Loss: 1.373201847076416, Accuracy: 0.6645816518697875\n",
      "Step: 1239, Loss: 1.232491135597229, Accuracy: 0.6645833333333333\n",
      "Step: 1240, Loss: 1.2333124876022339, Accuracy: 0.6645850120870266\n",
      "Step: 1241, Loss: 1.2805320024490356, Accuracy: 0.6645195920558239\n",
      "Step: 1242, Loss: 1.1862651109695435, Accuracy: 0.6645883614910164\n",
      "Step: 1243, Loss: 1.297533392906189, Accuracy: 0.6645230439442658\n",
      "Step: 1244, Loss: 1.3867889642715454, Accuracy: 0.6643908969210174\n",
      "Step: 1245, Loss: 1.062690019607544, Accuracy: 0.6645264847512039\n",
      "Step: 1246, Loss: 1.5675057172775269, Accuracy: 0.6642608928094093\n",
      "Step: 1247, Loss: 1.4120721817016602, Accuracy: 0.6641292735042735\n",
      "Step: 1248, Loss: 1.2327935695648193, Accuracy: 0.6641313050440353\n",
      "Step: 1249, Loss: 1.5158491134643555, Accuracy: 0.6638666666666667\n",
      "Step: 1250, Loss: 1.3116793632507324, Accuracy: 0.6638022915001333\n",
      "Step: 1251, Loss: 1.2520475387573242, Accuracy: 0.6638045793397231\n",
      "Step: 1252, Loss: 1.2178500890731812, Accuracy: 0.6638068635275339\n",
      "Step: 1253, Loss: 1.0893439054489136, Accuracy: 0.6639420520999468\n",
      "Step: 1254, Loss: 1.4135042428970337, Accuracy: 0.6638114209827357\n",
      "Step: 1255, Loss: 1.341789722442627, Accuracy: 0.6637473460721869\n",
      "Step: 1256, Loss: 1.061285138130188, Accuracy: 0.6639485547600106\n",
      "Step: 1257, Loss: 1.3056890964508057, Accuracy: 0.6638844727080021\n",
      "Step: 1258, Loss: 1.276908278465271, Accuracy: 0.6638866825522902\n",
      "Step: 1259, Loss: 1.1484922170639038, Accuracy: 0.6639550264550265\n",
      "Step: 1260, Loss: 1.3090018033981323, Accuracy: 0.6638910917261432\n",
      "Step: 1261, Loss: 1.1425570249557495, Accuracy: 0.663959323824617\n",
      "Step: 1262, Loss: 1.3889497518539429, Accuracy: 0.6638295064660861\n",
      "Step: 1263, Loss: 1.2477463483810425, Accuracy: 0.6637658227848101\n",
      "Step: 1264, Loss: 1.268931269645691, Accuracy: 0.6637022397891963\n",
      "Step: 1265, Loss: 1.0154273509979248, Accuracy: 0.6639020537124802\n",
      "Step: 1266, Loss: 1.331870436668396, Accuracy: 0.6638384635622204\n",
      "Step: 1267, Loss: 1.1351244449615479, Accuracy: 0.663906414300736\n",
      "Step: 1268, Loss: 1.121890664100647, Accuracy: 0.664039926451274\n",
      "Step: 1269, Loss: 1.0818185806274414, Accuracy: 0.6641732283464566\n",
      "Step: 1270, Loss: 1.423912525177002, Accuracy: 0.6639784946236559\n",
      "Step: 1271, Loss: 1.1485490798950195, Accuracy: 0.6640461215932913\n",
      "Step: 1272, Loss: 1.6716994047164917, Accuracy: 0.6636554071746531\n",
      "Step: 1273, Loss: 1.1768386363983154, Accuracy: 0.6637231815803244\n",
      "Step: 1274, Loss: 1.0861026048660278, Accuracy: 0.6638562091503268\n",
      "Step: 1275, Loss: 1.2518815994262695, Accuracy: 0.6637931034482759\n",
      "Step: 1276, Loss: 1.359412670135498, Accuracy: 0.663664839467502\n",
      "Step: 1277, Loss: 1.2498587369918823, Accuracy: 0.6636671883150757\n",
      "Step: 1278, Loss: 1.3225200176239014, Accuracy: 0.6636043784206411\n",
      "Step: 1279, Loss: 1.1201952695846558, Accuracy: 0.6637369791666666\n",
      "Step: 1280, Loss: 1.0944925546646118, Accuracy: 0.6638693728857663\n",
      "Step: 1281, Loss: 1.1656428575515747, Accuracy: 0.6639365574622985\n",
      "Step: 1282, Loss: 1.060691475868225, Accuracy: 0.6640685892439595\n",
      "Step: 1283, Loss: 1.3983383178710938, Accuracy: 0.6639408099688473\n",
      "Step: 1284, Loss: 1.323319911956787, Accuracy: 0.6638780804150454\n",
      "Step: 1285, Loss: 0.9956891536712646, Accuracy: 0.6640746500777605\n",
      "Step: 1286, Loss: 1.4196381568908691, Accuracy: 0.6639471639471639\n",
      "Step: 1287, Loss: 1.3571547269821167, Accuracy: 0.6638845755693582\n",
      "Step: 1288, Loss: 1.1180790662765503, Accuracy: 0.6640160331005948\n",
      "Step: 1289, Loss: 1.2667888402938843, Accuracy: 0.663953488372093\n",
      "Step: 1290, Loss: 1.1557650566101074, Accuracy: 0.6640201394268009\n",
      "Step: 1291, Loss: 0.9359282851219177, Accuracy: 0.6642801857585139\n",
      "Step: 1292, Loss: 1.5489047765731812, Accuracy: 0.6639597834493426\n",
      "Step: 1293, Loss: 1.1816157102584839, Accuracy: 0.6640262751159196\n",
      "Step: 1294, Loss: 1.159854769706726, Accuracy: 0.6640283140283141\n",
      "Step: 1295, Loss: 1.0820297002792358, Accuracy: 0.6641589506172839\n",
      "Step: 1296, Loss: 1.3319098949432373, Accuracy: 0.6640323824209715\n",
      "Step: 1297, Loss: 1.2634705305099487, Accuracy: 0.6640344119157678\n",
      "Step: 1298, Loss: 1.241011142730713, Accuracy: 0.6640364382858609\n",
      "Step: 1299, Loss: 1.220086932182312, Accuracy: 0.6640384615384616\n",
      "Step: 1300, Loss: 1.1340420246124268, Accuracy: 0.6641045349730976\n",
      "Step: 1301, Loss: 1.0939842462539673, Accuracy: 0.6642345110087046\n",
      "Step: 1302, Loss: 0.9917953610420227, Accuracy: 0.6644282425172678\n",
      "Step: 1303, Loss: 1.2437924146652222, Accuracy: 0.6644299591002045\n",
      "Step: 1304, Loss: 1.2981998920440674, Accuracy: 0.664367816091954\n",
      "Step: 1305, Loss: 1.2011421918869019, Accuracy: 0.6643695763144462\n",
      "Step: 1306, Loss: 1.0808608531951904, Accuracy: 0.6644988523335884\n",
      "Step: 1307, Loss: 1.2017446756362915, Accuracy: 0.6645642201834863\n",
      "Step: 1308, Loss: 1.421548843383789, Accuracy: 0.6643748408454291\n",
      "Step: 1309, Loss: 1.147288203239441, Accuracy: 0.664440203562341\n",
      "Step: 1310, Loss: 1.1680649518966675, Accuracy: 0.6645054665649631\n",
      "Step: 1311, Loss: 1.3041445016860962, Accuracy: 0.6644435975609756\n",
      "Step: 1312, Loss: 1.2845767736434937, Accuracy: 0.6644452906829145\n",
      "Step: 1313, Loss: 1.1635090112686157, Accuracy: 0.6644469812278031\n",
      "Step: 1314, Loss: 1.2223206758499146, Accuracy: 0.6644486692015209\n",
      "Step: 1315, Loss: 1.2714816331863403, Accuracy: 0.6644503546099291\n",
      "Step: 1316, Loss: 1.1368316411972046, Accuracy: 0.6645153125790939\n",
      "Step: 1317, Loss: 1.4475308656692505, Accuracy: 0.6643272635306019\n",
      "Step: 1318, Loss: 1.2343509197235107, Accuracy: 0.6643290371493555\n",
      "Step: 1319, Loss: 1.353980541229248, Accuracy: 0.6642676767676767\n",
      "Step: 1320, Loss: 1.2949146032333374, Accuracy: 0.6642064092858945\n",
      "Step: 1321, Loss: 1.232928991317749, Accuracy: 0.664208270297529\n",
      "Step: 1322, Loss: 1.1042149066925049, Accuracy: 0.6642731166540691\n",
      "Step: 1323, Loss: 1.243412733078003, Accuracy: 0.6643378650553877\n",
      "Step: 1324, Loss: 1.2605135440826416, Accuracy: 0.6642767295597485\n",
      "Step: 1325, Loss: 1.054450511932373, Accuracy: 0.6644042232277526\n",
      "Step: 1326, Loss: 1.182695746421814, Accuracy: 0.6644059281587541\n",
      "Step: 1327, Loss: 1.3662919998168945, Accuracy: 0.6642821285140562\n",
      "Step: 1328, Loss: 1.259747862815857, Accuracy: 0.6642212189616253\n",
      "Step: 1329, Loss: 1.1503050327301025, Accuracy: 0.6642857142857143\n",
      "Step: 1330, Loss: 1.04842209815979, Accuracy: 0.6644753318307037\n",
      "Step: 1331, Loss: 1.2293075323104858, Accuracy: 0.6644769769769769\n",
      "Step: 1332, Loss: 1.1816595792770386, Accuracy: 0.664541135283821\n",
      "Step: 1333, Loss: 1.1251564025878906, Accuracy: 0.6646051974012993\n",
      "Step: 1334, Loss: 1.0453755855560303, Accuracy: 0.6647315855181024\n",
      "Step: 1335, Loss: 1.2093547582626343, Accuracy: 0.6647330339321357\n",
      "Step: 1336, Loss: 1.1610227823257446, Accuracy: 0.6648591373722263\n",
      "Step: 1337, Loss: 1.0614148378372192, Accuracy: 0.6649850523168909\n",
      "Step: 1338, Loss: 1.335975170135498, Accuracy: 0.664924072691063\n",
      "Step: 1339, Loss: 1.1369577646255493, Accuracy: 0.6649875621890547\n",
      "Step: 1340, Loss: 1.3743311166763306, Accuracy: 0.6648645289584887\n",
      "Step: 1341, Loss: 1.3358278274536133, Accuracy: 0.6647416790859414\n",
      "Step: 1342, Loss: 1.2032943964004517, Accuracy: 0.6647431124348474\n",
      "Step: 1343, Loss: 1.3933372497558594, Accuracy: 0.6646205357142857\n",
      "Step: 1344, Loss: 1.2088912725448608, Accuracy: 0.6646840148698885\n",
      "Step: 1345, Loss: 1.191224455833435, Accuracy: 0.6647473997028231\n",
      "Step: 1346, Loss: 1.0644017457962036, Accuracy: 0.6649344221727296\n",
      "Step: 1347, Loss: 1.3332977294921875, Accuracy: 0.6648738872403561\n",
      "Step: 1348, Loss: 1.1788098812103271, Accuracy: 0.6649369903632321\n",
      "Step: 1349, Loss: 1.260372519493103, Accuracy: 0.6649382716049382\n",
      "Step: 1350, Loss: 1.125779390335083, Accuracy: 0.6650629163582531\n",
      "Step: 1351, Loss: 1.0706299543380737, Accuracy: 0.6651873767258383\n",
      "Step: 1352, Loss: 1.2628775835037231, Accuracy: 0.6651884700665188\n",
      "Step: 1353, Loss: 1.0518782138824463, Accuracy: 0.6653126538650911\n",
      "Step: 1354, Loss: 1.1541587114334106, Accuracy: 0.6653751537515376\n",
      "Step: 1355, Loss: 1.2773855924606323, Accuracy: 0.6653761061946902\n",
      "Step: 1356, Loss: 1.368762493133545, Accuracy: 0.6652542372881356\n",
      "Step: 1357, Loss: 1.3172723054885864, Accuracy: 0.6651325478645066\n",
      "Step: 1358, Loss: 1.0860439538955688, Accuracy: 0.6653176355163111\n",
      "Step: 1359, Loss: 1.1861873865127563, Accuracy: 0.6653799019607843\n",
      "Step: 1360, Loss: 1.067650556564331, Accuracy: 0.6655033063923586\n",
      "Step: 1361, Loss: 1.3682208061218262, Accuracy: 0.6654429760156633\n",
      "Step: 1362, Loss: 1.0372140407562256, Accuracy: 0.665566153093666\n",
      "Step: 1363, Loss: 1.206926703453064, Accuracy: 0.665628054740958\n",
      "Step: 1364, Loss: 1.3439922332763672, Accuracy: 0.6655677655677655\n",
      "Step: 1365, Loss: 1.0624234676361084, Accuracy: 0.6656905807711079\n",
      "Step: 1366, Loss: 1.3377119302749634, Accuracy: 0.6656303340648623\n",
      "Step: 1367, Loss: 1.079465389251709, Accuracy: 0.6657529239766082\n",
      "Step: 1368, Loss: 1.2373558282852173, Accuracy: 0.6657535914292672\n",
      "Step: 1369, Loss: 1.2016681432724, Accuracy: 0.6657542579075426\n",
      "Step: 1370, Loss: 1.3920024633407593, Accuracy: 0.6656333576464868\n",
      "Step: 1371, Loss: 1.474146842956543, Accuracy: 0.6654518950437318\n",
      "Step: 1372, Loss: 1.2220544815063477, Accuracy: 0.6655134741442098\n",
      "Step: 1373, Loss: 1.28977370262146, Accuracy: 0.6654536632702571\n",
      "Step: 1374, Loss: 1.1390316486358643, Accuracy: 0.6655151515151515\n",
      "Step: 1375, Loss: 1.2961748838424683, Accuracy: 0.6654554263565892\n",
      "Step: 1376, Loss: 1.0692261457443237, Accuracy: 0.6655773420479303\n",
      "Step: 1377, Loss: 1.304206132888794, Accuracy: 0.6655176584421868\n",
      "Step: 1378, Loss: 1.1562551259994507, Accuracy: 0.6655789219240996\n",
      "Step: 1379, Loss: 1.1951545476913452, Accuracy: 0.6656400966183574\n",
      "Step: 1380, Loss: 1.202944040298462, Accuracy: 0.6656408399710355\n",
      "Step: 1381, Loss: 1.0620553493499756, Accuracy: 0.6657621804148577\n",
      "Step: 1382, Loss: 1.347413182258606, Accuracy: 0.665642323451434\n",
      "Step: 1383, Loss: 1.0785263776779175, Accuracy: 0.6657634874759152\n",
      "Step: 1384, Loss: 1.628904938697815, Accuracy: 0.6654632972322503\n",
      "Step: 1385, Loss: 1.2269922494888306, Accuracy: 0.6654641654641654\n",
      "Step: 1386, Loss: 1.425265908241272, Accuracy: 0.6652847873107426\n",
      "Step: 1387, Loss: 1.3083988428115845, Accuracy: 0.665225744476465\n",
      "Step: 1388, Loss: 1.3290003538131714, Accuracy: 0.6651667866570674\n",
      "Step: 1389, Loss: 1.373381495475769, Accuracy: 0.6650479616306955\n",
      "Step: 1390, Loss: 1.176512598991394, Accuracy: 0.6651090342679128\n",
      "Step: 1391, Loss: 1.1392490863800049, Accuracy: 0.6651700191570882\n",
      "Step: 1392, Loss: 0.9729101657867432, Accuracy: 0.6654103852596315\n",
      "Step: 1393, Loss: 1.3727054595947266, Accuracy: 0.6652917264466762\n",
      "Step: 1394, Loss: 1.3163089752197266, Accuracy: 0.6652329749103942\n",
      "Step: 1395, Loss: 1.3305325508117676, Accuracy: 0.6651743075453678\n",
      "Step: 1396, Loss: 1.3039073944091797, Accuracy: 0.6651157241708423\n",
      "Step: 1397, Loss: 1.2092541456222534, Accuracy: 0.6651168335717692\n",
      "Step: 1398, Loss: 1.0180809497833252, Accuracy: 0.6652966404574696\n",
      "Step: 1399, Loss: 1.1465178728103638, Accuracy: 0.6653571428571429\n",
      "Step: 1400, Loss: 1.5913562774658203, Accuracy: 0.6650606709493219\n",
      "Step: 1401, Loss: 1.047686219215393, Accuracy: 0.6652401331431289\n",
      "Step: 1402, Loss: 1.4672232866287231, Accuracy: 0.6650629603231172\n",
      "Step: 1403, Loss: 1.1311306953430176, Accuracy: 0.6651234567901234\n",
      "Step: 1404, Loss: 1.078029751777649, Accuracy: 0.6652431791221827\n",
      "Step: 1405, Loss: 1.0942577123641968, Accuracy: 0.6653627311522048\n",
      "Step: 1406, Loss: 1.3034955263137817, Accuracy: 0.6653044302298033\n",
      "Step: 1407, Loss: 1.4286359548568726, Accuracy: 0.6651870265151515\n",
      "Step: 1408, Loss: 1.325313687324524, Accuracy: 0.6650697894487816\n",
      "Step: 1409, Loss: 1.6409052610397339, Accuracy: 0.6647754137115839\n",
      "Step: 1410, Loss: 1.2244724035263062, Accuracy: 0.664776754075124\n",
      "Step: 1411, Loss: 1.4094585180282593, Accuracy: 0.6646600566572238\n",
      "Step: 1412, Loss: 1.3380494117736816, Accuracy: 0.6646025005897618\n",
      "Step: 1413, Loss: 1.152267575263977, Accuracy: 0.6646628948609147\n",
      "Step: 1414, Loss: 1.2004410028457642, Accuracy: 0.6646643109540636\n",
      "Step: 1415, Loss: 1.3824462890625, Accuracy: 0.66454802259887\n",
      "Step: 1416, Loss: 1.3771371841430664, Accuracy: 0.6644318983768525\n",
      "Step: 1417, Loss: 1.3614373207092285, Accuracy: 0.6643159379407616\n",
      "Step: 1418, Loss: 1.3899701833724976, Accuracy: 0.6642588677472399\n",
      "Step: 1419, Loss: 1.2896140813827515, Accuracy: 0.6642018779342723\n",
      "Step: 1420, Loss: 1.1035159826278687, Accuracy: 0.6643209007741028\n",
      "Step: 1421, Loss: 1.1531277894973755, Accuracy: 0.664381153305204\n",
      "Step: 1422, Loss: 1.1604622602462769, Accuracy: 0.6644413211524948\n",
      "Step: 1423, Loss: 1.3627238273620605, Accuracy: 0.6643258426966292\n",
      "Step: 1424, Loss: 0.9135556221008301, Accuracy: 0.6645614035087719\n",
      "Step: 1425, Loss: 1.0782955884933472, Accuracy: 0.6647381954184198\n",
      "Step: 1426, Loss: 1.3442641496658325, Accuracy: 0.6644475589815464\n",
      "Epoch: 3, Val_Accuracy: 0.13426791277258568\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c865e824f55d47beb417206af3e7208e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1427 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0, Loss: 1.1177719831466675, Accuracy: 0.75\n",
      "Step: 1, Loss: 1.1211992502212524, Accuracy: 0.75\n",
      "Step: 2, Loss: 1.0784947872161865, Accuracy: 0.7777777777777778\n",
      "Step: 3, Loss: 1.165144920349121, Accuracy: 0.7708333333333334\n",
      "Step: 4, Loss: 1.0112468004226685, Accuracy: 0.8\n",
      "Step: 5, Loss: 1.2200955152511597, Accuracy: 0.7777777777777778\n",
      "Step: 6, Loss: 1.1527435779571533, Accuracy: 0.7738095238095238\n",
      "Step: 7, Loss: 1.0668305158615112, Accuracy: 0.78125\n",
      "Step: 8, Loss: 1.257228970527649, Accuracy: 0.7685185185185185\n",
      "Step: 9, Loss: 1.220579981803894, Accuracy: 0.7583333333333333\n",
      "Step: 10, Loss: 0.9840849041938782, Accuracy: 0.7727272727272727\n",
      "Step: 11, Loss: 1.1655620336532593, Accuracy: 0.7708333333333334\n",
      "Step: 12, Loss: 1.2156217098236084, Accuracy: 0.7628205128205128\n",
      "Step: 13, Loss: 1.2668970823287964, Accuracy: 0.75\n",
      "Step: 14, Loss: 1.2621673345565796, Accuracy: 0.7388888888888889\n",
      "Step: 15, Loss: 1.2497659921646118, Accuracy: 0.734375\n",
      "Step: 16, Loss: 1.2897368669509888, Accuracy: 0.7254901960784313\n",
      "Step: 17, Loss: 1.210992455482483, Accuracy: 0.7222222222222222\n",
      "Step: 18, Loss: 1.479782223701477, Accuracy: 0.706140350877193\n",
      "Step: 19, Loss: 1.1304413080215454, Accuracy: 0.7083333333333334\n",
      "Step: 20, Loss: 1.0715333223342896, Accuracy: 0.7142857142857143\n",
      "Step: 21, Loss: 1.511207938194275, Accuracy: 0.7007575757575758\n",
      "Step: 22, Loss: 1.1001372337341309, Accuracy: 0.7028985507246377\n",
      "Step: 23, Loss: 1.2191661596298218, Accuracy: 0.7013888888888888\n",
      "Step: 24, Loss: 1.4220436811447144, Accuracy: 0.69\n",
      "Step: 25, Loss: 1.2513010501861572, Accuracy: 0.6891025641025641\n",
      "Step: 26, Loss: 1.281294822692871, Accuracy: 0.6882716049382716\n",
      "Step: 27, Loss: 1.243039608001709, Accuracy: 0.6875\n",
      "Step: 28, Loss: 1.2124451398849487, Accuracy: 0.6867816091954023\n",
      "Step: 29, Loss: 1.163360357284546, Accuracy: 0.6888888888888889\n",
      "Step: 30, Loss: 1.2586642503738403, Accuracy: 0.6854838709677419\n",
      "Step: 31, Loss: 1.1943411827087402, Accuracy: 0.6875\n",
      "Step: 32, Loss: 1.2281593084335327, Accuracy: 0.6868686868686869\n",
      "Step: 33, Loss: 1.4234243631362915, Accuracy: 0.678921568627451\n",
      "Step: 34, Loss: 1.2344703674316406, Accuracy: 0.6785714285714286\n",
      "Step: 35, Loss: 1.3008379936218262, Accuracy: 0.6759259259259259\n",
      "Step: 36, Loss: 1.3334040641784668, Accuracy: 0.6734234234234234\n",
      "Step: 37, Loss: 1.238094449043274, Accuracy: 0.6732456140350878\n",
      "Step: 38, Loss: 1.3234012126922607, Accuracy: 0.6709401709401709\n",
      "Step: 39, Loss: 1.0258944034576416, Accuracy: 0.6770833333333334\n",
      "Step: 40, Loss: 1.2126165628433228, Accuracy: 0.676829268292683\n",
      "Step: 41, Loss: 1.1494091749191284, Accuracy: 0.6785714285714286\n",
      "Step: 42, Loss: 1.2586218118667603, Accuracy: 0.6782945736434108\n",
      "Step: 43, Loss: 1.082100749015808, Accuracy: 0.6818181818181818\n",
      "Step: 44, Loss: 1.2080886363983154, Accuracy: 0.6814814814814815\n",
      "Step: 45, Loss: 1.1553422212600708, Accuracy: 0.6847826086956522\n",
      "Step: 46, Loss: 1.1436429023742676, Accuracy: 0.6861702127659575\n",
      "Step: 47, Loss: 1.322778344154358, Accuracy: 0.6840277777777778\n",
      "Step: 48, Loss: 1.0755974054336548, Accuracy: 0.6870748299319728\n",
      "Step: 49, Loss: 1.1034101247787476, Accuracy: 0.69\n",
      "Step: 50, Loss: 1.403679370880127, Accuracy: 0.6862745098039216\n",
      "Step: 51, Loss: 1.2786412239074707, Accuracy: 0.6842948717948718\n",
      "Step: 52, Loss: 1.1753743886947632, Accuracy: 0.6839622641509434\n",
      "Step: 53, Loss: 1.1534929275512695, Accuracy: 0.6851851851851852\n",
      "Step: 54, Loss: 1.1420446634292603, Accuracy: 0.6863636363636364\n",
      "Step: 55, Loss: 1.199326992034912, Accuracy: 0.6860119047619048\n",
      "Step: 56, Loss: 1.3986107110977173, Accuracy: 0.6827485380116959\n",
      "Step: 57, Loss: 1.152491569519043, Accuracy: 0.6839080459770115\n",
      "Step: 58, Loss: 1.4036272764205933, Accuracy: 0.6807909604519774\n",
      "Step: 59, Loss: 1.1238915920257568, Accuracy: 0.6833333333333333\n",
      "Step: 60, Loss: 1.221709132194519, Accuracy: 0.6830601092896175\n",
      "Step: 61, Loss: 1.163320541381836, Accuracy: 0.6841397849462365\n",
      "Step: 62, Loss: 1.174124836921692, Accuracy: 0.6851851851851852\n",
      "Step: 63, Loss: 1.2096210718154907, Accuracy: 0.6848958333333334\n",
      "Step: 64, Loss: 1.147534966468811, Accuracy: 0.6858974358974359\n",
      "Step: 65, Loss: 1.0553627014160156, Accuracy: 0.6893939393939394\n",
      "Step: 66, Loss: 1.386949896812439, Accuracy: 0.6878109452736318\n",
      "Step: 67, Loss: 1.2487322092056274, Accuracy: 0.6862745098039216\n",
      "Step: 68, Loss: 1.3108689785003662, Accuracy: 0.6847826086956522\n",
      "Step: 69, Loss: 1.2044070959091187, Accuracy: 0.6845238095238095\n",
      "Step: 70, Loss: 1.0683515071868896, Accuracy: 0.6866197183098591\n",
      "Step: 71, Loss: 1.5515127182006836, Accuracy: 0.6817129629629629\n",
      "Step: 72, Loss: 1.1980903148651123, Accuracy: 0.6815068493150684\n",
      "Step: 73, Loss: 1.1293152570724487, Accuracy: 0.6824324324324325\n",
      "Step: 74, Loss: 1.1704217195510864, Accuracy: 0.6833333333333333\n",
      "Step: 75, Loss: 1.1743398904800415, Accuracy: 0.6831140350877193\n",
      "Step: 76, Loss: 1.2209311723709106, Accuracy: 0.6829004329004329\n",
      "Step: 77, Loss: 1.0527383089065552, Accuracy: 0.6848290598290598\n",
      "Step: 78, Loss: 1.1770751476287842, Accuracy: 0.6856540084388185\n",
      "Step: 79, Loss: 1.3632930517196655, Accuracy: 0.6833333333333333\n",
      "Step: 80, Loss: 1.2437390089035034, Accuracy: 0.6831275720164609\n",
      "Step: 81, Loss: 1.2285996675491333, Accuracy: 0.6829268292682927\n",
      "Step: 82, Loss: 1.0645849704742432, Accuracy: 0.6847389558232931\n",
      "Step: 83, Loss: 0.994652271270752, Accuracy: 0.6875\n",
      "Step: 84, Loss: 1.1687332391738892, Accuracy: 0.6882352941176471\n",
      "Step: 85, Loss: 1.1051421165466309, Accuracy: 0.688953488372093\n",
      "Step: 86, Loss: 1.1425296068191528, Accuracy: 0.6906130268199234\n",
      "Step: 87, Loss: 1.1823724508285522, Accuracy: 0.6903409090909091\n",
      "Step: 88, Loss: 1.0699149370193481, Accuracy: 0.6919475655430711\n",
      "Step: 89, Loss: 1.0579485893249512, Accuracy: 0.6935185185185185\n",
      "Step: 90, Loss: 1.1142903566360474, Accuracy: 0.695054945054945\n",
      "Step: 91, Loss: 1.170883059501648, Accuracy: 0.6956521739130435\n",
      "Step: 92, Loss: 1.1586047410964966, Accuracy: 0.696236559139785\n",
      "Step: 93, Loss: 0.9867863059043884, Accuracy: 0.6985815602836879\n",
      "Step: 94, Loss: 1.241470456123352, Accuracy: 0.6982456140350877\n",
      "Step: 95, Loss: 1.1506789922714233, Accuracy: 0.6987847222222222\n",
      "Step: 96, Loss: 1.2244757413864136, Accuracy: 0.6984536082474226\n",
      "Step: 97, Loss: 1.4367417097091675, Accuracy: 0.6955782312925171\n",
      "Step: 98, Loss: 1.0449895858764648, Accuracy: 0.696969696969697\n",
      "Step: 99, Loss: 1.1405287981033325, Accuracy: 0.6975\n",
      "Step: 100, Loss: 1.3153074979782104, Accuracy: 0.6963696369636964\n",
      "Step: 101, Loss: 1.2031127214431763, Accuracy: 0.696078431372549\n",
      "Step: 102, Loss: 1.2767283916473389, Accuracy: 0.6957928802588996\n",
      "Step: 103, Loss: 0.9530742168426514, Accuracy: 0.6987179487179487\n",
      "Step: 104, Loss: 1.2318323850631714, Accuracy: 0.6984126984126984\n",
      "Step: 105, Loss: 1.210809588432312, Accuracy: 0.6981132075471698\n",
      "Step: 106, Loss: 1.2497013807296753, Accuracy: 0.6970404984423676\n",
      "Step: 107, Loss: 1.1773627996444702, Accuracy: 0.6975308641975309\n",
      "Step: 108, Loss: 1.2783483266830444, Accuracy: 0.6964831804281345\n",
      "Step: 109, Loss: 1.1123204231262207, Accuracy: 0.696969696969697\n",
      "Step: 110, Loss: 1.3732010126113892, Accuracy: 0.6951951951951952\n",
      "Step: 111, Loss: 1.3860055208206177, Accuracy: 0.6934523809523809\n",
      "Step: 112, Loss: 1.3480526208877563, Accuracy: 0.6924778761061947\n",
      "Step: 113, Loss: 1.0140200853347778, Accuracy: 0.6944444444444444\n",
      "Step: 114, Loss: 1.3167600631713867, Accuracy: 0.6934782608695652\n",
      "Step: 115, Loss: 1.3084557056427002, Accuracy: 0.6925287356321839\n",
      "Step: 116, Loss: 1.166486144065857, Accuracy: 0.6930199430199431\n",
      "Step: 117, Loss: 1.3923169374465942, Accuracy: 0.6913841807909604\n",
      "Step: 118, Loss: 1.0305370092391968, Accuracy: 0.6932773109243697\n",
      "Step: 119, Loss: 1.0650361776351929, Accuracy: 0.6951388888888889\n",
      "Step: 120, Loss: 1.1643786430358887, Accuracy: 0.6955922865013774\n",
      "Step: 121, Loss: 1.2197314500808716, Accuracy: 0.6953551912568307\n",
      "Step: 122, Loss: 1.1700392961502075, Accuracy: 0.69579945799458\n",
      "Step: 123, Loss: 1.1506681442260742, Accuracy: 0.696236559139785\n",
      "Step: 124, Loss: 1.4786639213562012, Accuracy: 0.694\n",
      "Step: 125, Loss: 1.2314132452011108, Accuracy: 0.6937830687830688\n",
      "Step: 126, Loss: 1.2113356590270996, Accuracy: 0.6942257217847769\n",
      "Step: 127, Loss: 1.1840649843215942, Accuracy: 0.6946614583333334\n",
      "Step: 128, Loss: 1.09918212890625, Accuracy: 0.6950904392764858\n",
      "Step: 129, Loss: 1.2371183633804321, Accuracy: 0.6948717948717948\n",
      "Step: 130, Loss: 1.360589861869812, Accuracy: 0.693384223918575\n",
      "Step: 131, Loss: 1.3025521039962769, Accuracy: 0.6925505050505051\n",
      "Step: 132, Loss: 1.245371699333191, Accuracy: 0.6923558897243107\n",
      "Step: 133, Loss: 1.310560703277588, Accuracy: 0.6915422885572139\n",
      "Step: 134, Loss: 1.3096500635147095, Accuracy: 0.6907407407407408\n",
      "Step: 135, Loss: 1.0515459775924683, Accuracy: 0.6917892156862745\n",
      "Step: 136, Loss: 1.3319576978683472, Accuracy: 0.6909975669099757\n",
      "Step: 137, Loss: 1.3130618333816528, Accuracy: 0.6902173913043478\n",
      "Step: 138, Loss: 1.4185609817504883, Accuracy: 0.6882494004796164\n",
      "Step: 139, Loss: 1.2460516691207886, Accuracy: 0.6880952380952381\n",
      "Step: 140, Loss: 0.9397116303443909, Accuracy: 0.6903073286052009\n",
      "Step: 141, Loss: 1.1024597883224487, Accuracy: 0.6907276995305164\n",
      "Step: 142, Loss: 1.3265568017959595, Accuracy: 0.6899766899766899\n",
      "Step: 143, Loss: 1.4030656814575195, Accuracy: 0.6886574074074074\n",
      "Step: 144, Loss: 1.3226690292358398, Accuracy: 0.6879310344827586\n",
      "Step: 145, Loss: 1.1466468572616577, Accuracy: 0.6883561643835616\n",
      "Step: 146, Loss: 1.1015228033065796, Accuracy: 0.6893424036281179\n",
      "Step: 147, Loss: 1.0729610919952393, Accuracy: 0.6903153153153153\n",
      "Step: 148, Loss: 1.2706246376037598, Accuracy: 0.6895973154362416\n",
      "Step: 149, Loss: 1.468901515007019, Accuracy: 0.6877777777777778\n",
      "Step: 150, Loss: 1.3999353647232056, Accuracy: 0.6865342163355408\n",
      "Step: 151, Loss: 1.3223111629486084, Accuracy: 0.6858552631578947\n",
      "Step: 152, Loss: 1.375156044960022, Accuracy: 0.684640522875817\n",
      "Step: 153, Loss: 1.1099375486373901, Accuracy: 0.6856060606060606\n",
      "Step: 154, Loss: 1.1865991353988647, Accuracy: 0.6860215053763441\n",
      "Step: 155, Loss: 1.1351956129074097, Accuracy: 0.6864316239316239\n",
      "Step: 156, Loss: 1.2332522869110107, Accuracy: 0.6863057324840764\n",
      "Step: 157, Loss: 1.2147654294967651, Accuracy: 0.6867088607594937\n",
      "Step: 158, Loss: 1.5577603578567505, Accuracy: 0.6844863731656184\n",
      "Step: 159, Loss: 1.3004485368728638, Accuracy: 0.6838541666666667\n",
      "Step: 160, Loss: 1.1601189374923706, Accuracy: 0.6847826086956522\n",
      "Step: 161, Loss: 1.3804916143417358, Accuracy: 0.683641975308642\n",
      "Step: 162, Loss: 1.1300185918807983, Accuracy: 0.684560327198364\n",
      "Step: 163, Loss: 1.3133763074874878, Accuracy: 0.6839430894308943\n",
      "Step: 164, Loss: 1.2705150842666626, Accuracy: 0.6838383838383838\n",
      "Step: 165, Loss: 1.3395158052444458, Accuracy: 0.6832329317269076\n",
      "Step: 166, Loss: 1.1285490989685059, Accuracy: 0.6836327345309381\n",
      "Step: 167, Loss: 1.2694367170333862, Accuracy: 0.6830357142857143\n",
      "Step: 168, Loss: 1.1683837175369263, Accuracy: 0.6834319526627219\n",
      "Step: 169, Loss: 1.2958767414093018, Accuracy: 0.682843137254902\n",
      "Step: 170, Loss: 1.31696355342865, Accuracy: 0.682261208576998\n",
      "Step: 171, Loss: 1.3359575271606445, Accuracy: 0.6816860465116279\n",
      "Step: 172, Loss: 1.137378215789795, Accuracy: 0.6820809248554913\n",
      "Step: 173, Loss: 1.0783072710037231, Accuracy: 0.6829501915708812\n",
      "Step: 174, Loss: 1.1375442743301392, Accuracy: 0.6838095238095238\n",
      "Step: 175, Loss: 1.0920612812042236, Accuracy: 0.6846590909090909\n",
      "Step: 176, Loss: 1.2059366703033447, Accuracy: 0.6850282485875706\n",
      "Step: 177, Loss: 1.2900813817977905, Accuracy: 0.6844569288389513\n",
      "Step: 178, Loss: 1.4284319877624512, Accuracy: 0.6829608938547486\n",
      "Step: 179, Loss: 1.0056484937667847, Accuracy: 0.6842592592592592\n",
      "Step: 180, Loss: 1.3712631464004517, Accuracy: 0.6832412523020258\n",
      "Step: 181, Loss: 1.3608554601669312, Accuracy: 0.6822344322344323\n",
      "Step: 182, Loss: 1.2366575002670288, Accuracy: 0.6821493624772313\n",
      "Step: 183, Loss: 1.1068041324615479, Accuracy: 0.6829710144927537\n",
      "Step: 184, Loss: 1.127195954322815, Accuracy: 0.6833333333333333\n",
      "Step: 185, Loss: 1.2861090898513794, Accuracy: 0.6827956989247311\n",
      "Step: 186, Loss: 1.281363844871521, Accuracy: 0.6822638146167558\n",
      "Step: 187, Loss: 1.2296019792556763, Accuracy: 0.6821808510638298\n",
      "Step: 188, Loss: 1.0596976280212402, Accuracy: 0.6834215167548501\n",
      "Step: 189, Loss: 1.4508062601089478, Accuracy: 0.6820175438596491\n",
      "Step: 190, Loss: 1.0535696744918823, Accuracy: 0.6832460732984293\n",
      "Step: 191, Loss: 1.1231638193130493, Accuracy: 0.6840277777777778\n",
      "Step: 192, Loss: 1.3355213403701782, Accuracy: 0.6830742659758203\n",
      "Step: 193, Loss: 1.0890454053878784, Accuracy: 0.6834192439862543\n",
      "Step: 194, Loss: 1.419735312461853, Accuracy: 0.6824786324786325\n",
      "Step: 195, Loss: 1.1929432153701782, Accuracy: 0.6823979591836735\n",
      "Step: 196, Loss: 1.2449380159378052, Accuracy: 0.6823181049069373\n",
      "Step: 197, Loss: 1.250096321105957, Accuracy: 0.6822390572390572\n",
      "Step: 198, Loss: 1.1138428449630737, Accuracy: 0.682998324958124\n",
      "Step: 199, Loss: 1.3267061710357666, Accuracy: 0.6825\n",
      "Step: 200, Loss: 1.362113356590271, Accuracy: 0.681592039800995\n",
      "Step: 201, Loss: 1.2291085720062256, Accuracy: 0.6815181518151815\n",
      "Step: 202, Loss: 1.236898422241211, Accuracy: 0.6814449917898193\n",
      "Step: 203, Loss: 1.1332882642745972, Accuracy: 0.681781045751634\n",
      "Step: 204, Loss: 1.1749330759048462, Accuracy: 0.6821138211382114\n",
      "Step: 205, Loss: 1.3023301362991333, Accuracy: 0.6816343042071198\n",
      "Step: 206, Loss: 1.1922277212142944, Accuracy: 0.6815619967793881\n",
      "Step: 207, Loss: 1.1079024076461792, Accuracy: 0.6822916666666666\n",
      "Step: 208, Loss: 1.3266113996505737, Accuracy: 0.6818181818181818\n",
      "Step: 209, Loss: 1.2228914499282837, Accuracy: 0.6817460317460318\n",
      "Step: 210, Loss: 1.1741307973861694, Accuracy: 0.6820695102685624\n",
      "Step: 211, Loss: 1.0639232397079468, Accuracy: 0.6831761006289309\n",
      "Step: 212, Loss: 1.0427502393722534, Accuracy: 0.6838810641627543\n",
      "Step: 213, Loss: 1.2676998376846313, Accuracy: 0.6838006230529595\n",
      "Step: 214, Loss: 1.2572500705718994, Accuracy: 0.6833333333333333\n",
      "Step: 215, Loss: 1.116756558418274, Accuracy: 0.6840277777777778\n",
      "Step: 216, Loss: 1.0708842277526855, Accuracy: 0.684715821812596\n",
      "Step: 217, Loss: 1.1493395566940308, Accuracy: 0.6850152905198776\n",
      "Step: 218, Loss: 1.1975651979446411, Accuracy: 0.6853120243531202\n",
      "Step: 219, Loss: 1.1872305870056152, Accuracy: 0.6856060606060606\n",
      "Step: 220, Loss: 1.2461295127868652, Accuracy: 0.6851432880844646\n",
      "Step: 221, Loss: 1.2945398092269897, Accuracy: 0.6846846846846847\n",
      "Step: 222, Loss: 1.2821578979492188, Accuracy: 0.6842301943198804\n",
      "Step: 223, Loss: 1.4154024124145508, Accuracy: 0.6834077380952381\n",
      "Step: 224, Loss: 1.1105273962020874, Accuracy: 0.6840740740740741\n",
      "Step: 225, Loss: 1.3074489831924438, Accuracy: 0.6836283185840708\n",
      "Step: 226, Loss: 1.2317655086517334, Accuracy: 0.6835535976505139\n",
      "Step: 227, Loss: 1.1333166360855103, Accuracy: 0.6834795321637427\n",
      "Step: 228, Loss: 1.1402335166931152, Accuracy: 0.6837700145560408\n",
      "Step: 229, Loss: 1.2303215265274048, Accuracy: 0.683695652173913\n",
      "Step: 230, Loss: 1.168496012687683, Accuracy: 0.683982683982684\n",
      "Step: 231, Loss: 1.0239702463150024, Accuracy: 0.6849856321839081\n",
      "Step: 232, Loss: 1.2248789072036743, Accuracy: 0.6849070100143062\n",
      "Step: 233, Loss: 1.276225209236145, Accuracy: 0.6844729344729344\n",
      "Step: 234, Loss: 1.1835147142410278, Accuracy: 0.6847517730496454\n",
      "Step: 235, Loss: 1.3632497787475586, Accuracy: 0.6839689265536724\n",
      "Step: 236, Loss: 1.222237467765808, Accuracy: 0.6838959212376934\n",
      "Step: 237, Loss: 1.190819501876831, Accuracy: 0.6841736694677871\n",
      "Step: 238, Loss: 1.0398762226104736, Accuracy: 0.6851464435146444\n",
      "Step: 239, Loss: 1.1837924718856812, Accuracy: 0.6850694444444444\n",
      "Step: 240, Loss: 1.3668619394302368, Accuracy: 0.6843015214384509\n",
      "Step: 241, Loss: 1.355249285697937, Accuracy: 0.6835399449035813\n",
      "Step: 242, Loss: 1.4786123037338257, Accuracy: 0.6824417009602195\n",
      "Step: 243, Loss: 1.1670148372650146, Accuracy: 0.6827185792349727\n",
      "Step: 244, Loss: 1.363250732421875, Accuracy: 0.6819727891156463\n",
      "Step: 245, Loss: 1.0473490953445435, Accuracy: 0.6825880758807588\n",
      "Step: 246, Loss: 1.2274272441864014, Accuracy: 0.6825236167341431\n",
      "Step: 247, Loss: 1.1019477844238281, Accuracy: 0.6831317204301075\n",
      "Step: 248, Loss: 1.2455896139144897, Accuracy: 0.6830655957161981\n",
      "Step: 249, Loss: 0.9432231783866882, Accuracy: 0.6843333333333333\n",
      "Step: 250, Loss: 1.0916539430618286, Accuracy: 0.6849269588313412\n",
      "Step: 251, Loss: 1.4575437307357788, Accuracy: 0.6838624338624338\n",
      "Step: 252, Loss: 1.0659021139144897, Accuracy: 0.6844532279314888\n",
      "Step: 253, Loss: 1.275933027267456, Accuracy: 0.6840551181102362\n",
      "Step: 254, Loss: 1.0926774740219116, Accuracy: 0.684640522875817\n",
      "Step: 255, Loss: 1.2587661743164062, Accuracy: 0.6842447916666666\n",
      "Step: 256, Loss: 1.1382814645767212, Accuracy: 0.6845006485084306\n",
      "Step: 257, Loss: 1.1289256811141968, Accuracy: 0.6847545219638242\n",
      "Step: 258, Loss: 1.212796688079834, Accuracy: 0.6846846846846847\n",
      "Step: 259, Loss: 1.1190664768218994, Accuracy: 0.6849358974358974\n",
      "Step: 260, Loss: 1.350407600402832, Accuracy: 0.6845466155810983\n",
      "Step: 261, Loss: 1.413440227508545, Accuracy: 0.6835241730279898\n",
      "Step: 262, Loss: 1.2452746629714966, Accuracy: 0.6834600760456274\n",
      "Step: 263, Loss: 1.0805573463439941, Accuracy: 0.6840277777777778\n",
      "Step: 264, Loss: 1.1611074209213257, Accuracy: 0.6842767295597484\n",
      "Step: 265, Loss: 1.2206419706344604, Accuracy: 0.6842105263157895\n",
      "Step: 266, Loss: 1.0780717134475708, Accuracy: 0.684769038701623\n",
      "Step: 267, Loss: 1.2521408796310425, Accuracy: 0.6843905472636815\n",
      "Step: 268, Loss: 1.4739446640014648, Accuracy: 0.6833952912019826\n",
      "Step: 269, Loss: 1.148707628250122, Accuracy: 0.683641975308642\n",
      "Step: 270, Loss: 1.2061132192611694, Accuracy: 0.683579335793358\n",
      "Step: 271, Loss: 1.2334914207458496, Accuracy: 0.6835171568627451\n",
      "Step: 272, Loss: 1.209349274635315, Accuracy: 0.6837606837606838\n",
      "Step: 273, Loss: 1.1623560190200806, Accuracy: 0.6840024330900243\n",
      "Step: 274, Loss: 1.0733603239059448, Accuracy: 0.6845454545454546\n",
      "Step: 275, Loss: 1.323142647743225, Accuracy: 0.6841787439613527\n",
      "Step: 276, Loss: 1.1543861627578735, Accuracy: 0.6844163658243081\n",
      "Step: 277, Loss: 1.4327359199523926, Accuracy: 0.6837529976019184\n",
      "Step: 278, Loss: 1.157657265663147, Accuracy: 0.6839904420549582\n",
      "Step: 279, Loss: 1.4500256776809692, Accuracy: 0.6833333333333333\n",
      "Step: 280, Loss: 1.1652629375457764, Accuracy: 0.683570581257414\n",
      "Step: 281, Loss: 1.3052122592926025, Accuracy: 0.6832151300236406\n",
      "Step: 282, Loss: 1.4003872871398926, Accuracy: 0.682567726737338\n",
      "Step: 283, Loss: 1.13872492313385, Accuracy: 0.6828051643192489\n",
      "Step: 284, Loss: 1.1617761850357056, Accuracy: 0.6830409356725147\n",
      "Step: 285, Loss: 1.1477001905441284, Accuracy: 0.6832750582750583\n",
      "Step: 286, Loss: 1.0026129484176636, Accuracy: 0.6840882694541232\n",
      "Step: 287, Loss: 1.1563715934753418, Accuracy: 0.6843171296296297\n",
      "Step: 288, Loss: 1.2239651679992676, Accuracy: 0.6842560553633218\n",
      "Step: 289, Loss: 1.3242526054382324, Accuracy: 0.6839080459770115\n",
      "Step: 290, Loss: 1.1603189706802368, Accuracy: 0.684135166093929\n",
      "Step: 291, Loss: 1.3590021133422852, Accuracy: 0.6835045662100456\n",
      "Step: 292, Loss: 1.0524166822433472, Accuracy: 0.6840159271899886\n",
      "Step: 293, Loss: 1.3035908937454224, Accuracy: 0.6836734693877551\n",
      "Step: 294, Loss: 1.3004560470581055, Accuracy: 0.6833333333333333\n",
      "Step: 295, Loss: 1.3227375745773315, Accuracy: 0.682713963963964\n",
      "Step: 296, Loss: 1.0734308958053589, Accuracy: 0.6832210998877666\n",
      "Step: 297, Loss: 1.2066041231155396, Accuracy: 0.683165548098434\n",
      "Step: 298, Loss: 1.154281735420227, Accuracy: 0.6833890746934225\n",
      "Step: 299, Loss: 1.2187999486923218, Accuracy: 0.6833333333333333\n",
      "Step: 300, Loss: 1.2193719148635864, Accuracy: 0.6832779623477298\n",
      "Step: 301, Loss: 1.1867438554763794, Accuracy: 0.6834988962472406\n",
      "Step: 302, Loss: 1.0057628154754639, Accuracy: 0.6842684268426843\n",
      "Step: 303, Loss: 1.360650897026062, Accuracy: 0.6836622807017544\n",
      "Step: 304, Loss: 1.1681588888168335, Accuracy: 0.683879781420765\n",
      "Step: 305, Loss: 1.3671579360961914, Accuracy: 0.6832788671023965\n",
      "Step: 306, Loss: 1.15617835521698, Accuracy: 0.6834961997828447\n",
      "Step: 307, Loss: 1.0979785919189453, Accuracy: 0.683982683982684\n",
      "Step: 308, Loss: 1.1657739877700806, Accuracy: 0.6841963322545846\n",
      "Step: 309, Loss: 1.128416657447815, Accuracy: 0.6844086021505377\n",
      "Step: 310, Loss: 1.113290548324585, Accuracy: 0.684887459807074\n",
      "Step: 311, Loss: 1.0668069124221802, Accuracy: 0.6856303418803419\n",
      "Step: 312, Loss: 1.1510319709777832, Accuracy: 0.6858359957401491\n",
      "Step: 313, Loss: 1.380561351776123, Accuracy: 0.6849787685774947\n",
      "Step: 314, Loss: 1.7006059885025024, Accuracy: 0.6833333333333333\n",
      "Step: 315, Loss: 1.15735924243927, Accuracy: 0.6835443037974683\n",
      "Step: 316, Loss: 1.216282844543457, Accuracy: 0.6834910620399579\n",
      "Step: 317, Loss: 1.5311132669448853, Accuracy: 0.6823899371069182\n",
      "Step: 318, Loss: 1.1083141565322876, Accuracy: 0.6828631138975967\n",
      "Step: 319, Loss: 1.2544835805892944, Accuracy: 0.6825520833333333\n",
      "Step: 320, Loss: 1.3897112607955933, Accuracy: 0.6819833852544133\n",
      "Step: 321, Loss: 1.037265658378601, Accuracy: 0.682712215320911\n",
      "Step: 322, Loss: 1.2400301694869995, Accuracy: 0.6826625386996904\n",
      "Step: 323, Loss: 1.1381243467330933, Accuracy: 0.6828703703703703\n",
      "Step: 324, Loss: 1.148527979850769, Accuracy: 0.683076923076923\n",
      "Step: 325, Loss: 1.4090932607650757, Accuracy: 0.6825153374233128\n",
      "Step: 326, Loss: 1.2030380964279175, Accuracy: 0.682466870540265\n",
      "Step: 327, Loss: 1.1134498119354248, Accuracy: 0.6826727642276422\n",
      "Step: 328, Loss: 1.1756985187530518, Accuracy: 0.6828774062816616\n",
      "Step: 329, Loss: 1.24140202999115, Accuracy: 0.6828282828282828\n",
      "Step: 330, Loss: 1.1585596799850464, Accuracy: 0.6830312185297079\n",
      "Step: 331, Loss: 1.551308512687683, Accuracy: 0.6819779116465864\n",
      "Step: 332, Loss: 1.4690829515457153, Accuracy: 0.6811811811811812\n",
      "Step: 333, Loss: 1.3743723630905151, Accuracy: 0.6806387225548902\n",
      "Step: 334, Loss: 1.1886547803878784, Accuracy: 0.6805970149253732\n",
      "Step: 335, Loss: 1.3093756437301636, Accuracy: 0.6803075396825397\n",
      "Step: 336, Loss: 1.2877768278121948, Accuracy: 0.6800197823936697\n",
      "Step: 337, Loss: 1.2811658382415771, Accuracy: 0.6799802761341223\n",
      "Step: 338, Loss: 1.0807698965072632, Accuracy: 0.6804326450344149\n",
      "Step: 339, Loss: 1.183265209197998, Accuracy: 0.6806372549019608\n",
      "Step: 340, Loss: 1.2667477130889893, Accuracy: 0.6803519061583577\n",
      "Step: 341, Loss: 1.3322303295135498, Accuracy: 0.6800682261208577\n",
      "Step: 342, Loss: 1.362815022468567, Accuracy: 0.6797862001943634\n",
      "Step: 343, Loss: 1.2418321371078491, Accuracy: 0.6797480620155039\n",
      "Step: 344, Loss: 1.214622139930725, Accuracy: 0.6797101449275362\n",
      "Step: 345, Loss: 1.2326509952545166, Accuracy: 0.6796724470134875\n",
      "Step: 346, Loss: 1.3847250938415527, Accuracy: 0.6791546589817483\n",
      "Step: 347, Loss: 0.9990277290344238, Accuracy: 0.6798371647509579\n",
      "Step: 348, Loss: 1.1601612567901611, Accuracy: 0.6800382043935053\n",
      "Step: 349, Loss: 1.3694511651992798, Accuracy: 0.6797619047619048\n",
      "Step: 350, Loss: 1.1536526679992676, Accuracy: 0.6801994301994302\n",
      "Step: 351, Loss: 1.287676453590393, Accuracy: 0.6801609848484849\n",
      "Step: 352, Loss: 1.4551101922988892, Accuracy: 0.6794145420207743\n",
      "Step: 353, Loss: 1.1581181287765503, Accuracy: 0.6796139359698682\n",
      "Step: 354, Loss: 1.2978161573410034, Accuracy: 0.6793427230046948\n",
      "Step: 355, Loss: 1.0818835496902466, Accuracy: 0.6797752808988764\n",
      "Step: 356, Loss: 1.3631672859191895, Accuracy: 0.6792717086834734\n",
      "Step: 357, Loss: 1.3793936967849731, Accuracy: 0.6787709497206704\n",
      "Step: 358, Loss: 1.1619824171066284, Accuracy: 0.6789693593314763\n",
      "Step: 359, Loss: 0.9632844924926758, Accuracy: 0.6798611111111111\n",
      "Step: 360, Loss: 1.429721713066101, Accuracy: 0.6793628808864266\n",
      "Step: 361, Loss: 1.4079290628433228, Accuracy: 0.6788674033149171\n",
      "Step: 362, Loss: 1.3175983428955078, Accuracy: 0.6786042240587695\n",
      "Step: 363, Loss: 1.1556165218353271, Accuracy: 0.6788003663003663\n",
      "Step: 364, Loss: 1.4134869575500488, Accuracy: 0.678082191780822\n",
      "Step: 365, Loss: 1.466956615447998, Accuracy: 0.677367941712204\n",
      "Step: 366, Loss: 1.0698328018188477, Accuracy: 0.6777929155313351\n",
      "Step: 367, Loss: 1.3014284372329712, Accuracy: 0.677536231884058\n",
      "Step: 368, Loss: 1.0608290433883667, Accuracy: 0.6779584462511292\n",
      "Step: 369, Loss: 1.2201560735702515, Accuracy: 0.6781531531531532\n",
      "Step: 370, Loss: 1.3215841054916382, Accuracy: 0.6778975741239892\n",
      "Step: 371, Loss: 1.0630574226379395, Accuracy: 0.6783154121863799\n",
      "Step: 372, Loss: 1.1957818269729614, Accuracy: 0.6785075960679178\n",
      "Step: 373, Loss: 1.2358548641204834, Accuracy: 0.678475935828877\n",
      "Step: 374, Loss: 1.3143366575241089, Accuracy: 0.6782222222222222\n",
      "Step: 375, Loss: 1.281200647354126, Accuracy: 0.6779698581560284\n",
      "Step: 376, Loss: 1.0476423501968384, Accuracy: 0.6783819628647215\n",
      "Step: 377, Loss: 1.2510623931884766, Accuracy: 0.6781305114638448\n",
      "Step: 378, Loss: 1.4309622049331665, Accuracy: 0.6778803869832893\n",
      "Step: 379, Loss: 1.1543636322021484, Accuracy: 0.6780701754385965\n",
      "Step: 380, Loss: 1.3284363746643066, Accuracy: 0.6778215223097113\n",
      "Step: 381, Loss: 1.1627305746078491, Accuracy: 0.6780104712041884\n",
      "Step: 382, Loss: 1.332431674003601, Accuracy: 0.6775456919060052\n",
      "Step: 383, Loss: 1.0872784852981567, Accuracy: 0.6779513888888888\n",
      "Step: 384, Loss: 1.1587529182434082, Accuracy: 0.6781385281385282\n",
      "Step: 385, Loss: 1.2605088949203491, Accuracy: 0.6781088082901554\n",
      "Step: 386, Loss: 1.1961947679519653, Accuracy: 0.6780792420327304\n",
      "Step: 387, Loss: 1.0841996669769287, Accuracy: 0.678479381443299\n",
      "Step: 388, Loss: 1.0749653577804565, Accuracy: 0.6788774635818338\n",
      "Step: 389, Loss: 1.1170705556869507, Accuracy: 0.6790598290598291\n",
      "Step: 390, Loss: 1.1605173349380493, Accuracy: 0.6792412617220801\n",
      "Step: 391, Loss: 1.2225350141525269, Accuracy: 0.6794217687074829\n",
      "Step: 392, Loss: 1.1854511499404907, Accuracy: 0.6796013570822731\n",
      "Step: 393, Loss: 1.337412714958191, Accuracy: 0.6791455160744501\n",
      "Step: 394, Loss: 1.0548492670059204, Accuracy: 0.679535864978903\n",
      "Step: 395, Loss: 1.3641551733016968, Accuracy: 0.6790824915824916\n",
      "Step: 396, Loss: 1.1079859733581543, Accuracy: 0.679471032745592\n",
      "Step: 397, Loss: 1.1905676126480103, Accuracy: 0.6796482412060302\n",
      "Step: 398, Loss: 1.1799598932266235, Accuracy: 0.6798245614035088\n",
      "Step: 399, Loss: 1.2072083950042725, Accuracy: 0.6797916666666667\n",
      "Step: 400, Loss: 1.17267644405365, Accuracy: 0.6799667497921862\n",
      "Step: 401, Loss: 1.1741704940795898, Accuracy: 0.6801409618573798\n",
      "Step: 402, Loss: 1.4605728387832642, Accuracy: 0.6794871794871795\n",
      "Step: 403, Loss: 1.2413182258605957, Accuracy: 0.6794554455445545\n",
      "Step: 404, Loss: 1.3670867681503296, Accuracy: 0.6792181069958848\n",
      "Step: 405, Loss: 1.3146018981933594, Accuracy: 0.6789819376026273\n",
      "Step: 406, Loss: 1.236113429069519, Accuracy: 0.678951678951679\n",
      "Step: 407, Loss: 1.1932036876678467, Accuracy: 0.6791258169934641\n",
      "Step: 408, Loss: 1.0825105905532837, Accuracy: 0.6795028524857376\n",
      "Step: 409, Loss: 1.3106411695480347, Accuracy: 0.6792682926829269\n",
      "Step: 410, Loss: 1.3428350687026978, Accuracy: 0.6790348742903487\n",
      "Step: 411, Loss: 1.347809910774231, Accuracy: 0.6788025889967637\n",
      "Step: 412, Loss: 1.1014866828918457, Accuracy: 0.6791767554479419\n",
      "Step: 413, Loss: 1.075268030166626, Accuracy: 0.679549114331723\n",
      "Step: 414, Loss: 1.2789011001586914, Accuracy: 0.6795180722891566\n",
      "Step: 415, Loss: 1.4686002731323242, Accuracy: 0.678886217948718\n",
      "Step: 416, Loss: 1.053813099861145, Accuracy: 0.6792565947242206\n",
      "Step: 417, Loss: 1.2965372800827026, Accuracy: 0.6790271132376395\n",
      "Step: 418, Loss: 1.0221445560455322, Accuracy: 0.6795942720763724\n",
      "Step: 419, Loss: 1.1292372941970825, Accuracy: 0.6797619047619048\n",
      "Step: 420, Loss: 0.993266761302948, Accuracy: 0.6803246239113222\n",
      "Step: 421, Loss: 1.231300950050354, Accuracy: 0.6802922590837283\n",
      "Step: 422, Loss: 1.4493824243545532, Accuracy: 0.6796690307328606\n",
      "Step: 423, Loss: 1.22782301902771, Accuracy: 0.6796383647798742\n",
      "Step: 424, Loss: 1.2349281311035156, Accuracy: 0.6796078431372549\n",
      "Step: 425, Loss: 1.1592828035354614, Accuracy: 0.679773082942097\n",
      "Step: 426, Loss: 1.211930751800537, Accuracy: 0.6799375487900078\n",
      "Step: 427, Loss: 1.0827242136001587, Accuracy: 0.6802959501557633\n",
      "Step: 428, Loss: 1.3946436643600464, Accuracy: 0.6798756798756799\n",
      "Step: 429, Loss: 0.9979512691497803, Accuracy: 0.6804263565891473\n",
      "Step: 430, Loss: 1.0951284170150757, Accuracy: 0.6807811291569992\n",
      "Step: 431, Loss: 1.1597949266433716, Accuracy: 0.6809413580246914\n",
      "Step: 432, Loss: 1.2718950510025024, Accuracy: 0.6809083910700539\n",
      "Step: 433, Loss: 1.2397748231887817, Accuracy: 0.6808755760368663\n",
      "Step: 434, Loss: 1.1764494180679321, Accuracy: 0.6810344827586207\n",
      "Step: 435, Loss: 1.31336510181427, Accuracy: 0.6808103975535168\n",
      "Step: 436, Loss: 1.1619378328323364, Accuracy: 0.6809687261632341\n",
      "Step: 437, Loss: 1.2580780982971191, Accuracy: 0.6809360730593608\n",
      "Step: 438, Loss: 1.1765166521072388, Accuracy: 0.6810933940774487\n",
      "Step: 439, Loss: 1.322776436805725, Accuracy: 0.6808712121212122\n",
      "Step: 440, Loss: 1.0794721841812134, Accuracy: 0.6812169312169312\n",
      "Step: 441, Loss: 1.3502845764160156, Accuracy: 0.6808069381598794\n",
      "Step: 442, Loss: 1.2201030254364014, Accuracy: 0.6807750188111362\n",
      "Step: 443, Loss: 1.2491604089736938, Accuracy: 0.6807432432432432\n",
      "Step: 444, Loss: 1.0640592575073242, Accuracy: 0.6810861423220974\n",
      "Step: 445, Loss: 1.143318772315979, Accuracy: 0.6812406576980568\n",
      "Step: 446, Loss: 1.304112434387207, Accuracy: 0.6810216256524981\n",
      "Step: 447, Loss: 0.9839170575141907, Accuracy: 0.6815476190476191\n",
      "Step: 448, Loss: 1.39107084274292, Accuracy: 0.6813288789903489\n",
      "Step: 449, Loss: 1.0894047021865845, Accuracy: 0.6816666666666666\n",
      "Step: 450, Loss: 1.3714886903762817, Accuracy: 0.6812638580931264\n",
      "Step: 451, Loss: 1.1675626039505005, Accuracy: 0.6814159292035398\n",
      "Step: 452, Loss: 1.2450841665267944, Accuracy: 0.681383370125092\n",
      "Step: 453, Loss: 1.2398887872695923, Accuracy: 0.6813509544787077\n",
      "Step: 454, Loss: 1.3024992942810059, Accuracy: 0.6811355311355312\n",
      "Step: 455, Loss: 1.264206051826477, Accuracy: 0.6811038011695907\n",
      "Step: 456, Loss: 1.2159408330917358, Accuracy: 0.6810722100656456\n",
      "Step: 457, Loss: 1.3667041063308716, Accuracy: 0.6808588064046579\n",
      "Step: 458, Loss: 1.3533307313919067, Accuracy: 0.6806463326071169\n",
      "Step: 459, Loss: 1.5704587697982788, Accuracy: 0.6798913043478261\n",
      "Step: 460, Loss: 1.2052903175354004, Accuracy: 0.6798626174981923\n",
      "Step: 461, Loss: 1.2953001260757446, Accuracy: 0.6796536796536796\n",
      "Step: 462, Loss: 1.0861245393753052, Accuracy: 0.6799856011519079\n",
      "Step: 463, Loss: 1.2411351203918457, Accuracy: 0.6799568965517241\n",
      "Step: 464, Loss: 1.1203343868255615, Accuracy: 0.6802867383512545\n",
      "Step: 465, Loss: 1.2341147661209106, Accuracy: 0.6802575107296137\n",
      "Step: 466, Loss: 1.200757622718811, Accuracy: 0.6802284082798001\n",
      "Step: 467, Loss: 1.3835182189941406, Accuracy: 0.6798433048433048\n",
      "Step: 468, Loss: 1.2703200578689575, Accuracy: 0.679637526652452\n",
      "Step: 469, Loss: 1.182273507118225, Accuracy: 0.6797872340425531\n",
      "Step: 470, Loss: 1.121102213859558, Accuracy: 0.6799363057324841\n",
      "Step: 471, Loss: 1.2205970287322998, Accuracy: 0.6799081920903954\n",
      "Step: 472, Loss: 1.0820218324661255, Accuracy: 0.6802325581395349\n",
      "Step: 473, Loss: 1.378825068473816, Accuracy: 0.6798523206751055\n",
      "Step: 474, Loss: 1.1665065288543701, Accuracy: 0.68\n",
      "Step: 475, Loss: 1.3281023502349854, Accuracy: 0.6796218487394958\n",
      "Step: 476, Loss: 1.1114839315414429, Accuracy: 0.6799440950384347\n",
      "Step: 477, Loss: 1.314609169960022, Accuracy: 0.6797419804741981\n",
      "Step: 478, Loss: 1.3915380239486694, Accuracy: 0.6793667362560891\n",
      "Step: 479, Loss: 1.218541145324707, Accuracy: 0.6793402777777777\n",
      "Step: 480, Loss: 1.3236501216888428, Accuracy: 0.6791406791406791\n",
      "Step: 481, Loss: 1.1819515228271484, Accuracy: 0.6792876901798064\n",
      "Step: 482, Loss: 1.0811055898666382, Accuracy: 0.6796066252587992\n",
      "Step: 483, Loss: 1.1870232820510864, Accuracy: 0.6797520661157025\n",
      "Step: 484, Loss: 1.2891699075698853, Accuracy: 0.679725085910653\n",
      "Step: 485, Loss: 1.3649595975875854, Accuracy: 0.6793552812071331\n",
      "Step: 486, Loss: 1.4105876684188843, Accuracy: 0.6789869952087612\n",
      "Step: 487, Loss: 1.3379583358764648, Accuracy: 0.6787909836065574\n",
      "Step: 488, Loss: 1.2555593252182007, Accuracy: 0.6785957736877982\n",
      "Step: 489, Loss: 1.4111918210983276, Accuracy: 0.6782312925170068\n",
      "Step: 490, Loss: 1.2126808166503906, Accuracy: 0.6782077393075356\n",
      "Step: 491, Loss: 1.1393738985061646, Accuracy: 0.6783536585365854\n",
      "Step: 492, Loss: 1.407381534576416, Accuracy: 0.6779918864097363\n",
      "Step: 493, Loss: 1.558895230293274, Accuracy: 0.6772941970310391\n",
      "Step: 494, Loss: 1.1280266046524048, Accuracy: 0.6774410774410774\n",
      "Step: 495, Loss: 1.4029436111450195, Accuracy: 0.6770833333333334\n",
      "Step: 496, Loss: 1.3910980224609375, Accuracy: 0.6767270288397049\n",
      "Step: 497, Loss: 1.3369182348251343, Accuracy: 0.6763721552878179\n",
      "Step: 498, Loss: 1.1892575025558472, Accuracy: 0.6765197060788243\n",
      "Step: 499, Loss: 1.1310395002365112, Accuracy: 0.6766666666666666\n",
      "Step: 500, Loss: 1.2938989400863647, Accuracy: 0.676480372588157\n",
      "Step: 501, Loss: 1.1605499982833862, Accuracy: 0.6766268260292164\n",
      "Step: 502, Loss: 1.1404155492782593, Accuracy: 0.6767726971504308\n",
      "Step: 503, Loss: 1.0348039865493774, Accuracy: 0.6770833333333334\n",
      "Step: 504, Loss: 1.1533472537994385, Accuracy: 0.6772277227722773\n",
      "Step: 505, Loss: 1.4570211172103882, Accuracy: 0.6765480895915679\n",
      "Step: 506, Loss: 1.5645103454589844, Accuracy: 0.6758711374095989\n",
      "Step: 507, Loss: 1.0051612854003906, Accuracy: 0.6763451443569554\n",
      "Step: 508, Loss: 1.091138482093811, Accuracy: 0.676489849377865\n",
      "Step: 509, Loss: 1.1865788698196411, Accuracy: 0.6764705882352942\n",
      "Step: 510, Loss: 1.446825385093689, Accuracy: 0.6759621656881931\n",
      "Step: 511, Loss: 1.3354140520095825, Accuracy: 0.67578125\n",
      "Step: 512, Loss: 1.4962636232376099, Accuracy: 0.6752761533463287\n",
      "Step: 513, Loss: 1.3063654899597168, Accuracy: 0.6750972762645915\n",
      "Step: 514, Loss: 1.0654500722885132, Accuracy: 0.6754045307443366\n",
      "Step: 515, Loss: 1.1339324712753296, Accuracy: 0.6755490956072352\n",
      "Step: 516, Loss: 1.2767722606658936, Accuracy: 0.675531914893617\n",
      "Step: 517, Loss: 1.1799311637878418, Accuracy: 0.6756756756756757\n",
      "Step: 518, Loss: 1.0747052431106567, Accuracy: 0.6759794476557482\n",
      "Step: 519, Loss: 1.1819722652435303, Accuracy: 0.6761217948717949\n",
      "Step: 520, Loss: 1.4816950559616089, Accuracy: 0.6756238003838771\n",
      "Step: 521, Loss: 1.3018187284469604, Accuracy: 0.6754469987228607\n",
      "Step: 522, Loss: 1.3619886636734009, Accuracy: 0.6751115360101976\n",
      "Step: 523, Loss: 1.3773174285888672, Accuracy: 0.6747773536895675\n",
      "Step: 524, Loss: 1.493507742881775, Accuracy: 0.6741269841269841\n",
      "Step: 525, Loss: 1.1241159439086914, Accuracy: 0.6742712294043093\n",
      "Step: 526, Loss: 1.4441789388656616, Accuracy: 0.6737824161922834\n",
      "Step: 527, Loss: 1.2286611795425415, Accuracy: 0.6737689393939394\n",
      "Step: 528, Loss: 1.1375341415405273, Accuracy: 0.6739130434782609\n",
      "Step: 529, Loss: 1.2944387197494507, Accuracy: 0.6737421383647799\n",
      "Step: 530, Loss: 1.1594762802124023, Accuracy: 0.6738857501569366\n",
      "Step: 531, Loss: 1.2368338108062744, Accuracy: 0.6738721804511278\n",
      "Step: 532, Loss: 1.3545951843261719, Accuracy: 0.6735459662288931\n",
      "Step: 533, Loss: 1.2519198656082153, Accuracy: 0.6735330836454432\n",
      "Step: 534, Loss: 1.2059193849563599, Accuracy: 0.6735202492211838\n",
      "Step: 535, Loss: 1.238396406173706, Accuracy: 0.6735074626865671\n",
      "Step: 536, Loss: 1.1456459760665894, Accuracy: 0.6736499068901304\n",
      "Step: 537, Loss: 1.1457384824752808, Accuracy: 0.6737918215613383\n",
      "Step: 538, Loss: 1.4889289140701294, Accuracy: 0.6731601731601732\n",
      "Step: 539, Loss: 1.1076279878616333, Accuracy: 0.6734567901234568\n",
      "Step: 540, Loss: 1.2172605991363525, Accuracy: 0.6734442390634627\n",
      "Step: 541, Loss: 1.3013756275177002, Accuracy: 0.6732779827798278\n",
      "Step: 542, Loss: 1.0633466243743896, Accuracy: 0.6737262124002455\n",
      "Step: 543, Loss: 1.244848370552063, Accuracy: 0.6738664215686274\n",
      "Step: 544, Loss: 1.4582514762878418, Accuracy: 0.673394495412844\n",
      "Step: 545, Loss: 1.3029512166976929, Accuracy: 0.6732295482295483\n",
      "Step: 546, Loss: 1.198345422744751, Accuracy: 0.673217550274223\n",
      "Step: 547, Loss: 1.0954447984695435, Accuracy: 0.6735097323600974\n",
      "Step: 548, Loss: 1.1714223623275757, Accuracy: 0.6736490588949605\n",
      "Step: 549, Loss: 1.3053001165390015, Accuracy: 0.6734848484848485\n",
      "Step: 550, Loss: 1.3171762228012085, Accuracy: 0.6733212341197822\n",
      "Step: 551, Loss: 1.1628977060317993, Accuracy: 0.6734601449275363\n",
      "Step: 552, Loss: 1.367310881614685, Accuracy: 0.6731464737793852\n",
      "Step: 553, Loss: 0.9404566884040833, Accuracy: 0.6737364620938628\n",
      "Step: 554, Loss: 1.2039610147476196, Accuracy: 0.6738738738738739\n",
      "Step: 555, Loss: 1.205872893333435, Accuracy: 0.6740107913669064\n",
      "Step: 556, Loss: 1.0544707775115967, Accuracy: 0.6744464392579294\n",
      "Step: 557, Loss: 1.2841275930404663, Accuracy: 0.6744324970131422\n",
      "Step: 558, Loss: 1.313057541847229, Accuracy: 0.6742695289206917\n",
      "Step: 559, Loss: 1.3055481910705566, Accuracy: 0.6742559523809524\n",
      "Step: 560, Loss: 1.4213610887527466, Accuracy: 0.6737967914438503\n",
      "Step: 561, Loss: 1.1265417337417603, Accuracy: 0.6740806642941874\n",
      "Step: 562, Loss: 1.2994904518127441, Accuracy: 0.673919478981646\n",
      "Step: 563, Loss: 1.081133484840393, Accuracy: 0.6742021276595744\n",
      "Step: 564, Loss: 1.2557207345962524, Accuracy: 0.674188790560472\n",
      "Step: 565, Loss: 1.4232715368270874, Accuracy: 0.6738810365135454\n",
      "Step: 566, Loss: 1.2867857217788696, Accuracy: 0.673721340388007\n",
      "Step: 567, Loss: 1.5327153205871582, Accuracy: 0.6731220657276995\n",
      "Step: 568, Loss: 1.431515097618103, Accuracy: 0.6726713532513181\n",
      "Step: 569, Loss: 1.3395954370498657, Accuracy: 0.6723684210526316\n",
      "Step: 570, Loss: 1.3016196489334106, Accuracy: 0.6722124927028604\n",
      "Step: 571, Loss: 1.1732096672058105, Accuracy: 0.6723484848484849\n",
      "Step: 572, Loss: 1.2106791734695435, Accuracy: 0.6724840023269343\n",
      "Step: 573, Loss: 1.3142800331115723, Accuracy: 0.67232868757259\n",
      "Step: 574, Loss: 1.171212077140808, Accuracy: 0.6723188405797101\n",
      "Step: 575, Loss: 1.2236652374267578, Accuracy: 0.6723090277777778\n",
      "Step: 576, Loss: 1.3502984046936035, Accuracy: 0.6720103986135182\n",
      "Step: 577, Loss: 0.9890541434288025, Accuracy: 0.6725778546712803\n",
      "Step: 578, Loss: 1.33551824092865, Accuracy: 0.6722797927461139\n",
      "Step: 579, Loss: 1.3027184009552002, Accuracy: 0.6719827586206897\n",
      "Step: 580, Loss: 1.0591508150100708, Accuracy: 0.6722604704532416\n",
      "Step: 581, Loss: 1.031197190284729, Accuracy: 0.6725372279495991\n",
      "Step: 582, Loss: 1.5164775848388672, Accuracy: 0.6719554030874786\n",
      "Step: 583, Loss: 1.3027735948562622, Accuracy: 0.6718036529680366\n",
      "Step: 584, Loss: 1.4488391876220703, Accuracy: 0.6715099715099715\n",
      "Step: 585, Loss: 1.2977666854858398, Accuracy: 0.6713594994311718\n",
      "Step: 586, Loss: 1.158861756324768, Accuracy: 0.6714934696195344\n",
      "Step: 587, Loss: 1.1857393980026245, Accuracy: 0.671485260770975\n",
      "Step: 588, Loss: 1.2547812461853027, Accuracy: 0.6714770797962648\n",
      "Step: 589, Loss: 1.0796523094177246, Accuracy: 0.6717514124293785\n",
      "Step: 590, Loss: 1.3410677909851074, Accuracy: 0.6716018048505358\n",
      "Step: 591, Loss: 1.0238417387008667, Accuracy: 0.6720157657657657\n",
      "Step: 592, Loss: 1.207871437072754, Accuracy: 0.6720067453625632\n",
      "Step: 593, Loss: 1.2567477226257324, Accuracy: 0.6719977553310886\n",
      "Step: 594, Loss: 1.1528205871582031, Accuracy: 0.6721288515406163\n",
      "Step: 595, Loss: 1.40772545337677, Accuracy: 0.6719798657718121\n",
      "Step: 596, Loss: 1.1876169443130493, Accuracy: 0.6719709659408152\n",
      "Step: 597, Loss: 1.3413370847702026, Accuracy: 0.6716833890746934\n",
      "Step: 598, Loss: 1.1297277212142944, Accuracy: 0.6719532554257095\n",
      "Step: 599, Loss: 1.1503986120224, Accuracy: 0.6720833333333334\n",
      "Step: 600, Loss: 1.1350241899490356, Accuracy: 0.6722129783693843\n",
      "Step: 601, Loss: 1.3601864576339722, Accuracy: 0.6720653377630121\n",
      "Step: 602, Loss: 1.267425775527954, Accuracy: 0.67191818684356\n",
      "Step: 603, Loss: 1.258471965789795, Accuracy: 0.6719094922737306\n",
      "Step: 604, Loss: 1.1933072805404663, Accuracy: 0.671900826446281\n",
      "Step: 605, Loss: 1.0357242822647095, Accuracy: 0.6721672167216721\n",
      "Step: 606, Loss: 1.195388674736023, Accuracy: 0.6721581548599671\n",
      "Step: 607, Loss: 1.071468710899353, Accuracy: 0.6725603070175439\n",
      "Step: 608, Loss: 1.2350444793701172, Accuracy: 0.6725506294471811\n",
      "Step: 609, Loss: 1.1581352949142456, Accuracy: 0.6726775956284153\n",
      "Step: 610, Loss: 1.3312615156173706, Accuracy: 0.67253136933988\n",
      "Step: 611, Loss: 1.2903660535812378, Accuracy: 0.6723856209150327\n",
      "Step: 612, Loss: 1.1407973766326904, Accuracy: 0.6725122349102773\n",
      "Step: 613, Loss: 1.378764033317566, Accuracy: 0.6722312703583062\n",
      "Step: 614, Loss: 1.1892073154449463, Accuracy: 0.6722222222222223\n",
      "Step: 615, Loss: 1.2937915325164795, Accuracy: 0.672077922077922\n",
      "Step: 616, Loss: 1.3799200057983398, Accuracy: 0.6717990275526742\n",
      "Step: 617, Loss: 1.1939691305160522, Accuracy: 0.6719255663430421\n",
      "Step: 618, Loss: 1.1292717456817627, Accuracy: 0.6720516962843296\n",
      "Step: 619, Loss: 1.1478286981582642, Accuracy: 0.6721774193548387\n",
      "Step: 620, Loss: 1.5737501382827759, Accuracy: 0.6716317767042405\n",
      "Step: 621, Loss: 1.4051681756973267, Accuracy: 0.6713558413719185\n",
      "Step: 622, Loss: 1.1776851415634155, Accuracy: 0.671482075976458\n",
      "Step: 623, Loss: 1.0942307710647583, Accuracy: 0.6717414529914529\n",
      "Step: 624, Loss: 1.0854477882385254, Accuracy: 0.672\n",
      "Step: 625, Loss: 0.9348970055580139, Accuracy: 0.6725239616613419\n",
      "Step: 626, Loss: 1.328237771987915, Accuracy: 0.6723817118553961\n",
      "Step: 627, Loss: 1.253611445426941, Accuracy: 0.6723726114649682\n",
      "Step: 628, Loss: 1.2049278020858765, Accuracy: 0.6723635400105988\n",
      "Step: 629, Loss: 1.2391246557235718, Accuracy: 0.6723544973544974\n",
      "Step: 630, Loss: 1.3398990631103516, Accuracy: 0.6722134178552562\n",
      "Step: 631, Loss: 1.3319050073623657, Accuracy: 0.6720727848101266\n",
      "Step: 632, Loss: 1.3349695205688477, Accuracy: 0.6718009478672986\n",
      "Step: 633, Loss: 1.0441877841949463, Accuracy: 0.6720557308096741\n",
      "Step: 634, Loss: 1.4216176271438599, Accuracy: 0.6717847769028872\n",
      "Step: 635, Loss: 1.2862495183944702, Accuracy: 0.6716457023060797\n",
      "Step: 636, Loss: 1.093775749206543, Accuracy: 0.6718995290423861\n",
      "Step: 637, Loss: 1.159685730934143, Accuracy: 0.6721525600835946\n",
      "Step: 638, Loss: 1.1794111728668213, Accuracy: 0.6722743870631195\n",
      "Step: 639, Loss: 1.3337990045547485, Accuracy: 0.6721354166666667\n",
      "Step: 640, Loss: 1.6029375791549683, Accuracy: 0.6714768590743629\n",
      "Step: 641, Loss: 1.259576678276062, Accuracy: 0.6714693665628245\n",
      "Step: 642, Loss: 1.1420034170150757, Accuracy: 0.6715914981855884\n",
      "Step: 643, Loss: 1.1100648641586304, Accuracy: 0.6717132505175983\n",
      "Step: 644, Loss: 1.3066340684890747, Accuracy: 0.6715762273901809\n",
      "Step: 645, Loss: 1.0147061347961426, Accuracy: 0.6719556243550051\n",
      "Step: 646, Loss: 1.1088846921920776, Accuracy: 0.6722050489438434\n",
      "Step: 647, Loss: 1.194642186164856, Accuracy: 0.6723251028806584\n",
      "Step: 648, Loss: 1.0652954578399658, Accuracy: 0.672573189522342\n",
      "Step: 649, Loss: 1.230517029762268, Accuracy: 0.6725641025641026\n",
      "Step: 650, Loss: 1.3751214742660522, Accuracy: 0.6722990271377368\n",
      "Step: 651, Loss: 1.3809865713119507, Accuracy: 0.6720347648261759\n",
      "Step: 652, Loss: 1.2298049926757812, Accuracy: 0.6720265441551813\n",
      "Step: 653, Loss: 1.4648551940917969, Accuracy: 0.6716360856269113\n",
      "Step: 654, Loss: 1.2748101949691772, Accuracy: 0.6716284987277353\n",
      "Step: 655, Loss: 1.1542147397994995, Accuracy: 0.6717479674796748\n",
      "Step: 656, Loss: 1.633462905883789, Accuracy: 0.6711060375443937\n",
      "Step: 657, Loss: 1.268027663230896, Accuracy: 0.6709726443768997\n",
      "Step: 658, Loss: 1.2023082971572876, Accuracy: 0.670966110268083\n",
      "Step: 659, Loss: 1.2217451333999634, Accuracy: 0.670959595959596\n",
      "Step: 660, Loss: 1.138097882270813, Accuracy: 0.6710791729702471\n",
      "Step: 661, Loss: 1.1929389238357544, Accuracy: 0.6711983887210473\n",
      "Step: 662, Loss: 1.21979558467865, Accuracy: 0.6711915535444947\n",
      "Step: 663, Loss: 1.4428118467330933, Accuracy: 0.6708082329317269\n",
      "Step: 664, Loss: 1.1181540489196777, Accuracy: 0.6709273182957394\n",
      "Step: 665, Loss: 1.1788992881774902, Accuracy: 0.671046046046046\n",
      "Step: 666, Loss: 1.1329821348190308, Accuracy: 0.6711644177911045\n",
      "Step: 667, Loss: 1.155376672744751, Accuracy: 0.6712824351297405\n",
      "Step: 668, Loss: 1.1495054960250854, Accuracy: 0.6714000996512207\n",
      "Step: 669, Loss: 1.157140851020813, Accuracy: 0.6715174129353234\n",
      "Step: 670, Loss: 1.3466930389404297, Accuracy: 0.6712617983109787\n",
      "Step: 671, Loss: 1.3432135581970215, Accuracy: 0.6711309523809523\n",
      "Step: 672, Loss: 1.1860288381576538, Accuracy: 0.671124318969787\n",
      "Step: 673, Loss: 1.2622442245483398, Accuracy: 0.6709940652818991\n",
      "Step: 674, Loss: 1.0531870126724243, Accuracy: 0.6712345679012346\n",
      "Step: 675, Loss: 1.1903510093688965, Accuracy: 0.6712278106508875\n",
      "Step: 676, Loss: 1.39070463180542, Accuracy: 0.6709748892171344\n",
      "Step: 677, Loss: 1.088018536567688, Accuracy: 0.6712143559488692\n",
      "Step: 678, Loss: 1.1112909317016602, Accuracy: 0.6712076583210603\n",
      "Step: 679, Loss: 1.1123102903366089, Accuracy: 0.6714460784313725\n",
      "Step: 680, Loss: 1.0714728832244873, Accuracy: 0.6716837983357807\n",
      "Step: 681, Loss: 1.0217063426971436, Accuracy: 0.6720430107526881\n",
      "Step: 682, Loss: 1.2420214414596558, Accuracy: 0.6720351390922401\n",
      "Step: 683, Loss: 1.328750729560852, Accuracy: 0.6719054580896686\n",
      "Step: 684, Loss: 0.9914957880973816, Accuracy: 0.6722627737226278\n",
      "Step: 685, Loss: 1.2428597211837769, Accuracy: 0.6722546161321672\n",
      "Step: 686, Loss: 1.0558315515518188, Accuracy: 0.6724890829694323\n",
      "Step: 687, Loss: 1.2662444114685059, Accuracy: 0.6724806201550387\n",
      "Step: 688, Loss: 1.2994152307510376, Accuracy: 0.6723512336719883\n",
      "Step: 689, Loss: 1.3021602630615234, Accuracy: 0.6722222222222223\n",
      "Step: 690, Loss: 1.4879642724990845, Accuracy: 0.6718523878437048\n",
      "Step: 691, Loss: 1.263607144355774, Accuracy: 0.671844894026975\n",
      "Step: 692, Loss: 1.327959656715393, Accuracy: 0.6715969215969216\n",
      "Step: 693, Loss: 1.1085081100463867, Accuracy: 0.6718299711815562\n",
      "Step: 694, Loss: 1.07789146900177, Accuracy: 0.6720623501199041\n",
      "Step: 695, Loss: 1.2562047243118286, Accuracy: 0.6720545977011494\n",
      "Step: 696, Loss: 1.3528923988342285, Accuracy: 0.6718077474892395\n",
      "Step: 697, Loss: 1.1607764959335327, Accuracy: 0.671919770773639\n",
      "Step: 698, Loss: 1.532171607017517, Accuracy: 0.6714353838817358\n",
      "Step: 699, Loss: 1.1872268915176392, Accuracy: 0.6715476190476191\n",
      "Step: 700, Loss: 1.318607211112976, Accuracy: 0.6714217784117926\n",
      "Step: 701, Loss: 1.3059879541397095, Accuracy: 0.6712962962962963\n",
      "Step: 702, Loss: 1.4651527404785156, Accuracy: 0.6709340919867236\n",
      "Step: 703, Loss: 1.3059210777282715, Accuracy: 0.6708096590909091\n",
      "Step: 704, Loss: 1.1538339853286743, Accuracy: 0.6709219858156028\n",
      "Step: 705, Loss: 1.2914174795150757, Accuracy: 0.6706798866855525\n",
      "Step: 706, Loss: 1.1514205932617188, Accuracy: 0.6707920792079208\n",
      "Step: 707, Loss: 1.1000982522964478, Accuracy: 0.6709039548022598\n",
      "Step: 708, Loss: 1.2575031518936157, Accuracy: 0.6708979783732957\n",
      "Step: 709, Loss: 1.0585967302322388, Accuracy: 0.6711267605633803\n",
      "Step: 710, Loss: 1.1276979446411133, Accuracy: 0.6712376933895922\n",
      "Step: 711, Loss: 1.1497377157211304, Accuracy: 0.6713483146067416\n",
      "Step: 712, Loss: 1.273853063583374, Accuracy: 0.6713417484805985\n",
      "Step: 713, Loss: 1.3266692161560059, Accuracy: 0.6712184873949579\n",
      "Step: 714, Loss: 1.492264747619629, Accuracy: 0.6708624708624709\n",
      "Step: 715, Loss: 1.1685270071029663, Accuracy: 0.6709729981378026\n",
      "Step: 716, Loss: 1.2909815311431885, Accuracy: 0.6708507670850767\n",
      "Step: 717, Loss: 1.5622243881225586, Accuracy: 0.6703806870937791\n",
      "Step: 718, Loss: 1.4889835119247437, Accuracy: 0.6700278164116829\n",
      "Step: 719, Loss: 1.156962513923645, Accuracy: 0.6702546296296297\n",
      "Step: 720, Loss: 1.2267844676971436, Accuracy: 0.670249653259362\n",
      "Step: 721, Loss: 1.149375557899475, Accuracy: 0.6703601108033241\n",
      "Step: 722, Loss: 1.0174847841262817, Accuracy: 0.6707007837713231\n",
      "Step: 723, Loss: 1.2745341062545776, Accuracy: 0.6705801104972375\n",
      "Step: 724, Loss: 1.4662729501724243, Accuracy: 0.6702298850574713\n",
      "Step: 725, Loss: 1.1225440502166748, Accuracy: 0.6704545454545454\n",
      "Step: 726, Loss: 1.091408610343933, Accuracy: 0.6706785878037598\n",
      "Step: 727, Loss: 1.2008272409439087, Accuracy: 0.6706730769230769\n",
      "Step: 728, Loss: 1.0901398658752441, Accuracy: 0.6708962048468221\n",
      "Step: 729, Loss: 1.270012378692627, Accuracy: 0.6707762557077626\n",
      "Step: 730, Loss: 1.4375630617141724, Accuracy: 0.6705426356589147\n",
      "Step: 731, Loss: 1.2844165563583374, Accuracy: 0.6704234972677595\n",
      "Step: 732, Loss: 1.1109447479248047, Accuracy: 0.6705320600272852\n",
      "Step: 733, Loss: 1.1862658262252808, Accuracy: 0.6705267938237965\n",
      "Step: 734, Loss: 1.1915863752365112, Accuracy: 0.6705215419501134\n",
      "Step: 735, Loss: 1.3449949026107788, Accuracy: 0.6704030797101449\n",
      "Step: 736, Loss: 1.0443836450576782, Accuracy: 0.670737222976029\n",
      "Step: 737, Loss: 1.0940933227539062, Accuracy: 0.6709575429087624\n",
      "Step: 738, Loss: 1.0753929615020752, Accuracy: 0.6711772665764547\n",
      "Step: 739, Loss: 1.3096271753311157, Accuracy: 0.6710585585585586\n",
      "Step: 740, Loss: 1.2886645793914795, Accuracy: 0.6710526315789473\n",
      "Step: 741, Loss: 1.3076390027999878, Accuracy: 0.6709344115004492\n",
      "Step: 742, Loss: 1.5273680686950684, Accuracy: 0.6704800358905338\n",
      "Step: 743, Loss: 1.0622692108154297, Accuracy: 0.6706989247311828\n",
      "Step: 744, Loss: 1.0445671081542969, Accuracy: 0.670917225950783\n",
      "Step: 745, Loss: 1.1352518796920776, Accuracy: 0.6711349419124218\n",
      "Step: 746, Loss: 1.3360017538070679, Accuracy: 0.6710174029451138\n",
      "Step: 747, Loss: 1.2026458978652954, Accuracy: 0.6711229946524064\n",
      "Step: 748, Loss: 1.3456507921218872, Accuracy: 0.6710057854917668\n",
      "Step: 749, Loss: 1.108518123626709, Accuracy: 0.6712222222222223\n",
      "Step: 750, Loss: 1.2376168966293335, Accuracy: 0.6712161562361296\n",
      "Step: 751, Loss: 1.2164398431777954, Accuracy: 0.6712101063829787\n",
      "Step: 752, Loss: 1.4230517148971558, Accuracy: 0.6709827357237715\n",
      "Step: 753, Loss: 1.3939437866210938, Accuracy: 0.6707559681697612\n",
      "Step: 754, Loss: 1.2363618612289429, Accuracy: 0.6707505518763797\n",
      "Step: 755, Loss: 1.1479086875915527, Accuracy: 0.6709656084656085\n",
      "Step: 756, Loss: 1.1633366346359253, Accuracy: 0.6710700132100397\n",
      "Step: 757, Loss: 1.5580469369888306, Accuracy: 0.6706244503078276\n",
      "Step: 758, Loss: 1.0810067653656006, Accuracy: 0.670838823012736\n",
      "Step: 759, Loss: 1.2649705410003662, Accuracy: 0.6707236842105263\n",
      "Step: 760, Loss: 1.0863958597183228, Accuracy: 0.6709373631187034\n",
      "Step: 761, Loss: 1.3671637773513794, Accuracy: 0.6707130358705162\n",
      "Step: 762, Loss: 0.9967033863067627, Accuracy: 0.6710353866317169\n",
      "Step: 763, Loss: 1.2950562238693237, Accuracy: 0.6709205933682374\n",
      "Step: 764, Loss: 1.0931270122528076, Accuracy: 0.6711328976034858\n",
      "Step: 765, Loss: 1.1820834875106812, Accuracy: 0.6712358572671888\n",
      "Step: 766, Loss: 1.3219062089920044, Accuracy: 0.6711212516297262\n",
      "Step: 767, Loss: 1.5005568265914917, Accuracy: 0.6706814236111112\n",
      "Step: 768, Loss: 1.197640061378479, Accuracy: 0.6706762028608583\n",
      "Step: 769, Loss: 1.3029409646987915, Accuracy: 0.6705627705627706\n",
      "Step: 770, Loss: 1.29055655002594, Accuracy: 0.6704496325118893\n",
      "Step: 771, Loss: 1.0849484205245972, Accuracy: 0.6706606217616581\n",
      "Step: 772, Loss: 1.2372876405715942, Accuracy: 0.670655454937473\n",
      "Step: 773, Loss: 1.1159800291061401, Accuracy: 0.6708656330749354\n",
      "Step: 774, Loss: 1.3701163530349731, Accuracy: 0.6706451612903226\n",
      "Step: 775, Loss: 1.4287843704223633, Accuracy: 0.6703178694158075\n",
      "Step: 776, Loss: 1.0708857774734497, Accuracy: 0.6705276705276705\n",
      "Step: 777, Loss: 1.2752717733383179, Accuracy: 0.670522707797772\n",
      "Step: 778, Loss: 1.3123373985290527, Accuracy: 0.670410783055199\n",
      "Step: 779, Loss: 1.0232223272323608, Accuracy: 0.6707264957264957\n",
      "Step: 780, Loss: 1.3679800033569336, Accuracy: 0.6705078958600086\n",
      "Step: 781, Loss: 1.2494646310806274, Accuracy: 0.6705029838022165\n",
      "Step: 782, Loss: 1.3206323385238647, Accuracy: 0.6703916560238399\n",
      "Step: 783, Loss: 1.0792955160140991, Accuracy: 0.6705994897959183\n",
      "Step: 784, Loss: 1.080710768699646, Accuracy: 0.6708067940552017\n",
      "Step: 785, Loss: 1.077745795249939, Accuracy: 0.6710135708227312\n",
      "Step: 786, Loss: 1.0236159563064575, Accuracy: 0.6713257094451504\n",
      "Step: 787, Loss: 1.453762173652649, Accuracy: 0.671002538071066\n",
      "Step: 788, Loss: 1.1848243474960327, Accuracy: 0.6709970426700465\n",
      "Step: 789, Loss: 1.1856704950332642, Accuracy: 0.6710970464135021\n",
      "Step: 790, Loss: 1.2208071947097778, Accuracy: 0.6710914454277286\n",
      "Step: 791, Loss: 1.2346982955932617, Accuracy: 0.6710858585858586\n",
      "Step: 792, Loss: 1.1962363719940186, Accuracy: 0.6710802858343842\n",
      "Step: 793, Loss: 1.4668024778366089, Accuracy: 0.6707598656591099\n",
      "Step: 794, Loss: 1.5496907234191895, Accuracy: 0.6703354297693921\n",
      "Step: 795, Loss: 1.2949624061584473, Accuracy: 0.6702261306532663\n",
      "Step: 796, Loss: 1.1444777250289917, Accuracy: 0.6703262233375157\n",
      "Step: 797, Loss: 1.241349220275879, Accuracy: 0.6703216374269005\n",
      "Step: 798, Loss: 1.2793740034103394, Accuracy: 0.6702127659574468\n",
      "Step: 799, Loss: 1.1531325578689575, Accuracy: 0.6703125\n",
      "Step: 800, Loss: 1.4133424758911133, Accuracy: 0.670099875156055\n",
      "Step: 801, Loss: 1.2067495584487915, Accuracy: 0.6700955943474647\n",
      "Step: 802, Loss: 1.3249049186706543, Accuracy: 0.6700913242009132\n",
      "Step: 803, Loss: 1.0812708139419556, Accuracy: 0.6702943615257048\n",
      "Step: 804, Loss: 1.6075439453125, Accuracy: 0.6697722567287785\n",
      "Step: 805, Loss: 1.1918752193450928, Accuracy: 0.6697684036393714\n",
      "Step: 806, Loss: 1.0791778564453125, Accuracy: 0.6699710863279636\n",
      "Step: 807, Loss: 1.1626707315444946, Accuracy: 0.6700701320132013\n",
      "Step: 808, Loss: 1.0825756788253784, Accuracy: 0.6701689328388958\n",
      "Step: 809, Loss: 1.2546948194503784, Accuracy: 0.670164609053498\n",
      "Step: 810, Loss: 1.0946415662765503, Accuracy: 0.6703658035347307\n",
      "Step: 811, Loss: 1.2240186929702759, Accuracy: 0.6703612479474549\n",
      "Step: 812, Loss: 1.3684592247009277, Accuracy: 0.6701517015170152\n",
      "Step: 813, Loss: 1.1002675294876099, Accuracy: 0.6703521703521703\n",
      "Step: 814, Loss: 1.0127582550048828, Accuracy: 0.6706543967280164\n",
      "Step: 815, Loss: 1.2641006708145142, Accuracy: 0.6706495098039216\n",
      "Step: 816, Loss: 1.339206337928772, Accuracy: 0.6704406364749081\n",
      "Step: 817, Loss: 1.2320284843444824, Accuracy: 0.6704360228198859\n",
      "Step: 818, Loss: 1.3547178506851196, Accuracy: 0.6702279202279202\n",
      "Step: 819, Loss: 1.1534011363983154, Accuracy: 0.6703252032520325\n",
      "Step: 820, Loss: 1.1279140710830688, Accuracy: 0.6704222492894844\n",
      "Step: 821, Loss: 1.0324655771255493, Accuracy: 0.6707218167072182\n",
      "Step: 822, Loss: 1.2686067819595337, Accuracy: 0.6707168894289186\n",
      "Step: 823, Loss: 1.1333872079849243, Accuracy: 0.6708131067961165\n",
      "Step: 824, Loss: 1.2276954650878906, Accuracy: 0.6708080808080809\n",
      "Step: 825, Loss: 1.2142080068588257, Accuracy: 0.6708030669895076\n",
      "Step: 826, Loss: 1.0670617818832397, Accuracy: 0.6709995969367191\n",
      "Step: 827, Loss: 0.9967924952507019, Accuracy: 0.6712962962962963\n",
      "Step: 828, Loss: 1.302616000175476, Accuracy: 0.6711901889827101\n",
      "Step: 829, Loss: 1.1671372652053833, Accuracy: 0.671285140562249\n",
      "Step: 830, Loss: 1.157423734664917, Accuracy: 0.6713798636181307\n",
      "Step: 831, Loss: 1.3143380880355835, Accuracy: 0.6712740384615384\n",
      "Step: 832, Loss: 0.9793276786804199, Accuracy: 0.6715686274509803\n",
      "Step: 833, Loss: 1.1097525358200073, Accuracy: 0.6717625899280576\n",
      "Step: 834, Loss: 1.304146409034729, Accuracy: 0.6717564870259481\n",
      "Step: 835, Loss: 1.3254005908966064, Accuracy: 0.6716507177033493\n",
      "Step: 836, Loss: 1.3912334442138672, Accuracy: 0.6714456391875747\n",
      "Step: 837, Loss: 1.0618001222610474, Accuracy: 0.6716388225934765\n",
      "Step: 838, Loss: 1.3997198343276978, Accuracy: 0.6714342471195868\n",
      "Step: 839, Loss: 1.224726676940918, Accuracy: 0.6714285714285714\n",
      "Step: 840, Loss: 1.1777223348617554, Accuracy: 0.6715219976218787\n",
      "Step: 841, Loss: 1.1451773643493652, Accuracy: 0.6716152019002375\n",
      "Step: 842, Loss: 1.158189058303833, Accuracy: 0.6717081850533808\n",
      "Step: 843, Loss: 1.095302939414978, Accuracy: 0.6718009478672986\n",
      "Step: 844, Loss: 1.159237265586853, Accuracy: 0.6718934911242603\n",
      "Step: 845, Loss: 1.1070884466171265, Accuracy: 0.6720843183609141\n",
      "Step: 846, Loss: 1.2174973487854004, Accuracy: 0.672077922077922\n",
      "Step: 847, Loss: 1.3288897275924683, Accuracy: 0.6719732704402516\n",
      "Step: 848, Loss: 1.40145742893219, Accuracy: 0.6717707106399686\n",
      "Step: 849, Loss: 1.1760200262069702, Accuracy: 0.6718627450980392\n",
      "Step: 850, Loss: 1.1183583736419678, Accuracy: 0.6719545632589111\n",
      "Step: 851, Loss: 1.0394463539123535, Accuracy: 0.6721439749608764\n",
      "Step: 852, Loss: 1.2635866403579712, Accuracy: 0.6721375537319265\n",
      "Step: 853, Loss: 1.098883032798767, Accuracy: 0.6723263075722092\n",
      "Step: 854, Loss: 1.1371837854385376, Accuracy: 0.6724171539961014\n",
      "Step: 855, Loss: 1.2226343154907227, Accuracy: 0.6725077881619937\n",
      "Step: 856, Loss: 1.336470603942871, Accuracy: 0.6724037339556592\n",
      "Step: 857, Loss: 1.2385703325271606, Accuracy: 0.6723970473970474\n",
      "Step: 858, Loss: 1.0735955238342285, Accuracy: 0.6725844004656577\n",
      "Step: 859, Loss: 1.1248670816421509, Accuracy: 0.6726744186046512\n",
      "Step: 860, Loss: 1.076514720916748, Accuracy: 0.6728610143244289\n",
      "Step: 861, Loss: 1.0555133819580078, Accuracy: 0.6731438515081206\n",
      "Step: 862, Loss: 1.4149279594421387, Accuracy: 0.6729432213209734\n",
      "Step: 863, Loss: 1.2526625394821167, Accuracy: 0.6729359567901234\n",
      "Step: 864, Loss: 1.2501660585403442, Accuracy: 0.6729287090558767\n",
      "Step: 865, Loss: 1.3995293378829956, Accuracy: 0.6727290223248653\n",
      "Step: 866, Loss: 1.2145949602127075, Accuracy: 0.6728181468665898\n",
      "Step: 867, Loss: 1.0231071710586548, Accuracy: 0.6730990783410138\n",
      "Step: 868, Loss: 0.9969310760498047, Accuracy: 0.673379363252781\n",
      "Step: 869, Loss: 1.5388489961624146, Accuracy: 0.6729885057471264\n",
      "Step: 870, Loss: 1.1632118225097656, Accuracy: 0.6730769230769231\n",
      "Step: 871, Loss: 1.1761629581451416, Accuracy: 0.6731651376146789\n",
      "Step: 872, Loss: 1.558611512184143, Accuracy: 0.6727758686521573\n",
      "Step: 873, Loss: 1.16335928440094, Accuracy: 0.6727688787185355\n",
      "Step: 874, Loss: 1.1360620260238647, Accuracy: 0.6728571428571428\n",
      "Step: 875, Loss: 1.2430843114852905, Accuracy: 0.6728500761035008\n",
      "Step: 876, Loss: 1.1166809797286987, Accuracy: 0.6729380463702015\n",
      "Step: 877, Loss: 1.2313119173049927, Accuracy: 0.6729309035687168\n",
      "Step: 878, Loss: 1.174734115600586, Accuracy: 0.6730185817216534\n",
      "Step: 879, Loss: 1.154166579246521, Accuracy: 0.6731060606060606\n",
      "Step: 880, Loss: 1.1500591039657593, Accuracy: 0.6731933409004919\n",
      "Step: 881, Loss: 1.072068691253662, Accuracy: 0.6733749055177627\n",
      "Step: 882, Loss: 1.3224023580551147, Accuracy: 0.6732729331823329\n",
      "Step: 883, Loss: 1.1613755226135254, Accuracy: 0.6733597285067874\n",
      "Step: 884, Loss: 1.1676162481307983, Accuracy: 0.6734463276836158\n",
      "Step: 885, Loss: 1.1765100955963135, Accuracy: 0.6735327313769752\n",
      "Step: 886, Loss: 1.0944117307662964, Accuracy: 0.673618940248027\n",
      "Step: 887, Loss: 1.164183259010315, Accuracy: 0.673704954954955\n",
      "Step: 888, Loss: 1.1531517505645752, Accuracy: 0.6737907761529809\n",
      "Step: 889, Loss: 1.186566710472107, Accuracy: 0.6737827715355805\n",
      "Step: 890, Loss: 1.0561600923538208, Accuracy: 0.6739618406285073\n",
      "Step: 891, Loss: 1.5593231916427612, Accuracy: 0.6735799701046338\n",
      "Step: 892, Loss: 1.1475272178649902, Accuracy: 0.673665546845838\n",
      "Step: 893, Loss: 1.1094766855239868, Accuracy: 0.6738441461595824\n",
      "Step: 894, Loss: 1.0724506378173828, Accuracy: 0.674022346368715\n",
      "Step: 895, Loss: 1.1799639463424683, Accuracy: 0.6741071428571429\n",
      "Step: 896, Loss: 1.2324992418289185, Accuracy: 0.6740988480118915\n",
      "Step: 897, Loss: 1.2089776992797852, Accuracy: 0.674090571640683\n",
      "Step: 898, Loss: 1.2717903852462769, Accuracy: 0.6739896180941787\n",
      "Step: 899, Loss: 1.2996087074279785, Accuracy: 0.6738888888888889\n",
      "Step: 900, Loss: 1.0961779356002808, Accuracy: 0.6740658527561968\n",
      "Step: 901, Loss: 1.236101746559143, Accuracy: 0.6740576496674058\n",
      "Step: 902, Loss: 1.114248514175415, Accuracy: 0.674234034699151\n",
      "Step: 903, Loss: 1.146121859550476, Accuracy: 0.6743178466076696\n",
      "Step: 904, Loss: 1.2500872611999512, Accuracy: 0.6743093922651934\n",
      "Step: 905, Loss: 1.3819656372070312, Accuracy: 0.6741169977924945\n",
      "Step: 906, Loss: 1.2486504316329956, Accuracy: 0.6741087835354649\n",
      "Step: 907, Loss: 1.2136987447738647, Accuracy: 0.6741005873715125\n",
      "Step: 908, Loss: 1.347300410270691, Accuracy: 0.6739090575724239\n",
      "Step: 909, Loss: 1.2158230543136597, Accuracy: 0.673901098901099\n",
      "Step: 910, Loss: 1.389011025428772, Accuracy: 0.6737102085620198\n",
      "Step: 911, Loss: 1.352578043937683, Accuracy: 0.6736111111111112\n",
      "Step: 912, Loss: 1.039553165435791, Accuracy: 0.6737860533041256\n",
      "Step: 913, Loss: 1.1774457693099976, Accuracy: 0.6738694383661561\n",
      "Step: 914, Loss: 1.2056435346603394, Accuracy: 0.6738615664845173\n",
      "Step: 915, Loss: 1.4054235219955444, Accuracy: 0.6736717612809315\n",
      "Step: 916, Loss: 1.3039332628250122, Accuracy: 0.67357324609233\n",
      "Step: 917, Loss: 1.5071302652359009, Accuracy: 0.673202614379085\n",
      "Step: 918, Loss: 1.2371774911880493, Accuracy: 0.6731955023576351\n",
      "Step: 919, Loss: 1.2228515148162842, Accuracy: 0.6731884057971015\n",
      "Step: 920, Loss: 1.3540266752243042, Accuracy: 0.6730003619254433\n",
      "Step: 921, Loss: 1.2360552549362183, Accuracy: 0.6729934924078091\n",
      "Step: 922, Loss: 1.1621202230453491, Accuracy: 0.6730769230769231\n",
      "Step: 923, Loss: 1.1318705081939697, Accuracy: 0.6731601731601732\n",
      "Step: 924, Loss: 1.1133772134780884, Accuracy: 0.6733333333333333\n",
      "Step: 925, Loss: 1.2730437517166138, Accuracy: 0.6732361411087113\n",
      "Step: 926, Loss: 1.1391092538833618, Accuracy: 0.6733189500179791\n",
      "Step: 927, Loss: 1.3751295804977417, Accuracy: 0.673132183908046\n",
      "Step: 928, Loss: 1.3294206857681274, Accuracy: 0.6730355220667384\n",
      "Step: 929, Loss: 1.277875304222107, Accuracy: 0.6730286738351254\n",
      "Step: 930, Loss: 1.38512122631073, Accuracy: 0.6728428213390619\n",
      "Step: 931, Loss: 1.155593752861023, Accuracy: 0.672925608011445\n",
      "Step: 932, Loss: 1.436948299407959, Accuracy: 0.6726509467667023\n",
      "Step: 933, Loss: 1.3078628778457642, Accuracy: 0.6725553176302641\n",
      "Step: 934, Loss: 1.647031307220459, Accuracy: 0.6721033868092692\n",
      "Step: 935, Loss: 1.1869416236877441, Accuracy: 0.6721866096866097\n",
      "Step: 936, Loss: 1.2479265928268433, Accuracy: 0.6720917822838848\n",
      "Step: 937, Loss: 1.4439595937728882, Accuracy: 0.67181947405828\n",
      "Step: 938, Loss: 1.1719204187393188, Accuracy: 0.6719027334043308\n",
      "Step: 939, Loss: 1.0817577838897705, Accuracy: 0.6720744680851064\n",
      "Step: 940, Loss: 1.2262815237045288, Accuracy: 0.6720687212185618\n",
      "Step: 941, Loss: 1.0634864568710327, Accuracy: 0.67223991507431\n",
      "Step: 942, Loss: 1.3895288705825806, Accuracy: 0.6720572640509014\n",
      "Step: 943, Loss: 1.1760832071304321, Accuracy: 0.6721398305084746\n",
      "Step: 944, Loss: 1.1259609460830688, Accuracy: 0.6722222222222223\n",
      "Step: 945, Loss: 1.3159767389297485, Accuracy: 0.6721282593375617\n",
      "Step: 946, Loss: 1.1610082387924194, Accuracy: 0.6722104892643436\n",
      "Step: 947, Loss: 1.4733692407608032, Accuracy: 0.6719409282700421\n",
      "Step: 948, Loss: 1.4613620042800903, Accuracy: 0.6716719353705655\n",
      "Step: 949, Loss: 1.3668104410171509, Accuracy: 0.6714035087719298\n",
      "Step: 950, Loss: 1.3725509643554688, Accuracy: 0.671223273746933\n",
      "Step: 951, Loss: 1.2199515104293823, Accuracy: 0.6712184873949579\n",
      "Step: 952, Loss: 1.2870208024978638, Accuracy: 0.6712137110877929\n",
      "Step: 953, Loss: 1.3231732845306396, Accuracy: 0.6711215932914046\n",
      "Step: 954, Loss: 1.2589627504348755, Accuracy: 0.6711169284467714\n",
      "Step: 955, Loss: 1.3345280885696411, Accuracy: 0.6709379358437936\n",
      "Step: 956, Loss: 1.07618248462677, Accuracy: 0.6711076280041798\n",
      "Step: 957, Loss: 1.3158947229385376, Accuracy: 0.6710160055671538\n",
      "Step: 958, Loss: 1.008217692375183, Accuracy: 0.6712721584984359\n",
      "Step: 959, Loss: 1.4588016271591187, Accuracy: 0.6710069444444444\n",
      "Step: 960, Loss: 1.2150965929031372, Accuracy: 0.6710024280263615\n",
      "Step: 961, Loss: 1.564523696899414, Accuracy: 0.6705647955647955\n",
      "Step: 962, Loss: 1.2063237428665161, Accuracy: 0.6705607476635514\n",
      "Step: 963, Loss: 1.10489022731781, Accuracy: 0.6707295988934993\n",
      "Step: 964, Loss: 1.3112822771072388, Accuracy: 0.6706390328151987\n",
      "Step: 965, Loss: 1.0810039043426514, Accuracy: 0.6708937198067633\n",
      "Step: 966, Loss: 1.2843751907348633, Accuracy: 0.6708893485005171\n",
      "Step: 967, Loss: 1.5541898012161255, Accuracy: 0.6705406336088154\n",
      "Step: 968, Loss: 1.1941254138946533, Accuracy: 0.6705366357069144\n",
      "Step: 969, Loss: 1.2541592121124268, Accuracy: 0.6705326460481099\n",
      "Step: 970, Loss: 1.1392614841461182, Accuracy: 0.6706144867833849\n",
      "Step: 971, Loss: 1.2673083543777466, Accuracy: 0.6705246913580247\n",
      "Step: 972, Loss: 1.180710792541504, Accuracy: 0.6705207262761219\n",
      "Step: 973, Loss: 1.3333073854446411, Accuracy: 0.6704312114989733\n",
      "Step: 974, Loss: 1.2140640020370483, Accuracy: 0.6704273504273505\n",
      "Step: 975, Loss: 1.3763231039047241, Accuracy: 0.6702527322404371\n",
      "Step: 976, Loss: 1.3148481845855713, Accuracy: 0.6701637666325486\n",
      "Step: 977, Loss: 1.303704023361206, Accuracy: 0.6700749829584185\n",
      "Step: 978, Loss: 1.094342589378357, Accuracy: 0.6702417432754512\n",
      "Step: 979, Loss: 1.2450772523880005, Accuracy: 0.6702380952380952\n",
      "Step: 980, Loss: 1.3080973625183105, Accuracy: 0.6701495073054706\n",
      "Step: 981, Loss: 1.366249680519104, Accuracy: 0.6699762389680923\n",
      "Step: 982, Loss: 1.142823338508606, Accuracy: 0.6701424211597151\n",
      "Step: 983, Loss: 1.1565901041030884, Accuracy: 0.6702235772357723\n",
      "Step: 984, Loss: 1.2374640703201294, Accuracy: 0.6702199661590524\n",
      "Step: 985, Loss: 1.3255640268325806, Accuracy: 0.670131845841785\n",
      "Step: 986, Loss: 1.1630593538284302, Accuracy: 0.6702127659574468\n",
      "Step: 987, Loss: 1.330245852470398, Accuracy: 0.6700404858299596\n",
      "Step: 988, Loss: 1.3335704803466797, Accuracy: 0.6699528142905291\n",
      "Step: 989, Loss: 1.1994433403015137, Accuracy: 0.67003367003367\n",
      "Step: 990, Loss: 1.0670475959777832, Accuracy: 0.6701143625967036\n",
      "Step: 991, Loss: 1.152093529701233, Accuracy: 0.6701948924731183\n",
      "Step: 992, Loss: 1.2684472799301147, Accuracy: 0.6701074185968445\n",
      "Step: 993, Loss: 1.0851194858551025, Accuracy: 0.670271629778672\n",
      "Step: 994, Loss: 1.0546618700027466, Accuracy: 0.6705192629815745\n",
      "Step: 995, Loss: 1.1673455238342285, Accuracy: 0.6705153949129853\n",
      "Step: 996, Loss: 1.0177831649780273, Accuracy: 0.6707622868605817\n",
      "Step: 997, Loss: 1.2970409393310547, Accuracy: 0.6707581830327322\n",
      "Step: 998, Loss: 1.2639944553375244, Accuracy: 0.670754087420754\n",
      "Step: 999, Loss: 1.1354840993881226, Accuracy: 0.6708333333333333\n",
      "Step: 1000, Loss: 1.0569370985031128, Accuracy: 0.670995670995671\n",
      "Step: 1001, Loss: 0.998591423034668, Accuracy: 0.6712408516300732\n",
      "Step: 1002, Loss: 1.2351679801940918, Accuracy: 0.6712362911266201\n",
      "Step: 1003, Loss: 1.2937763929367065, Accuracy: 0.671148738379814\n",
      "Step: 1004, Loss: 1.3974608182907104, Accuracy: 0.6709784411276949\n",
      "Step: 1005, Loss: 1.2398804426193237, Accuracy: 0.6709741550695825\n",
      "Step: 1006, Loss: 1.1978873014450073, Accuracy: 0.6709698775239987\n",
      "Step: 1007, Loss: 1.1473342180252075, Accuracy: 0.6710482804232805\n",
      "Step: 1008, Loss: 1.3110781908035278, Accuracy: 0.6709613478691774\n",
      "Step: 1009, Loss: 1.0170046091079712, Accuracy: 0.6712046204620462\n",
      "Step: 1010, Loss: 1.1250569820404053, Accuracy: 0.6712825585229146\n",
      "Step: 1011, Loss: 1.1162067651748657, Accuracy: 0.6714426877470355\n",
      "Step: 1012, Loss: 1.1304574012756348, Accuracy: 0.6715202369200395\n",
      "Step: 1013, Loss: 1.1281847953796387, Accuracy: 0.6715976331360947\n",
      "Step: 1014, Loss: 1.2814525365829468, Accuracy: 0.6715106732348112\n",
      "Step: 1015, Loss: 1.2473667860031128, Accuracy: 0.671505905511811\n",
      "Step: 1016, Loss: 1.2316950559616089, Accuracy: 0.671501147164864\n",
      "Step: 1017, Loss: 1.314089059829712, Accuracy: 0.6714145383104125\n",
      "Step: 1018, Loss: 1.3143354654312134, Accuracy: 0.6713280994438993\n",
      "Step: 1019, Loss: 1.339502215385437, Accuracy: 0.6712418300653594\n",
      "Step: 1020, Loss: 1.134626030921936, Accuracy: 0.6713189683317009\n",
      "Step: 1021, Loss: 1.1097310781478882, Accuracy: 0.6714774951076321\n",
      "Step: 1022, Loss: 1.2578362226486206, Accuracy: 0.6713913326816553\n",
      "Step: 1023, Loss: 1.2047163248062134, Accuracy: 0.6714680989583334\n",
      "Step: 1024, Loss: 1.2291598320007324, Accuracy: 0.6714634146341464\n",
      "Step: 1025, Loss: 1.056770920753479, Accuracy: 0.6716211825860948\n",
      "Step: 1026, Loss: 1.3848010301589966, Accuracy: 0.6714540733528075\n",
      "Step: 1027, Loss: 1.2518362998962402, Accuracy: 0.6714494163424124\n",
      "Step: 1028, Loss: 1.0976910591125488, Accuracy: 0.6716067379332685\n",
      "Step: 1029, Loss: 1.5400195121765137, Accuracy: 0.6712783171521035\n",
      "Step: 1030, Loss: 1.389865517616272, Accuracy: 0.6711121888134497\n",
      "Step: 1031, Loss: 1.1275396347045898, Accuracy: 0.6712693798449613\n",
      "Step: 1032, Loss: 1.2189074754714966, Accuracy: 0.6712649241690868\n",
      "Step: 1033, Loss: 1.2090094089508057, Accuracy: 0.6712604771115409\n",
      "Step: 1034, Loss: 1.5241438150405884, Accuracy: 0.6709339774557166\n",
      "Step: 1035, Loss: 1.1262201070785522, Accuracy: 0.671010296010296\n",
      "Step: 1036, Loss: 1.2898974418640137, Accuracy: 0.6710061073609772\n",
      "Step: 1037, Loss: 1.151471495628357, Accuracy: 0.671082209377007\n",
      "Step: 1038, Loss: 1.3410638570785522, Accuracy: 0.6709175489252487\n",
      "Step: 1039, Loss: 1.109112024307251, Accuracy: 0.6709935897435897\n",
      "Step: 1040, Loss: 1.2321892976760864, Accuracy: 0.6709894332372719\n",
      "Step: 1041, Loss: 1.1190699338912964, Accuracy: 0.6710652591170825\n",
      "Step: 1042, Loss: 1.362770676612854, Accuracy: 0.6709012464046021\n",
      "Step: 1043, Loss: 1.3731342554092407, Accuracy: 0.6707375478927203\n",
      "Step: 1044, Loss: 1.0681779384613037, Accuracy: 0.6708931419457735\n",
      "Step: 1045, Loss: 1.4417486190795898, Accuracy: 0.6706500956022945\n",
      "Step: 1046, Loss: 1.2594172954559326, Accuracy: 0.6705666985036612\n",
      "Step: 1047, Loss: 1.3149223327636719, Accuracy: 0.6704834605597965\n",
      "Step: 1048, Loss: 1.3669978380203247, Accuracy: 0.6703209405783286\n",
      "Step: 1049, Loss: 1.3005967140197754, Accuracy: 0.6701587301587302\n",
      "Step: 1050, Loss: 1.3751450777053833, Accuracy: 0.6699968284173803\n",
      "Step: 1051, Loss: 1.2900196313858032, Accuracy: 0.6699144486692015\n",
      "Step: 1052, Loss: 1.243606448173523, Accuracy: 0.6699113643558088\n",
      "Step: 1053, Loss: 1.4182147979736328, Accuracy: 0.6697501581277673\n",
      "Step: 1054, Loss: 1.1764575242996216, Accuracy: 0.669826224328594\n",
      "Step: 1055, Loss: 1.2256548404693604, Accuracy: 0.6698232323232324\n",
      "Step: 1056, Loss: 1.3940783739089966, Accuracy: 0.6696625670135604\n",
      "Step: 1057, Loss: 1.100060224533081, Accuracy: 0.6698172652804033\n",
      "Step: 1058, Loss: 1.1194556951522827, Accuracy: 0.669971671388102\n",
      "Step: 1059, Loss: 1.2742480039596558, Accuracy: 0.6698899371069182\n",
      "Step: 1060, Loss: 1.1590620279312134, Accuracy: 0.6699654414074773\n",
      "Step: 1061, Loss: 1.1776723861694336, Accuracy: 0.6700408035153798\n",
      "Step: 1062, Loss: 1.1616508960723877, Accuracy: 0.6701160238319223\n",
      "Step: 1063, Loss: 1.103390097618103, Accuracy: 0.6702694235588973\n",
      "Step: 1064, Loss: 1.1296401023864746, Accuracy: 0.6703442879499217\n",
      "Step: 1065, Loss: 1.2415720224380493, Accuracy: 0.6704190118824265\n",
      "Step: 1066, Loss: 1.0959892272949219, Accuracy: 0.6704935957513277\n",
      "Step: 1067, Loss: 1.1197997331619263, Accuracy: 0.6705680399500624\n",
      "Step: 1068, Loss: 1.211614966392517, Accuracy: 0.6706423448705956\n",
      "Step: 1069, Loss: 1.148391604423523, Accuracy: 0.6707165109034268\n",
      "Step: 1070, Loss: 1.2270551919937134, Accuracy: 0.6707127295362589\n",
      "Step: 1071, Loss: 1.259329080581665, Accuracy: 0.6707089552238806\n",
      "Step: 1072, Loss: 1.192711353302002, Accuracy: 0.6707828518173345\n",
      "Step: 1073, Loss: 1.0539636611938477, Accuracy: 0.6709342023587833\n",
      "Step: 1074, Loss: 1.0693987607955933, Accuracy: 0.6710852713178295\n",
      "Step: 1075, Loss: 1.293967366218567, Accuracy: 0.671003717472119\n",
      "Step: 1076, Loss: 0.9988603591918945, Accuracy: 0.6712318167749922\n",
      "Step: 1077, Loss: 1.2414873838424683, Accuracy: 0.6712275819418676\n",
      "Step: 1078, Loss: 1.2146220207214355, Accuracy: 0.6712233549582948\n",
      "Step: 1079, Loss: 1.1142295598983765, Accuracy: 0.6713734567901235\n",
      "Step: 1080, Loss: 1.338509202003479, Accuracy: 0.6712920135676842\n",
      "Step: 1081, Loss: 1.4774012565612793, Accuracy: 0.6710566851509551\n",
      "Step: 1082, Loss: 1.1007814407348633, Accuracy: 0.6712065250846414\n",
      "Step: 1083, Loss: 1.2215629816055298, Accuracy: 0.6712023370233703\n",
      "Step: 1084, Loss: 1.0978829860687256, Accuracy: 0.6713517665130568\n",
      "Step: 1085, Loss: 1.1494032144546509, Accuracy: 0.6714241866175568\n",
      "Step: 1086, Loss: 1.3295234441757202, Accuracy: 0.6712664826740263\n",
      "Step: 1087, Loss: 1.0793629884719849, Accuracy: 0.6714154411764706\n",
      "Step: 1088, Loss: 1.288252830505371, Accuracy: 0.6714110805019896\n",
      "Step: 1089, Loss: 1.22955322265625, Accuracy: 0.6714831804281346\n",
      "Step: 1090, Loss: 1.1994304656982422, Accuracy: 0.6714787656584174\n",
      "Step: 1091, Loss: 1.2307928800582886, Accuracy: 0.6714743589743589\n",
      "Step: 1092, Loss: 1.3373031616210938, Accuracy: 0.6713174748398902\n",
      "Step: 1093, Loss: 1.335012435913086, Accuracy: 0.6712370505789153\n",
      "Step: 1094, Loss: 1.0415974855422974, Accuracy: 0.6714611872146119\n",
      "Step: 1095, Loss: 1.2434815168380737, Accuracy: 0.6714568126520681\n",
      "Step: 1096, Loss: 1.1557714939117432, Accuracy: 0.6714524460650259\n",
      "Step: 1097, Loss: 1.1570721864700317, Accuracy: 0.6715998785670917\n",
      "Step: 1098, Loss: 0.9998102784156799, Accuracy: 0.6718228692750986\n",
      "Step: 1099, Loss: 1.340572476387024, Accuracy: 0.6717424242424243\n",
      "Step: 1100, Loss: 1.2477878332138062, Accuracy: 0.6717378141083863\n",
      "Step: 1101, Loss: 1.1678229570388794, Accuracy: 0.6717332123411979\n",
      "Step: 1102, Loss: 1.246854543685913, Accuracy: 0.6717286189181021\n",
      "Step: 1103, Loss: 1.190735101699829, Accuracy: 0.6717240338164251\n",
      "Step: 1104, Loss: 1.0324925184249878, Accuracy: 0.6719457013574661\n",
      "Step: 1105, Loss: 1.308024525642395, Accuracy: 0.6718655816757083\n",
      "Step: 1106, Loss: 1.1710747480392456, Accuracy: 0.6720114423366456\n",
      "Step: 1107, Loss: 1.1422537565231323, Accuracy: 0.6720818291215404\n",
      "Step: 1108, Loss: 1.314422369003296, Accuracy: 0.6720018034265104\n",
      "Step: 1109, Loss: 1.1164058446884155, Accuracy: 0.6721471471471472\n",
      "Step: 1110, Loss: 1.2556744813919067, Accuracy: 0.6721422142214222\n",
      "Step: 1111, Loss: 1.2066922187805176, Accuracy: 0.6721372901678657\n",
      "Step: 1112, Loss: 1.1137853860855103, Accuracy: 0.6722072476789458\n",
      "Step: 1113, Loss: 1.1597191095352173, Accuracy: 0.672277079593058\n",
      "Step: 1114, Loss: 1.2184475660324097, Accuracy: 0.672272047832586\n",
      "Step: 1115, Loss: 1.2371045351028442, Accuracy: 0.6722670250896058\n",
      "Step: 1116, Loss: 1.18825101852417, Accuracy: 0.6723366159355416\n",
      "Step: 1117, Loss: 1.321773648262024, Accuracy: 0.6722570065593322\n",
      "Step: 1118, Loss: 1.1114319562911987, Accuracy: 0.6724009532320524\n",
      "Step: 1119, Loss: 1.3200552463531494, Accuracy: 0.6722470238095238\n",
      "Step: 1120, Loss: 1.4244297742843628, Accuracy: 0.672019030627416\n",
      "Step: 1121, Loss: 1.3319658041000366, Accuracy: 0.6719399881164587\n",
      "Step: 1122, Loss: 1.077487587928772, Accuracy: 0.6720837043633126\n",
      "Step: 1123, Loss: 1.0248053073883057, Accuracy: 0.6723013048635824\n",
      "Step: 1124, Loss: 1.2076436281204224, Accuracy: 0.6723703703703704\n",
      "Step: 1125, Loss: 1.1996477842330933, Accuracy: 0.6724393132030787\n",
      "Step: 1126, Loss: 1.2239214181900024, Accuracy: 0.6724341910677314\n",
      "Step: 1127, Loss: 1.2429457902908325, Accuracy: 0.6724290780141844\n",
      "Step: 1128, Loss: 1.193892002105713, Accuracy: 0.6724977856510186\n",
      "Step: 1129, Loss: 1.1358096599578857, Accuracy: 0.672566371681416\n",
      "Step: 1130, Loss: 1.0815938711166382, Accuracy: 0.6727085175361037\n",
      "Step: 1131, Loss: 1.389233112335205, Accuracy: 0.6725559481743227\n",
      "Step: 1132, Loss: 1.329384684562683, Accuracy: 0.6724771991762283\n",
      "Step: 1133, Loss: 1.2498012781143188, Accuracy: 0.672472075249853\n",
      "Step: 1134, Loss: 1.3076601028442383, Accuracy: 0.6723935389133627\n",
      "Step: 1135, Loss: 1.197998285293579, Accuracy: 0.6724618544600939\n",
      "Step: 1136, Loss: 1.1879940032958984, Accuracy: 0.672530049838757\n",
      "Step: 1137, Loss: 1.2647429704666138, Accuracy: 0.6724516695957821\n",
      "Step: 1138, Loss: 1.1589232683181763, Accuracy: 0.6725197541703248\n",
      "Step: 1139, Loss: 1.2722424268722534, Accuracy: 0.6724415204678362\n",
      "Step: 1140, Loss: 1.376090168952942, Accuracy: 0.6722903885480572\n",
      "Step: 1141, Loss: 1.2952395677566528, Accuracy: 0.6722124927028604\n",
      "Step: 1142, Loss: 1.1556044816970825, Accuracy: 0.6722805482648002\n",
      "Step: 1143, Loss: 1.173296570777893, Accuracy: 0.6723484848484849\n",
      "Step: 1144, Loss: 1.1685028076171875, Accuracy: 0.6724163027656478\n",
      "Step: 1145, Loss: 0.9564110636711121, Accuracy: 0.6727021524141943\n",
      "Step: 1146, Loss: 1.3085907697677612, Accuracy: 0.6726242371403661\n",
      "Step: 1147, Loss: 1.2483816146850586, Accuracy: 0.6726190476190477\n",
      "Step: 1148, Loss: 1.2460143566131592, Accuracy: 0.6726138671308384\n",
      "Step: 1149, Loss: 1.0589301586151123, Accuracy: 0.6727536231884058\n",
      "Step: 1150, Loss: 1.2235208749771118, Accuracy: 0.6727483347813495\n",
      "Step: 1151, Loss: 1.1221067905426025, Accuracy: 0.6728877314814815\n",
      "Step: 1152, Loss: 1.0256062746047974, Accuracy: 0.6730268863833477\n",
      "Step: 1153, Loss: 1.215238094329834, Accuracy: 0.6730213749277874\n",
      "Step: 1154, Loss: 1.1856247186660767, Accuracy: 0.6730880230880231\n",
      "Step: 1155, Loss: 1.1599289178848267, Accuracy: 0.673154555940023\n",
      "Step: 1156, Loss: 1.1745609045028687, Accuracy: 0.6732209737827716\n",
      "Step: 1157, Loss: 1.2259565591812134, Accuracy: 0.67328727691422\n",
      "Step: 1158, Loss: 1.3374041318893433, Accuracy: 0.6732096635030198\n",
      "Step: 1159, Loss: 1.3254432678222656, Accuracy: 0.673132183908046\n",
      "Step: 1160, Loss: 1.2242637872695923, Accuracy: 0.6731266149870802\n",
      "Step: 1161, Loss: 1.121331810951233, Accuracy: 0.6732644865174986\n",
      "Step: 1162, Loss: 1.2203401327133179, Accuracy: 0.6732588134135855\n",
      "Step: 1163, Loss: 1.1486233472824097, Accuracy: 0.6733247422680413\n",
      "Step: 1164, Loss: 1.2705903053283691, Accuracy: 0.6733190271816881\n",
      "Step: 1165, Loss: 1.2269495725631714, Accuracy: 0.6733133218982276\n",
      "Step: 1166, Loss: 1.1642961502075195, Accuracy: 0.6733790345615538\n",
      "Step: 1167, Loss: 1.128873348236084, Accuracy: 0.6734446347031964\n",
      "Step: 1168, Loss: 1.479982852935791, Accuracy: 0.6731536926147704\n",
      "Step: 1169, Loss: 0.9812390208244324, Accuracy: 0.6733618233618234\n",
      "Step: 1170, Loss: 1.4138984680175781, Accuracy: 0.6732137773982352\n",
      "Step: 1171, Loss: 1.2235866785049438, Accuracy: 0.6732081911262798\n",
      "Step: 1172, Loss: 1.0586509704589844, Accuracy: 0.6734157431088378\n",
      "Step: 1173, Loss: 1.3231576681137085, Accuracy: 0.6733390119250426\n",
      "Step: 1174, Loss: 1.0619888305664062, Accuracy: 0.6734751773049645\n",
      "Step: 1175, Loss: 1.383774757385254, Accuracy: 0.673327664399093\n",
      "Step: 1176, Loss: 1.418944001197815, Accuracy: 0.6731804021523647\n",
      "Step: 1177, Loss: 1.3064377307891846, Accuracy: 0.6731041312959819\n",
      "Step: 1178, Loss: 0.9961324334144592, Accuracy: 0.6733107152954482\n",
      "Step: 1179, Loss: 1.3029032945632935, Accuracy: 0.6732344632768361\n",
      "Step: 1180, Loss: 1.1476329565048218, Accuracy: 0.6732994637313011\n",
      "Step: 1181, Loss: 1.0752171277999878, Accuracy: 0.6734348561759729\n",
      "Step: 1182, Loss: 1.463731288909912, Accuracy: 0.6732178078331924\n",
      "Step: 1183, Loss: 1.1483856439590454, Accuracy: 0.6732826576576577\n",
      "Step: 1184, Loss: 1.221012830734253, Accuracy: 0.6733473980309423\n",
      "Step: 1185, Loss: 1.2600950002670288, Accuracy: 0.6733417650365374\n",
      "Step: 1186, Loss: 1.2381681203842163, Accuracy: 0.6733361415332771\n",
      "Step: 1187, Loss: 1.4101004600524902, Accuracy: 0.6731902356902357\n",
      "Step: 1188, Loss: 1.0164958238601685, Accuracy: 0.673395009812167\n",
      "Step: 1189, Loss: 1.2614991664886475, Accuracy: 0.673389355742297\n",
      "Step: 1190, Loss: 1.1595228910446167, Accuracy: 0.6734536803806325\n",
      "Step: 1191, Loss: 1.314167857170105, Accuracy: 0.6733780760626398\n",
      "Step: 1192, Loss: 1.2387439012527466, Accuracy: 0.6733724504051412\n",
      "Step: 1193, Loss: 1.1796507835388184, Accuracy: 0.6734366275823562\n",
      "Step: 1194, Loss: 1.2913649082183838, Accuracy: 0.6733612273361227\n",
      "Step: 1195, Loss: 1.1754509210586548, Accuracy: 0.6734253065774805\n",
      "Step: 1196, Loss: 0.9819790720939636, Accuracy: 0.6736285157337789\n",
      "Step: 1197, Loss: 1.3404436111450195, Accuracy: 0.673553144129104\n",
      "Step: 1198, Loss: 1.236807942390442, Accuracy: 0.6735474006116208\n",
      "Step: 1199, Loss: 1.2881391048431396, Accuracy: 0.6734722222222222\n",
      "Step: 1200, Loss: 1.3583449125289917, Accuracy: 0.6733277824035526\n",
      "Step: 1201, Loss: 1.2507238388061523, Accuracy: 0.6732529118136439\n",
      "Step: 1202, Loss: 1.2583776712417603, Accuracy: 0.6732474369631477\n",
      "Step: 1203, Loss: 1.034878134727478, Accuracy: 0.6734496124031008\n",
      "Step: 1204, Loss: 1.2296254634857178, Accuracy: 0.6734439834024897\n",
      "Step: 1205, Loss: 1.0879943370819092, Accuracy: 0.6735074626865671\n",
      "Step: 1206, Loss: 1.2096377611160278, Accuracy: 0.6735708367854184\n",
      "Step: 1207, Loss: 1.157597541809082, Accuracy: 0.6736341059602649\n",
      "Step: 1208, Loss: 1.0692505836486816, Accuracy: 0.6738351254480287\n",
      "Step: 1209, Loss: 1.339308738708496, Accuracy: 0.6737603305785124\n",
      "Step: 1210, Loss: 1.5333552360534668, Accuracy: 0.6734792182769062\n",
      "Step: 1211, Loss: 0.9624068140983582, Accuracy: 0.6737486248624862\n",
      "Step: 1212, Loss: 1.4513691663742065, Accuracy: 0.6735366859027205\n",
      "Step: 1213, Loss: 1.2407770156860352, Accuracy: 0.6735310269082921\n",
      "Step: 1214, Loss: 1.256750226020813, Accuracy: 0.6735253772290809\n",
      "Step: 1215, Loss: 1.276500940322876, Accuracy: 0.6735197368421053\n",
      "Step: 1216, Loss: 1.4698296785354614, Accuracy: 0.6733086825527252\n",
      "Step: 1217, Loss: 1.13367760181427, Accuracy: 0.6733716475095786\n",
      "Step: 1218, Loss: 1.3878406286239624, Accuracy: 0.6732294230243369\n",
      "Step: 1219, Loss: 1.1639500856399536, Accuracy: 0.673292349726776\n",
      "Step: 1220, Loss: 1.2767250537872314, Accuracy: 0.6732186732186732\n",
      "Step: 1221, Loss: 1.3823614120483398, Accuracy: 0.6730769230769231\n",
      "Step: 1222, Loss: 0.9906928539276123, Accuracy: 0.6732760970291632\n",
      "Step: 1223, Loss: 1.1772165298461914, Accuracy: 0.673338779956427\n",
      "Step: 1224, Loss: 1.1351146697998047, Accuracy: 0.6734013605442177\n",
      "Step: 1225, Loss: 1.187469244003296, Accuracy: 0.6734638390429581\n",
      "Step: 1226, Loss: 1.3233857154846191, Accuracy: 0.6733903830480847\n",
      "Step: 1227, Loss: 1.296099305152893, Accuracy: 0.6733170466883822\n",
      "Step: 1228, Loss: 1.0902297496795654, Accuracy: 0.6734472470843504\n",
      "Step: 1229, Loss: 1.174190640449524, Accuracy: 0.673509485094851\n",
      "Step: 1230, Loss: 1.3396881818771362, Accuracy: 0.6733685350663418\n",
      "Step: 1231, Loss: 1.4002189636230469, Accuracy: 0.6732278138528138\n",
      "Step: 1232, Loss: 1.2696236371994019, Accuracy: 0.6732224925655582\n",
      "Step: 1233, Loss: 1.2007395029067993, Accuracy: 0.6732847109670448\n",
      "Step: 1234, Loss: 1.112855315208435, Accuracy: 0.6734143049932524\n",
      "Step: 1235, Loss: 1.04990816116333, Accuracy: 0.6735436893203883\n",
      "Step: 1236, Loss: 1.2213081121444702, Accuracy: 0.6735381298841283\n",
      "Step: 1237, Loss: 1.0397893190383911, Accuracy: 0.6736672051696284\n",
      "Step: 1238, Loss: 1.2156018018722534, Accuracy: 0.6736615550174873\n",
      "Step: 1239, Loss: 1.0779342651367188, Accuracy: 0.6737903225806452\n",
      "Step: 1240, Loss: 1.140844464302063, Accuracy: 0.6738517324738115\n",
      "Step: 1241, Loss: 1.2082898616790771, Accuracy: 0.6739130434782609\n",
      "Step: 1242, Loss: 0.9437485337257385, Accuracy: 0.674175382139984\n",
      "Step: 1243, Loss: 1.2374311685562134, Accuracy: 0.6741693461950696\n",
      "Step: 1244, Loss: 1.4012848138809204, Accuracy: 0.6740294511378849\n",
      "Step: 1245, Loss: 1.102073073387146, Accuracy: 0.6741573033707865\n",
      "Step: 1246, Loss: 1.0993366241455078, Accuracy: 0.6742849505479819\n",
      "Step: 1247, Loss: 1.372367262840271, Accuracy: 0.6741452991452992\n",
      "Step: 1248, Loss: 1.5796103477478027, Accuracy: 0.6738057112356551\n",
      "Step: 1249, Loss: 1.2624250650405884, Accuracy: 0.6738\n",
      "Step: 1250, Loss: 0.9339752197265625, Accuracy: 0.6740607513988809\n",
      "Step: 1251, Loss: 1.0996228456497192, Accuracy: 0.6741879659211928\n",
      "Step: 1252, Loss: 1.3007136583328247, Accuracy: 0.6741154562383612\n",
      "Step: 1253, Loss: 1.0910097360610962, Accuracy: 0.6742424242424242\n",
      "Step: 1254, Loss: 1.166435718536377, Accuracy: 0.6743027888446215\n",
      "Step: 1255, Loss: 1.3892003297805786, Accuracy: 0.6741640127388535\n",
      "Step: 1256, Loss: 1.2683748006820679, Accuracy: 0.6740917528507028\n",
      "Step: 1257, Loss: 1.2172763347625732, Accuracy: 0.6740858505564388\n",
      "Step: 1258, Loss: 1.4300408363342285, Accuracy: 0.6738813873444532\n",
      "Step: 1259, Loss: 1.3031206130981445, Accuracy: 0.6738095238095239\n",
      "Step: 1260, Loss: 1.2207435369491577, Accuracy: 0.6738038593708697\n",
      "Step: 1261, Loss: 1.189428448677063, Accuracy: 0.6738642366613841\n",
      "Step: 1262, Loss: 1.2823954820632935, Accuracy: 0.6737925574030087\n",
      "Step: 1263, Loss: 1.3110984563827515, Accuracy: 0.6737209915611815\n",
      "Step: 1264, Loss: 1.3575986623764038, Accuracy: 0.6735836627140975\n",
      "Step: 1265, Loss: 1.1902931928634644, Accuracy: 0.6735781990521327\n",
      "Step: 1266, Loss: 1.319131851196289, Accuracy: 0.6735069718495132\n",
      "Step: 1267, Loss: 1.086199164390564, Accuracy: 0.6736330178759201\n",
      "Step: 1268, Loss: 1.2926136255264282, Accuracy: 0.6735618597320725\n",
      "Step: 1269, Loss: 1.5944151878356934, Accuracy: 0.6732283464566929\n",
      "Step: 1270, Loss: 1.1807273626327515, Accuracy: 0.6732887490165225\n",
      "Step: 1271, Loss: 1.3629202842712402, Accuracy: 0.6731525157232704\n",
      "Step: 1272, Loss: 1.1788381338119507, Accuracy: 0.6732128829536528\n",
      "Step: 1273, Loss: 1.3637834787368774, Accuracy: 0.6730769230769231\n",
      "Step: 1274, Loss: 1.4561229944229126, Accuracy: 0.6728758169934641\n",
      "Step: 1275, Loss: 1.0535787343978882, Accuracy: 0.6730015673981191\n",
      "Step: 1276, Loss: 1.226545810699463, Accuracy: 0.6729966066301227\n",
      "Step: 1277, Loss: 1.1419326066970825, Accuracy: 0.673056859676578\n",
      "Step: 1278, Loss: 1.2144581079483032, Accuracy: 0.6730518634349753\n",
      "Step: 1279, Loss: 0.9814934730529785, Accuracy: 0.6732421875\n",
      "Step: 1280, Loss: 1.5586203336715698, Accuracy: 0.6729768410096278\n",
      "Step: 1281, Loss: 1.1660236120224, Accuracy: 0.6730369214768591\n",
      "Step: 1282, Loss: 1.3252334594726562, Accuracy: 0.6729670044167316\n",
      "Step: 1283, Loss: 1.179604411125183, Accuracy: 0.6730269989615784\n",
      "Step: 1284, Loss: 1.2298334836959839, Accuracy: 0.6730220492866408\n",
      "Step: 1285, Loss: 1.2276344299316406, Accuracy: 0.6730171073094868\n",
      "Step: 1286, Loss: 1.2835661172866821, Accuracy: 0.6729474229474229\n",
      "Step: 1287, Loss: 1.209082007408142, Accuracy: 0.6730072463768116\n",
      "Step: 1288, Loss: 1.1703052520751953, Accuracy: 0.6730669769847427\n",
      "Step: 1289, Loss: 1.4495383501052856, Accuracy: 0.6728682170542636\n",
      "Step: 1290, Loss: 1.3038289546966553, Accuracy: 0.6727988639297702\n",
      "Step: 1291, Loss: 0.9830278754234314, Accuracy: 0.6729876160990712\n",
      "Step: 1292, Loss: 1.161932110786438, Accuracy: 0.673047177107502\n",
      "Step: 1293, Loss: 1.2104469537734985, Accuracy: 0.6730422462648119\n",
      "Step: 1294, Loss: 1.1554945707321167, Accuracy: 0.6731016731016731\n",
      "Step: 1295, Loss: 1.1603375673294067, Accuracy: 0.6731610082304527\n",
      "Step: 1296, Loss: 1.1115654706954956, Accuracy: 0.6732202518632743\n",
      "Step: 1297, Loss: 1.1736096143722534, Accuracy: 0.6732794042116076\n",
      "Step: 1298, Loss: 1.1359604597091675, Accuracy: 0.6733384654862715\n",
      "Step: 1299, Loss: 1.0877496004104614, Accuracy: 0.6734615384615384\n",
      "Step: 1300, Loss: 1.3638089895248413, Accuracy: 0.6733282090699462\n",
      "Step: 1301, Loss: 1.1029119491577148, Accuracy: 0.6734511008704557\n",
      "Step: 1302, Loss: 1.2523136138916016, Accuracy: 0.6733819391148631\n",
      "Step: 1303, Loss: 1.1725101470947266, Accuracy: 0.6734406952965235\n",
      "Step: 1304, Loss: 1.2623261213302612, Accuracy: 0.6734355044699872\n",
      "Step: 1305, Loss: 1.3588756322860718, Accuracy: 0.6733665135273098\n",
      "Step: 1306, Loss: 1.3693128824234009, Accuracy: 0.6731701096659015\n",
      "Step: 1307, Loss: 1.5395737886428833, Accuracy: 0.6729102956167177\n",
      "Step: 1308, Loss: 1.1258488893508911, Accuracy: 0.6729691876750701\n",
      "Step: 1309, Loss: 1.2257142066955566, Accuracy: 0.6729643765903308\n",
      "Step: 1310, Loss: 1.0924456119537354, Accuracy: 0.6730867022629037\n",
      "Step: 1311, Loss: 1.2487057447433472, Accuracy: 0.6730818089430894\n",
      "Step: 1312, Loss: 1.4033724069595337, Accuracy: 0.6728865194211728\n",
      "Step: 1313, Loss: 1.3239248991012573, Accuracy: 0.672818366311517\n",
      "Step: 1314, Loss: 1.0403791666030884, Accuracy: 0.6729404309252218\n",
      "Step: 1315, Loss: 1.1702302694320679, Accuracy: 0.672998986828774\n",
      "Step: 1316, Loss: 1.387062907218933, Accuracy: 0.672867628448494\n",
      "Step: 1317, Loss: 1.4275561571121216, Accuracy: 0.6727364693980779\n",
      "Step: 1318, Loss: 1.0742506980895996, Accuracy: 0.6728582259287339\n",
      "Step: 1319, Loss: 1.2394899129867554, Accuracy: 0.6728535353535353\n",
      "Step: 1320, Loss: 1.230450987815857, Accuracy: 0.672848851879889\n",
      "Step: 1321, Loss: 1.1072914600372314, Accuracy: 0.6729072112960162\n",
      "Step: 1322, Loss: 1.2162507772445679, Accuracy: 0.6729024943310657\n",
      "Step: 1323, Loss: 1.4307661056518555, Accuracy: 0.6727089627391742\n",
      "Step: 1324, Loss: 1.1477327346801758, Accuracy: 0.6727672955974843\n",
      "Step: 1325, Loss: 1.2172348499298096, Accuracy: 0.6727626948215184\n",
      "Step: 1326, Loss: 1.1303884983062744, Accuracy: 0.6728208992715398\n",
      "Step: 1327, Loss: 0.9784155488014221, Accuracy: 0.6730672690763052\n",
      "Step: 1328, Loss: 1.1456035375595093, Accuracy: 0.6731251567594683\n",
      "Step: 1329, Loss: 1.2292547225952148, Accuracy: 0.6731203007518797\n",
      "Step: 1330, Loss: 1.151687502861023, Accuracy: 0.6731780616078137\n",
      "Step: 1331, Loss: 1.080381155014038, Accuracy: 0.6732982982982983\n",
      "Step: 1332, Loss: 1.3267245292663574, Accuracy: 0.6732308077019254\n",
      "Step: 1333, Loss: 1.2835853099822998, Accuracy: 0.6731634182908546\n",
      "Step: 1334, Loss: 1.128076434135437, Accuracy: 0.6732209737827716\n",
      "Step: 1335, Loss: 1.148413896560669, Accuracy: 0.6732784431137725\n",
      "Step: 1336, Loss: 1.1184676885604858, Accuracy: 0.6733981550735477\n",
      "Step: 1337, Loss: 1.3092783689498901, Accuracy: 0.6733308420528151\n",
      "Step: 1338, Loss: 1.0195192098617554, Accuracy: 0.673512571570824\n",
      "Step: 1339, Loss: 1.1759661436080933, Accuracy: 0.6735696517412936\n",
      "Step: 1340, Loss: 1.2082418203353882, Accuracy: 0.6735645041014169\n",
      "Step: 1341, Loss: 1.2929458618164062, Accuracy: 0.6734972677595629\n",
      "Step: 1342, Loss: 1.2910405397415161, Accuracy: 0.6733680814097791\n",
      "Step: 1343, Loss: 1.1836895942687988, Accuracy: 0.6734250992063492\n",
      "Step: 1344, Loss: 1.0236316919326782, Accuracy: 0.6736059479553903\n",
      "Step: 1345, Loss: 1.2718256711959839, Accuracy: 0.6735388806339773\n",
      "Step: 1346, Loss: 1.30898916721344, Accuracy: 0.6734719128928484\n",
      "Step: 1347, Loss: 1.1329370737075806, Accuracy: 0.6735286844708209\n",
      "Step: 1348, Loss: 1.0890854597091675, Accuracy: 0.6736471460340994\n",
      "Step: 1349, Loss: 1.1009339094161987, Accuracy: 0.6737654320987654\n",
      "Step: 1350, Loss: 1.2236446142196655, Accuracy: 0.673760177646188\n",
      "Step: 1351, Loss: 1.2781046628952026, Accuracy: 0.6736932938856016\n",
      "Step: 1352, Loss: 0.9728628993034363, Accuracy: 0.6738728750923872\n",
      "Step: 1353, Loss: 1.291096568107605, Accuracy: 0.6738060068931561\n",
      "Step: 1354, Loss: 1.2407467365264893, Accuracy: 0.673739237392374\n",
      "Step: 1355, Loss: 1.1163098812103271, Accuracy: 0.6738569321533924\n",
      "Step: 1356, Loss: 1.260043740272522, Accuracy: 0.6737902235323017\n",
      "Step: 1357, Loss: 1.379594326019287, Accuracy: 0.6736622484045165\n",
      "Step: 1358, Loss: 1.3607460260391235, Accuracy: 0.6735957812116753\n",
      "Step: 1359, Loss: 1.2767372131347656, Accuracy: 0.6735294117647059\n",
      "Step: 1360, Loss: 1.1500054597854614, Accuracy: 0.6735855988243938\n",
      "Step: 1361, Loss: 1.1834347248077393, Accuracy: 0.673580518844836\n",
      "Step: 1362, Loss: 1.2415212392807007, Accuracy: 0.6735754463193935\n",
      "Step: 1363, Loss: 1.2416287660598755, Accuracy: 0.6735703812316716\n",
      "Step: 1364, Loss: 1.3232096433639526, Accuracy: 0.6735042735042736\n",
      "Step: 1365, Loss: 1.1636754274368286, Accuracy: 0.6735602733040508\n",
      "Step: 1366, Loss: 1.1502867937088013, Accuracy: 0.6736161911728846\n",
      "Step: 1367, Loss: 1.4606672525405884, Accuracy: 0.6734283625730995\n",
      "Step: 1368, Loss: 0.9692928194999695, Accuracy: 0.6736669101533966\n",
      "Step: 1369, Loss: 1.326499342918396, Accuracy: 0.6736009732360098\n",
      "Step: 1370, Loss: 1.0716758966445923, Accuracy: 0.6737174811573061\n",
      "Step: 1371, Loss: 1.4588021039962769, Accuracy: 0.6735301263362488\n",
      "Step: 1372, Loss: 1.15900456905365, Accuracy: 0.6735858218014081\n",
      "Step: 1373, Loss: 1.1591979265213013, Accuracy: 0.6736414361960213\n",
      "Step: 1374, Loss: 1.1287091970443726, Accuracy: 0.6736969696969697\n",
      "Step: 1375, Loss: 1.205835223197937, Accuracy: 0.6736918604651163\n",
      "Step: 1376, Loss: 1.2298911809921265, Accuracy: 0.6736867586540789\n",
      "Step: 1377, Loss: 1.0674833059310913, Accuracy: 0.6738026124818578\n",
      "Step: 1378, Loss: 1.1622822284698486, Accuracy: 0.6738578680203046\n",
      "Step: 1379, Loss: 1.2750334739685059, Accuracy: 0.673792270531401\n",
      "Step: 1380, Loss: 1.4297270774841309, Accuracy: 0.6736664252956794\n",
      "Step: 1381, Loss: 1.3398667573928833, Accuracy: 0.6735407621804148\n",
      "Step: 1382, Loss: 1.2640209197998047, Accuracy: 0.6735357917570499\n",
      "Step: 1383, Loss: 1.192358136177063, Accuracy: 0.6735910404624278\n",
      "Step: 1384, Loss: 1.2051445245742798, Accuracy: 0.6736462093862816\n",
      "Step: 1385, Loss: 1.257455825805664, Accuracy: 0.6736411736411736\n",
      "Step: 1386, Loss: 1.2136242389678955, Accuracy: 0.6736361451574141\n",
      "Step: 1387, Loss: 1.010170340538025, Accuracy: 0.6738112391930836\n",
      "Step: 1388, Loss: 1.1548713445663452, Accuracy: 0.673866090712743\n",
      "Step: 1389, Loss: 1.3336453437805176, Accuracy: 0.673800959232614\n",
      "Step: 1390, Loss: 1.24652898311615, Accuracy: 0.6737958303378864\n",
      "Step: 1391, Loss: 1.3256891965866089, Accuracy: 0.6737308429118773\n",
      "Step: 1392, Loss: 1.1690198183059692, Accuracy: 0.673785594639866\n",
      "Step: 1393, Loss: 1.2193931341171265, Accuracy: 0.6737804878048781\n",
      "Step: 1394, Loss: 1.4808086156845093, Accuracy: 0.6735364396654719\n",
      "Step: 1395, Loss: 1.4129714965820312, Accuracy: 0.673352435530086\n",
      "Step: 1396, Loss: 1.2525218725204468, Accuracy: 0.6732879980911477\n",
      "Step: 1397, Loss: 1.156911849975586, Accuracy: 0.6733428707677634\n",
      "Step: 1398, Loss: 1.1198757886886597, Accuracy: 0.6733976649988087\n",
      "Step: 1399, Loss: 1.3926633596420288, Accuracy: 0.6732738095238096\n",
      "Step: 1400, Loss: 1.1214579343795776, Accuracy: 0.6733880561503688\n",
      "Step: 1401, Loss: 1.2636862993240356, Accuracy: 0.6733832620066571\n",
      "Step: 1402, Loss: 1.2514100074768066, Accuracy: 0.6733784746970777\n",
      "Step: 1403, Loss: 1.1610541343688965, Accuracy: 0.6734330484330484\n",
      "Step: 1404, Loss: 1.2342157363891602, Accuracy: 0.6734282325029656\n",
      "Step: 1405, Loss: 1.1524885892868042, Accuracy: 0.6734826932195354\n",
      "Step: 1406, Loss: 1.2373090982437134, Accuracy: 0.6734778488509832\n",
      "Step: 1407, Loss: 1.0856865644454956, Accuracy: 0.6735913825757576\n",
      "Step: 1408, Loss: 1.4807041883468628, Accuracy: 0.6734090371421813\n",
      "Step: 1409, Loss: 1.07820725440979, Accuracy: 0.6735224586288416\n",
      "Step: 1410, Loss: 1.257341980934143, Accuracy: 0.6735175998110088\n",
      "Step: 1411, Loss: 1.2525666952133179, Accuracy: 0.6734537299339\n",
      "Step: 1412, Loss: 1.1007976531982422, Accuracy: 0.6735668789808917\n",
      "Step: 1413, Loss: 1.1228488683700562, Accuracy: 0.6736209335219236\n",
      "Step: 1414, Loss: 1.4871243238449097, Accuracy: 0.6734393404004712\n",
      "Step: 1415, Loss: 1.2101250886917114, Accuracy: 0.6734345574387948\n",
      "Step: 1416, Loss: 1.2251731157302856, Accuracy: 0.6734297812279464\n",
      "Step: 1417, Loss: 1.2021503448486328, Accuracy: 0.6734837799717912\n",
      "Step: 1418, Loss: 1.0259865522384644, Accuracy: 0.6736551562132957\n",
      "Step: 1419, Loss: 1.0371778011322021, Accuracy: 0.6738262910798122\n",
      "Step: 1420, Loss: 1.1094990968704224, Accuracy: 0.6739385409336148\n",
      "Step: 1421, Loss: 1.109336018562317, Accuracy: 0.6740506329113924\n",
      "Step: 1422, Loss: 1.2781723737716675, Accuracy: 0.6739868821738112\n",
      "Step: 1423, Loss: 1.3476041555404663, Accuracy: 0.6739232209737828\n",
      "Step: 1424, Loss: 1.322432518005371, Accuracy: 0.6739181286549708\n",
      "Step: 1425, Loss: 1.045003056526184, Accuracy: 0.6740299205236092\n",
      "Step: 1426, Loss: 1.0613422393798828, Accuracy: 0.6738495678579771\n",
      "Epoch: 4, Val_Accuracy: 0.12087227414330218\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bff706fe065a4c34a639c20dce83e2b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1427 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0, Loss: 1.0416934490203857, Accuracy: 0.8333333333333334\n",
      "Step: 1, Loss: 1.542259693145752, Accuracy: 0.5833333333333334\n",
      "Step: 2, Loss: 1.0213700532913208, Accuracy: 0.6944444444444444\n",
      "Step: 3, Loss: 1.1803377866744995, Accuracy: 0.7083333333333334\n",
      "Step: 4, Loss: 1.367872714996338, Accuracy: 0.6666666666666666\n",
      "Step: 5, Loss: 1.2079051733016968, Accuracy: 0.6805555555555556\n",
      "Step: 6, Loss: 1.1822892427444458, Accuracy: 0.6904761904761905\n",
      "Step: 7, Loss: 1.1402537822723389, Accuracy: 0.6979166666666666\n",
      "Step: 8, Loss: 1.0685665607452393, Accuracy: 0.7129629629629629\n",
      "Step: 9, Loss: 1.3326153755187988, Accuracy: 0.7\n",
      "Step: 10, Loss: 1.2680188417434692, Accuracy: 0.696969696969697\n",
      "Step: 11, Loss: 1.3089183568954468, Accuracy: 0.6875\n",
      "Step: 12, Loss: 1.0740231275558472, Accuracy: 0.6987179487179487\n",
      "Step: 13, Loss: 1.413521409034729, Accuracy: 0.6845238095238095\n",
      "Step: 14, Loss: 1.2514692544937134, Accuracy: 0.6833333333333333\n",
      "Step: 15, Loss: 1.0955621004104614, Accuracy: 0.6927083333333334\n",
      "Step: 16, Loss: 1.2115176916122437, Accuracy: 0.6911764705882353\n",
      "Step: 17, Loss: 1.17159903049469, Accuracy: 0.6944444444444444\n",
      "Step: 18, Loss: 1.2253299951553345, Accuracy: 0.6973684210526315\n",
      "Step: 19, Loss: 1.1077462434768677, Accuracy: 0.7083333333333334\n",
      "Step: 20, Loss: 1.41446053981781, Accuracy: 0.6944444444444444\n",
      "Step: 21, Loss: 1.2112250328063965, Accuracy: 0.6931818181818182\n",
      "Step: 22, Loss: 1.4097784757614136, Accuracy: 0.6811594202898551\n",
      "Step: 23, Loss: 1.4173874855041504, Accuracy: 0.6736111111111112\n",
      "Step: 24, Loss: 1.1747225522994995, Accuracy: 0.6766666666666666\n",
      "Step: 25, Loss: 1.1872705221176147, Accuracy: 0.6794871794871795\n",
      "Step: 26, Loss: 1.2936257123947144, Accuracy: 0.6790123456790124\n",
      "Step: 27, Loss: 1.3051313161849976, Accuracy: 0.6726190476190477\n",
      "Step: 28, Loss: 1.2513822317123413, Accuracy: 0.6724137931034483\n",
      "Step: 29, Loss: 1.2309657335281372, Accuracy: 0.6722222222222223\n",
      "Step: 30, Loss: 1.217448353767395, Accuracy: 0.6747311827956989\n",
      "Step: 31, Loss: 1.3606551885604858, Accuracy: 0.6692708333333334\n",
      "Step: 32, Loss: 1.1235238313674927, Accuracy: 0.6742424242424242\n",
      "Step: 33, Loss: 1.288112998008728, Accuracy: 0.6740196078431373\n",
      "Step: 34, Loss: 1.2177772521972656, Accuracy: 0.6738095238095239\n",
      "Step: 35, Loss: 1.1862765550613403, Accuracy: 0.6759259259259259\n",
      "Step: 36, Loss: 1.073921799659729, Accuracy: 0.6801801801801802\n",
      "Step: 37, Loss: 1.071025013923645, Accuracy: 0.6842105263157895\n",
      "Step: 38, Loss: 1.3823350667953491, Accuracy: 0.6794871794871795\n",
      "Step: 39, Loss: 1.2629166841506958, Accuracy: 0.6791666666666667\n",
      "Step: 40, Loss: 1.3341203927993774, Accuracy: 0.676829268292683\n",
      "Step: 41, Loss: 1.3091903924942017, Accuracy: 0.6746031746031746\n",
      "Step: 42, Loss: 1.2883065938949585, Accuracy: 0.6724806201550387\n",
      "Step: 43, Loss: 1.1320288181304932, Accuracy: 0.6742424242424242\n",
      "Step: 44, Loss: 1.2953563928604126, Accuracy: 0.6722222222222223\n",
      "Step: 45, Loss: 1.1816123723983765, Accuracy: 0.6739130434782609\n",
      "Step: 46, Loss: 1.4151934385299683, Accuracy: 0.6702127659574468\n",
      "Step: 47, Loss: 1.3650360107421875, Accuracy: 0.6684027777777778\n",
      "Step: 48, Loss: 1.1881651878356934, Accuracy: 0.6700680272108843\n",
      "Step: 49, Loss: 1.0999085903167725, Accuracy: 0.6733333333333333\n",
      "Step: 50, Loss: 1.3616276979446411, Accuracy: 0.6699346405228758\n",
      "Step: 51, Loss: 1.2712894678115845, Accuracy: 0.6682692307692307\n",
      "Step: 52, Loss: 1.4607691764831543, Accuracy: 0.6635220125786163\n",
      "Step: 53, Loss: 1.1247893571853638, Accuracy: 0.6651234567901234\n",
      "Step: 54, Loss: 1.3388543128967285, Accuracy: 0.6636363636363637\n",
      "Step: 55, Loss: 1.2485965490341187, Accuracy: 0.6636904761904762\n",
      "Step: 56, Loss: 0.998562753200531, Accuracy: 0.6681286549707602\n",
      "Step: 57, Loss: 1.2631138563156128, Accuracy: 0.6681034482758621\n",
      "Step: 58, Loss: 1.488349437713623, Accuracy: 0.6638418079096046\n",
      "Step: 59, Loss: 1.081634283065796, Accuracy: 0.6666666666666666\n",
      "Step: 60, Loss: 1.1298279762268066, Accuracy: 0.6680327868852459\n",
      "Step: 61, Loss: 1.2395697832107544, Accuracy: 0.6666666666666666\n",
      "Step: 62, Loss: 1.0518511533737183, Accuracy: 0.6693121693121693\n",
      "Step: 63, Loss: 1.1565197706222534, Accuracy: 0.6705729166666666\n",
      "Step: 64, Loss: 1.2269644737243652, Accuracy: 0.6705128205128205\n",
      "Step: 65, Loss: 1.126875400543213, Accuracy: 0.6717171717171717\n",
      "Step: 66, Loss: 1.2786825895309448, Accuracy: 0.6703980099502488\n",
      "Step: 67, Loss: 1.0872310400009155, Accuracy: 0.6727941176470589\n",
      "Step: 68, Loss: 1.2308359146118164, Accuracy: 0.6727053140096618\n",
      "Step: 69, Loss: 1.1926108598709106, Accuracy: 0.6738095238095239\n",
      "Step: 70, Loss: 1.1747251749038696, Accuracy: 0.6748826291079812\n",
      "Step: 71, Loss: 1.2497254610061646, Accuracy: 0.6736111111111112\n",
      "Step: 72, Loss: 1.0580741167068481, Accuracy: 0.6757990867579908\n",
      "Step: 73, Loss: 1.141255259513855, Accuracy: 0.6768018018018018\n",
      "Step: 74, Loss: 1.275457739830017, Accuracy: 0.6755555555555556\n",
      "Step: 75, Loss: 1.261013388633728, Accuracy: 0.6754385964912281\n",
      "Step: 76, Loss: 1.319359302520752, Accuracy: 0.6742424242424242\n",
      "Step: 77, Loss: 1.305023193359375, Accuracy: 0.6730769230769231\n",
      "Step: 78, Loss: 1.1754803657531738, Accuracy: 0.6740506329113924\n",
      "Step: 79, Loss: 1.3720539808273315, Accuracy: 0.671875\n",
      "Step: 80, Loss: 1.264711618423462, Accuracy: 0.6718106995884774\n",
      "Step: 81, Loss: 1.1127585172653198, Accuracy: 0.6737804878048781\n",
      "Step: 82, Loss: 1.1735694408416748, Accuracy: 0.6736947791164659\n",
      "Step: 83, Loss: 1.3668575286865234, Accuracy: 0.6716269841269841\n",
      "Step: 84, Loss: 1.3816124200820923, Accuracy: 0.6696078431372549\n",
      "Step: 85, Loss: 1.3087865114212036, Accuracy: 0.6686046511627907\n",
      "Step: 86, Loss: 1.2477171421051025, Accuracy: 0.6685823754789272\n",
      "Step: 87, Loss: 1.3162158727645874, Accuracy: 0.6676136363636364\n",
      "Step: 88, Loss: 1.366060733795166, Accuracy: 0.6657303370786517\n",
      "Step: 89, Loss: 1.079748272895813, Accuracy: 0.6675925925925926\n",
      "Step: 90, Loss: 1.1264539957046509, Accuracy: 0.6694139194139194\n",
      "Step: 91, Loss: 1.0115619897842407, Accuracy: 0.6721014492753623\n",
      "Step: 92, Loss: 1.2284187078475952, Accuracy: 0.6720430107526881\n",
      "Step: 93, Loss: 1.1691699028015137, Accuracy: 0.6728723404255319\n",
      "Step: 94, Loss: 1.3137949705123901, Accuracy: 0.6719298245614035\n",
      "Step: 95, Loss: 1.3085023164749146, Accuracy: 0.6710069444444444\n",
      "Step: 96, Loss: 1.2347992658615112, Accuracy: 0.6701030927835051\n",
      "Step: 97, Loss: 1.1406904458999634, Accuracy: 0.6709183673469388\n",
      "Step: 98, Loss: 1.2003892660140991, Accuracy: 0.6708754208754208\n",
      "Step: 99, Loss: 1.2474313974380493, Accuracy: 0.6708333333333333\n",
      "Step: 100, Loss: 1.1864334344863892, Accuracy: 0.6716171617161716\n",
      "Step: 101, Loss: 1.078134536743164, Accuracy: 0.673202614379085\n",
      "Step: 102, Loss: 1.0737377405166626, Accuracy: 0.6739482200647249\n",
      "Step: 103, Loss: 1.1293946504592896, Accuracy: 0.6754807692307693\n",
      "Step: 104, Loss: 1.002647876739502, Accuracy: 0.6777777777777778\n",
      "Step: 105, Loss: 1.2962092161178589, Accuracy: 0.6768867924528302\n",
      "Step: 106, Loss: 1.165906310081482, Accuracy: 0.677570093457944\n",
      "Step: 107, Loss: 1.1725751161575317, Accuracy: 0.6782407407407407\n",
      "Step: 108, Loss: 1.5685406923294067, Accuracy: 0.6750764525993884\n",
      "Step: 109, Loss: 1.3340740203857422, Accuracy: 0.6742424242424242\n",
      "Step: 110, Loss: 1.0503243207931519, Accuracy: 0.6756756756756757\n",
      "Step: 111, Loss: 1.3241820335388184, Accuracy: 0.6748511904761905\n",
      "Step: 112, Loss: 1.0063245296478271, Accuracy: 0.6769911504424779\n",
      "Step: 113, Loss: 1.2872368097305298, Accuracy: 0.6761695906432749\n",
      "Step: 114, Loss: 1.207972526550293, Accuracy: 0.6768115942028986\n",
      "Step: 115, Loss: 1.2873843908309937, Accuracy: 0.6760057471264368\n",
      "Step: 116, Loss: 1.0703235864639282, Accuracy: 0.6773504273504274\n",
      "Step: 117, Loss: 1.4420822858810425, Accuracy: 0.6751412429378532\n",
      "Step: 118, Loss: 1.430885672569275, Accuracy: 0.6729691876750701\n",
      "Step: 119, Loss: 1.1764854192733765, Accuracy: 0.6736111111111112\n",
      "Step: 120, Loss: 1.1439515352249146, Accuracy: 0.6742424242424242\n",
      "Step: 121, Loss: 1.2255173921585083, Accuracy: 0.6741803278688525\n",
      "Step: 122, Loss: 1.20150625705719, Accuracy: 0.674119241192412\n",
      "Step: 123, Loss: 0.9505593180656433, Accuracy: 0.676747311827957\n",
      "Step: 124, Loss: 1.2908357381820679, Accuracy: 0.676\n",
      "Step: 125, Loss: 1.1581783294677734, Accuracy: 0.6765873015873016\n",
      "Step: 126, Loss: 1.2795077562332153, Accuracy: 0.6758530183727034\n",
      "Step: 127, Loss: 1.1296778917312622, Accuracy: 0.6764322916666666\n",
      "Step: 128, Loss: 1.3023182153701782, Accuracy: 0.6757105943152455\n",
      "Step: 129, Loss: 1.5279632806777954, Accuracy: 0.6730769230769231\n",
      "Step: 130, Loss: 1.0770665407180786, Accuracy: 0.6743002544529262\n",
      "Step: 131, Loss: 1.4594818353652954, Accuracy: 0.6723484848484849\n",
      "Step: 132, Loss: 1.3928674459457397, Accuracy: 0.6710526315789473\n",
      "Step: 133, Loss: 1.3543978929519653, Accuracy: 0.6703980099502488\n",
      "Step: 134, Loss: 1.021423578262329, Accuracy: 0.6722222222222223\n",
      "Step: 135, Loss: 1.2415707111358643, Accuracy: 0.6721813725490197\n",
      "Step: 136, Loss: 1.155993938446045, Accuracy: 0.6727493917274939\n",
      "Step: 137, Loss: 1.157463788986206, Accuracy: 0.6733091787439613\n",
      "Step: 138, Loss: 1.1201270818710327, Accuracy: 0.6744604316546763\n",
      "Step: 139, Loss: 1.2613471746444702, Accuracy: 0.674404761904762\n",
      "Step: 140, Loss: 1.124625563621521, Accuracy: 0.6749408983451537\n",
      "Step: 141, Loss: 1.2292367219924927, Accuracy: 0.6748826291079812\n",
      "Step: 142, Loss: 1.2236334085464478, Accuracy: 0.6748251748251748\n",
      "Step: 143, Loss: 1.2197178602218628, Accuracy: 0.6747685185185185\n",
      "Step: 144, Loss: 1.1891969442367554, Accuracy: 0.6752873563218391\n",
      "Step: 145, Loss: 1.2750743627548218, Accuracy: 0.6746575342465754\n",
      "Step: 146, Loss: 1.2316302061080933, Accuracy: 0.6746031746031746\n",
      "Step: 147, Loss: 1.3114603757858276, Accuracy: 0.6739864864864865\n",
      "Step: 148, Loss: 1.0358905792236328, Accuracy: 0.6756152125279642\n",
      "Step: 149, Loss: 1.1540563106536865, Accuracy: 0.6761111111111111\n",
      "Step: 150, Loss: 1.3232835531234741, Accuracy: 0.6754966887417219\n",
      "Step: 151, Loss: 1.2450083494186401, Accuracy: 0.6754385964912281\n",
      "Step: 152, Loss: 1.1849387884140015, Accuracy: 0.6759259259259259\n",
      "Step: 153, Loss: 1.2147080898284912, Accuracy: 0.6758658008658008\n",
      "Step: 154, Loss: 1.0579129457473755, Accuracy: 0.6768817204301075\n",
      "Step: 155, Loss: 1.0466264486312866, Accuracy: 0.6784188034188035\n",
      "Step: 156, Loss: 1.452497124671936, Accuracy: 0.6767515923566879\n",
      "Step: 157, Loss: 1.134191870689392, Accuracy: 0.6772151898734177\n",
      "Step: 158, Loss: 1.2124391794204712, Accuracy: 0.6771488469601677\n",
      "Step: 159, Loss: 1.248968243598938, Accuracy: 0.6770833333333334\n",
      "Step: 160, Loss: 1.483973503112793, Accuracy: 0.6749482401656315\n",
      "Step: 161, Loss: 1.0259034633636475, Accuracy: 0.676440329218107\n",
      "Step: 162, Loss: 1.2358230352401733, Accuracy: 0.6763803680981595\n",
      "Step: 163, Loss: 1.1359783411026, Accuracy: 0.676829268292683\n",
      "Step: 164, Loss: 1.185234546661377, Accuracy: 0.6772727272727272\n",
      "Step: 165, Loss: 1.0787732601165771, Accuracy: 0.6782128514056225\n",
      "Step: 166, Loss: 1.1468428373336792, Accuracy: 0.6786427145708582\n",
      "Step: 167, Loss: 1.311396837234497, Accuracy: 0.6780753968253969\n",
      "Step: 168, Loss: 1.1520311832427979, Accuracy: 0.6785009861932939\n",
      "Step: 169, Loss: 1.331137776374817, Accuracy: 0.6779411764705883\n",
      "Step: 170, Loss: 1.286016821861267, Accuracy: 0.6773879142300195\n",
      "Step: 171, Loss: 1.149055004119873, Accuracy: 0.6778100775193798\n",
      "Step: 172, Loss: 1.227070689201355, Accuracy: 0.6777456647398844\n",
      "Step: 173, Loss: 1.3174232244491577, Accuracy: 0.6772030651340997\n",
      "Step: 174, Loss: 1.2945048809051514, Accuracy: 0.6771428571428572\n",
      "Step: 175, Loss: 1.310773253440857, Accuracy: 0.6766098484848485\n",
      "Step: 176, Loss: 1.250671625137329, Accuracy: 0.6765536723163842\n",
      "Step: 177, Loss: 1.245108962059021, Accuracy: 0.6764981273408239\n",
      "Step: 178, Loss: 1.511832356452942, Accuracy: 0.6750465549348231\n",
      "Step: 179, Loss: 1.213802695274353, Accuracy: 0.675\n",
      "Step: 180, Loss: 1.242516040802002, Accuracy: 0.6749539594843462\n",
      "Step: 181, Loss: 1.2252620458602905, Accuracy: 0.674908424908425\n",
      "Step: 182, Loss: 1.3340754508972168, Accuracy: 0.674408014571949\n",
      "Step: 183, Loss: 1.0714064836502075, Accuracy: 0.6752717391304348\n",
      "Step: 184, Loss: 1.078359842300415, Accuracy: 0.6761261261261261\n",
      "Step: 185, Loss: 1.3985697031021118, Accuracy: 0.6751792114695341\n",
      "Step: 186, Loss: 1.1571468114852905, Accuracy: 0.6755793226381461\n",
      "Step: 187, Loss: 1.1807135343551636, Accuracy: 0.6759751773049646\n",
      "Step: 188, Loss: 1.1389421224594116, Accuracy: 0.6763668430335097\n",
      "Step: 189, Loss: 1.0949831008911133, Accuracy: 0.6771929824561403\n",
      "Step: 190, Loss: 1.0664806365966797, Accuracy: 0.6780104712041884\n",
      "Step: 191, Loss: 1.4457091093063354, Accuracy: 0.6766493055555556\n",
      "Step: 192, Loss: 1.0947209596633911, Accuracy: 0.677461139896373\n",
      "Step: 193, Loss: 1.1116782426834106, Accuracy: 0.6778350515463918\n",
      "Step: 194, Loss: 1.1011320352554321, Accuracy: 0.6786324786324787\n",
      "Step: 195, Loss: 1.1774414777755737, Accuracy: 0.6789965986394558\n",
      "Step: 196, Loss: 1.2040296792984009, Accuracy: 0.6793570219966159\n",
      "Step: 197, Loss: 1.1941677331924438, Accuracy: 0.6792929292929293\n",
      "Step: 198, Loss: 1.320946216583252, Accuracy: 0.6788107202680067\n",
      "Step: 199, Loss: 1.0981990098953247, Accuracy: 0.6795833333333333\n",
      "Step: 200, Loss: 0.9143505096435547, Accuracy: 0.6811774461028193\n",
      "Step: 201, Loss: 1.1480780839920044, Accuracy: 0.6815181518151815\n",
      "Step: 202, Loss: 1.2071342468261719, Accuracy: 0.6814449917898193\n",
      "Step: 203, Loss: 1.361130714416504, Accuracy: 0.6809640522875817\n",
      "Step: 204, Loss: 1.363804817199707, Accuracy: 0.6800813008130081\n",
      "Step: 205, Loss: 1.1170481443405151, Accuracy: 0.6808252427184466\n",
      "Step: 206, Loss: 1.0911872386932373, Accuracy: 0.6815619967793881\n",
      "Step: 207, Loss: 1.208439826965332, Accuracy: 0.6814903846153846\n",
      "Step: 208, Loss: 1.3547271490097046, Accuracy: 0.680622009569378\n",
      "Step: 209, Loss: 1.3111668825149536, Accuracy: 0.6801587301587302\n",
      "Step: 210, Loss: 1.2986764907836914, Accuracy: 0.6796998420221169\n",
      "Step: 211, Loss: 1.3736075162887573, Accuracy: 0.6788522012578616\n",
      "Step: 212, Loss: 1.4588346481323242, Accuracy: 0.6776212832550861\n",
      "Step: 213, Loss: 1.3036561012268066, Accuracy: 0.6771806853582555\n",
      "Step: 214, Loss: 1.1562024354934692, Accuracy: 0.6775193798449612\n",
      "Step: 215, Loss: 1.0524901151657104, Accuracy: 0.6782407407407407\n",
      "Step: 216, Loss: 1.0645207166671753, Accuracy: 0.6789554531490015\n",
      "Step: 217, Loss: 1.2055861949920654, Accuracy: 0.6792813455657493\n",
      "Step: 218, Loss: 1.331514596939087, Accuracy: 0.6788432267884322\n",
      "Step: 219, Loss: 1.1812134981155396, Accuracy: 0.6787878787878788\n",
      "Step: 220, Loss: 1.3260632753372192, Accuracy: 0.6783559577677225\n",
      "Step: 221, Loss: 1.303104281425476, Accuracy: 0.6779279279279279\n",
      "Step: 222, Loss: 1.2507089376449585, Accuracy: 0.6778774289985052\n",
      "Step: 223, Loss: 1.4066753387451172, Accuracy: 0.6770833333333334\n",
      "Step: 224, Loss: 1.3124984502792358, Accuracy: 0.6766666666666666\n",
      "Step: 225, Loss: 1.2235231399536133, Accuracy: 0.676622418879056\n",
      "Step: 226, Loss: 1.2192353010177612, Accuracy: 0.6769456681350955\n",
      "Step: 227, Loss: 1.108157753944397, Accuracy: 0.6776315789473685\n",
      "Step: 228, Loss: 1.147729516029358, Accuracy: 0.6779475982532751\n",
      "Step: 229, Loss: 1.3339556455612183, Accuracy: 0.677536231884058\n",
      "Step: 230, Loss: 1.4592458009719849, Accuracy: 0.6764069264069265\n",
      "Step: 231, Loss: 1.4779200553894043, Accuracy: 0.6752873563218391\n",
      "Step: 232, Loss: 1.310628056526184, Accuracy: 0.674892703862661\n",
      "Step: 233, Loss: 1.255323886871338, Accuracy: 0.6748575498575499\n",
      "Step: 234, Loss: 1.2387088537216187, Accuracy: 0.674822695035461\n",
      "Step: 235, Loss: 1.5385265350341797, Accuracy: 0.6733757062146892\n",
      "Step: 236, Loss: 1.3732548952102661, Accuracy: 0.6726441631504922\n",
      "Step: 237, Loss: 1.177677035331726, Accuracy: 0.6729691876750701\n",
      "Step: 238, Loss: 1.1696559190750122, Accuracy: 0.6732914923291492\n",
      "Step: 239, Loss: 1.1550136804580688, Accuracy: 0.6736111111111112\n",
      "Step: 240, Loss: 1.1303526163101196, Accuracy: 0.6739280774550485\n",
      "Step: 241, Loss: 1.111057162284851, Accuracy: 0.6745867768595041\n",
      "Step: 242, Loss: 1.1496362686157227, Accuracy: 0.6748971193415638\n",
      "Step: 243, Loss: 1.2191119194030762, Accuracy: 0.6752049180327869\n",
      "Step: 244, Loss: 1.2267135381698608, Accuracy: 0.6751700680272109\n",
      "Step: 245, Loss: 1.0219801664352417, Accuracy: 0.6761517615176151\n",
      "Step: 246, Loss: 1.1926953792572021, Accuracy: 0.6761133603238867\n",
      "Step: 247, Loss: 0.9859259724617004, Accuracy: 0.6770833333333334\n",
      "Step: 248, Loss: 1.3875246047973633, Accuracy: 0.6763721552878179\n",
      "Step: 249, Loss: 1.2235634326934814, Accuracy: 0.6763333333333333\n",
      "Step: 250, Loss: 1.2967876195907593, Accuracy: 0.6759628154050464\n",
      "Step: 251, Loss: 1.1477031707763672, Accuracy: 0.6762566137566137\n",
      "Step: 252, Loss: 1.2964378595352173, Accuracy: 0.6758893280632411\n",
      "Step: 253, Loss: 1.3300222158432007, Accuracy: 0.675524934383202\n",
      "Step: 254, Loss: 1.274247646331787, Accuracy: 0.6751633986928105\n",
      "Step: 255, Loss: 1.2299007177352905, Accuracy: 0.6751302083333334\n",
      "Step: 256, Loss: 1.0006331205368042, Accuracy: 0.6760700389105059\n",
      "Step: 257, Loss: 1.2944812774658203, Accuracy: 0.6757105943152455\n",
      "Step: 258, Loss: 1.3373247385025024, Accuracy: 0.6753539253539254\n",
      "Step: 259, Loss: 1.0767749547958374, Accuracy: 0.6759615384615385\n",
      "Step: 260, Loss: 1.2538772821426392, Accuracy: 0.6759259259259259\n",
      "Step: 261, Loss: 1.1465965509414673, Accuracy: 0.6765267175572519\n",
      "Step: 262, Loss: 1.383339762687683, Accuracy: 0.6758555133079848\n",
      "Step: 263, Loss: 1.1237064599990845, Accuracy: 0.6764520202020202\n",
      "Step: 264, Loss: 1.224541425704956, Accuracy: 0.6764150943396227\n",
      "Step: 265, Loss: 1.3104201555252075, Accuracy: 0.6760651629072681\n",
      "Step: 266, Loss: 1.0350346565246582, Accuracy: 0.6772784019975031\n",
      "Step: 267, Loss: 1.1381675004959106, Accuracy: 0.6775497512437811\n",
      "Step: 268, Loss: 0.9996166229248047, Accuracy: 0.6784386617100372\n",
      "Step: 269, Loss: 1.1135166883468628, Accuracy: 0.6787037037037037\n",
      "Step: 270, Loss: 1.1325116157531738, Accuracy: 0.6792742927429274\n",
      "Step: 271, Loss: 1.2179361581802368, Accuracy: 0.6792279411764706\n",
      "Step: 272, Loss: 1.2114101648330688, Accuracy: 0.6791819291819292\n",
      "Step: 273, Loss: 1.280351996421814, Accuracy: 0.6791362530413625\n",
      "Step: 274, Loss: 1.3530672788619995, Accuracy: 0.6784848484848485\n",
      "Step: 275, Loss: 1.2771717309951782, Accuracy: 0.6784420289855072\n",
      "Step: 276, Loss: 1.0703274011611938, Accuracy: 0.6790012033694344\n",
      "Step: 277, Loss: 1.138346552848816, Accuracy: 0.6792565947242206\n",
      "Step: 278, Loss: 1.3208192586898804, Accuracy: 0.6789127837514934\n",
      "Step: 279, Loss: 1.2946704626083374, Accuracy: 0.6788690476190476\n",
      "Step: 280, Loss: 1.3205478191375732, Accuracy: 0.6785290628706999\n",
      "Step: 281, Loss: 1.388339638710022, Accuracy: 0.6778959810874704\n",
      "Step: 282, Loss: 1.0979746580123901, Accuracy: 0.6784452296819788\n",
      "Step: 283, Loss: 1.1134920120239258, Accuracy: 0.6786971830985915\n",
      "Step: 284, Loss: 1.083577275276184, Accuracy: 0.6792397660818713\n",
      "Step: 285, Loss: 1.1954783201217651, Accuracy: 0.6791958041958042\n",
      "Step: 286, Loss: 1.2208797931671143, Accuracy: 0.6791521486643438\n",
      "Step: 287, Loss: 1.2689462900161743, Accuracy: 0.6791087962962963\n",
      "Step: 288, Loss: 1.2912299633026123, Accuracy: 0.6790657439446367\n",
      "Step: 289, Loss: 1.3387633562088013, Accuracy: 0.678448275862069\n",
      "Step: 290, Loss: 1.2119027376174927, Accuracy: 0.6784077892325315\n",
      "Step: 291, Loss: 1.4765901565551758, Accuracy: 0.6772260273972602\n",
      "Step: 292, Loss: 1.2613459825515747, Accuracy: 0.6771899886234357\n",
      "Step: 293, Loss: 1.2173672914505005, Accuracy: 0.6771541950113379\n",
      "Step: 294, Loss: 1.0754660367965698, Accuracy: 0.677683615819209\n",
      "Step: 295, Loss: 1.3171268701553345, Accuracy: 0.6773648648648649\n",
      "Step: 296, Loss: 1.3556007146835327, Accuracy: 0.6770482603815937\n",
      "Step: 297, Loss: 1.3657231330871582, Accuracy: 0.6764541387024608\n",
      "Step: 298, Loss: 1.1485291719436646, Accuracy: 0.6767001114827201\n",
      "Step: 299, Loss: 0.9850459098815918, Accuracy: 0.6775\n",
      "Step: 300, Loss: 1.098214864730835, Accuracy: 0.6780177187153932\n",
      "Step: 301, Loss: 1.0887731313705444, Accuracy: 0.6785320088300221\n",
      "Step: 302, Loss: 1.2125210762023926, Accuracy: 0.6784928492849285\n",
      "Step: 303, Loss: 1.2329506874084473, Accuracy: 0.678453947368421\n",
      "Step: 304, Loss: 1.170464277267456, Accuracy: 0.6786885245901639\n",
      "Step: 305, Loss: 1.1510626077651978, Accuracy: 0.678921568627451\n",
      "Step: 306, Loss: 1.3390955924987793, Accuracy: 0.6783387622149837\n",
      "Step: 307, Loss: 1.0750013589859009, Accuracy: 0.6788419913419913\n",
      "Step: 308, Loss: 1.202247977256775, Accuracy: 0.6788025889967637\n",
      "Step: 309, Loss: 1.3449548482894897, Accuracy: 0.6782258064516129\n",
      "Step: 310, Loss: 1.099584698677063, Accuracy: 0.6784565916398714\n",
      "Step: 311, Loss: 1.0713508129119873, Accuracy: 0.6789529914529915\n",
      "Step: 312, Loss: 1.0877925157546997, Accuracy: 0.6794462193823216\n",
      "Step: 313, Loss: 1.2960163354873657, Accuracy: 0.679140127388535\n",
      "Step: 314, Loss: 1.3081620931625366, Accuracy: 0.6788359788359788\n",
      "Step: 315, Loss: 1.2329208850860596, Accuracy: 0.6787974683544303\n",
      "Step: 316, Loss: 1.256193995475769, Accuracy: 0.6784963196635121\n",
      "Step: 317, Loss: 1.1137160062789917, Accuracy: 0.6789832285115304\n",
      "Step: 318, Loss: 1.15057373046875, Accuracy: 0.6792058516196448\n",
      "Step: 319, Loss: 1.2405506372451782, Accuracy: 0.6791666666666667\n",
      "Step: 320, Loss: 1.2574713230133057, Accuracy: 0.6791277258566978\n",
      "Step: 321, Loss: 1.2852929830551147, Accuracy: 0.6788302277432712\n",
      "Step: 322, Loss: 1.2122496366500854, Accuracy: 0.6790505675954592\n",
      "Step: 323, Loss: 1.1632790565490723, Accuracy: 0.6792695473251029\n",
      "Step: 324, Loss: 1.3101526498794556, Accuracy: 0.678974358974359\n",
      "Step: 325, Loss: 1.3242387771606445, Accuracy: 0.678680981595092\n",
      "Step: 326, Loss: 1.1030471324920654, Accuracy: 0.6791539245667686\n",
      "Step: 327, Loss: 1.1543760299682617, Accuracy: 0.679369918699187\n",
      "Step: 328, Loss: 1.086691975593567, Accuracy: 0.6798378926038501\n",
      "Step: 329, Loss: 1.1494591236114502, Accuracy: 0.680050505050505\n",
      "Step: 330, Loss: 1.2799873352050781, Accuracy: 0.6797583081570997\n",
      "Step: 331, Loss: 1.245310664176941, Accuracy: 0.679718875502008\n",
      "Step: 332, Loss: 1.3787082433700562, Accuracy: 0.6791791791791791\n",
      "Step: 333, Loss: 1.3542438745498657, Accuracy: 0.6788922155688623\n",
      "Step: 334, Loss: 1.4045332670211792, Accuracy: 0.6783582089552239\n",
      "Step: 335, Loss: 1.4603382349014282, Accuracy: 0.6778273809523809\n",
      "Step: 336, Loss: 1.3729063272476196, Accuracy: 0.6772997032640949\n",
      "Step: 337, Loss: 1.3518129587173462, Accuracy: 0.6767751479289941\n",
      "Step: 338, Loss: 1.2778581380844116, Accuracy: 0.6764995083579154\n",
      "Step: 339, Loss: 1.2816983461380005, Accuracy: 0.6764705882352942\n",
      "Step: 340, Loss: 1.221798300743103, Accuracy: 0.6764418377321603\n",
      "Step: 341, Loss: 1.4810925722122192, Accuracy: 0.675682261208577\n",
      "Step: 342, Loss: 1.1616871356964111, Accuracy: 0.6758989310009719\n",
      "Step: 343, Loss: 1.3595857620239258, Accuracy: 0.6756298449612403\n",
      "Step: 344, Loss: 1.167235255241394, Accuracy: 0.6758454106280193\n",
      "Step: 345, Loss: 1.2661004066467285, Accuracy: 0.6758188824662813\n",
      "Step: 346, Loss: 1.2024129629135132, Accuracy: 0.6760326609029779\n",
      "Step: 347, Loss: 1.1257364749908447, Accuracy: 0.6762452107279694\n",
      "Step: 348, Loss: 1.4282938241958618, Accuracy: 0.6757402101241643\n",
      "Step: 349, Loss: 1.2960302829742432, Accuracy: 0.6757142857142857\n",
      "Step: 350, Loss: 1.1276912689208984, Accuracy: 0.6759259259259259\n",
      "Step: 351, Loss: 1.1392923593521118, Accuracy: 0.6761363636363636\n",
      "Step: 352, Loss: 1.2648850679397583, Accuracy: 0.676109537299339\n",
      "Step: 353, Loss: 1.0032199621200562, Accuracy: 0.676789077212806\n",
      "Step: 354, Loss: 0.9970209002494812, Accuracy: 0.6774647887323944\n",
      "Step: 355, Loss: 1.3422876596450806, Accuracy: 0.6772003745318352\n",
      "Step: 356, Loss: 1.222248911857605, Accuracy: 0.677170868347339\n",
      "Step: 357, Loss: 1.1832159757614136, Accuracy: 0.6773743016759777\n",
      "Step: 358, Loss: 1.540346622467041, Accuracy: 0.6764159702878366\n",
      "Step: 359, Loss: 1.1551231145858765, Accuracy: 0.6766203703703704\n",
      "Step: 360, Loss: 1.0846977233886719, Accuracy: 0.6770544783010157\n",
      "Step: 361, Loss: 1.313571572303772, Accuracy: 0.6767955801104972\n",
      "Step: 362, Loss: 1.3638747930526733, Accuracy: 0.6763085399449036\n",
      "Step: 363, Loss: 1.07439124584198, Accuracy: 0.6767399267399268\n",
      "Step: 364, Loss: 1.3134626150131226, Accuracy: 0.6764840182648402\n",
      "Step: 365, Loss: 1.3311266899108887, Accuracy: 0.6762295081967213\n",
      "Step: 366, Loss: 1.1541202068328857, Accuracy: 0.6764305177111717\n",
      "Step: 367, Loss: 1.3357125520706177, Accuracy: 0.676177536231884\n",
      "Step: 368, Loss: 1.1341856718063354, Accuracy: 0.6763775971093045\n",
      "Step: 369, Loss: 1.1385300159454346, Accuracy: 0.6765765765765765\n",
      "Step: 370, Loss: 1.1560825109481812, Accuracy: 0.676774483378257\n",
      "Step: 371, Loss: 1.1610625982284546, Accuracy: 0.6769713261648745\n",
      "Step: 372, Loss: 1.2115384340286255, Accuracy: 0.6769436997319035\n",
      "Step: 373, Loss: 1.1331161260604858, Accuracy: 0.6771390374331551\n",
      "Step: 374, Loss: 1.359507441520691, Accuracy: 0.6766666666666666\n",
      "Step: 375, Loss: 1.1569819450378418, Accuracy: 0.6770833333333334\n",
      "Step: 376, Loss: 1.0599576234817505, Accuracy: 0.6774977895667551\n",
      "Step: 377, Loss: 1.404320240020752, Accuracy: 0.6770282186948854\n",
      "Step: 378, Loss: 1.273779034614563, Accuracy: 0.6770008795074758\n",
      "Step: 379, Loss: 1.2631343603134155, Accuracy: 0.6767543859649123\n",
      "Step: 380, Loss: 1.184960961341858, Accuracy: 0.6769466316710411\n",
      "Step: 381, Loss: 1.059727668762207, Accuracy: 0.6773560209424084\n",
      "Step: 382, Loss: 1.5096782445907593, Accuracy: 0.6766753698868582\n",
      "Step: 383, Loss: 1.323281168937683, Accuracy: 0.6764322916666666\n",
      "Step: 384, Loss: 1.2356362342834473, Accuracy: 0.6764069264069265\n",
      "Step: 385, Loss: 1.323581337928772, Accuracy: 0.6761658031088082\n",
      "Step: 386, Loss: 1.1748024225234985, Accuracy: 0.6763565891472868\n",
      "Step: 387, Loss: 1.3420580625534058, Accuracy: 0.6759020618556701\n",
      "Step: 388, Loss: 1.4106392860412598, Accuracy: 0.6754498714652957\n",
      "Step: 389, Loss: 1.3703089952468872, Accuracy: 0.6752136752136753\n",
      "Step: 390, Loss: 1.393985390663147, Accuracy: 0.6747655583972719\n",
      "Step: 391, Loss: 1.1775492429733276, Accuracy: 0.6749574829931972\n",
      "Step: 392, Loss: 1.422498106956482, Accuracy: 0.6745122985581001\n",
      "Step: 393, Loss: 1.2301453351974487, Accuracy: 0.674492385786802\n",
      "Step: 394, Loss: 1.4387820959091187, Accuracy: 0.6738396624472573\n",
      "Step: 395, Loss: 0.9720069766044617, Accuracy: 0.6746632996632996\n",
      "Step: 396, Loss: 1.0756527185440063, Accuracy: 0.6750629722921915\n",
      "Step: 397, Loss: 1.2634533643722534, Accuracy: 0.6748324958123953\n",
      "Step: 398, Loss: 1.2282921075820923, Accuracy: 0.674812030075188\n",
      "Step: 399, Loss: 1.0199241638183594, Accuracy: 0.6754166666666667\n",
      "Step: 400, Loss: 1.2920355796813965, Accuracy: 0.6751870324189526\n",
      "Step: 401, Loss: 1.1556569337844849, Accuracy: 0.6755804311774462\n",
      "Step: 402, Loss: 1.0580166578292847, Accuracy: 0.6759718775847808\n",
      "Step: 403, Loss: 1.2867863178253174, Accuracy: 0.6759488448844885\n",
      "Step: 404, Loss: 1.121361494064331, Accuracy: 0.6763374485596708\n",
      "Step: 405, Loss: 1.4631470441818237, Accuracy: 0.6756978653530378\n",
      "Step: 406, Loss: 1.149793267250061, Accuracy: 0.6758804258804259\n",
      "Step: 407, Loss: 1.2362686395645142, Accuracy: 0.6758578431372549\n",
      "Step: 408, Loss: 1.186981439590454, Accuracy: 0.676039119804401\n",
      "Step: 409, Loss: 1.2365082502365112, Accuracy: 0.6760162601626016\n",
      "Step: 410, Loss: 1.238507866859436, Accuracy: 0.6759935117599352\n",
      "Step: 411, Loss: 1.018227219581604, Accuracy: 0.6765776699029126\n",
      "Step: 412, Loss: 1.2717825174331665, Accuracy: 0.6763518966908797\n",
      "Step: 413, Loss: 1.3088544607162476, Accuracy: 0.6761272141706924\n",
      "Step: 414, Loss: 1.1724234819412231, Accuracy: 0.6763052208835342\n",
      "Step: 415, Loss: 1.1652419567108154, Accuracy: 0.6764823717948718\n",
      "Step: 416, Loss: 1.1922680139541626, Accuracy: 0.676458832933653\n",
      "Step: 417, Loss: 1.026486873626709, Accuracy: 0.6770334928229665\n",
      "Step: 418, Loss: 1.0903257131576538, Accuracy: 0.677406523468576\n",
      "Step: 419, Loss: 1.3138333559036255, Accuracy: 0.6771825396825397\n",
      "Step: 420, Loss: 1.3600640296936035, Accuracy: 0.6769596199524941\n",
      "Step: 421, Loss: 1.2401407957077026, Accuracy: 0.6769352290679305\n",
      "Step: 422, Loss: 1.1329916715621948, Accuracy: 0.6771079590228526\n",
      "Step: 423, Loss: 1.3123301267623901, Accuracy: 0.6766902515723271\n",
      "Step: 424, Loss: 1.150386929512024, Accuracy: 0.6768627450980392\n",
      "Step: 425, Loss: 1.1200133562088013, Accuracy: 0.6772300469483568\n",
      "Step: 426, Loss: 1.1177020072937012, Accuracy: 0.677400468384075\n",
      "Step: 427, Loss: 1.3522419929504395, Accuracy: 0.6769859813084113\n",
      "Step: 428, Loss: 1.293337106704712, Accuracy: 0.6767676767676768\n",
      "Step: 429, Loss: 1.0331459045410156, Accuracy: 0.6771317829457364\n",
      "Step: 430, Loss: 1.2060325145721436, Accuracy: 0.6773008507347255\n",
      "Step: 431, Loss: 1.2495026588439941, Accuracy: 0.6772762345679012\n",
      "Step: 432, Loss: 1.1773548126220703, Accuracy: 0.6774441878367975\n",
      "Step: 433, Loss: 1.3625340461730957, Accuracy: 0.6770353302611367\n",
      "Step: 434, Loss: 1.2859805822372437, Accuracy: 0.6768199233716475\n",
      "Step: 435, Loss: 1.15510892868042, Accuracy: 0.6769877675840978\n",
      "Step: 436, Loss: 1.5603584051132202, Accuracy: 0.6762013729977117\n",
      "Step: 437, Loss: 1.234148621559143, Accuracy: 0.676179604261796\n",
      "Step: 438, Loss: 1.3101840019226074, Accuracy: 0.6757782839787395\n",
      "Step: 439, Loss: 1.297878623008728, Accuracy: 0.6755681818181818\n",
      "Step: 440, Loss: 1.185414433479309, Accuracy: 0.6757369614512472\n",
      "Step: 441, Loss: 1.279529094696045, Accuracy: 0.67552790346908\n",
      "Step: 442, Loss: 1.229950189590454, Accuracy: 0.6755079006772009\n",
      "Step: 443, Loss: 1.2441519498825073, Accuracy: 0.675487987987988\n",
      "Step: 444, Loss: 1.3946565389633179, Accuracy: 0.6750936329588015\n",
      "Step: 445, Loss: 1.3171842098236084, Accuracy: 0.6748878923766816\n",
      "Step: 446, Loss: 1.266816258430481, Accuracy: 0.6746830723340791\n",
      "Step: 447, Loss: 1.143375039100647, Accuracy: 0.6748511904761905\n",
      "Step: 448, Loss: 1.320971131324768, Accuracy: 0.6746473645137342\n",
      "Step: 449, Loss: 1.3695249557495117, Accuracy: 0.6742592592592592\n",
      "Step: 450, Loss: 1.1649342775344849, Accuracy: 0.6744271988174427\n",
      "Step: 451, Loss: 1.13593327999115, Accuracy: 0.674594395280236\n",
      "Step: 452, Loss: 1.1437811851501465, Accuracy: 0.6747608535688006\n",
      "Step: 453, Loss: 1.0827804803848267, Accuracy: 0.6751101321585903\n",
      "Step: 454, Loss: 1.342279076576233, Accuracy: 0.674908424908425\n",
      "Step: 455, Loss: 1.4189611673355103, Accuracy: 0.6745248538011696\n",
      "Step: 456, Loss: 1.1077200174331665, Accuracy: 0.674872355944566\n",
      "Step: 457, Loss: 1.33718740940094, Accuracy: 0.6746724890829694\n",
      "Step: 458, Loss: 1.1322386264801025, Accuracy: 0.6748366013071896\n",
      "Step: 459, Loss: 1.4807300567626953, Accuracy: 0.6742753623188406\n",
      "Step: 460, Loss: 1.440676212310791, Accuracy: 0.6737165582067968\n",
      "Step: 461, Loss: 1.170650601387024, Accuracy: 0.6738816738816739\n",
      "Step: 462, Loss: 1.2472834587097168, Accuracy: 0.673866090712743\n",
      "Step: 463, Loss: 1.4547780752182007, Accuracy: 0.6733117816091954\n",
      "Step: 464, Loss: 1.1275192499160767, Accuracy: 0.6734767025089605\n",
      "Step: 465, Loss: 1.2161344289779663, Accuracy: 0.6734620886981402\n",
      "Step: 466, Loss: 1.2778652906417847, Accuracy: 0.6732690935046396\n",
      "Step: 467, Loss: 0.9935477375984192, Accuracy: 0.6737891737891738\n",
      "Step: 468, Loss: 1.2772243022918701, Accuracy: 0.6735963041933192\n",
      "Step: 469, Loss: 1.280432105064392, Accuracy: 0.6734042553191489\n",
      "Step: 470, Loss: 1.1633726358413696, Accuracy: 0.6735668789808917\n",
      "Step: 471, Loss: 1.366582989692688, Accuracy: 0.6731991525423728\n",
      "Step: 472, Loss: 1.3058592081069946, Accuracy: 0.673185341789993\n",
      "Step: 473, Loss: 0.9695158004760742, Accuracy: 0.6738748241912799\n",
      "Step: 474, Loss: 1.026515007019043, Accuracy: 0.6743859649122808\n",
      "Step: 475, Loss: 1.1480234861373901, Accuracy: 0.6745448179271709\n",
      "Step: 476, Loss: 1.2079229354858398, Accuracy: 0.6745283018867925\n",
      "Step: 477, Loss: 1.1088742017745972, Accuracy: 0.674860529986053\n",
      "Step: 478, Loss: 1.141649603843689, Accuracy: 0.675017397355602\n",
      "Step: 479, Loss: 1.1703165769577026, Accuracy: 0.6751736111111111\n",
      "Step: 480, Loss: 1.2098721265792847, Accuracy: 0.6751559251559252\n",
      "Step: 481, Loss: 1.229231595993042, Accuracy: 0.6751383125864454\n",
      "Step: 482, Loss: 1.2657939195632935, Accuracy: 0.6751207729468599\n",
      "Step: 483, Loss: 1.1191481351852417, Accuracy: 0.6754476584022039\n",
      "Step: 484, Loss: 1.1624834537506104, Accuracy: 0.6756013745704468\n",
      "Step: 485, Loss: 1.0655829906463623, Accuracy: 0.6759259259259259\n",
      "Step: 486, Loss: 1.2795054912567139, Accuracy: 0.6757357973990418\n",
      "Step: 487, Loss: 1.0199192762374878, Accuracy: 0.6762295081967213\n",
      "Step: 488, Loss: 1.1528418064117432, Accuracy: 0.6765507839127471\n",
      "Step: 489, Loss: 1.4821995496749878, Accuracy: 0.6760204081632653\n",
      "Step: 490, Loss: 1.2541178464889526, Accuracy: 0.6760013577732519\n",
      "Step: 491, Loss: 1.383506178855896, Accuracy: 0.6756436314363143\n",
      "Step: 492, Loss: 1.162024736404419, Accuracy: 0.6757944557133199\n",
      "Step: 493, Loss: 1.1734610795974731, Accuracy: 0.675944669365722\n",
      "Step: 494, Loss: 1.1518678665161133, Accuracy: 0.6760942760942761\n",
      "Step: 495, Loss: 1.2654659748077393, Accuracy: 0.6760752688172043\n",
      "Step: 496, Loss: 1.2585738897323608, Accuracy: 0.676056338028169\n",
      "Step: 497, Loss: 1.1002687215805054, Accuracy: 0.6763721552878179\n",
      "Step: 498, Loss: 1.1739007234573364, Accuracy: 0.6765197060788243\n",
      "Step: 499, Loss: 1.2856649160385132, Accuracy: 0.6763333333333333\n",
      "Step: 500, Loss: 1.348357081413269, Accuracy: 0.675981370592149\n",
      "Step: 501, Loss: 1.0062551498413086, Accuracy: 0.6764608233731739\n",
      "Step: 502, Loss: 1.1054915189743042, Accuracy: 0.6766070245195493\n",
      "Step: 503, Loss: 1.2913345098495483, Accuracy: 0.6764219576719577\n",
      "Step: 504, Loss: 1.3365846872329712, Accuracy: 0.6762376237623763\n",
      "Step: 505, Loss: 1.333276629447937, Accuracy: 0.6760540184453228\n",
      "Step: 506, Loss: 1.156689167022705, Accuracy: 0.6761998685075609\n",
      "Step: 507, Loss: 1.1197315454483032, Accuracy: 0.6763451443569554\n",
      "Step: 508, Loss: 1.153854489326477, Accuracy: 0.676489849377865\n",
      "Step: 509, Loss: 1.2954022884368896, Accuracy: 0.6763071895424837\n",
      "Step: 510, Loss: 1.3561391830444336, Accuracy: 0.6757990867579908\n",
      "Step: 511, Loss: 1.1508923768997192, Accuracy: 0.6759440104166666\n",
      "Step: 512, Loss: 1.224008560180664, Accuracy: 0.6759259259259259\n",
      "Step: 513, Loss: 1.1437803506851196, Accuracy: 0.6760700389105059\n",
      "Step: 514, Loss: 1.0901830196380615, Accuracy: 0.6762135922330097\n",
      "Step: 515, Loss: 1.1002506017684937, Accuracy: 0.6765180878552972\n",
      "Step: 516, Loss: 1.3452163934707642, Accuracy: 0.6763378465506125\n",
      "Step: 517, Loss: 1.137020468711853, Accuracy: 0.6764800514800515\n",
      "Step: 518, Loss: 1.2339524030685425, Accuracy: 0.676461143224149\n",
      "Step: 519, Loss: 1.1605390310287476, Accuracy: 0.6766025641025641\n",
      "Step: 520, Loss: 1.3581050634384155, Accuracy: 0.6764235444657709\n",
      "Step: 521, Loss: 1.1495240926742554, Accuracy: 0.6765644955300127\n",
      "Step: 522, Loss: 1.342604637145996, Accuracy: 0.6763862332695985\n",
      "Step: 523, Loss: 1.1230411529541016, Accuracy: 0.6766857506361323\n",
      "Step: 524, Loss: 1.2454129457473755, Accuracy: 0.6766666666666666\n",
      "Step: 525, Loss: 1.1752619743347168, Accuracy: 0.6768060836501901\n",
      "Step: 526, Loss: 1.1674227714538574, Accuracy: 0.676786843769766\n",
      "Step: 527, Loss: 1.2432252168655396, Accuracy: 0.6767676767676768\n",
      "Step: 528, Loss: 1.0966390371322632, Accuracy: 0.6770636420919974\n",
      "Step: 529, Loss: 1.1343317031860352, Accuracy: 0.6772012578616352\n",
      "Step: 530, Loss: 1.066759705543518, Accuracy: 0.6776522284996861\n",
      "Step: 531, Loss: 1.3033801317214966, Accuracy: 0.6774749373433584\n",
      "Step: 532, Loss: 1.2402973175048828, Accuracy: 0.6774546591619762\n",
      "Step: 533, Loss: 1.2173594236373901, Accuracy: 0.677434456928839\n",
      "Step: 534, Loss: 1.5078076124191284, Accuracy: 0.6769470404984423\n",
      "Step: 535, Loss: 1.1900360584259033, Accuracy: 0.6770833333333334\n",
      "Step: 536, Loss: 1.0701197385787964, Accuracy: 0.6773743016759777\n",
      "Step: 537, Loss: 1.2259622812271118, Accuracy: 0.6773543990086741\n",
      "Step: 538, Loss: 1.1494685411453247, Accuracy: 0.6774891774891775\n",
      "Step: 539, Loss: 1.312092661857605, Accuracy: 0.6773148148148148\n",
      "Step: 540, Loss: 1.107833743095398, Accuracy: 0.6776032039433149\n",
      "Step: 541, Loss: 1.2382829189300537, Accuracy: 0.6775830258302583\n",
      "Step: 542, Loss: 1.2047468423843384, Accuracy: 0.6775629220380601\n",
      "Step: 543, Loss: 1.218622088432312, Accuracy: 0.6775428921568627\n",
      "Step: 544, Loss: 1.0984100103378296, Accuracy: 0.67782874617737\n",
      "Step: 545, Loss: 1.2342520952224731, Accuracy: 0.6778083028083028\n",
      "Step: 546, Loss: 1.1284711360931396, Accuracy: 0.6780926264472883\n",
      "Step: 547, Loss: 1.0707271099090576, Accuracy: 0.6783759124087592\n",
      "Step: 548, Loss: 1.1713114976882935, Accuracy: 0.6785063752276868\n",
      "Step: 549, Loss: 1.2018908262252808, Accuracy: 0.6786363636363636\n",
      "Step: 550, Loss: 1.2920533418655396, Accuracy: 0.6784633998790078\n",
      "Step: 551, Loss: 1.2490805387496948, Accuracy: 0.6782910628019324\n",
      "Step: 552, Loss: 1.322449803352356, Accuracy: 0.6781193490054249\n",
      "Step: 553, Loss: 1.693089485168457, Accuracy: 0.6771961492178099\n",
      "Step: 554, Loss: 1.1715795993804932, Accuracy: 0.6773273273273274\n",
      "Step: 555, Loss: 1.2832034826278687, Accuracy: 0.677158273381295\n",
      "Step: 556, Loss: 1.026843547821045, Accuracy: 0.6775882704967086\n",
      "Step: 557, Loss: 1.3837660551071167, Accuracy: 0.6772700119474313\n",
      "Step: 558, Loss: 1.1545159816741943, Accuracy: 0.6774001192605844\n",
      "Step: 559, Loss: 1.2707573175430298, Accuracy: 0.6772321428571428\n",
      "Step: 560, Loss: 1.198760986328125, Accuracy: 0.6772133095662507\n",
      "Step: 561, Loss: 1.2028894424438477, Accuracy: 0.6773428232502966\n",
      "Step: 562, Loss: 1.2684876918792725, Accuracy: 0.6771758436944938\n",
      "Step: 563, Loss: 1.25002920627594, Accuracy: 0.6771572104018913\n",
      "Step: 564, Loss: 1.348341464996338, Accuracy: 0.6769911504424779\n",
      "Step: 565, Loss: 1.373956561088562, Accuracy: 0.676678445229682\n",
      "Step: 566, Loss: 1.1266608238220215, Accuracy: 0.6768077601410935\n",
      "Step: 567, Loss: 1.2306767702102661, Accuracy: 0.6767899061032864\n",
      "Step: 568, Loss: 1.5471845865249634, Accuracy: 0.6761862917398945\n",
      "Step: 569, Loss: 1.2465767860412598, Accuracy: 0.6761695906432749\n",
      "Step: 570, Loss: 1.1327916383743286, Accuracy: 0.6764448336252189\n",
      "Step: 571, Loss: 1.1104098558425903, Accuracy: 0.6765734265734266\n",
      "Step: 572, Loss: 1.2363697290420532, Accuracy: 0.6765561372891216\n",
      "Step: 573, Loss: 1.1749593019485474, Accuracy: 0.6766840882694541\n",
      "Step: 574, Loss: 1.1685246229171753, Accuracy: 0.6768115942028986\n",
      "Step: 575, Loss: 1.1308695077896118, Accuracy: 0.6769386574074074\n",
      "Step: 576, Loss: 1.2804588079452515, Accuracy: 0.6769208549971115\n",
      "Step: 577, Loss: 1.0575169324874878, Accuracy: 0.6771914648212226\n",
      "Step: 578, Loss: 1.2758656740188599, Accuracy: 0.6771732872769142\n",
      "Step: 579, Loss: 1.4050034284591675, Accuracy: 0.676867816091954\n",
      "Step: 580, Loss: 1.2062995433807373, Accuracy: 0.6768502581755593\n",
      "Step: 581, Loss: 1.2155057191848755, Accuracy: 0.6769759450171822\n",
      "Step: 582, Loss: 1.1868116855621338, Accuracy: 0.6771012006861064\n",
      "Step: 583, Loss: 1.1334398984909058, Accuracy: 0.6772260273972602\n",
      "Step: 584, Loss: 1.2822073698043823, Accuracy: 0.677065527065527\n",
      "Step: 585, Loss: 1.4985108375549316, Accuracy: 0.6766211604095563\n",
      "Step: 586, Loss: 1.1431465148925781, Accuracy: 0.6767461669505963\n",
      "Step: 587, Loss: 1.165527105331421, Accuracy: 0.6768707482993197\n",
      "Step: 588, Loss: 1.1088203191757202, Accuracy: 0.6769949066213922\n",
      "Step: 589, Loss: 1.0420163869857788, Accuracy: 0.6772598870056498\n",
      "Step: 590, Loss: 1.2189122438430786, Accuracy: 0.6772419627749577\n",
      "Step: 591, Loss: 1.2507823705673218, Accuracy: 0.6772240990990991\n",
      "Step: 592, Loss: 1.0414081811904907, Accuracy: 0.677627880831928\n",
      "Step: 593, Loss: 1.241054892539978, Accuracy: 0.6776094276094277\n",
      "Step: 594, Loss: 1.2310385704040527, Accuracy: 0.6775910364145659\n",
      "Step: 595, Loss: 1.2073299884796143, Accuracy: 0.6775727069351231\n",
      "Step: 596, Loss: 1.1568220853805542, Accuracy: 0.6776940256839754\n",
      "Step: 597, Loss: 0.9995579123497009, Accuracy: 0.6780936454849499\n",
      "Step: 598, Loss: 1.131951093673706, Accuracy: 0.6782136894824707\n",
      "Step: 599, Loss: 0.9752178192138672, Accuracy: 0.67875\n",
      "Step: 600, Loss: 1.2695317268371582, Accuracy: 0.6787298946200776\n",
      "Step: 601, Loss: 1.173962116241455, Accuracy: 0.6788482834994463\n",
      "Step: 602, Loss: 1.1595875024795532, Accuracy: 0.6789662797125484\n",
      "Step: 603, Loss: 1.0728484392166138, Accuracy: 0.6792218543046358\n",
      "Step: 604, Loss: 1.130543828010559, Accuracy: 0.6793388429752066\n",
      "Step: 605, Loss: 1.2825610637664795, Accuracy: 0.6791804180418042\n",
      "Step: 606, Loss: 1.30043363571167, Accuracy: 0.6790225151015925\n",
      "Step: 607, Loss: 1.1332560777664185, Accuracy: 0.6791392543859649\n",
      "Step: 608, Loss: 1.1343191862106323, Accuracy: 0.6792556102900931\n",
      "Step: 609, Loss: 1.4952611923217773, Accuracy: 0.6788251366120218\n",
      "Step: 610, Loss: 1.0986461639404297, Accuracy: 0.6790780141843972\n",
      "Step: 611, Loss: 1.0231300592422485, Accuracy: 0.6794662309368191\n",
      "Step: 612, Loss: 1.2181669473648071, Accuracy: 0.6794453507340946\n",
      "Step: 613, Loss: 1.2317086458206177, Accuracy: 0.6794245385450597\n",
      "Step: 614, Loss: 1.484367847442627, Accuracy: 0.6789972899728998\n",
      "Step: 615, Loss: 1.3089114427566528, Accuracy: 0.6788419913419913\n",
      "Step: 616, Loss: 1.1953182220458984, Accuracy: 0.678957320367369\n",
      "Step: 617, Loss: 1.1541069746017456, Accuracy: 0.6790722761596548\n",
      "Step: 618, Loss: 1.2326732873916626, Accuracy: 0.6791868605277329\n",
      "Step: 619, Loss: 1.2624043226242065, Accuracy: 0.6791666666666667\n",
      "Step: 620, Loss: 1.035446047782898, Accuracy: 0.679549114331723\n",
      "Step: 621, Loss: 1.4236029386520386, Accuracy: 0.6792604501607717\n",
      "Step: 622, Loss: 1.1285723447799683, Accuracy: 0.6793739967897271\n",
      "Step: 623, Loss: 1.3900974988937378, Accuracy: 0.6790865384615384\n",
      "Step: 624, Loss: 1.3065532445907593, Accuracy: 0.6789333333333334\n",
      "Step: 625, Loss: 1.1221035718917847, Accuracy: 0.6791799787007454\n",
      "Step: 626, Loss: 1.2762486934661865, Accuracy: 0.6790271132376395\n",
      "Step: 627, Loss: 1.3413716554641724, Accuracy: 0.6788747346072187\n",
      "Step: 628, Loss: 1.0685720443725586, Accuracy: 0.6791202967673556\n",
      "Step: 629, Loss: 1.2602359056472778, Accuracy: 0.678968253968254\n",
      "Step: 630, Loss: 1.307365894317627, Accuracy: 0.6788166930797676\n",
      "Step: 631, Loss: 1.0567904710769653, Accuracy: 0.6790611814345991\n",
      "Step: 632, Loss: 1.1861430406570435, Accuracy: 0.6790416008425487\n",
      "Step: 633, Loss: 1.2916802167892456, Accuracy: 0.6788906414300736\n",
      "Step: 634, Loss: 1.0744776725769043, Accuracy: 0.6791338582677166\n",
      "Step: 635, Loss: 1.2860407829284668, Accuracy: 0.6789832285115304\n",
      "Step: 636, Loss: 1.3383780717849731, Accuracy: 0.6787022501308215\n",
      "Step: 637, Loss: 1.2330342531204224, Accuracy: 0.6786833855799373\n",
      "Step: 638, Loss: 1.0686814785003662, Accuracy: 0.6789254042775169\n",
      "Step: 639, Loss: 1.1975370645523071, Accuracy: 0.67890625\n",
      "Step: 640, Loss: 1.2668746709823608, Accuracy: 0.6788871554862195\n",
      "Step: 641, Loss: 1.3711949586868286, Accuracy: 0.6786085150571132\n",
      "Step: 642, Loss: 1.3239256143569946, Accuracy: 0.6784603421461898\n",
      "Step: 643, Loss: 1.2508060932159424, Accuracy: 0.6783126293995859\n",
      "Step: 644, Loss: 1.0461548566818237, Accuracy: 0.6785529715762274\n",
      "Step: 645, Loss: 1.0814341306686401, Accuracy: 0.6787925696594427\n",
      "Step: 646, Loss: 1.4605540037155151, Accuracy: 0.678516228748068\n",
      "Step: 647, Loss: 1.268370270729065, Accuracy: 0.6784979423868313\n",
      "Step: 648, Loss: 1.042776107788086, Accuracy: 0.6787365177195686\n",
      "Step: 649, Loss: 1.2277249097824097, Accuracy: 0.6787179487179488\n",
      "Step: 650, Loss: 1.007081389427185, Accuracy: 0.6790834613415259\n",
      "Step: 651, Loss: 1.145662784576416, Accuracy: 0.6791922290388548\n",
      "Step: 652, Loss: 1.1914056539535522, Accuracy: 0.6791730474732006\n",
      "Step: 653, Loss: 1.2322441339492798, Accuracy: 0.6791539245667686\n",
      "Step: 654, Loss: 1.2965961694717407, Accuracy: 0.6790076335877863\n",
      "Step: 655, Loss: 1.4824990034103394, Accuracy: 0.6786077235772358\n",
      "Step: 656, Loss: 1.3727068901062012, Accuracy: 0.678335870116692\n",
      "Step: 657, Loss: 1.3433326482772827, Accuracy: 0.6781914893617021\n",
      "Step: 658, Loss: 1.331087589263916, Accuracy: 0.6780475467880627\n",
      "Step: 659, Loss: 1.2536909580230713, Accuracy: 0.678030303030303\n",
      "Step: 660, Loss: 1.1006360054016113, Accuracy: 0.6782652546646495\n",
      "Step: 661, Loss: 1.20779550075531, Accuracy: 0.6782477341389728\n",
      "Step: 662, Loss: 1.108566403388977, Accuracy: 0.6783559577677225\n",
      "Step: 663, Loss: 1.215725064277649, Accuracy: 0.6782128514056225\n",
      "Step: 664, Loss: 1.352333426475525, Accuracy: 0.6780701754385965\n",
      "Step: 665, Loss: 1.2496064901351929, Accuracy: 0.6780530530530531\n",
      "Step: 666, Loss: 1.1018553972244263, Accuracy: 0.6782858570714643\n",
      "Step: 667, Loss: 1.0809946060180664, Accuracy: 0.6785179640718563\n",
      "Step: 668, Loss: 1.091163158416748, Accuracy: 0.6787493771798705\n",
      "Step: 669, Loss: 1.0665431022644043, Accuracy: 0.6789800995024876\n",
      "Step: 670, Loss: 1.3986462354660034, Accuracy: 0.6787133631395926\n",
      "Step: 671, Loss: 1.203161597251892, Accuracy: 0.6786954365079365\n",
      "Step: 672, Loss: 1.337263584136963, Accuracy: 0.6785537394749876\n",
      "Step: 673, Loss: 1.1502478122711182, Accuracy: 0.6786597428288823\n",
      "Step: 674, Loss: 1.2490136623382568, Accuracy: 0.678641975308642\n",
      "Step: 675, Loss: 1.2719131708145142, Accuracy: 0.6786242603550295\n",
      "Step: 676, Loss: 1.0395363569259644, Accuracy: 0.6788527818808469\n",
      "Step: 677, Loss: 1.384781837463379, Accuracy: 0.6785889872173058\n",
      "Step: 678, Loss: 1.1000689268112183, Accuracy: 0.6788168875797742\n",
      "Step: 679, Loss: 1.3019918203353882, Accuracy: 0.6787990196078432\n",
      "Step: 680, Loss: 1.4683305025100708, Accuracy: 0.6784140969162996\n",
      "Step: 681, Loss: 1.2444216012954712, Accuracy: 0.678396871945259\n",
      "Step: 682, Loss: 1.2017974853515625, Accuracy: 0.6785017081503172\n",
      "Step: 683, Loss: 1.245094895362854, Accuracy: 0.6784844054580896\n",
      "Step: 684, Loss: 1.1971818208694458, Accuracy: 0.6784671532846716\n",
      "Step: 685, Loss: 1.2372618913650513, Accuracy: 0.6784499514091351\n",
      "Step: 686, Loss: 1.3866785764694214, Accuracy: 0.6783114992721979\n",
      "Step: 687, Loss: 1.2604299783706665, Accuracy: 0.6782945736434108\n",
      "Step: 688, Loss: 1.29879629611969, Accuracy: 0.6781567489114659\n",
      "Step: 689, Loss: 1.1932405233383179, Accuracy: 0.6782608695652174\n",
      "Step: 690, Loss: 1.2700474262237549, Accuracy: 0.6782440906898215\n",
      "Step: 691, Loss: 1.0540492534637451, Accuracy: 0.6784682080924855\n",
      "Step: 692, Loss: 1.0784953832626343, Accuracy: 0.6786916786916787\n",
      "Step: 693, Loss: 1.1536930799484253, Accuracy: 0.6787944284341979\n",
      "Step: 694, Loss: 1.312171459197998, Accuracy: 0.6786570743405276\n",
      "Step: 695, Loss: 1.1172770261764526, Accuracy: 0.6788793103448276\n",
      "Step: 696, Loss: 1.3969945907592773, Accuracy: 0.6786226685796269\n",
      "Step: 697, Loss: 1.3529945611953735, Accuracy: 0.6783667621776505\n",
      "Step: 698, Loss: 1.2812803983688354, Accuracy: 0.6782308059132094\n",
      "Step: 699, Loss: 1.1377525329589844, Accuracy: 0.6784523809523809\n",
      "Step: 700, Loss: 1.2783783674240112, Accuracy: 0.6784355682358535\n",
      "Step: 701, Loss: 1.1918418407440186, Accuracy: 0.6784188034188035\n",
      "Step: 702, Loss: 1.2629573345184326, Accuracy: 0.6784020862968232\n",
      "Step: 703, Loss: 1.2611734867095947, Accuracy: 0.6783854166666666\n",
      "Step: 704, Loss: 1.300047755241394, Accuracy: 0.6782505910165485\n",
      "Step: 705, Loss: 1.4170364141464233, Accuracy: 0.6779981114258735\n",
      "Step: 706, Loss: 1.11052405834198, Accuracy: 0.6782178217821783\n",
      "Step: 707, Loss: 1.080727458000183, Accuracy: 0.678436911487759\n",
      "Step: 708, Loss: 1.183140754699707, Accuracy: 0.6785378467324871\n",
      "Step: 709, Loss: 1.3166530132293701, Accuracy: 0.6784037558685446\n",
      "Step: 710, Loss: 1.1654690504074097, Accuracy: 0.6785044538209095\n",
      "Step: 711, Loss: 1.065553069114685, Accuracy: 0.6787219101123596\n",
      "Step: 712, Loss: 0.9736763834953308, Accuracy: 0.6790556334735858\n",
      "Step: 713, Loss: 1.2337146997451782, Accuracy: 0.6790382819794585\n",
      "Step: 714, Loss: 1.1589224338531494, Accuracy: 0.6791375291375291\n",
      "Step: 715, Loss: 1.335532307624817, Accuracy: 0.6790037243947858\n",
      "Step: 716, Loss: 1.143820881843567, Accuracy: 0.6791027429102743\n",
      "Step: 717, Loss: 1.1714375019073486, Accuracy: 0.6792014856081708\n",
      "Step: 718, Loss: 1.3850547075271606, Accuracy: 0.6789522484932777\n",
      "Step: 719, Loss: 1.3081862926483154, Accuracy: 0.6788194444444444\n",
      "Step: 720, Loss: 1.2757951021194458, Accuracy: 0.6786870087840962\n",
      "Step: 721, Loss: 1.1994363069534302, Accuracy: 0.6787857802400739\n",
      "Step: 722, Loss: 1.1482847929000854, Accuracy: 0.6788842784693407\n",
      "Step: 723, Loss: 1.252013087272644, Accuracy: 0.6788674033149171\n",
      "Step: 724, Loss: 1.1823548078536987, Accuracy: 0.6789655172413793\n",
      "Step: 725, Loss: 1.2003527879714966, Accuracy: 0.6790633608815427\n",
      "Step: 726, Loss: 1.022720456123352, Accuracy: 0.6793901879871619\n",
      "Step: 727, Loss: 0.9517938494682312, Accuracy: 0.6798305860805861\n",
      "Step: 728, Loss: 1.4838711023330688, Accuracy: 0.6793552812071331\n",
      "Step: 729, Loss: 1.5819865465164185, Accuracy: 0.6788812785388127\n",
      "Step: 730, Loss: 1.153920292854309, Accuracy: 0.6789785681714546\n",
      "Step: 731, Loss: 1.2664374113082886, Accuracy: 0.6788479052823315\n",
      "Step: 732, Loss: 1.3044084310531616, Accuracy: 0.6787175989085948\n",
      "Step: 733, Loss: 1.3351227045059204, Accuracy: 0.6784741144414169\n",
      "Step: 734, Loss: 1.0267549753189087, Accuracy: 0.6787981859410431\n",
      "Step: 735, Loss: 1.1537977457046509, Accuracy: 0.6788949275362319\n",
      "Step: 736, Loss: 1.2341662645339966, Accuracy: 0.6788783355947535\n",
      "Step: 737, Loss: 1.006060242652893, Accuracy: 0.67920054200542\n",
      "Step: 738, Loss: 1.341025710105896, Accuracy: 0.6790708164185837\n",
      "Step: 739, Loss: 1.161634922027588, Accuracy: 0.6791666666666667\n",
      "Step: 740, Loss: 1.146714448928833, Accuracy: 0.6792622582096266\n",
      "Step: 741, Loss: 1.079243779182434, Accuracy: 0.6794699011680144\n",
      "Step: 742, Loss: 1.1438920497894287, Accuracy: 0.6796769851951547\n",
      "Step: 743, Loss: 1.578275203704834, Accuracy: 0.6792114695340502\n",
      "Step: 744, Loss: 1.319078803062439, Accuracy: 0.679082774049217\n",
      "Step: 745, Loss: 1.3973785638809204, Accuracy: 0.6788427167113494\n",
      "Step: 746, Loss: 1.0208635330200195, Accuracy: 0.679161088799643\n",
      "Step: 747, Loss: 1.1651870012283325, Accuracy: 0.6792557932263814\n",
      "Step: 748, Loss: 1.1329829692840576, Accuracy: 0.6794615042278593\n",
      "Step: 749, Loss: 1.0766611099243164, Accuracy: 0.6796666666666666\n",
      "Step: 750, Loss: 1.1595028638839722, Accuracy: 0.6797603195739015\n",
      "Step: 751, Loss: 1.1594278812408447, Accuracy: 0.6799645390070922\n",
      "Step: 752, Loss: 1.013736367225647, Accuracy: 0.6802788844621513\n",
      "Step: 753, Loss: 1.1998652219772339, Accuracy: 0.6803713527851459\n",
      "Step: 754, Loss: 1.2638455629348755, Accuracy: 0.6803532008830022\n",
      "Step: 755, Loss: 1.2304010391235352, Accuracy: 0.6803350970017636\n",
      "Step: 756, Loss: 1.3653172254562378, Accuracy: 0.6800968736239542\n",
      "Step: 757, Loss: 0.9982275366783142, Accuracy: 0.6804089709762533\n",
      "Step: 758, Loss: 1.2293895483016968, Accuracy: 0.6803908651734739\n",
      "Step: 759, Loss: 1.1986545324325562, Accuracy: 0.6804824561403509\n",
      "Step: 760, Loss: 1.237414002418518, Accuracy: 0.6804643013578625\n",
      "Step: 761, Loss: 1.19564950466156, Accuracy: 0.6804461942257218\n",
      "Step: 762, Loss: 1.417322039604187, Accuracy: 0.6802096985583224\n",
      "Step: 763, Loss: 1.3136965036392212, Accuracy: 0.6799738219895288\n",
      "Step: 764, Loss: 1.1166774034500122, Accuracy: 0.6800653594771242\n",
      "Step: 765, Loss: 1.303179144859314, Accuracy: 0.6799390774586597\n",
      "Step: 766, Loss: 1.2901268005371094, Accuracy: 0.679813124728379\n",
      "Step: 767, Loss: 1.0846599340438843, Accuracy: 0.6800130208333334\n",
      "Step: 768, Loss: 1.2913581132888794, Accuracy: 0.6798872995231903\n",
      "Step: 769, Loss: 1.344761848449707, Accuracy: 0.6797619047619048\n",
      "Step: 770, Loss: 1.2335164546966553, Accuracy: 0.6797449200172936\n",
      "Step: 771, Loss: 1.2494611740112305, Accuracy: 0.6797279792746114\n",
      "Step: 772, Loss: 1.3698729276657104, Accuracy: 0.6794954721862871\n",
      "Step: 773, Loss: 1.3913370370864868, Accuracy: 0.6792635658914729\n",
      "Step: 774, Loss: 1.0434383153915405, Accuracy: 0.6794623655913978\n",
      "Step: 775, Loss: 1.129535436630249, Accuracy: 0.6796606529209622\n",
      "Step: 776, Loss: 1.0307000875473022, Accuracy: 0.6799656799656799\n",
      "Step: 777, Loss: 1.1842197179794312, Accuracy: 0.6800556983718937\n",
      "Step: 778, Loss: 1.162400245666504, Accuracy: 0.680145485665383\n",
      "Step: 779, Loss: 1.2466801404953003, Accuracy: 0.6801282051282052\n",
      "Step: 780, Loss: 1.4210621118545532, Accuracy: 0.6797908664105847\n",
      "Step: 781, Loss: 1.1661019325256348, Accuracy: 0.6797740835464621\n",
      "Step: 782, Loss: 1.1514285802841187, Accuracy: 0.679757343550447\n",
      "Step: 783, Loss: 1.155799388885498, Accuracy: 0.6798469387755102\n",
      "Step: 784, Loss: 1.0830309391021729, Accuracy: 0.6800424628450106\n",
      "Step: 785, Loss: 1.170548439025879, Accuracy: 0.6801314673452078\n",
      "Step: 786, Loss: 1.2713731527328491, Accuracy: 0.68000847098687\n",
      "Step: 787, Loss: 1.1386319398880005, Accuracy: 0.6800972927241963\n",
      "Step: 788, Loss: 1.1397358179092407, Accuracy: 0.6801858893113646\n",
      "Step: 789, Loss: 1.0573756694793701, Accuracy: 0.680379746835443\n",
      "Step: 790, Loss: 1.2378129959106445, Accuracy: 0.6803624104509061\n",
      "Step: 791, Loss: 1.4328840970993042, Accuracy: 0.6801346801346801\n",
      "Step: 792, Loss: 1.196395754814148, Accuracy: 0.680222782681799\n",
      "Step: 793, Loss: 1.3023426532745361, Accuracy: 0.6801007556675063\n",
      "Step: 794, Loss: 1.1958460807800293, Accuracy: 0.680083857442348\n",
      "Step: 795, Loss: 1.1859400272369385, Accuracy: 0.6801716917922948\n",
      "Step: 796, Loss: 1.2781990766525269, Accuracy: 0.6801547469677959\n",
      "Step: 797, Loss: 1.143691897392273, Accuracy: 0.6802422723475355\n",
      "Step: 798, Loss: 1.147926688194275, Accuracy: 0.6803295786399666\n",
      "Step: 799, Loss: 1.312425136566162, Accuracy: 0.6802083333333333\n",
      "Step: 800, Loss: 1.2300188541412354, Accuracy: 0.6802954640033292\n",
      "Step: 801, Loss: 1.5228878259658813, Accuracy: 0.6798628428927681\n",
      "Step: 802, Loss: 1.0875391960144043, Accuracy: 0.6800539643005397\n",
      "Step: 803, Loss: 1.5584344863891602, Accuracy: 0.67962271973466\n",
      "Step: 804, Loss: 1.3588091135025024, Accuracy: 0.6793995859213251\n",
      "Step: 805, Loss: 1.2579578161239624, Accuracy: 0.679383788254756\n",
      "Step: 806, Loss: 1.5223203897476196, Accuracy: 0.6789549772821148\n",
      "Step: 807, Loss: 1.237687110900879, Accuracy: 0.679042904290429\n",
      "Step: 808, Loss: 1.3051871061325073, Accuracy: 0.6789245982694685\n",
      "Step: 809, Loss: 1.2350658178329468, Accuracy: 0.6789094650205761\n",
      "Step: 810, Loss: 1.319849967956543, Accuracy: 0.6787916152897657\n",
      "Step: 811, Loss: 1.2448657751083374, Accuracy: 0.6787766830870279\n",
      "Step: 812, Loss: 1.1834416389465332, Accuracy: 0.6787617876178762\n",
      "Step: 813, Loss: 1.2609000205993652, Accuracy: 0.6787469287469288\n",
      "Step: 814, Loss: 1.1001092195510864, Accuracy: 0.6789366053169734\n",
      "Step: 815, Loss: 1.2320541143417358, Accuracy: 0.678921568627451\n",
      "Step: 816, Loss: 1.089711308479309, Accuracy: 0.679110567115463\n",
      "Step: 817, Loss: 1.13925302028656, Accuracy: 0.6791972290138549\n",
      "Step: 818, Loss: 1.2330893278121948, Accuracy: 0.6791819291819292\n",
      "Step: 819, Loss: 1.2412630319595337, Accuracy: 0.6791666666666667\n",
      "Step: 820, Loss: 1.09403657913208, Accuracy: 0.6793544457978076\n",
      "Step: 821, Loss: 1.2398492097854614, Accuracy: 0.6793390105433901\n",
      "Step: 822, Loss: 1.4300737380981445, Accuracy: 0.679019846091535\n",
      "Step: 823, Loss: 1.0927973985671997, Accuracy: 0.6792071197411004\n",
      "Step: 824, Loss: 1.1630213260650635, Accuracy: 0.6792929292929293\n",
      "Step: 825, Loss: 1.4574705362319946, Accuracy: 0.6789749798224375\n",
      "Step: 826, Loss: 1.2171082496643066, Accuracy: 0.6790608625554212\n",
      "Step: 827, Loss: 1.1898871660232544, Accuracy: 0.6790458937198067\n",
      "Step: 828, Loss: 1.2050269842147827, Accuracy: 0.6791314837153196\n",
      "Step: 829, Loss: 1.1853886842727661, Accuracy: 0.6792168674698795\n",
      "Step: 830, Loss: 1.2338160276412964, Accuracy: 0.6792017649418371\n",
      "Step: 831, Loss: 1.083411693572998, Accuracy: 0.6793870192307693\n",
      "Step: 832, Loss: 1.2106088399887085, Accuracy: 0.6793717486994798\n",
      "Step: 833, Loss: 1.0510395765304565, Accuracy: 0.6795563549160671\n",
      "Step: 834, Loss: 1.0475798845291138, Accuracy: 0.6797405189620759\n",
      "Step: 835, Loss: 1.1523667573928833, Accuracy: 0.6798245614035088\n",
      "Step: 836, Loss: 1.074161410331726, Accuracy: 0.6800079649542015\n",
      "Step: 837, Loss: 1.3606287240982056, Accuracy: 0.6797931583134447\n",
      "Step: 838, Loss: 1.0989891290664673, Accuracy: 0.6799761620977354\n",
      "Step: 839, Loss: 1.1440366506576538, Accuracy: 0.6800595238095238\n",
      "Step: 840, Loss: 1.2937910556793213, Accuracy: 0.679944510503369\n",
      "Step: 841, Loss: 1.2386353015899658, Accuracy: 0.6799287410926366\n",
      "Step: 842, Loss: 1.3326102495193481, Accuracy: 0.6797153024911032\n",
      "Step: 843, Loss: 1.2677266597747803, Accuracy: 0.6796998420221169\n",
      "Step: 844, Loss: 1.2020736932754517, Accuracy: 0.6797830374753452\n",
      "Step: 845, Loss: 1.2648131847381592, Accuracy: 0.6797675334909378\n",
      "Step: 846, Loss: 1.2201899290084839, Accuracy: 0.6797520661157025\n",
      "Step: 847, Loss: 1.0817950963974, Accuracy: 0.679933176100629\n",
      "Step: 848, Loss: 1.2348941564559937, Accuracy: 0.6799175500588928\n",
      "Step: 849, Loss: 1.1653684377670288, Accuracy: 0.68\n",
      "Step: 850, Loss: 1.6318050622940063, Accuracy: 0.6794947121034077\n",
      "Step: 851, Loss: 1.0830177068710327, Accuracy: 0.6796752738654147\n",
      "Step: 852, Loss: 1.2107014656066895, Accuracy: 0.6796600234466589\n",
      "Step: 853, Loss: 1.2681444883346558, Accuracy: 0.6796448087431693\n",
      "Step: 854, Loss: 1.3233435153961182, Accuracy: 0.6795321637426901\n",
      "Step: 855, Loss: 1.392040729522705, Accuracy: 0.679322429906542\n",
      "Step: 856, Loss: 1.473861813545227, Accuracy: 0.6790159471022948\n",
      "Step: 857, Loss: 1.318289041519165, Accuracy: 0.6789044289044289\n",
      "Step: 858, Loss: 0.9802635312080383, Accuracy: 0.6791812184710904\n",
      "Step: 859, Loss: 1.3165454864501953, Accuracy: 0.6790697674418604\n",
      "Step: 860, Loss: 1.2263952493667603, Accuracy: 0.6790553619821913\n",
      "Step: 861, Loss: 1.0162731409072876, Accuracy: 0.6793310131477185\n",
      "Step: 862, Loss: 1.1828211545944214, Accuracy: 0.679316338354577\n",
      "Step: 863, Loss: 1.3026267290115356, Accuracy: 0.6793016975308642\n",
      "Step: 864, Loss: 0.9229540824890137, Accuracy: 0.6796724470134875\n",
      "Step: 865, Loss: 1.1620064973831177, Accuracy: 0.6797536566589685\n",
      "Step: 866, Loss: 1.073673963546753, Accuracy: 0.6799307958477508\n",
      "Step: 867, Loss: 1.0259621143341064, Accuracy: 0.6802035330261137\n",
      "Step: 868, Loss: 1.1654428243637085, Accuracy: 0.6802838511699271\n",
      "Step: 869, Loss: 1.3153985738754272, Accuracy: 0.6801724137931034\n",
      "Step: 870, Loss: 1.3345527648925781, Accuracy: 0.6800612323000382\n",
      "Step: 871, Loss: 1.2237683534622192, Accuracy: 0.680045871559633\n",
      "Step: 872, Loss: 1.2282477617263794, Accuracy: 0.6800305460099274\n",
      "Step: 873, Loss: 1.2164772748947144, Accuracy: 0.6800152555301296\n",
      "Step: 874, Loss: 1.3049923181533813, Accuracy: 0.6799047619047619\n",
      "Step: 875, Loss: 1.167532205581665, Accuracy: 0.6799847792998478\n",
      "Step: 876, Loss: 1.0758793354034424, Accuracy: 0.6801596351197263\n",
      "Step: 877, Loss: 1.2219806909561157, Accuracy: 0.6801442672741078\n",
      "Step: 878, Loss: 1.007952332496643, Accuracy: 0.6804133485020857\n",
      "Step: 879, Loss: 1.407950758934021, Accuracy: 0.6802083333333333\n",
      "Step: 880, Loss: 1.1678744554519653, Accuracy: 0.6802875520242149\n",
      "Step: 881, Loss: 1.1743654012680054, Accuracy: 0.6803665910808768\n",
      "Step: 882, Loss: 1.069502353668213, Accuracy: 0.6806342015855039\n",
      "Step: 883, Loss: 1.1688776016235352, Accuracy: 0.6807126696832579\n",
      "Step: 884, Loss: 1.2241578102111816, Accuracy: 0.6806967984934087\n",
      "Step: 885, Loss: 1.172049641609192, Accuracy: 0.6807750188111362\n",
      "Step: 886, Loss: 1.2890639305114746, Accuracy: 0.6807591131153702\n",
      "Step: 887, Loss: 1.0785249471664429, Accuracy: 0.6809309309309309\n",
      "Step: 888, Loss: 1.3361783027648926, Accuracy: 0.6808211473565804\n",
      "Step: 889, Loss: 1.2993100881576538, Accuracy: 0.6807116104868914\n",
      "Step: 890, Loss: 1.2397269010543823, Accuracy: 0.680695847362514\n",
      "Step: 891, Loss: 1.0078736543655396, Accuracy: 0.6809603886397608\n",
      "Step: 892, Loss: 1.2416568994522095, Accuracy: 0.6809443822321762\n",
      "Step: 893, Loss: 1.164483666419983, Accuracy: 0.6810216256524981\n",
      "Step: 894, Loss: 1.0819635391235352, Accuracy: 0.6811918063314711\n",
      "Step: 895, Loss: 1.2013823986053467, Accuracy: 0.6812686011904762\n",
      "Step: 896, Loss: 1.1159323453903198, Accuracy: 0.6813452248234857\n",
      "Step: 897, Loss: 1.3096370697021484, Accuracy: 0.6812360801781737\n",
      "Step: 898, Loss: 1.2868934869766235, Accuracy: 0.6811271783463108\n",
      "Step: 899, Loss: 1.0898534059524536, Accuracy: 0.6812962962962963\n",
      "Step: 900, Loss: 1.127007007598877, Accuracy: 0.6813725490196079\n",
      "Step: 901, Loss: 1.3450393676757812, Accuracy: 0.6812638580931264\n",
      "Step: 902, Loss: 1.1179075241088867, Accuracy: 0.6814322628276117\n",
      "Step: 903, Loss: 0.9673991203308105, Accuracy: 0.6816924778761062\n",
      "Step: 904, Loss: 1.0653797388076782, Accuracy: 0.6818600368324125\n",
      "Step: 905, Loss: 1.1692280769348145, Accuracy: 0.6819352465047829\n",
      "Step: 906, Loss: 1.2795380353927612, Accuracy: 0.6819184123484013\n",
      "Step: 907, Loss: 1.0856884717941284, Accuracy: 0.6820851688693098\n",
      "Step: 908, Loss: 0.9171350002288818, Accuracy: 0.6824349101576824\n",
      "Step: 909, Loss: 1.2249096632003784, Accuracy: 0.6823260073260073\n",
      "Step: 910, Loss: 1.4918044805526733, Accuracy: 0.6820343944383461\n",
      "Step: 911, Loss: 1.3558272123336792, Accuracy: 0.6818347953216374\n",
      "Step: 912, Loss: 1.026813268661499, Accuracy: 0.682092004381161\n",
      "Step: 913, Loss: 1.2679598331451416, Accuracy: 0.6819839533187454\n",
      "Step: 914, Loss: 1.2298613786697388, Accuracy: 0.6819672131147541\n",
      "Step: 915, Loss: 1.1787559986114502, Accuracy: 0.6819505094614265\n",
      "Step: 916, Loss: 1.5561202764511108, Accuracy: 0.6815703380588877\n",
      "Step: 917, Loss: 1.163107991218567, Accuracy: 0.6816448801742919\n",
      "Step: 918, Loss: 1.091192364692688, Accuracy: 0.681809938338774\n",
      "Step: 919, Loss: 1.2056829929351807, Accuracy: 0.6817934782608696\n",
      "Step: 920, Loss: 1.1509153842926025, Accuracy: 0.6818675352877307\n",
      "Step: 921, Loss: 1.23957359790802, Accuracy: 0.6818510484454086\n",
      "Step: 922, Loss: 1.4483007192611694, Accuracy: 0.6815637414228963\n",
      "Step: 923, Loss: 1.166467547416687, Accuracy: 0.6815476190476191\n",
      "Step: 924, Loss: 1.1745892763137817, Accuracy: 0.6816216216216217\n",
      "Step: 925, Loss: 1.1278131008148193, Accuracy: 0.681695464362851\n",
      "Step: 926, Loss: 1.172315001487732, Accuracy: 0.6816792520676016\n",
      "Step: 927, Loss: 1.1335835456848145, Accuracy: 0.6818426724137931\n",
      "Step: 928, Loss: 1.1275368928909302, Accuracy: 0.6820057409400789\n",
      "Step: 929, Loss: 1.2962900400161743, Accuracy: 0.681899641577061\n",
      "Step: 930, Loss: 1.4153027534484863, Accuracy: 0.681704260651629\n",
      "Step: 931, Loss: 1.3637827634811401, Accuracy: 0.6815092989985694\n",
      "Step: 932, Loss: 1.2913349866867065, Accuracy: 0.6814040728831725\n",
      "Step: 933, Loss: 1.3660697937011719, Accuracy: 0.6812098501070664\n",
      "Step: 934, Loss: 1.2703585624694824, Accuracy: 0.6811942959001782\n",
      "Step: 935, Loss: 1.226606845855713, Accuracy: 0.6811787749287749\n",
      "Step: 936, Loss: 1.1829115152359009, Accuracy: 0.6812522234080398\n",
      "Step: 937, Loss: 1.1446361541748047, Accuracy: 0.6813255152807391\n",
      "Step: 938, Loss: 1.208275556564331, Accuracy: 0.6813099041533547\n",
      "Step: 939, Loss: 1.237022042274475, Accuracy: 0.6812943262411347\n",
      "Step: 940, Loss: 1.1686019897460938, Accuracy: 0.6813673397095289\n",
      "Step: 941, Loss: 1.2387349605560303, Accuracy: 0.6813517338995047\n",
      "Step: 942, Loss: 1.135388731956482, Accuracy: 0.6815129020855426\n",
      "Step: 943, Loss: 1.2545464038848877, Accuracy: 0.681497175141243\n",
      "Step: 944, Loss: 1.0762956142425537, Accuracy: 0.681657848324515\n",
      "Step: 945, Loss: 1.1289483308792114, Accuracy: 0.6817300916138126\n",
      "Step: 946, Loss: 1.2412477731704712, Accuracy: 0.6817141851460753\n",
      "Step: 947, Loss: 1.4257181882858276, Accuracy: 0.6815225035161744\n",
      "Step: 948, Loss: 1.1310982704162598, Accuracy: 0.6816824727783631\n",
      "Step: 949, Loss: 1.2943041324615479, Accuracy: 0.6816666666666666\n",
      "Step: 950, Loss: 1.5136160850524902, Accuracy: 0.6813003855590606\n",
      "Step: 951, Loss: 1.125996470451355, Accuracy: 0.6813725490196079\n",
      "Step: 952, Loss: 1.3288270235061646, Accuracy: 0.6812696747114375\n",
      "Step: 953, Loss: 1.1259349584579468, Accuracy: 0.6812543675751223\n",
      "Step: 954, Loss: 1.3069367408752441, Accuracy: 0.681151832460733\n",
      "Step: 955, Loss: 1.297652244567871, Accuracy: 0.6810495118549512\n",
      "Step: 956, Loss: 1.300348162651062, Accuracy: 0.6809474050853361\n",
      "Step: 957, Loss: 1.0924233198165894, Accuracy: 0.6811064718162839\n",
      "Step: 958, Loss: 1.127174735069275, Accuracy: 0.681265206812652\n",
      "Step: 959, Loss: 1.359972357749939, Accuracy: 0.6810763888888889\n",
      "Step: 960, Loss: 1.3841108083724976, Accuracy: 0.6808879639264654\n",
      "Step: 961, Loss: 0.9170346856117249, Accuracy: 0.6812196812196812\n",
      "Step: 962, Loss: 0.9618481993675232, Accuracy: 0.6815507095880927\n",
      "Step: 963, Loss: 1.273537516593933, Accuracy: 0.6814488243430152\n",
      "Step: 964, Loss: 1.1684269905090332, Accuracy: 0.6815198618307426\n",
      "Step: 965, Loss: 1.3028022050857544, Accuracy: 0.6814182194616977\n",
      "Step: 966, Loss: 1.1009246110916138, Accuracy: 0.681575318855567\n",
      "Step: 967, Loss: 1.274396300315857, Accuracy: 0.6814738292011019\n",
      "Step: 968, Loss: 1.3067172765731812, Accuracy: 0.6812865497076024\n",
      "Step: 969, Loss: 1.20419180393219, Accuracy: 0.6812714776632303\n",
      "Step: 970, Loss: 1.4325346946716309, Accuracy: 0.6809989701338826\n",
      "Step: 971, Loss: 1.2059544324874878, Accuracy: 0.6809842249657064\n",
      "Step: 972, Loss: 1.2381337881088257, Accuracy: 0.6809695101062008\n",
      "Step: 973, Loss: 1.1013891696929932, Accuracy: 0.6811259411362081\n",
      "Step: 974, Loss: 1.166979432106018, Accuracy: 0.6811965811965812\n",
      "Step: 975, Loss: 1.2697707414627075, Accuracy: 0.681181693989071\n",
      "Step: 976, Loss: 1.3033438920974731, Accuracy: 0.6810815421357899\n",
      "Step: 977, Loss: 1.2238328456878662, Accuracy: 0.6810668029993183\n",
      "Step: 978, Loss: 1.0934323072433472, Accuracy: 0.6812223357167178\n",
      "Step: 979, Loss: 1.0747350454330444, Accuracy: 0.6813775510204082\n",
      "Step: 980, Loss: 1.3152390718460083, Accuracy: 0.6812776078831124\n",
      "Step: 981, Loss: 0.995819628238678, Accuracy: 0.6815173116089613\n",
      "Step: 982, Loss: 1.189314842224121, Accuracy: 0.6815022041369956\n",
      "Step: 983, Loss: 1.2358312606811523, Accuracy: 0.6814871273712737\n",
      "Step: 984, Loss: 1.1566684246063232, Accuracy: 0.6815566835871404\n",
      "Step: 985, Loss: 1.1787880659103394, Accuracy: 0.6816260987153482\n",
      "Step: 986, Loss: 1.2554799318313599, Accuracy: 0.6816109422492401\n",
      "Step: 987, Loss: 1.3997799158096313, Accuracy: 0.6814271255060729\n",
      "Step: 988, Loss: 1.0519298315048218, Accuracy: 0.6815807212672733\n",
      "Step: 989, Loss: 1.3394819498062134, Accuracy: 0.6814814814814815\n",
      "Step: 990, Loss: 1.188187837600708, Accuracy: 0.6815506222670703\n",
      "Step: 991, Loss: 1.068358063697815, Accuracy: 0.6817036290322581\n",
      "Step: 992, Loss: 1.4615235328674316, Accuracy: 0.6814367237327963\n",
      "Step: 993, Loss: 1.0718588829040527, Accuracy: 0.68158953722334\n",
      "Step: 994, Loss: 1.3393610715866089, Accuracy: 0.6814907872696817\n",
      "Step: 995, Loss: 1.3701833486557007, Accuracy: 0.6813922356091031\n",
      "Step: 996, Loss: 1.2137542963027954, Accuracy: 0.6813774657305249\n",
      "Step: 997, Loss: 0.9512579441070557, Accuracy: 0.6816132264529058\n",
      "Step: 998, Loss: 1.1759679317474365, Accuracy: 0.6816816816816816\n",
      "Step: 999, Loss: 1.1344956159591675, Accuracy: 0.68175\n",
      "Step: 1000, Loss: 1.393680453300476, Accuracy: 0.6815684315684316\n",
      "Step: 1001, Loss: 1.270114779472351, Accuracy: 0.6814703925482368\n",
      "Step: 1002, Loss: 1.1745201349258423, Accuracy: 0.6815387171817879\n",
      "Step: 1003, Loss: 1.1786776781082153, Accuracy: 0.6816069057104913\n",
      "Step: 1004, Loss: 1.2667044401168823, Accuracy: 0.6815091210613599\n",
      "Step: 1005, Loss: 1.4614243507385254, Accuracy: 0.681245858184228\n",
      "Step: 1006, Loss: 1.1475231647491455, Accuracy: 0.6813141343925853\n",
      "Step: 1007, Loss: 1.4153803586959839, Accuracy: 0.6811342592592593\n",
      "Step: 1008, Loss: 1.1650136709213257, Accuracy: 0.681202510736703\n",
      "Step: 1009, Loss: 1.139654278755188, Accuracy: 0.6813531353135314\n",
      "Step: 1010, Loss: 1.2150932550430298, Accuracy: 0.6813386086383119\n",
      "Step: 1011, Loss: 1.1607385873794556, Accuracy: 0.6814064558629777\n",
      "Step: 1012, Loss: 0.9979633688926697, Accuracy: 0.6816386969397829\n",
      "Step: 1013, Loss: 1.072684407234192, Accuracy: 0.6817882971729126\n",
      "Step: 1014, Loss: 1.3476027250289917, Accuracy: 0.6816091954022988\n",
      "Step: 1015, Loss: 1.1540628671646118, Accuracy: 0.6816765091863517\n",
      "Step: 1016, Loss: 1.3227378129959106, Accuracy: 0.681579809898394\n",
      "Step: 1017, Loss: 1.1871581077575684, Accuracy: 0.6815651604453176\n",
      "Step: 1018, Loss: 1.2788701057434082, Accuracy: 0.6815505397448479\n",
      "Step: 1019, Loss: 1.0831445455551147, Accuracy: 0.6816993464052288\n",
      "Step: 1020, Loss: 1.1700412034988403, Accuracy: 0.6816846229187071\n",
      "Step: 1021, Loss: 1.3219064474105835, Accuracy: 0.6815883887801696\n",
      "Step: 1022, Loss: 1.243073582649231, Accuracy: 0.6815738025415444\n",
      "Step: 1023, Loss: 1.0299328565597534, Accuracy: 0.6818033854166666\n",
      "Step: 1024, Loss: 1.1705478429794312, Accuracy: 0.6817886178861788\n",
      "Step: 1025, Loss: 1.4623132944107056, Accuracy: 0.6815302144249513\n",
      "Step: 1026, Loss: 1.0834916830062866, Accuracy: 0.6816780266147354\n",
      "Step: 1027, Loss: 1.0737438201904297, Accuracy: 0.681825551232166\n",
      "Step: 1028, Loss: 1.2278766632080078, Accuracy: 0.6818108195659216\n",
      "Step: 1029, Loss: 1.255651593208313, Accuracy: 0.681715210355987\n",
      "Step: 1030, Loss: 1.0203496217727661, Accuracy: 0.6819430973165211\n",
      "Step: 1031, Loss: 1.1657921075820923, Accuracy: 0.6820090439276486\n",
      "Step: 1032, Loss: 1.2242337465286255, Accuracy: 0.6819941916747337\n",
      "Step: 1033, Loss: 1.083279013633728, Accuracy: 0.68214055448098\n",
      "Step: 1034, Loss: 1.2006802558898926, Accuracy: 0.6821256038647343\n",
      "Step: 1035, Loss: 1.1782792806625366, Accuracy: 0.6821911196911197\n",
      "Step: 1036, Loss: 1.060743808746338, Accuracy: 0.682336869173899\n",
      "Step: 1037, Loss: 1.3269094228744507, Accuracy: 0.6821612074502248\n",
      "Step: 1038, Loss: 1.294501781463623, Accuracy: 0.6820660891883221\n",
      "Step: 1039, Loss: 1.188096046447754, Accuracy: 0.6821314102564102\n",
      "Step: 1040, Loss: 1.2249891757965088, Accuracy: 0.6821966058277298\n",
      "Step: 1041, Loss: 1.2022520303726196, Accuracy: 0.6822616762635957\n",
      "Step: 1042, Loss: 1.2396122217178345, Accuracy: 0.6822467241930329\n",
      "Step: 1043, Loss: 1.082419514656067, Accuracy: 0.6823914431673053\n",
      "Step: 1044, Loss: 1.0874658823013306, Accuracy: 0.6825358851674641\n",
      "Step: 1045, Loss: 1.2551672458648682, Accuracy: 0.6825207138304653\n",
      "Step: 1046, Loss: 1.1955269575119019, Accuracy: 0.6825055714740529\n",
      "Step: 1047, Loss: 1.1284395456314087, Accuracy: 0.6825699745547074\n",
      "Step: 1048, Loss: 1.4145541191101074, Accuracy: 0.6823959326342548\n",
      "Step: 1049, Loss: 1.045812964439392, Accuracy: 0.6825396825396826\n",
      "Step: 1050, Loss: 1.0028120279312134, Accuracy: 0.6827624484617825\n",
      "Step: 1051, Loss: 1.1660958528518677, Accuracy: 0.6828263624841572\n",
      "Step: 1052, Loss: 0.9430816173553467, Accuracy: 0.6831275720164609\n",
      "Step: 1053, Loss: 1.1559416055679321, Accuracy: 0.683191018342821\n",
      "Step: 1054, Loss: 1.2391586303710938, Accuracy: 0.683175355450237\n",
      "Step: 1055, Loss: 1.1028004884719849, Accuracy: 0.6832386363636364\n",
      "Step: 1056, Loss: 1.2810624837875366, Accuracy: 0.6831441185745821\n",
      "Step: 1057, Loss: 1.077187180519104, Accuracy: 0.6832860743541272\n",
      "Step: 1058, Loss: 1.1737133264541626, Accuracy: 0.6833490714510545\n",
      "Step: 1059, Loss: 0.9259031414985657, Accuracy: 0.6836477987421383\n",
      "Step: 1060, Loss: 1.1668943166732788, Accuracy: 0.6837103361608545\n",
      "Step: 1061, Loss: 1.200147032737732, Accuracy: 0.6836942875078468\n",
      "Step: 1062, Loss: 1.331250786781311, Accuracy: 0.6835998745688303\n",
      "Step: 1063, Loss: 1.0949515104293823, Accuracy: 0.6838189223057645\n",
      "Step: 1064, Loss: 1.0469588041305542, Accuracy: 0.6839593114241002\n",
      "Step: 1065, Loss: 1.1074438095092773, Accuracy: 0.6840994371482176\n",
      "Step: 1066, Loss: 1.2590305805206299, Accuracy: 0.6840830990315526\n",
      "Step: 1067, Loss: 1.1808050870895386, Accuracy: 0.6840667915106118\n",
      "Step: 1068, Loss: 1.345453143119812, Accuracy: 0.6838946055503586\n",
      "Step: 1069, Loss: 1.3952871561050415, Accuracy: 0.6837227414330218\n",
      "Step: 1070, Loss: 1.3081568479537964, Accuracy: 0.6836290071584189\n",
      "Step: 1071, Loss: 1.3579374551773071, Accuracy: 0.683535447761194\n",
      "Step: 1072, Loss: 1.3134902715682983, Accuracy: 0.6834420627524076\n",
      "Step: 1073, Loss: 1.3113032579421997, Accuracy: 0.6833488516449411\n",
      "Step: 1074, Loss: 1.4449437856674194, Accuracy: 0.6831782945736434\n",
      "Step: 1075, Loss: 1.1146382093429565, Accuracy: 0.683317843866171\n",
      "Step: 1076, Loss: 1.5046137571334839, Accuracy: 0.682992881460848\n",
      "Step: 1077, Loss: 1.1867777109146118, Accuracy: 0.6829777365491652\n",
      "Step: 1078, Loss: 1.1484520435333252, Accuracy: 0.6830398517145505\n",
      "Step: 1079, Loss: 1.1666556596755981, Accuracy: 0.6831018518518519\n",
      "Step: 1080, Loss: 1.2852731943130493, Accuracy: 0.6830095590502621\n",
      "Step: 1081, Loss: 1.2913702726364136, Accuracy: 0.6829944547134935\n",
      "Step: 1082, Loss: 1.074294924736023, Accuracy: 0.6831332717759311\n",
      "Step: 1083, Loss: 1.2304247617721558, Accuracy: 0.6831180811808119\n",
      "Step: 1084, Loss: 1.1508822441101074, Accuracy: 0.6831797235023042\n",
      "Step: 1085, Loss: 1.0574010610580444, Accuracy: 0.683317986494782\n",
      "Step: 1086, Loss: 1.218396782875061, Accuracy: 0.6833026678932843\n",
      "Step: 1087, Loss: 1.093466877937317, Accuracy: 0.6834405637254902\n",
      "Step: 1088, Loss: 1.1909857988357544, Accuracy: 0.6835016835016835\n",
      "Step: 1089, Loss: 1.0847214460372925, Accuracy: 0.6836391437308869\n",
      "Step: 1090, Loss: 1.263873815536499, Accuracy: 0.6835472043996333\n",
      "Step: 1091, Loss: 1.2946994304656982, Accuracy: 0.6834554334554335\n",
      "Step: 1092, Loss: 1.1113656759262085, Accuracy: 0.6835925587069228\n",
      "Step: 1093, Loss: 1.3825746774673462, Accuracy: 0.6834247410115784\n",
      "Step: 1094, Loss: 1.1141330003738403, Accuracy: 0.6834855403348554\n",
      "Step: 1095, Loss: 1.4180551767349243, Accuracy: 0.6833181265206812\n",
      "Step: 1096, Loss: 1.1973527669906616, Accuracy: 0.6833029474323914\n",
      "Step: 1097, Loss: 1.167617678642273, Accuracy: 0.6834395871281117\n",
      "Step: 1098, Loss: 1.2807241678237915, Accuracy: 0.6834243251440704\n",
      "Step: 1099, Loss: 1.2435375452041626, Accuracy: 0.6834090909090909\n",
      "Step: 1100, Loss: 1.0910545587539673, Accuracy: 0.6835452618831366\n",
      "Step: 1101, Loss: 1.0618336200714111, Accuracy: 0.683681185722928\n",
      "Step: 1102, Loss: 1.0005606412887573, Accuracy: 0.6838168631006346\n",
      "Step: 1103, Loss: 1.2255288362503052, Accuracy: 0.6838013285024155\n",
      "Step: 1104, Loss: 1.1764506101608276, Accuracy: 0.6837858220211162\n",
      "Step: 1105, Loss: 1.0920933485031128, Accuracy: 0.6838456901748041\n",
      "Step: 1106, Loss: 1.1599204540252686, Accuracy: 0.6838301716350497\n",
      "Step: 1107, Loss: 1.118458867073059, Accuracy: 0.683965102286402\n",
      "Step: 1108, Loss: 1.3166176080703735, Accuracy: 0.6838743612864443\n",
      "Step: 1109, Loss: 1.200110912322998, Accuracy: 0.6838588588588589\n",
      "Step: 1110, Loss: 1.1740347146987915, Accuracy: 0.6839183918391839\n",
      "Step: 1111, Loss: 1.2951685190200806, Accuracy: 0.6838279376498801\n",
      "Step: 1112, Loss: 1.3505696058273315, Accuracy: 0.683737646001797\n",
      "Step: 1113, Loss: 1.3997602462768555, Accuracy: 0.683572710951526\n",
      "Step: 1114, Loss: 1.2786465883255005, Accuracy: 0.6835575485799701\n",
      "Step: 1115, Loss: 1.3042322397232056, Accuracy: 0.6834677419354839\n",
      "Step: 1116, Loss: 1.2923386096954346, Accuracy: 0.6833780960907192\n",
      "Step: 1117, Loss: 1.143511414527893, Accuracy: 0.6834376863446631\n",
      "Step: 1118, Loss: 1.303563117980957, Accuracy: 0.6833482275841525\n",
      "Step: 1119, Loss: 1.302957534790039, Accuracy: 0.6832589285714286\n",
      "Step: 1120, Loss: 1.131393313407898, Accuracy: 0.6833184656556646\n",
      "Step: 1121, Loss: 0.9474311470985413, Accuracy: 0.6835264408793821\n",
      "Step: 1122, Loss: 1.1316578388214111, Accuracy: 0.6835856337192046\n",
      "Step: 1123, Loss: 1.1767159700393677, Accuracy: 0.683570581257414\n",
      "Step: 1124, Loss: 1.0902966260910034, Accuracy: 0.6836296296296296\n",
      "Step: 1125, Loss: 1.0128628015518188, Accuracy: 0.6838365896980462\n",
      "Step: 1126, Loss: 1.2500108480453491, Accuracy: 0.683821354628808\n",
      "Step: 1127, Loss: 1.1584607362747192, Accuracy: 0.6838800236406619\n",
      "Step: 1128, Loss: 1.300419807434082, Accuracy: 0.6837909654561559\n",
      "Step: 1129, Loss: 1.2313790321350098, Accuracy: 0.6837758112094395\n",
      "Step: 1130, Loss: 1.0697295665740967, Accuracy: 0.6839080459770115\n",
      "Step: 1131, Loss: 1.2252833843231201, Accuracy: 0.6838928150765606\n",
      "Step: 1132, Loss: 1.1540049314498901, Accuracy: 0.6839511621065019\n",
      "Step: 1133, Loss: 1.3501309156417847, Accuracy: 0.6837889476778366\n",
      "Step: 1134, Loss: 1.220863699913025, Accuracy: 0.6837738619676945\n",
      "Step: 1135, Loss: 1.2275447845458984, Accuracy: 0.6837588028169014\n",
      "Step: 1136, Loss: 1.272760272026062, Accuracy: 0.6837437701553797\n",
      "Step: 1137, Loss: 1.2879009246826172, Accuracy: 0.6836555360281195\n",
      "Step: 1138, Loss: 1.3564683198928833, Accuracy: 0.6835674568334796\n",
      "Step: 1139, Loss: 1.11496102809906, Accuracy: 0.6836988304093568\n",
      "Step: 1140, Loss: 1.0568469762802124, Accuracy: 0.6838299737072743\n",
      "Step: 1141, Loss: 1.2420636415481567, Accuracy: 0.6838149445417396\n",
      "Step: 1142, Loss: 1.317971110343933, Accuracy: 0.6837270341207349\n",
      "Step: 1143, Loss: 1.0881673097610474, Accuracy: 0.6838578088578089\n",
      "Step: 1144, Loss: 1.17178213596344, Accuracy: 0.6839155749636099\n",
      "Step: 1145, Loss: 1.050585389137268, Accuracy: 0.6840459569517161\n",
      "Step: 1146, Loss: 1.1624850034713745, Accuracy: 0.6841034582970067\n",
      "Step: 1147, Loss: 1.2406667470932007, Accuracy: 0.6840882694541232\n",
      "Step: 1148, Loss: 1.3231427669525146, Accuracy: 0.6839280533797505\n",
      "Step: 1149, Loss: 1.3208097219467163, Accuracy: 0.6838405797101449\n",
      "Step: 1150, Loss: 1.15318763256073, Accuracy: 0.6838980596582682\n",
      "Step: 1151, Loss: 1.4070876836776733, Accuracy: 0.6837384259259259\n",
      "Step: 1152, Loss: 1.171030879020691, Accuracy: 0.6837958947672738\n",
      "Step: 1153, Loss: 1.1643139123916626, Accuracy: 0.6838532640092432\n",
      "Step: 1154, Loss: 1.140400767326355, Accuracy: 0.6839105339105339\n",
      "Step: 1155, Loss: 1.271670937538147, Accuracy: 0.6838956170703575\n",
      "Step: 1156, Loss: 1.2264710664749146, Accuracy: 0.6838807260155575\n",
      "Step: 1157, Loss: 1.1002367734909058, Accuracy: 0.6840097869890616\n",
      "Step: 1158, Loss: 1.1539870500564575, Accuracy: 0.684066724187518\n",
      "Step: 1159, Loss: 1.0655535459518433, Accuracy: 0.6842672413793104\n",
      "Step: 1160, Loss: 1.1194581985473633, Accuracy: 0.6843238587424634\n",
      "Step: 1161, Loss: 1.3582830429077148, Accuracy: 0.6841652323580034\n",
      "Step: 1162, Loss: 1.1603707075119019, Accuracy: 0.6841501862997994\n",
      "Step: 1163, Loss: 1.25725257396698, Accuracy: 0.684135166093929\n",
      "Step: 1164, Loss: 1.0664567947387695, Accuracy: 0.6842632331902718\n",
      "Step: 1165, Loss: 1.2825568914413452, Accuracy: 0.6841766723842195\n",
      "Step: 1166, Loss: 1.478073000907898, Accuracy: 0.6839474435875464\n",
      "Step: 1167, Loss: 0.9460048079490662, Accuracy: 0.6842180365296804\n",
      "Step: 1168, Loss: 1.2172763347625732, Accuracy: 0.6842030225263758\n",
      "Step: 1169, Loss: 1.3213509321212769, Accuracy: 0.6841168091168092\n",
      "Step: 1170, Loss: 1.2692168951034546, Accuracy: 0.6841019072018218\n",
      "Step: 1171, Loss: 1.1544654369354248, Accuracy: 0.6841581342434585\n",
      "Step: 1172, Loss: 1.243491291999817, Accuracy: 0.6841432225063938\n",
      "Step: 1173, Loss: 1.1412793397903442, Accuracy: 0.6841993185689949\n",
      "Step: 1174, Loss: 1.2917299270629883, Accuracy: 0.6841843971631205\n",
      "Step: 1175, Loss: 1.2368088960647583, Accuracy: 0.6841695011337868\n",
      "Step: 1176, Loss: 1.0660656690597534, Accuracy: 0.6843670348343246\n",
      "Step: 1177, Loss: 1.2290555238723755, Accuracy: 0.6843520090548953\n",
      "Step: 1178, Loss: 1.1384588479995728, Accuracy: 0.6844076901328809\n",
      "Step: 1179, Loss: 0.9144459366798401, Accuracy: 0.6846751412429378\n",
      "Step: 1180, Loss: 1.3079544305801392, Accuracy: 0.6845893310753599\n",
      "Step: 1181, Loss: 1.2507911920547485, Accuracy: 0.6845741680767061\n",
      "Step: 1182, Loss: 1.0465203523635864, Accuracy: 0.6847703578472809\n",
      "Step: 1183, Loss: 1.1262896060943604, Accuracy: 0.6848254504504504\n",
      "Step: 1184, Loss: 1.2412852048873901, Accuracy: 0.6847398030942334\n",
      "Step: 1185, Loss: 1.2373729944229126, Accuracy: 0.684654300168634\n",
      "Step: 1186, Loss: 1.38015878200531, Accuracy: 0.6844987363100252\n",
      "Step: 1187, Loss: 1.0967944860458374, Accuracy: 0.6846240179573513\n",
      "Step: 1188, Loss: 1.3423393964767456, Accuracy: 0.6845388281469021\n",
      "Step: 1189, Loss: 1.1859080791473389, Accuracy: 0.684593837535014\n",
      "Step: 1190, Loss: 1.217177391052246, Accuracy: 0.6845787853344528\n",
      "Step: 1191, Loss: 1.1658991575241089, Accuracy: 0.6845637583892618\n",
      "Step: 1192, Loss: 0.9400436282157898, Accuracy: 0.6848281642917016\n",
      "Step: 1193, Loss: 1.2978373765945435, Accuracy: 0.6847431602456728\n",
      "Step: 1194, Loss: 1.195165753364563, Accuracy: 0.6847977684797768\n",
      "Step: 1195, Loss: 1.1086212396621704, Accuracy: 0.6848522853957637\n",
      "Step: 1196, Loss: 1.0061310529708862, Accuracy: 0.6850459482038429\n",
      "Step: 1197, Loss: 1.2400487661361694, Accuracy: 0.6850306065664997\n",
      "Step: 1198, Loss: 1.5886949300765991, Accuracy: 0.6846677787044759\n",
      "Step: 1199, Loss: 1.3811802864074707, Accuracy: 0.6844444444444444\n",
      "Step: 1200, Loss: 1.191452145576477, Accuracy: 0.6844990285872884\n",
      "Step: 1201, Loss: 1.3340550661087036, Accuracy: 0.6844148641153632\n",
      "Step: 1202, Loss: 1.1924335956573486, Accuracy: 0.6844693821003048\n",
      "Step: 1203, Loss: 1.4720573425292969, Accuracy: 0.6842469545957918\n",
      "Step: 1204, Loss: 1.1197726726531982, Accuracy: 0.6843015214384509\n",
      "Step: 1205, Loss: 1.1925114393234253, Accuracy: 0.6843559977888336\n",
      "Step: 1206, Loss: 1.1772054433822632, Accuracy: 0.6843413421706711\n",
      "Step: 1207, Loss: 1.1106767654418945, Accuracy: 0.6843956953642384\n",
      "Step: 1208, Loss: 1.0245141983032227, Accuracy: 0.6845878136200717\n",
      "Step: 1209, Loss: 1.1856199502944946, Accuracy: 0.6845730027548209\n",
      "Step: 1210, Loss: 1.2908862829208374, Accuracy: 0.6844894026974951\n",
      "Step: 1211, Loss: 1.378321647644043, Accuracy: 0.6843371837183718\n",
      "Step: 1212, Loss: 1.2375085353851318, Accuracy: 0.6843226161033251\n",
      "Step: 1213, Loss: 1.1852712631225586, Accuracy: 0.6843767160900605\n",
      "Step: 1214, Loss: 1.1721522808074951, Accuracy: 0.6844307270233196\n",
      "Step: 1215, Loss: 1.1112865209579468, Accuracy: 0.684484649122807\n",
      "Step: 1216, Loss: 1.1438835859298706, Accuracy: 0.6845384826075048\n",
      "Step: 1217, Loss: 1.2318745851516724, Accuracy: 0.6845238095238095\n",
      "Step: 1218, Loss: 1.1509569883346558, Accuracy: 0.684577522559475\n",
      "Step: 1219, Loss: 1.1782541275024414, Accuracy: 0.6846311475409836\n",
      "Step: 1220, Loss: 1.068429946899414, Accuracy: 0.6847529347529347\n",
      "Step: 1221, Loss: 1.2988901138305664, Accuracy: 0.6846699399890889\n",
      "Step: 1222, Loss: 1.2264903783798218, Accuracy: 0.6846552194058326\n",
      "Step: 1223, Loss: 1.2988080978393555, Accuracy: 0.684572440087146\n",
      "Step: 1224, Loss: 1.3614521026611328, Accuracy: 0.684421768707483\n",
      "Step: 1225, Loss: 1.3676995038986206, Accuracy: 0.6843393148450244\n",
      "Step: 1226, Loss: 1.0709401369094849, Accuracy: 0.6844607443629449\n",
      "Step: 1227, Loss: 1.347272515296936, Accuracy: 0.6843783930510315\n",
      "Step: 1228, Loss: 1.0467318296432495, Accuracy: 0.684499593165175\n",
      "Step: 1229, Loss: 1.3335188627243042, Accuracy: 0.6844173441734417\n",
      "Step: 1230, Loss: 1.2385019063949585, Accuracy: 0.6844029244516653\n",
      "Step: 1231, Loss: 1.0959804058074951, Accuracy: 0.6845238095238095\n",
      "Step: 1232, Loss: 1.3870338201522827, Accuracy: 0.6843741551770749\n",
      "Step: 1233, Loss: 1.0783008337020874, Accuracy: 0.684494867639114\n",
      "Step: 1234, Loss: 1.1035337448120117, Accuracy: 0.6846153846153846\n",
      "Step: 1235, Loss: 1.2741879224777222, Accuracy: 0.6846008629989212\n",
      "Step: 1236, Loss: 1.244849443435669, Accuracy: 0.6845863648612234\n",
      "Step: 1237, Loss: 1.2339280843734741, Accuracy: 0.6845718901453958\n",
      "Step: 1238, Loss: 1.097774624824524, Accuracy: 0.6846919558783966\n",
      "Step: 1239, Loss: 1.1264058351516724, Accuracy: 0.6847446236559139\n",
      "Step: 1240, Loss: 1.2644518613815308, Accuracy: 0.6846629062583938\n",
      "Step: 1241, Loss: 1.0875778198242188, Accuracy: 0.6847826086956522\n",
      "Step: 1242, Loss: 1.3074169158935547, Accuracy: 0.6847009922231161\n",
      "Step: 1243, Loss: 1.2344015836715698, Accuracy: 0.6846864951768489\n",
      "Step: 1244, Loss: 1.3168588876724243, Accuracy: 0.6846050870147256\n",
      "Step: 1245, Loss: 1.16225266456604, Accuracy: 0.684657570893526\n",
      "Step: 1246, Loss: 1.071703314781189, Accuracy: 0.6847767976476877\n",
      "Step: 1247, Loss: 1.213055968284607, Accuracy: 0.6847622863247863\n",
      "Step: 1248, Loss: 0.9509317874908447, Accuracy: 0.6850146784093942\n",
      "Step: 1249, Loss: 1.1273952722549438, Accuracy: 0.6850666666666667\n",
      "Step: 1250, Loss: 1.2099639177322388, Accuracy: 0.6851185718092193\n",
      "Step: 1251, Loss: 1.3836127519607544, Accuracy: 0.6849707135250266\n",
      "Step: 1252, Loss: 1.3347917795181274, Accuracy: 0.6848230912476723\n",
      "Step: 1253, Loss: 1.1202374696731567, Accuracy: 0.6848750664540139\n",
      "Step: 1254, Loss: 1.4764775037765503, Accuracy: 0.6846613545816733\n",
      "Step: 1255, Loss: 1.1501468420028687, Accuracy: 0.6847133757961783\n",
      "Step: 1256, Loss: 1.2626190185546875, Accuracy: 0.6846327234155396\n",
      "Step: 1257, Loss: 1.179245114326477, Accuracy: 0.6846846846846847\n",
      "Step: 1258, Loss: 1.291068196296692, Accuracy: 0.6846041832141911\n",
      "Step: 1259, Loss: 1.1275511980056763, Accuracy: 0.6846560846560846\n",
      "Step: 1260, Loss: 1.2359637022018433, Accuracy: 0.6846418186624372\n",
      "Step: 1261, Loss: 1.3658944368362427, Accuracy: 0.6844955097728473\n",
      "Step: 1262, Loss: 1.4250621795654297, Accuracy: 0.684283452098179\n",
      "Step: 1263, Loss: 1.3130970001220703, Accuracy: 0.6842035864978903\n",
      "Step: 1264, Loss: 1.0877288579940796, Accuracy: 0.6842555994729907\n",
      "Step: 1265, Loss: 1.2679334878921509, Accuracy: 0.6842417061611374\n",
      "Step: 1266, Loss: 1.2519408464431763, Accuracy: 0.6841620626151013\n",
      "Step: 1267, Loss: 1.2399829626083374, Accuracy: 0.6841482649842271\n",
      "Step: 1268, Loss: 1.4259451627731323, Accuracy: 0.6840031520882585\n",
      "Step: 1269, Loss: 1.2073849439620972, Accuracy: 0.6839895013123359\n",
      "Step: 1270, Loss: 1.2373883724212646, Accuracy: 0.6839758720167847\n",
      "Step: 1271, Loss: 1.277413010597229, Accuracy: 0.683896750524109\n",
      "Step: 1272, Loss: 1.171555757522583, Accuracy: 0.6838832155014402\n",
      "Step: 1273, Loss: 1.0977586507797241, Accuracy: 0.6840005232862376\n",
      "Step: 1274, Loss: 1.2281187772750854, Accuracy: 0.6839869281045752\n",
      "Step: 1275, Loss: 1.2639588117599487, Accuracy: 0.6839733542319749\n",
      "Step: 1276, Loss: 1.3987048864364624, Accuracy: 0.6838292873923257\n",
      "Step: 1277, Loss: 1.1381272077560425, Accuracy: 0.6838810641627543\n",
      "Step: 1278, Loss: 1.1911320686340332, Accuracy: 0.6839327599687256\n",
      "Step: 1279, Loss: 1.293300747871399, Accuracy: 0.6838541666666667\n",
      "Step: 1280, Loss: 1.3244949579238892, Accuracy: 0.683775696070778\n",
      "Step: 1281, Loss: 1.3380693197250366, Accuracy: 0.6836973478939158\n",
      "Step: 1282, Loss: 1.1843329668045044, Accuracy: 0.6836840737853989\n",
      "Step: 1283, Loss: 1.024774193763733, Accuracy: 0.6838655244029076\n",
      "Step: 1284, Loss: 1.208074927330017, Accuracy: 0.683852140077821\n",
      "Step: 1285, Loss: 1.2637184858322144, Accuracy: 0.68383877656817\n",
      "Step: 1286, Loss: 1.271223545074463, Accuracy: 0.6838254338254338\n",
      "Step: 1287, Loss: 1.1076635122299194, Accuracy: 0.6838768115942029\n",
      "Step: 1288, Loss: 1.3679765462875366, Accuracy: 0.6837341608482027\n",
      "Step: 1289, Loss: 0.9831594824790955, Accuracy: 0.6839793281653747\n",
      "Step: 1290, Loss: 1.264215111732483, Accuracy: 0.6839013684482314\n",
      "Step: 1291, Loss: 1.3175760507583618, Accuracy: 0.6838235294117647\n",
      "Step: 1292, Loss: 1.0311931371688843, Accuracy: 0.6840036091776232\n",
      "Step: 1293, Loss: 1.3271758556365967, Accuracy: 0.6839258114374034\n",
      "Step: 1294, Loss: 1.3794794082641602, Accuracy: 0.6837837837837838\n",
      "Step: 1295, Loss: 1.2901374101638794, Accuracy: 0.6837062757201646\n",
      "Step: 1296, Loss: 1.0645734071731567, Accuracy: 0.6838858905165767\n",
      "Step: 1297, Loss: 1.1770504713058472, Accuracy: 0.6839368258859785\n",
      "Step: 1298, Loss: 0.9556767344474792, Accuracy: 0.6841159866564024\n",
      "Step: 1299, Loss: 1.1862872838974, Accuracy: 0.6841666666666667\n",
      "Step: 1300, Loss: 1.0565298795700073, Accuracy: 0.6842813220599538\n",
      "Step: 1301, Loss: 1.2407852411270142, Accuracy: 0.6842677931387608\n",
      "Step: 1302, Loss: 1.1963664293289185, Accuracy: 0.6842542849833717\n",
      "Step: 1303, Loss: 1.0915864706039429, Accuracy: 0.684368609406953\n",
      "Step: 1304, Loss: 1.254231333732605, Accuracy: 0.6843550446998723\n",
      "Step: 1305, Loss: 1.1503287553787231, Accuracy: 0.6844053088310362\n",
      "Step: 1306, Loss: 1.1345258951187134, Accuracy: 0.6845192552920173\n",
      "Step: 1307, Loss: 1.2521435022354126, Accuracy: 0.6845056065239552\n",
      "Step: 1308, Loss: 1.1761034727096558, Accuracy: 0.6845556404379933\n",
      "Step: 1309, Loss: 1.3329154253005981, Accuracy: 0.6844783715012722\n",
      "Step: 1310, Loss: 1.1542152166366577, Accuracy: 0.6845283498601576\n",
      "Step: 1311, Loss: 1.23343825340271, Accuracy: 0.6845147357723578\n",
      "Step: 1312, Loss: 1.1913174390792847, Accuracy: 0.6845646103071845\n",
      "Step: 1313, Loss: 1.180050253868103, Accuracy: 0.6845509893455098\n",
      "Step: 1314, Loss: 1.07823646068573, Accuracy: 0.6846641318124208\n",
      "Step: 1315, Loss: 1.231691598892212, Accuracy: 0.6846504559270516\n",
      "Step: 1316, Loss: 1.2640739679336548, Accuracy: 0.6846368008099215\n",
      "Step: 1317, Loss: 1.0786020755767822, Accuracy: 0.6847496206373292\n",
      "Step: 1318, Loss: 1.2304576635360718, Accuracy: 0.68473591104372\n",
      "Step: 1319, Loss: 1.188392996788025, Accuracy: 0.6847853535353535\n",
      "Step: 1320, Loss: 1.0456677675247192, Accuracy: 0.684960888215998\n",
      "Step: 1321, Loss: 1.0697991847991943, Accuracy: 0.6850731215330308\n",
      "Step: 1322, Loss: 1.2286534309387207, Accuracy: 0.6850592088687327\n",
      "Step: 1323, Loss: 1.298087239265442, Accuracy: 0.6849823766364552\n",
      "Step: 1324, Loss: 1.3080683946609497, Accuracy: 0.6849056603773584\n",
      "Step: 1325, Loss: 1.0706785917282104, Accuracy: 0.6850175967823027\n",
      "Step: 1326, Loss: 1.1449507474899292, Accuracy: 0.6850665661893997\n",
      "Step: 1327, Loss: 1.1317936182022095, Accuracy: 0.6851154618473896\n",
      "Step: 1328, Loss: 1.3461508750915527, Accuracy: 0.6849761725608227\n",
      "Step: 1329, Loss: 1.1120792627334595, Accuracy: 0.6850877192982456\n",
      "Step: 1330, Loss: 1.2192543745040894, Accuracy: 0.6850738792887553\n",
      "Step: 1331, Loss: 1.1886045932769775, Accuracy: 0.68506006006006\n",
      "Step: 1332, Loss: 1.2283755540847778, Accuracy: 0.6850462615653914\n",
      "Step: 1333, Loss: 1.3319271802902222, Accuracy: 0.6849700149925038\n",
      "Step: 1334, Loss: 1.1812678575515747, Accuracy: 0.6850187265917603\n",
      "Step: 1335, Loss: 1.08158540725708, Accuracy: 0.685129740518962\n",
      "Step: 1336, Loss: 1.045052409172058, Accuracy: 0.6852405883819497\n",
      "Step: 1337, Loss: 1.2003411054611206, Accuracy: 0.6852889885401097\n",
      "Step: 1338, Loss: 1.2194641828536987, Accuracy: 0.6852750809061489\n",
      "Step: 1339, Loss: 1.3295412063598633, Accuracy: 0.685136815920398\n",
      "Step: 1340, Loss: 0.9776312708854675, Accuracy: 0.6853716132239622\n",
      "Step: 1341, Loss: 1.3179357051849365, Accuracy: 0.6852955787382017\n",
      "Step: 1342, Loss: 1.289388656616211, Accuracy: 0.6852196574832464\n",
      "Step: 1343, Loss: 1.1421557664871216, Accuracy: 0.6852678571428571\n",
      "Step: 1344, Loss: 1.2450119256973267, Accuracy: 0.6852540272614622\n",
      "Step: 1345, Loss: 1.3907337188720703, Accuracy: 0.6851163942545815\n",
      "Step: 1346, Loss: 1.3483800888061523, Accuracy: 0.6849789656025737\n",
      "Step: 1347, Loss: 1.3508507013320923, Accuracy: 0.684841740850643\n",
      "Step: 1348, Loss: 1.2114907503128052, Accuracy: 0.6848282678527304\n",
      "Step: 1349, Loss: 1.4686542749404907, Accuracy: 0.6846296296296296\n",
      "Step: 1350, Loss: 1.3471568822860718, Accuracy: 0.6844929681717247\n",
      "Step: 1351, Loss: 1.0934966802597046, Accuracy: 0.684603057199211\n",
      "Step: 1352, Loss: 1.3100519180297852, Accuracy: 0.6845282089184528\n",
      "Step: 1353, Loss: 0.9234981536865234, Accuracy: 0.6847612013786312\n",
      "Step: 1354, Loss: 0.995918333530426, Accuracy: 0.6849323493234932\n",
      "Step: 1355, Loss: 1.1989892721176147, Accuracy: 0.6849188790560472\n",
      "Step: 1356, Loss: 1.1678403615951538, Accuracy: 0.684966838614591\n",
      "Step: 1357, Loss: 1.2683273553848267, Accuracy: 0.6848919980363279\n",
      "Step: 1358, Loss: 1.071295142173767, Accuracy: 0.6850012263919548\n",
      "Step: 1359, Loss: 1.1846868991851807, Accuracy: 0.6850490196078431\n",
      "Step: 1360, Loss: 1.1511565446853638, Accuracy: 0.6851579720793534\n",
      "Step: 1361, Loss: 1.0936585664749146, Accuracy: 0.6852667645619187\n",
      "Step: 1362, Loss: 1.1626936197280884, Accuracy: 0.6853142577647346\n",
      "Step: 1363, Loss: 1.2303813695907593, Accuracy: 0.6853005865102639\n",
      "Step: 1364, Loss: 1.0788195133209229, Accuracy: 0.6854090354090354\n",
      "Step: 1365, Loss: 1.3416844606399536, Accuracy: 0.6853343094192289\n",
      "Step: 1366, Loss: 1.153901219367981, Accuracy: 0.6853816142404292\n",
      "Step: 1367, Loss: 1.1990571022033691, Accuracy: 0.6853679337231969\n",
      "Step: 1368, Loss: 1.1613295078277588, Accuracy: 0.6854760165570977\n",
      "Step: 1369, Loss: 1.0622950792312622, Accuracy: 0.6856447688564477\n",
      "Step: 1370, Loss: 1.3139206171035767, Accuracy: 0.6855701434476051\n",
      "Step: 1371, Loss: 1.4715956449508667, Accuracy: 0.685374149659864\n",
      "Step: 1372, Loss: 1.2590739727020264, Accuracy: 0.685360524399126\n",
      "Step: 1373, Loss: 1.245814323425293, Accuracy: 0.6853469189713731\n",
      "Step: 1374, Loss: 1.2103265523910522, Accuracy: 0.6852727272727273\n",
      "Step: 1375, Loss: 1.2281237840652466, Accuracy: 0.6852592054263565\n",
      "Step: 1376, Loss: 1.1446716785430908, Accuracy: 0.6853062212539337\n",
      "Step: 1377, Loss: 1.127609372138977, Accuracy: 0.6853531688437349\n",
      "Step: 1378, Loss: 1.15647554397583, Accuracy: 0.6854604786076868\n",
      "Step: 1379, Loss: 1.0809584856033325, Accuracy: 0.6855676328502416\n",
      "Step: 1380, Loss: 1.232417106628418, Accuracy: 0.6855539464156408\n",
      "Step: 1381, Loss: 1.1468960046768188, Accuracy: 0.6856005788712012\n",
      "Step: 1382, Loss: 1.1474177837371826, Accuracy: 0.685647143890094\n",
      "Step: 1383, Loss: 1.1289530992507935, Accuracy: 0.6856936416184971\n",
      "Step: 1384, Loss: 1.3566585779190063, Accuracy: 0.6856197352587244\n",
      "Step: 1385, Loss: 1.1471534967422485, Accuracy: 0.6856661856661856\n",
      "Step: 1386, Loss: 1.0355125665664673, Accuracy: 0.685772650805095\n",
      "Step: 1387, Loss: 1.323870062828064, Accuracy: 0.6856988472622478\n",
      "Step: 1388, Loss: 1.1780617237091064, Accuracy: 0.6857451403887689\n",
      "Step: 1389, Loss: 1.176552414894104, Accuracy: 0.6857314148681055\n",
      "Step: 1390, Loss: 1.0322567224502563, Accuracy: 0.6858974358974359\n",
      "Step: 1391, Loss: 1.2436705827713013, Accuracy: 0.6858836206896551\n",
      "Step: 1392, Loss: 1.1879701614379883, Accuracy: 0.6858698253170615\n",
      "Step: 1393, Loss: 1.3396997451782227, Accuracy: 0.6857962697274032\n",
      "Step: 1394, Loss: 1.1118544340133667, Accuracy: 0.6859020310633214\n",
      "Step: 1395, Loss: 1.138800024986267, Accuracy: 0.685947946513849\n",
      "Step: 1396, Loss: 1.4213045835494995, Accuracy: 0.6857551896921976\n",
      "Step: 1397, Loss: 1.1986461877822876, Accuracy: 0.6857415355269433\n",
      "Step: 1398, Loss: 1.3190265893936157, Accuracy: 0.6856683345246605\n",
      "Step: 1399, Loss: 1.3068771362304688, Accuracy: 0.6855952380952381\n",
      "Step: 1400, Loss: 1.2774800062179565, Accuracy: 0.685581727337616\n",
      "Step: 1401, Loss: 1.4101606607437134, Accuracy: 0.6854493580599144\n",
      "Step: 1402, Loss: 1.528175950050354, Accuracy: 0.6851389878831077\n",
      "Step: 1403, Loss: 1.2896685600280762, Accuracy: 0.6850664767331434\n",
      "Step: 1404, Loss: 1.0562843084335327, Accuracy: 0.6851720047449584\n",
      "Step: 1405, Loss: 1.0578893423080444, Accuracy: 0.6853366524419156\n",
      "Step: 1406, Loss: 1.2434009313583374, Accuracy: 0.6853233830845771\n",
      "Step: 1407, Loss: 1.0989009141921997, Accuracy: 0.6854285037878788\n",
      "Step: 1408, Loss: 1.3166626691818237, Accuracy: 0.6853560444759877\n",
      "Step: 1409, Loss: 1.0490905046463013, Accuracy: 0.6855200945626477\n",
      "Step: 1410, Loss: 1.0896657705307007, Accuracy: 0.6856248523505788\n",
      "Step: 1411, Loss: 1.094201683998108, Accuracy: 0.685729461756374\n",
      "Step: 1412, Loss: 1.4756722450256348, Accuracy: 0.6855390422269403\n",
      "Step: 1413, Loss: 1.2398771047592163, Accuracy: 0.6855256954266855\n",
      "Step: 1414, Loss: 1.2959994077682495, Accuracy: 0.6854534746760895\n",
      "Step: 1415, Loss: 1.1018961668014526, Accuracy: 0.6855579096045198\n",
      "Step: 1416, Loss: 1.1798971891403198, Accuracy: 0.6856033874382498\n",
      "Step: 1417, Loss: 1.1136902570724487, Accuracy: 0.6857075693464975\n",
      "Step: 1418, Loss: 1.2187172174453735, Accuracy: 0.6856941508104298\n",
      "Step: 1419, Loss: 1.480508804321289, Accuracy: 0.6855046948356808\n",
      "Step: 1420, Loss: 1.2833595275878906, Accuracy: 0.685432793807178\n",
      "Step: 1421, Loss: 1.2213249206542969, Accuracy: 0.6854195968120019\n",
      "Step: 1422, Loss: 1.08729088306427, Accuracy: 0.6855821035371281\n",
      "Step: 1423, Loss: 1.2492295503616333, Accuracy: 0.6855102996254682\n",
      "Step: 1424, Loss: 1.4379863739013672, Accuracy: 0.6853216374269006\n",
      "Step: 1425, Loss: 1.0264317989349365, Accuracy: 0.6854838709677419\n",
      "Step: 1426, Loss: 1.3898520469665527, Accuracy: 0.6851786965662229\n",
      "Epoch: 5, Val_Accuracy: 0.11962616822429907\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "568fb44ecb2b402895102618b6c42b36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1427 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0, Loss: 1.211787462234497, Accuracy: 0.6666666666666666\n",
      "Step: 1, Loss: 1.2415269613265991, Accuracy: 0.6666666666666666\n",
      "Step: 2, Loss: 1.118996500968933, Accuracy: 0.7222222222222222\n",
      "Step: 3, Loss: 1.1647453308105469, Accuracy: 0.7291666666666666\n",
      "Step: 4, Loss: 1.2293450832366943, Accuracy: 0.7166666666666667\n",
      "Step: 5, Loss: 1.337175726890564, Accuracy: 0.6944444444444444\n",
      "Step: 6, Loss: 1.1734076738357544, Accuracy: 0.6904761904761905\n",
      "Step: 7, Loss: 1.230141520500183, Accuracy: 0.6875\n",
      "Step: 8, Loss: 1.2219648361206055, Accuracy: 0.6851851851851852\n",
      "Step: 9, Loss: 1.008162260055542, Accuracy: 0.7083333333333334\n",
      "Step: 10, Loss: 1.2413091659545898, Accuracy: 0.7045454545454546\n",
      "Step: 11, Loss: 1.071208119392395, Accuracy: 0.7152777777777778\n",
      "Step: 12, Loss: 1.3923486471176147, Accuracy: 0.6987179487179487\n",
      "Step: 13, Loss: 1.0842052698135376, Accuracy: 0.7083333333333334\n",
      "Step: 14, Loss: 1.2075461149215698, Accuracy: 0.7111111111111111\n",
      "Step: 15, Loss: 1.4806767702102661, Accuracy: 0.6927083333333334\n",
      "Step: 16, Loss: 1.2870668172836304, Accuracy: 0.6862745098039216\n",
      "Step: 17, Loss: 1.0183807611465454, Accuracy: 0.6990740740740741\n",
      "Step: 18, Loss: 1.2151588201522827, Accuracy: 0.6973684210526315\n",
      "Step: 19, Loss: 1.1022018194198608, Accuracy: 0.7\n",
      "Step: 20, Loss: 1.2158616781234741, Accuracy: 0.6984126984126984\n",
      "Step: 21, Loss: 1.2771459817886353, Accuracy: 0.6931818181818182\n",
      "Step: 22, Loss: 1.1634868383407593, Accuracy: 0.6956521739130435\n",
      "Step: 23, Loss: 1.3823617696762085, Accuracy: 0.6875\n",
      "Step: 24, Loss: 1.262059211730957, Accuracy: 0.6833333333333333\n",
      "Step: 25, Loss: 1.3784624338150024, Accuracy: 0.6762820512820513\n",
      "Step: 26, Loss: 1.388494610786438, Accuracy: 0.6697530864197531\n",
      "Step: 27, Loss: 1.0432201623916626, Accuracy: 0.6785714285714286\n",
      "Step: 28, Loss: 1.0591933727264404, Accuracy: 0.6839080459770115\n",
      "Step: 29, Loss: 1.1500760316848755, Accuracy: 0.6861111111111111\n",
      "Step: 30, Loss: 1.31122624874115, Accuracy: 0.6827956989247311\n",
      "Step: 31, Loss: 1.2314006090164185, Accuracy: 0.6822916666666666\n",
      "Step: 32, Loss: 1.3111344575881958, Accuracy: 0.6792929292929293\n",
      "Step: 33, Loss: 1.0919303894042969, Accuracy: 0.6838235294117647\n",
      "Step: 34, Loss: 1.417453408241272, Accuracy: 0.6761904761904762\n",
      "Step: 35, Loss: 1.1733758449554443, Accuracy: 0.6782407407407407\n",
      "Step: 36, Loss: 1.2183088064193726, Accuracy: 0.6779279279279279\n",
      "Step: 37, Loss: 1.1005152463912964, Accuracy: 0.6820175438596491\n",
      "Step: 38, Loss: 1.1515686511993408, Accuracy: 0.6837606837606838\n",
      "Step: 39, Loss: 1.2903746366500854, Accuracy: 0.68125\n",
      "Step: 40, Loss: 1.1534956693649292, Accuracy: 0.6829268292682927\n",
      "Step: 41, Loss: 1.2528921365737915, Accuracy: 0.6825396825396826\n",
      "Step: 42, Loss: 1.2343658208847046, Accuracy: 0.6821705426356589\n",
      "Step: 43, Loss: 1.121221899986267, Accuracy: 0.6856060606060606\n",
      "Step: 44, Loss: 1.173410415649414, Accuracy: 0.6851851851851852\n",
      "Step: 45, Loss: 1.0924973487854004, Accuracy: 0.6884057971014492\n",
      "Step: 46, Loss: 1.211906909942627, Accuracy: 0.6879432624113475\n",
      "Step: 47, Loss: 1.0530074834823608, Accuracy: 0.6927083333333334\n",
      "Step: 48, Loss: 1.2873986959457397, Accuracy: 0.6921768707482994\n",
      "Step: 49, Loss: 1.1969786882400513, Accuracy: 0.6933333333333334\n",
      "Step: 50, Loss: 1.2886052131652832, Accuracy: 0.6911764705882353\n",
      "Step: 51, Loss: 1.225760579109192, Accuracy: 0.6907051282051282\n",
      "Step: 52, Loss: 1.0858513116836548, Accuracy: 0.6933962264150944\n",
      "Step: 53, Loss: 1.3876785039901733, Accuracy: 0.6898148148148148\n",
      "Step: 54, Loss: 1.155117392539978, Accuracy: 0.6893939393939394\n",
      "Step: 55, Loss: 1.464103102684021, Accuracy: 0.6845238095238095\n",
      "Step: 56, Loss: 1.269713282585144, Accuracy: 0.6842105263157895\n",
      "Step: 57, Loss: 1.2454652786254883, Accuracy: 0.6839080459770115\n",
      "Step: 58, Loss: 1.1950427293777466, Accuracy: 0.6850282485875706\n",
      "Step: 59, Loss: 1.1534193754196167, Accuracy: 0.6875\n",
      "Step: 60, Loss: 1.3094449043273926, Accuracy: 0.6857923497267759\n",
      "Step: 61, Loss: 1.1730645895004272, Accuracy: 0.6868279569892473\n",
      "Step: 62, Loss: 1.1642330884933472, Accuracy: 0.6878306878306878\n",
      "Step: 63, Loss: 1.0828392505645752, Accuracy: 0.6901041666666666\n",
      "Step: 64, Loss: 1.1693767309188843, Accuracy: 0.691025641025641\n",
      "Step: 65, Loss: 1.1830312013626099, Accuracy: 0.6919191919191919\n",
      "Step: 66, Loss: 1.2241216897964478, Accuracy: 0.6915422885572139\n",
      "Step: 67, Loss: 1.1152677536010742, Accuracy: 0.6936274509803921\n",
      "Step: 68, Loss: 1.0986559391021729, Accuracy: 0.6944444444444444\n",
      "Step: 69, Loss: 1.1690590381622314, Accuracy: 0.6952380952380952\n",
      "Step: 70, Loss: 1.1558983325958252, Accuracy: 0.6960093896713615\n",
      "Step: 71, Loss: 1.095530390739441, Accuracy: 0.6979166666666666\n",
      "Step: 72, Loss: 1.3498815298080444, Accuracy: 0.6952054794520548\n",
      "Step: 73, Loss: 1.2498723268508911, Accuracy: 0.6936936936936937\n",
      "Step: 74, Loss: 1.0864918231964111, Accuracy: 0.6955555555555556\n",
      "Step: 75, Loss: 1.088620662689209, Accuracy: 0.6973684210526315\n",
      "Step: 76, Loss: 1.1709681749343872, Accuracy: 0.696969696969697\n",
      "Step: 77, Loss: 1.0634490251541138, Accuracy: 0.6987179487179487\n",
      "Step: 78, Loss: 1.1524815559387207, Accuracy: 0.6993670886075949\n",
      "Step: 79, Loss: 1.2868462800979614, Accuracy: 0.6979166666666666\n",
      "Step: 80, Loss: 1.2951079607009888, Accuracy: 0.6965020576131687\n",
      "Step: 81, Loss: 1.2414366006851196, Accuracy: 0.6961382113821138\n",
      "Step: 82, Loss: 1.2263832092285156, Accuracy: 0.6957831325301205\n",
      "Step: 83, Loss: 1.2039532661437988, Accuracy: 0.6954365079365079\n",
      "Step: 84, Loss: 1.09738290309906, Accuracy: 0.6970588235294117\n",
      "Step: 85, Loss: 1.2337568998336792, Accuracy: 0.6967054263565892\n",
      "Step: 86, Loss: 1.3433822393417358, Accuracy: 0.6944444444444444\n",
      "Step: 87, Loss: 1.2451461553573608, Accuracy: 0.6931818181818182\n",
      "Step: 88, Loss: 1.22732675075531, Accuracy: 0.6928838951310862\n",
      "Step: 89, Loss: 1.105210781097412, Accuracy: 0.6944444444444444\n",
      "Step: 90, Loss: 1.1042207479476929, Accuracy: 0.6959706959706959\n",
      "Step: 91, Loss: 1.2735925912857056, Accuracy: 0.6947463768115942\n",
      "Step: 92, Loss: 1.4408715963363647, Accuracy: 0.6917562724014337\n",
      "Step: 93, Loss: 1.350693702697754, Accuracy: 0.6897163120567376\n",
      "Step: 94, Loss: 1.1523979902267456, Accuracy: 0.6903508771929825\n",
      "Step: 95, Loss: 1.128149390220642, Accuracy: 0.6909722222222222\n",
      "Step: 96, Loss: 1.1667712926864624, Accuracy: 0.6915807560137457\n",
      "Step: 97, Loss: 1.2492449283599854, Accuracy: 0.6913265306122449\n",
      "Step: 98, Loss: 1.2844980955123901, Accuracy: 0.6910774410774411\n",
      "Step: 99, Loss: 1.2641493082046509, Accuracy: 0.69\n",
      "Step: 100, Loss: 0.9163423180580139, Accuracy: 0.693069306930693\n",
      "Step: 101, Loss: 1.1601876020431519, Accuracy: 0.6936274509803921\n",
      "Step: 102, Loss: 1.561880111694336, Accuracy: 0.6901294498381877\n",
      "Step: 103, Loss: 1.2435604333877563, Accuracy: 0.6899038461538461\n",
      "Step: 104, Loss: 1.3687543869018555, Accuracy: 0.6880952380952381\n",
      "Step: 105, Loss: 1.102627158164978, Accuracy: 0.6894654088050315\n",
      "Step: 106, Loss: 1.2863777875900269, Accuracy: 0.6884735202492211\n",
      "Step: 107, Loss: 1.2747809886932373, Accuracy: 0.6875\n",
      "Step: 108, Loss: 1.1729964017868042, Accuracy: 0.6880733944954128\n",
      "Step: 109, Loss: 1.0779398679733276, Accuracy: 0.6893939393939394\n",
      "Step: 110, Loss: 1.1807032823562622, Accuracy: 0.68993993993994\n",
      "Step: 111, Loss: 1.2348474264144897, Accuracy: 0.6897321428571429\n",
      "Step: 112, Loss: 1.1826432943344116, Accuracy: 0.68952802359882\n",
      "Step: 113, Loss: 1.511042594909668, Accuracy: 0.6864035087719298\n",
      "Step: 114, Loss: 1.2481722831726074, Accuracy: 0.6855072463768116\n",
      "Step: 115, Loss: 1.3300920724868774, Accuracy: 0.6846264367816092\n",
      "Step: 116, Loss: 1.2325446605682373, Accuracy: 0.6844729344729344\n",
      "Step: 117, Loss: 1.2465370893478394, Accuracy: 0.684322033898305\n",
      "Step: 118, Loss: 1.3115936517715454, Accuracy: 0.6834733893557423\n",
      "Step: 119, Loss: 1.1393013000488281, Accuracy: 0.6840277777777778\n",
      "Step: 120, Loss: 1.245652437210083, Accuracy: 0.6838842975206612\n",
      "Step: 121, Loss: 1.2684439420700073, Accuracy: 0.6837431693989071\n",
      "Step: 122, Loss: 1.1423007249832153, Accuracy: 0.6842818428184282\n",
      "Step: 123, Loss: 1.1605459451675415, Accuracy: 0.6848118279569892\n",
      "Step: 124, Loss: 1.387945532798767, Accuracy: 0.6833333333333333\n",
      "Step: 125, Loss: 1.3159723281860352, Accuracy: 0.6825396825396826\n",
      "Step: 126, Loss: 1.2683826684951782, Accuracy: 0.681758530183727\n",
      "Step: 127, Loss: 1.243644118309021, Accuracy: 0.681640625\n",
      "Step: 128, Loss: 1.144677758216858, Accuracy: 0.6821705426356589\n",
      "Step: 129, Loss: 1.248508334159851, Accuracy: 0.6820512820512821\n",
      "Step: 130, Loss: 1.0860694646835327, Accuracy: 0.683206106870229\n",
      "Step: 131, Loss: 1.3080400228500366, Accuracy: 0.6818181818181818\n",
      "Step: 132, Loss: 1.2374733686447144, Accuracy: 0.681704260651629\n",
      "Step: 133, Loss: 1.2899023294448853, Accuracy: 0.681592039800995\n",
      "Step: 134, Loss: 1.156164526939392, Accuracy: 0.6820987654320988\n",
      "Step: 135, Loss: 1.2300736904144287, Accuracy: 0.6819852941176471\n",
      "Step: 136, Loss: 1.1298235654830933, Accuracy: 0.6824817518248175\n",
      "Step: 137, Loss: 1.383334755897522, Accuracy: 0.6811594202898551\n",
      "Step: 138, Loss: 1.182913899421692, Accuracy: 0.6816546762589928\n",
      "Step: 139, Loss: 1.236806035041809, Accuracy: 0.6815476190476191\n",
      "Step: 140, Loss: 1.0006335973739624, Accuracy: 0.6832151300236406\n",
      "Step: 141, Loss: 1.1709682941436768, Accuracy: 0.6836854460093896\n",
      "Step: 142, Loss: 1.1363155841827393, Accuracy: 0.6841491841491841\n",
      "Step: 143, Loss: 1.2181974649429321, Accuracy: 0.6846064814814815\n",
      "Step: 144, Loss: 1.0799411535263062, Accuracy: 0.6856321839080459\n",
      "Step: 145, Loss: 1.235794186592102, Accuracy: 0.6855022831050228\n",
      "Step: 146, Loss: 1.0991170406341553, Accuracy: 0.6865079365079365\n",
      "Step: 147, Loss: 1.153810977935791, Accuracy: 0.6869369369369369\n",
      "Step: 148, Loss: 1.1088544130325317, Accuracy: 0.6873601789709173\n",
      "Step: 149, Loss: 1.2369883060455322, Accuracy: 0.6872222222222222\n",
      "Step: 150, Loss: 1.0488101243972778, Accuracy: 0.6881898454746137\n",
      "Step: 151, Loss: 1.0839346647262573, Accuracy: 0.6891447368421053\n",
      "Step: 152, Loss: 1.4190319776535034, Accuracy: 0.6879084967320261\n",
      "Step: 153, Loss: 1.203037142753601, Accuracy: 0.6877705627705628\n",
      "Step: 154, Loss: 1.432856559753418, Accuracy: 0.6865591397849462\n",
      "Step: 155, Loss: 1.104962944984436, Accuracy: 0.6875\n",
      "Step: 156, Loss: 1.20706045627594, Accuracy: 0.6878980891719745\n",
      "Step: 157, Loss: 1.1562057733535767, Accuracy: 0.6882911392405063\n",
      "Step: 158, Loss: 1.3300509452819824, Accuracy: 0.6876310272536688\n",
      "Step: 159, Loss: 1.0241113901138306, Accuracy: 0.6885416666666667\n",
      "Step: 160, Loss: 1.1978627443313599, Accuracy: 0.6889233954451346\n",
      "Step: 161, Loss: 0.9998332858085632, Accuracy: 0.6903292181069959\n",
      "Step: 162, Loss: 1.2192232608795166, Accuracy: 0.6901840490797546\n",
      "Step: 163, Loss: 1.4771562814712524, Accuracy: 0.6885162601626016\n",
      "Step: 164, Loss: 1.404530644416809, Accuracy: 0.6873737373737374\n",
      "Step: 165, Loss: 1.2675632238388062, Accuracy: 0.6872489959839357\n",
      "Step: 166, Loss: 1.4014800786972046, Accuracy: 0.686127744510978\n",
      "Step: 167, Loss: 1.1577590703964233, Accuracy: 0.6865079365079365\n",
      "Step: 168, Loss: 1.1887129545211792, Accuracy: 0.6863905325443787\n",
      "Step: 169, Loss: 1.1838499307632446, Accuracy: 0.6867647058823529\n",
      "Step: 170, Loss: 1.3370944261550903, Accuracy: 0.6861598440545809\n",
      "Step: 171, Loss: 1.1493754386901855, Accuracy: 0.686531007751938\n",
      "Step: 172, Loss: 1.4344607591629028, Accuracy: 0.6854527938342967\n",
      "Step: 173, Loss: 1.186031460762024, Accuracy: 0.685823754789272\n",
      "Step: 174, Loss: 1.254116415977478, Accuracy: 0.6857142857142857\n",
      "Step: 175, Loss: 1.238015055656433, Accuracy: 0.6856060606060606\n",
      "Step: 176, Loss: 1.2139278650283813, Accuracy: 0.6854990583804144\n",
      "Step: 177, Loss: 1.4952635765075684, Accuracy: 0.6839887640449438\n",
      "Step: 178, Loss: 1.2496025562286377, Accuracy: 0.6838919925512105\n",
      "Step: 179, Loss: 0.9948294758796692, Accuracy: 0.6851851851851852\n",
      "Step: 180, Loss: 0.952913224697113, Accuracy: 0.6869244935543278\n",
      "Step: 181, Loss: 1.1313210725784302, Accuracy: 0.6872710622710623\n",
      "Step: 182, Loss: 1.391924500465393, Accuracy: 0.686247723132969\n",
      "Step: 183, Loss: 1.017389178276062, Accuracy: 0.6875\n",
      "Step: 184, Loss: 1.2418795824050903, Accuracy: 0.6873873873873874\n",
      "Step: 185, Loss: 1.1347911357879639, Accuracy: 0.6877240143369175\n",
      "Step: 186, Loss: 1.2501603364944458, Accuracy: 0.6876114081996435\n",
      "Step: 187, Loss: 1.0368666648864746, Accuracy: 0.6888297872340425\n",
      "Step: 188, Loss: 1.3193997144699097, Accuracy: 0.6882716049382716\n",
      "Step: 189, Loss: 1.2758804559707642, Accuracy: 0.6881578947368421\n",
      "Step: 190, Loss: 1.2694171667099, Accuracy: 0.6880453752181501\n",
      "Step: 191, Loss: 1.337203860282898, Accuracy: 0.6875\n",
      "Step: 192, Loss: 1.345931887626648, Accuracy: 0.6869602763385146\n",
      "Step: 193, Loss: 1.5340466499328613, Accuracy: 0.6851374570446735\n",
      "Step: 194, Loss: 1.036676049232483, Accuracy: 0.6863247863247863\n",
      "Step: 195, Loss: 1.1720558404922485, Accuracy: 0.6862244897959183\n",
      "Step: 196, Loss: 1.245932936668396, Accuracy: 0.6861252115059222\n",
      "Step: 197, Loss: 1.162035346031189, Accuracy: 0.6864478114478114\n",
      "Step: 198, Loss: 1.1629022359848022, Accuracy: 0.6867671691792295\n",
      "Step: 199, Loss: 1.2788220643997192, Accuracy: 0.68625\n",
      "Step: 200, Loss: 1.0912963151931763, Accuracy: 0.6869817578772802\n",
      "Step: 201, Loss: 1.255597710609436, Accuracy: 0.6868811881188119\n",
      "Step: 202, Loss: 1.0371836423873901, Accuracy: 0.6876026272577996\n",
      "Step: 203, Loss: 1.04833984375, Accuracy: 0.6887254901960784\n",
      "Step: 204, Loss: 1.149438500404358, Accuracy: 0.6890243902439024\n",
      "Step: 205, Loss: 1.1991260051727295, Accuracy: 0.6893203883495146\n",
      "Step: 206, Loss: 1.2287291288375854, Accuracy: 0.6892109500805152\n",
      "Step: 207, Loss: 1.190311074256897, Accuracy: 0.6895032051282052\n",
      "Step: 208, Loss: 1.1849781274795532, Accuracy: 0.689792663476874\n",
      "Step: 209, Loss: 1.2765265703201294, Accuracy: 0.6892857142857143\n",
      "Step: 210, Loss: 1.307273507118225, Accuracy: 0.688783570300158\n",
      "Step: 211, Loss: 1.2450993061065674, Accuracy: 0.6882861635220126\n",
      "Step: 212, Loss: 1.3727086782455444, Accuracy: 0.6874021909233177\n",
      "Step: 213, Loss: 1.0909894704818726, Accuracy: 0.6880841121495327\n",
      "Step: 214, Loss: 1.092575192451477, Accuracy: 0.6887596899224806\n",
      "Step: 215, Loss: 1.190256118774414, Accuracy: 0.6890432098765432\n",
      "Step: 216, Loss: 1.266008973121643, Accuracy: 0.6885560675883257\n",
      "Step: 217, Loss: 0.9406254291534424, Accuracy: 0.6899847094801224\n",
      "Step: 218, Loss: 1.1522444486618042, Accuracy: 0.6902587519025876\n",
      "Step: 219, Loss: 1.0298973321914673, Accuracy: 0.6912878787878788\n",
      "Step: 220, Loss: 1.0870553255081177, Accuracy: 0.6919306184012066\n",
      "Step: 221, Loss: 1.4290090799331665, Accuracy: 0.6906906906906907\n",
      "Step: 222, Loss: 1.2297359704971313, Accuracy: 0.6905829596412556\n",
      "Step: 223, Loss: 1.2340255975723267, Accuracy: 0.6904761904761905\n",
      "Step: 224, Loss: 1.2354717254638672, Accuracy: 0.6903703703703704\n",
      "Step: 225, Loss: 1.3471657037734985, Accuracy: 0.68952802359882\n",
      "Step: 226, Loss: 1.2346248626708984, Accuracy: 0.6894273127753304\n",
      "Step: 227, Loss: 1.3185807466506958, Accuracy: 0.6889619883040936\n",
      "Step: 228, Loss: 1.0706443786621094, Accuracy: 0.6899563318777293\n",
      "Step: 229, Loss: 0.9997236132621765, Accuracy: 0.6909420289855073\n",
      "Step: 230, Loss: 1.3772801160812378, Accuracy: 0.6901154401154401\n",
      "Step: 231, Loss: 1.4229832887649536, Accuracy: 0.6892959770114943\n",
      "Step: 232, Loss: 1.1508675813674927, Accuracy: 0.6895565092989986\n",
      "Step: 233, Loss: 1.1936227083206177, Accuracy: 0.6894586894586895\n",
      "Step: 234, Loss: 1.1076170206069946, Accuracy: 0.6900709219858157\n",
      "Step: 235, Loss: 1.1635823249816895, Accuracy: 0.6903248587570622\n",
      "Step: 236, Loss: 1.191710352897644, Accuracy: 0.6905766526019691\n",
      "Step: 237, Loss: 1.3158981800079346, Accuracy: 0.6901260504201681\n",
      "Step: 238, Loss: 1.1943074464797974, Accuracy: 0.6903765690376569\n",
      "Step: 239, Loss: 1.139617919921875, Accuracy: 0.690625\n",
      "Step: 240, Loss: 1.3359850645065308, Accuracy: 0.690179806362379\n",
      "Step: 241, Loss: 1.0776768922805786, Accuracy: 0.690771349862259\n",
      "Step: 242, Loss: 1.3414267301559448, Accuracy: 0.6903292181069959\n",
      "Step: 243, Loss: 1.1708670854568481, Accuracy: 0.6905737704918032\n",
      "Step: 244, Loss: 1.2769442796707153, Accuracy: 0.6901360544217687\n",
      "Step: 245, Loss: 1.155105471611023, Accuracy: 0.690379403794038\n",
      "Step: 246, Loss: 1.2143607139587402, Accuracy: 0.6906207827260459\n",
      "Step: 247, Loss: 1.2609981298446655, Accuracy: 0.6905241935483871\n",
      "Step: 248, Loss: 1.2606446743011475, Accuracy: 0.6904283801874164\n",
      "Step: 249, Loss: 1.3523120880126953, Accuracy: 0.6896666666666667\n",
      "Step: 250, Loss: 1.2583024501800537, Accuracy: 0.6895750332005313\n",
      "Step: 251, Loss: 1.3896894454956055, Accuracy: 0.6888227513227513\n",
      "Step: 252, Loss: 1.3601919412612915, Accuracy: 0.6884057971014492\n",
      "Step: 253, Loss: 1.399051308631897, Accuracy: 0.6876640419947506\n",
      "Step: 254, Loss: 1.4206446409225464, Accuracy: 0.6866013071895425\n",
      "Step: 255, Loss: 1.2446223497390747, Accuracy: 0.6865234375\n",
      "Step: 256, Loss: 1.0733214616775513, Accuracy: 0.687094682230869\n",
      "Step: 257, Loss: 1.2134524583816528, Accuracy: 0.687015503875969\n",
      "Step: 258, Loss: 1.1951546669006348, Accuracy: 0.6869369369369369\n",
      "Step: 259, Loss: 1.4387694597244263, Accuracy: 0.6858974358974359\n",
      "Step: 260, Loss: 1.3866630792617798, Accuracy: 0.6851851851851852\n",
      "Step: 261, Loss: 1.155184030532837, Accuracy: 0.6854325699745547\n",
      "Step: 262, Loss: 1.224293828010559, Accuracy: 0.685361216730038\n",
      "Step: 263, Loss: 1.1147034168243408, Accuracy: 0.6859217171717171\n",
      "Step: 264, Loss: 1.3738726377487183, Accuracy: 0.6855345911949685\n",
      "Step: 265, Loss: 1.3815813064575195, Accuracy: 0.6848370927318296\n",
      "Step: 266, Loss: 1.2259141206741333, Accuracy: 0.684769038701623\n",
      "Step: 267, Loss: 1.2438215017318726, Accuracy: 0.6847014925373134\n",
      "Step: 268, Loss: 1.3031514883041382, Accuracy: 0.6843246592317225\n",
      "Step: 269, Loss: 1.0005543231964111, Accuracy: 0.6851851851851852\n",
      "Step: 270, Loss: 1.2090147733688354, Accuracy: 0.6851168511685117\n",
      "Step: 271, Loss: 1.280922770500183, Accuracy: 0.6847426470588235\n",
      "Step: 272, Loss: 0.9702410697937012, Accuracy: 0.6858974358974359\n",
      "Step: 273, Loss: 1.1405733823776245, Accuracy: 0.6861313868613139\n",
      "Step: 274, Loss: 1.4575880765914917, Accuracy: 0.6851515151515152\n",
      "Step: 275, Loss: 1.4646679162979126, Accuracy: 0.6841787439613527\n",
      "Step: 276, Loss: 1.273979663848877, Accuracy: 0.6841155234657039\n",
      "Step: 277, Loss: 1.0675510168075562, Accuracy: 0.684652278177458\n",
      "Step: 278, Loss: 1.2119122743606567, Accuracy: 0.6845878136200717\n",
      "Step: 279, Loss: 1.1088870763778687, Accuracy: 0.6851190476190476\n",
      "Step: 280, Loss: 1.2077540159225464, Accuracy: 0.6850533807829181\n",
      "Step: 281, Loss: 1.5024031400680542, Accuracy: 0.6841016548463357\n",
      "Step: 282, Loss: 1.045875906944275, Accuracy: 0.6849234393404005\n",
      "Step: 283, Loss: 1.3752641677856445, Accuracy: 0.6842723004694836\n",
      "Step: 284, Loss: 1.001686930656433, Accuracy: 0.6850877192982456\n",
      "Step: 285, Loss: 1.254037618637085, Accuracy: 0.6850233100233101\n",
      "Step: 286, Loss: 1.2856338024139404, Accuracy: 0.6849593495934959\n",
      "Step: 287, Loss: 1.1499600410461426, Accuracy: 0.6851851851851852\n",
      "Step: 288, Loss: 1.0634068250656128, Accuracy: 0.6856978085351788\n",
      "Step: 289, Loss: 1.2884314060211182, Accuracy: 0.6853448275862069\n",
      "Step: 290, Loss: 0.9580489993095398, Accuracy: 0.686426116838488\n",
      "Step: 291, Loss: 1.1817861795425415, Accuracy: 0.6866438356164384\n",
      "Step: 292, Loss: 1.2297974824905396, Accuracy: 0.6865756541524459\n",
      "Step: 293, Loss: 1.0623992681503296, Accuracy: 0.6870748299319728\n",
      "Step: 294, Loss: 1.1703537702560425, Accuracy: 0.6870056497175141\n",
      "Step: 295, Loss: 1.2011991739273071, Accuracy: 0.6869369369369369\n",
      "Step: 296, Loss: 1.1962319612503052, Accuracy: 0.6868686868686869\n",
      "Step: 297, Loss: 1.4533666372299194, Accuracy: 0.6859619686800895\n",
      "Step: 298, Loss: 1.0505027770996094, Accuracy: 0.6867335562987736\n",
      "Step: 299, Loss: 1.124043583869934, Accuracy: 0.6872222222222222\n",
      "Step: 300, Loss: 1.1673163175582886, Accuracy: 0.6874307862679956\n",
      "Step: 301, Loss: 1.2314949035644531, Accuracy: 0.6873620309050773\n",
      "Step: 302, Loss: 1.1297651529312134, Accuracy: 0.6878437843784379\n",
      "Step: 303, Loss: 1.3166134357452393, Accuracy: 0.6875\n",
      "Step: 304, Loss: 1.2167774438858032, Accuracy: 0.687431693989071\n",
      "Step: 305, Loss: 1.1674466133117676, Accuracy: 0.6876361655773421\n",
      "Step: 306, Loss: 1.2476047277450562, Accuracy: 0.6875678610206297\n",
      "Step: 307, Loss: 1.25153648853302, Accuracy: 0.6875\n",
      "Step: 308, Loss: 1.1561883687973022, Accuracy: 0.6877022653721683\n",
      "Step: 309, Loss: 1.166224479675293, Accuracy: 0.6879032258064516\n",
      "Step: 310, Loss: 1.2554728984832764, Accuracy: 0.6878349410503751\n",
      "Step: 311, Loss: 1.2310264110565186, Accuracy: 0.687767094017094\n",
      "Step: 312, Loss: 1.2331005334854126, Accuracy: 0.6876996805111821\n",
      "Step: 313, Loss: 1.1168115139007568, Accuracy: 0.6881634819532909\n",
      "Step: 314, Loss: 0.9349691867828369, Accuracy: 0.6891534391534392\n",
      "Step: 315, Loss: 1.3690623044967651, Accuracy: 0.6885548523206751\n",
      "Step: 316, Loss: 1.2479143142700195, Accuracy: 0.6884858044164038\n",
      "Step: 317, Loss: 1.2540560960769653, Accuracy: 0.6884171907756813\n",
      "Step: 318, Loss: 1.1636457443237305, Accuracy: 0.6886102403343782\n",
      "Step: 319, Loss: 1.2362781763076782, Accuracy: 0.6885416666666667\n",
      "Step: 320, Loss: 1.1448657512664795, Accuracy: 0.6887331256490135\n",
      "Step: 321, Loss: 1.009189248085022, Accuracy: 0.6896997929606625\n",
      "Step: 322, Loss: 1.192479133605957, Accuracy: 0.6896284829721362\n",
      "Step: 323, Loss: 1.2794321775436401, Accuracy: 0.6893004115226338\n",
      "Step: 324, Loss: 1.1884971857070923, Accuracy: 0.6894871794871795\n",
      "Step: 325, Loss: 1.1880980730056763, Accuracy: 0.6896728016359919\n",
      "Step: 326, Loss: 1.2001858949661255, Accuracy: 0.6898572884811417\n",
      "Step: 327, Loss: 1.1130489110946655, Accuracy: 0.6902947154471545\n",
      "Step: 328, Loss: 1.0786914825439453, Accuracy: 0.6907294832826748\n",
      "Step: 329, Loss: 1.2748016119003296, Accuracy: 0.6904040404040404\n",
      "Step: 330, Loss: 1.2966176271438599, Accuracy: 0.6900805639476334\n",
      "Step: 331, Loss: 1.102378010749817, Accuracy: 0.6905120481927711\n",
      "Step: 332, Loss: 1.3278584480285645, Accuracy: 0.6901901901901902\n",
      "Step: 333, Loss: 1.11793851852417, Accuracy: 0.6903692614770459\n",
      "Step: 334, Loss: 1.2429972887039185, Accuracy: 0.6902985074626866\n",
      "Step: 335, Loss: 1.190759301185608, Accuracy: 0.6904761904761905\n",
      "Step: 336, Loss: 1.1472195386886597, Accuracy: 0.6906528189910979\n",
      "Step: 337, Loss: 1.1269782781600952, Accuracy: 0.6908284023668639\n",
      "Step: 338, Loss: 1.1977472305297852, Accuracy: 0.6907571288102261\n",
      "Step: 339, Loss: 1.3219338655471802, Accuracy: 0.6904411764705882\n",
      "Step: 340, Loss: 1.3309173583984375, Accuracy: 0.6901270772238515\n",
      "Step: 341, Loss: 1.3006612062454224, Accuracy: 0.6898148148148148\n",
      "Step: 342, Loss: 1.0507673025131226, Accuracy: 0.6902332361516035\n",
      "Step: 343, Loss: 1.1626496315002441, Accuracy: 0.690406976744186\n",
      "Step: 344, Loss: 1.25189208984375, Accuracy: 0.6903381642512078\n",
      "Step: 345, Loss: 1.315689206123352, Accuracy: 0.690028901734104\n",
      "Step: 346, Loss: 1.1486164331436157, Accuracy: 0.6902017291066282\n",
      "Step: 347, Loss: 1.0631548166275024, Accuracy: 0.6906130268199234\n",
      "Step: 348, Loss: 1.2254316806793213, Accuracy: 0.6905444126074498\n",
      "Step: 349, Loss: 1.1430765390396118, Accuracy: 0.6907142857142857\n",
      "Step: 350, Loss: 1.2191041707992554, Accuracy: 0.6906457739791073\n",
      "Step: 351, Loss: 1.0074315071105957, Accuracy: 0.6912878787878788\n",
      "Step: 352, Loss: 1.3205920457839966, Accuracy: 0.690982058545798\n",
      "Step: 353, Loss: 1.2461462020874023, Accuracy: 0.6909133709981168\n",
      "Step: 354, Loss: 1.1243411302566528, Accuracy: 0.6910798122065728\n",
      "Step: 355, Loss: 1.1757755279541016, Accuracy: 0.6910112359550562\n",
      "Step: 356, Loss: 1.3906255960464478, Accuracy: 0.6904761904761905\n",
      "Step: 357, Loss: 1.1729995012283325, Accuracy: 0.6906424581005587\n",
      "Step: 358, Loss: 1.3043678998947144, Accuracy: 0.6903435468895079\n",
      "Step: 359, Loss: 1.1586984395980835, Accuracy: 0.6905092592592592\n",
      "Step: 360, Loss: 1.1522071361541748, Accuracy: 0.6906740535549399\n",
      "Step: 361, Loss: 1.1000854969024658, Accuracy: 0.6910681399631676\n",
      "Step: 362, Loss: 1.0822981595993042, Accuracy: 0.6914600550964187\n",
      "Step: 363, Loss: 1.2483662366867065, Accuracy: 0.6913919413919414\n",
      "Step: 364, Loss: 1.3335148096084595, Accuracy: 0.6910958904109589\n",
      "Step: 365, Loss: 1.230729579925537, Accuracy: 0.6910291438979964\n",
      "Step: 366, Loss: 1.1554678678512573, Accuracy: 0.6911898274296094\n",
      "Step: 367, Loss: 1.7233792543411255, Accuracy: 0.6897644927536232\n",
      "Step: 368, Loss: 1.4568204879760742, Accuracy: 0.6890243902439024\n",
      "Step: 369, Loss: 1.2343932390213013, Accuracy: 0.688963963963964\n",
      "Step: 370, Loss: 1.228061318397522, Accuracy: 0.6889038634321654\n",
      "Step: 371, Loss: 1.1335722208023071, Accuracy: 0.6892921146953405\n",
      "Step: 372, Loss: 1.2406877279281616, Accuracy: 0.6892314566577301\n",
      "Step: 373, Loss: 1.3453588485717773, Accuracy: 0.6889483065953654\n",
      "Step: 374, Loss: 1.3027185201644897, Accuracy: 0.6886666666666666\n",
      "Step: 375, Loss: 1.2636677026748657, Accuracy: 0.6886081560283688\n",
      "Step: 376, Loss: 1.3121633529663086, Accuracy: 0.6883289124668435\n",
      "Step: 377, Loss: 1.0815620422363281, Accuracy: 0.6887125220458554\n",
      "Step: 378, Loss: 1.2130078077316284, Accuracy: 0.6886543535620053\n",
      "Step: 379, Loss: 1.1618748903274536, Accuracy: 0.6888157894736842\n",
      "Step: 380, Loss: 1.5351086854934692, Accuracy: 0.6878827646544182\n",
      "Step: 381, Loss: 1.007073163986206, Accuracy: 0.6884816753926701\n",
      "Step: 382, Loss: 1.0330291986465454, Accuracy: 0.689077458659704\n",
      "Step: 383, Loss: 1.1066807508468628, Accuracy: 0.6892361111111112\n",
      "Step: 384, Loss: 1.0584031343460083, Accuracy: 0.6896103896103896\n",
      "Step: 385, Loss: 1.039202332496643, Accuracy: 0.6901986183074266\n",
      "Step: 386, Loss: 1.1688976287841797, Accuracy: 0.6903531438415159\n",
      "Step: 387, Loss: 1.0242048501968384, Accuracy: 0.6909364261168385\n",
      "Step: 388, Loss: 1.457655906677246, Accuracy: 0.6902313624678663\n",
      "Step: 389, Loss: 1.4631348848342896, Accuracy: 0.6895299145299145\n",
      "Step: 390, Loss: 1.3943414688110352, Accuracy: 0.6890451832907076\n",
      "Step: 391, Loss: 1.2534939050674438, Accuracy: 0.6889880952380952\n",
      "Step: 392, Loss: 1.2817779779434204, Accuracy: 0.6889312977099237\n",
      "Step: 393, Loss: 1.2363407611846924, Accuracy: 0.6888747884940778\n",
      "Step: 394, Loss: 1.0695573091506958, Accuracy: 0.6892405063291139\n",
      "Step: 395, Loss: 1.002966284751892, Accuracy: 0.6898148148148148\n",
      "Step: 396, Loss: 1.1406186819076538, Accuracy: 0.6899664147774979\n",
      "Step: 397, Loss: 1.1228731870651245, Accuracy: 0.6903266331658291\n",
      "Step: 398, Loss: 1.0225300788879395, Accuracy: 0.6908939014202172\n",
      "Step: 399, Loss: 1.2731152772903442, Accuracy: 0.6908333333333333\n",
      "Step: 400, Loss: 1.1816054582595825, Accuracy: 0.690980881130507\n",
      "Step: 401, Loss: 1.2649306058883667, Accuracy: 0.6909203980099502\n",
      "Step: 402, Loss: 1.368873119354248, Accuracy: 0.6904466501240695\n",
      "Step: 403, Loss: 1.1012165546417236, Accuracy: 0.6908003300330033\n",
      "Step: 404, Loss: 1.2413297891616821, Accuracy: 0.6905349794238683\n",
      "Step: 405, Loss: 1.1921168565750122, Accuracy: 0.6904761904761905\n",
      "Step: 406, Loss: 1.0019283294677734, Accuracy: 0.691031941031941\n",
      "Step: 407, Loss: 1.3150240182876587, Accuracy: 0.6907679738562091\n",
      "Step: 408, Loss: 1.3948639631271362, Accuracy: 0.6903015484922576\n",
      "Step: 409, Loss: 1.1768003702163696, Accuracy: 0.6904471544715447\n",
      "Step: 410, Loss: 1.1480506658554077, Accuracy: 0.6905920519059205\n",
      "Step: 411, Loss: 1.1486138105392456, Accuracy: 0.6907362459546925\n",
      "Step: 412, Loss: 1.2824443578720093, Accuracy: 0.6904761904761905\n",
      "Step: 413, Loss: 1.229238748550415, Accuracy: 0.6904186795491143\n",
      "Step: 414, Loss: 1.332340121269226, Accuracy: 0.6901606425702811\n",
      "Step: 415, Loss: 1.1512521505355835, Accuracy: 0.6903044871794872\n",
      "Step: 416, Loss: 1.262984275817871, Accuracy: 0.6900479616306955\n",
      "Step: 417, Loss: 1.226739525794983, Accuracy: 0.6899920255183413\n",
      "Step: 418, Loss: 1.0141128301620483, Accuracy: 0.690533015115354\n",
      "Step: 419, Loss: 1.2340182065963745, Accuracy: 0.6904761904761905\n",
      "Step: 420, Loss: 1.161606788635254, Accuracy: 0.6906175771971497\n",
      "Step: 421, Loss: 1.2905045747756958, Accuracy: 0.6903633491311216\n",
      "Step: 422, Loss: 1.151728868484497, Accuracy: 0.6905043341213554\n",
      "Step: 423, Loss: 1.1314970254898071, Accuracy: 0.6908411949685535\n",
      "Step: 424, Loss: 1.2021534442901611, Accuracy: 0.6909803921568628\n",
      "Step: 425, Loss: 1.1686136722564697, Accuracy: 0.6911189358372457\n",
      "Step: 426, Loss: 1.2421156167984009, Accuracy: 0.6910616705698673\n",
      "Step: 427, Loss: 1.3340444564819336, Accuracy: 0.690809968847352\n",
      "Step: 428, Loss: 1.3171030282974243, Accuracy: 0.6905594405594405\n",
      "Step: 429, Loss: 1.410598874092102, Accuracy: 0.6901162790697675\n",
      "Step: 430, Loss: 1.2464016675949097, Accuracy: 0.690061871616396\n",
      "Step: 431, Loss: 1.1720727682113647, Accuracy: 0.6902006172839507\n",
      "Step: 432, Loss: 1.2326523065567017, Accuracy: 0.6901462663587374\n",
      "Step: 433, Loss: 1.2976105213165283, Accuracy: 0.689900153609831\n",
      "Step: 434, Loss: 1.0908926725387573, Accuracy: 0.6902298850574713\n",
      "Step: 435, Loss: 1.1453769207000732, Accuracy: 0.6903669724770642\n",
      "Step: 436, Loss: 1.233370304107666, Accuracy: 0.6901220442410374\n",
      "Step: 437, Loss: 1.0133627653121948, Accuracy: 0.6906392694063926\n",
      "Step: 438, Loss: 1.449135661125183, Accuracy: 0.6900151860288535\n",
      "Step: 439, Loss: 1.2536994218826294, Accuracy: 0.6899621212121212\n",
      "Step: 440, Loss: 1.1830185651779175, Accuracy: 0.690098261526833\n",
      "Step: 441, Loss: 1.0782793760299683, Accuracy: 0.6904223227752639\n",
      "Step: 442, Loss: 1.2746882438659668, Accuracy: 0.6901805869074492\n",
      "Step: 443, Loss: 1.3071045875549316, Accuracy: 0.68993993993994\n",
      "Step: 444, Loss: 1.1451921463012695, Accuracy: 0.6900749063670412\n",
      "Step: 445, Loss: 1.2553213834762573, Accuracy: 0.6900224215246636\n",
      "Step: 446, Loss: 1.1037272214889526, Accuracy: 0.6903430275913497\n",
      "Step: 447, Loss: 0.9396523833274841, Accuracy: 0.6910342261904762\n",
      "Step: 448, Loss: 1.0662944316864014, Accuracy: 0.691351150705271\n",
      "Step: 449, Loss: 1.444675087928772, Accuracy: 0.6907407407407408\n",
      "Step: 450, Loss: 1.2245992422103882, Accuracy: 0.6906873614190687\n",
      "Step: 451, Loss: 1.1268097162246704, Accuracy: 0.6908185840707964\n",
      "Step: 452, Loss: 1.1433563232421875, Accuracy: 0.6909492273730684\n",
      "Step: 453, Loss: 1.4640628099441528, Accuracy: 0.6903450807635829\n",
      "Step: 454, Loss: 1.3555755615234375, Accuracy: 0.6901098901098901\n",
      "Step: 455, Loss: 1.147958755493164, Accuracy: 0.6902412280701754\n",
      "Step: 456, Loss: 1.1562389135360718, Accuracy: 0.6903719912472648\n",
      "Step: 457, Loss: 1.1180392503738403, Accuracy: 0.6905021834061136\n",
      "Step: 458, Loss: 0.9562501907348633, Accuracy: 0.6911764705882353\n",
      "Step: 459, Loss: 1.2179468870162964, Accuracy: 0.6911231884057971\n",
      "Step: 460, Loss: 1.091461420059204, Accuracy: 0.6914316702819957\n",
      "Step: 461, Loss: 1.0614644289016724, Accuracy: 0.6917388167388168\n",
      "Step: 462, Loss: 1.0887142419815063, Accuracy: 0.6920446364290856\n",
      "Step: 463, Loss: 1.451195240020752, Accuracy: 0.6914511494252874\n",
      "Step: 464, Loss: 1.2223511934280396, Accuracy: 0.6915770609318996\n",
      "Step: 465, Loss: 1.244269609451294, Accuracy: 0.6915236051502146\n",
      "Step: 466, Loss: 1.2936267852783203, Accuracy: 0.6912919343326196\n",
      "Step: 467, Loss: 1.163927435874939, Accuracy: 0.6914173789173789\n",
      "Step: 468, Loss: 1.2298325300216675, Accuracy: 0.69136460554371\n",
      "Step: 469, Loss: 1.1511857509613037, Accuracy: 0.6913120567375887\n",
      "Step: 470, Loss: 1.3669356107711792, Accuracy: 0.6910828025477707\n",
      "Step: 471, Loss: 1.2344495058059692, Accuracy: 0.6910310734463276\n",
      "Step: 472, Loss: 1.3557485342025757, Accuracy: 0.6906272022551092\n",
      "Step: 473, Loss: 1.1545418500900269, Accuracy: 0.6907524613220816\n",
      "Step: 474, Loss: 1.149162769317627, Accuracy: 0.6908771929824561\n",
      "Step: 475, Loss: 1.4877632856369019, Accuracy: 0.6903011204481793\n",
      "Step: 476, Loss: 1.093772053718567, Accuracy: 0.6906009783368274\n",
      "Step: 477, Loss: 1.2869271039962769, Accuracy: 0.6903765690376569\n",
      "Step: 478, Loss: 1.330593466758728, Accuracy: 0.6901530967292971\n",
      "Step: 479, Loss: 1.4364904165267944, Accuracy: 0.6897569444444445\n",
      "Step: 480, Loss: 1.2622723579406738, Accuracy: 0.6897089397089398\n",
      "Step: 481, Loss: 1.026769995689392, Accuracy: 0.6900069156293223\n",
      "Step: 482, Loss: 1.3408231735229492, Accuracy: 0.6896135265700483\n",
      "Step: 483, Loss: 1.3316823244094849, Accuracy: 0.6893939393939394\n",
      "Step: 484, Loss: 1.3257356882095337, Accuracy: 0.6891752577319588\n",
      "Step: 485, Loss: 1.313272476196289, Accuracy: 0.6889574759945131\n",
      "Step: 486, Loss: 1.2412548065185547, Accuracy: 0.688911704312115\n",
      "Step: 487, Loss: 1.2509137392044067, Accuracy: 0.6888661202185792\n",
      "Step: 488, Loss: 1.0481594800949097, Accuracy: 0.689161554192229\n",
      "Step: 489, Loss: 1.2244333028793335, Accuracy: 0.6891156462585034\n",
      "Step: 490, Loss: 1.3880316019058228, Accuracy: 0.6887304820095044\n",
      "Step: 491, Loss: 1.1450358629226685, Accuracy: 0.6888550135501355\n",
      "Step: 492, Loss: 1.2389068603515625, Accuracy: 0.6886409736308317\n",
      "Step: 493, Loss: 1.2503050565719604, Accuracy: 0.6884278002699056\n",
      "Step: 494, Loss: 1.174444317817688, Accuracy: 0.6885521885521886\n",
      "Step: 495, Loss: 1.0991986989974976, Accuracy: 0.6888440860215054\n",
      "Step: 496, Loss: 1.0428038835525513, Accuracy: 0.6893024815560027\n",
      "Step: 497, Loss: 1.1846922636032104, Accuracy: 0.6894243641231593\n",
      "Step: 498, Loss: 1.069994568824768, Accuracy: 0.6897127588510354\n",
      "Step: 499, Loss: 1.2009055614471436, Accuracy: 0.6896666666666667\n",
      "Step: 500, Loss: 1.139274001121521, Accuracy: 0.6897870924817032\n",
      "Step: 501, Loss: 1.017924427986145, Accuracy: 0.6902390438247012\n",
      "Step: 502, Loss: 1.2132474184036255, Accuracy: 0.6901921802518224\n",
      "Step: 503, Loss: 1.2296404838562012, Accuracy: 0.6901455026455027\n",
      "Step: 504, Loss: 1.1000932455062866, Accuracy: 0.6904290429042904\n",
      "Step: 505, Loss: 1.080354928970337, Accuracy: 0.6907114624505929\n",
      "Step: 506, Loss: 1.411697268486023, Accuracy: 0.6903353057199211\n",
      "Step: 507, Loss: 1.2332757711410522, Accuracy: 0.6902887139107612\n",
      "Step: 508, Loss: 1.2709286212921143, Accuracy: 0.6902423051735429\n",
      "Step: 509, Loss: 1.2214950323104858, Accuracy: 0.6901960784313725\n",
      "Step: 510, Loss: 1.1552284955978394, Accuracy: 0.6903131115459883\n",
      "Step: 511, Loss: 1.1727863550186157, Accuracy: 0.6904296875\n",
      "Step: 512, Loss: 1.2698708772659302, Accuracy: 0.690220922677063\n",
      "Step: 513, Loss: 1.105407953262329, Accuracy: 0.6904993514915694\n",
      "Step: 514, Loss: 1.0446897745132446, Accuracy: 0.6909385113268608\n",
      "Step: 515, Loss: 1.0090399980545044, Accuracy: 0.6913759689922481\n",
      "Step: 516, Loss: 1.3172096014022827, Accuracy: 0.6911669890393295\n",
      "Step: 517, Loss: 1.0383940935134888, Accuracy: 0.6916023166023166\n",
      "Step: 518, Loss: 1.4522333145141602, Accuracy: 0.691072575465639\n",
      "Step: 519, Loss: 1.1488879919052124, Accuracy: 0.6911858974358974\n",
      "Step: 520, Loss: 1.1053365468978882, Accuracy: 0.6914587332053743\n",
      "Step: 521, Loss: 1.4649595022201538, Accuracy: 0.6909323116219668\n",
      "Step: 522, Loss: 1.5545568466186523, Accuracy: 0.690089228808158\n",
      "Step: 523, Loss: 1.0669118165969849, Accuracy: 0.6905216284987278\n",
      "Step: 524, Loss: 1.222357153892517, Accuracy: 0.6904761904761905\n",
      "Step: 525, Loss: 1.011250615119934, Accuracy: 0.6909062103929025\n",
      "Step: 526, Loss: 1.2074633836746216, Accuracy: 0.6908602150537635\n",
      "Step: 527, Loss: 1.1507371664047241, Accuracy: 0.6909722222222222\n",
      "Step: 528, Loss: 1.13866126537323, Accuracy: 0.6912413358538122\n",
      "Step: 529, Loss: 1.2125593423843384, Accuracy: 0.6911949685534591\n",
      "Step: 530, Loss: 1.2526878118515015, Accuracy: 0.6911487758945386\n",
      "Step: 531, Loss: 1.0900723934173584, Accuracy: 0.6914160401002506\n",
      "Step: 532, Loss: 1.3045014142990112, Accuracy: 0.691213258286429\n",
      "Step: 533, Loss: 1.1178596019744873, Accuracy: 0.6914794007490637\n",
      "Step: 534, Loss: 1.169575810432434, Accuracy: 0.6915887850467289\n",
      "Step: 535, Loss: 1.2567507028579712, Accuracy: 0.691386815920398\n",
      "Step: 536, Loss: 1.1430500745773315, Accuracy: 0.691495965238982\n",
      "Step: 537, Loss: 1.3940025568008423, Accuracy: 0.6911400247831475\n",
      "Step: 538, Loss: 1.0733171701431274, Accuracy: 0.6914038342609771\n",
      "Step: 539, Loss: 1.2319647073745728, Accuracy: 0.691358024691358\n",
      "Step: 540, Loss: 1.0845316648483276, Accuracy: 0.6916204559457794\n",
      "Step: 541, Loss: 1.1928811073303223, Accuracy: 0.6915744157441575\n",
      "Step: 542, Loss: 1.1766413450241089, Accuracy: 0.691682013505218\n",
      "Step: 543, Loss: 1.09571373462677, Accuracy: 0.6919424019607843\n",
      "Step: 544, Loss: 1.2111667394638062, Accuracy: 0.6918960244648318\n",
      "Step: 545, Loss: 1.1712340116500854, Accuracy: 0.6918498168498168\n",
      "Step: 546, Loss: 1.228980541229248, Accuracy: 0.6918037781840342\n",
      "Step: 547, Loss: 1.10979163646698, Accuracy: 0.6920620437956204\n",
      "Step: 548, Loss: 1.3642302751541138, Accuracy: 0.691712204007286\n",
      "Step: 549, Loss: 1.1401010751724243, Accuracy: 0.691969696969697\n",
      "Step: 550, Loss: 1.1024795770645142, Accuracy: 0.692226255293406\n",
      "Step: 551, Loss: 1.131096363067627, Accuracy: 0.6923309178743962\n",
      "Step: 552, Loss: 1.479049563407898, Accuracy: 0.6918324291742013\n",
      "Step: 553, Loss: 1.243252158164978, Accuracy: 0.6917870036101083\n",
      "Step: 554, Loss: 1.152966856956482, Accuracy: 0.6918918918918919\n",
      "Step: 555, Loss: 1.3039602041244507, Accuracy: 0.6916966426858513\n",
      "Step: 556, Loss: 1.115873098373413, Accuracy: 0.6918013165769\n",
      "Step: 557, Loss: 1.2634035348892212, Accuracy: 0.6917562724014337\n",
      "Step: 558, Loss: 1.1755809783935547, Accuracy: 0.6918604651162791\n",
      "Step: 559, Loss: 1.1451963186264038, Accuracy: 0.6919642857142857\n",
      "Step: 560, Loss: 1.1356102228164673, Accuracy: 0.6920677361853832\n",
      "Step: 561, Loss: 1.1451832056045532, Accuracy: 0.6921708185053381\n",
      "Step: 562, Loss: 1.2171379327774048, Accuracy: 0.6921255180580225\n",
      "Step: 563, Loss: 1.2407238483428955, Accuracy: 0.692080378250591\n",
      "Step: 564, Loss: 1.0996698141098022, Accuracy: 0.692330383480826\n",
      "Step: 565, Loss: 1.0224400758743286, Accuracy: 0.6927267373380448\n",
      "Step: 566, Loss: 1.0890687704086304, Accuracy: 0.6929747207524986\n",
      "Step: 567, Loss: 1.3008052110671997, Accuracy: 0.6927816901408451\n",
      "Step: 568, Loss: 1.236189842224121, Accuracy: 0.6927357937902754\n",
      "Step: 569, Loss: 1.1605594158172607, Accuracy: 0.6928362573099415\n",
      "Step: 570, Loss: 1.2326419353485107, Accuracy: 0.6927904261529481\n",
      "Step: 571, Loss: 1.1719235181808472, Accuracy: 0.6928904428904429\n",
      "Step: 572, Loss: 1.3298197984695435, Accuracy: 0.6926992437463642\n",
      "Step: 573, Loss: 1.2675788402557373, Accuracy: 0.6926538908246226\n",
      "Step: 574, Loss: 1.0514119863510132, Accuracy: 0.6928985507246377\n",
      "Step: 575, Loss: 1.2889901399612427, Accuracy: 0.6927083333333334\n",
      "Step: 576, Loss: 1.1552459001541138, Accuracy: 0.6928076256499134\n",
      "Step: 577, Loss: 1.0228632688522339, Accuracy: 0.6931949250288351\n",
      "Step: 578, Loss: 1.2117348909378052, Accuracy: 0.6931491076568796\n",
      "Step: 579, Loss: 1.192742943763733, Accuracy: 0.6932471264367817\n",
      "Step: 580, Loss: 1.1683729887008667, Accuracy: 0.6933448078026391\n",
      "Step: 581, Loss: 1.1985667943954468, Accuracy: 0.6932989690721649\n",
      "Step: 582, Loss: 1.207290530204773, Accuracy: 0.6932532875929103\n",
      "Step: 583, Loss: 1.0501426458358765, Accuracy: 0.6936358447488584\n",
      "Step: 584, Loss: 1.1636707782745361, Accuracy: 0.6937321937321937\n",
      "Step: 585, Loss: 1.2569313049316406, Accuracy: 0.6936860068259386\n",
      "Step: 586, Loss: 0.9987444877624512, Accuracy: 0.6940658716638274\n",
      "Step: 587, Loss: 1.398671269416809, Accuracy: 0.6937358276643991\n",
      "Step: 588, Loss: 1.1008177995681763, Accuracy: 0.6938313525749858\n",
      "Step: 589, Loss: 1.1961671113967896, Accuracy: 0.6939265536723164\n",
      "Step: 590, Loss: 1.1487412452697754, Accuracy: 0.6940214326001128\n",
      "Step: 591, Loss: 1.2510020732879639, Accuracy: 0.6939752252252253\n",
      "Step: 592, Loss: 1.3222485780715942, Accuracy: 0.6937886453063519\n",
      "Step: 593, Loss: 1.1897590160369873, Accuracy: 0.6937429854096521\n",
      "Step: 594, Loss: 1.2247966527938843, Accuracy: 0.6936974789915966\n",
      "Step: 595, Loss: 1.1047841310501099, Accuracy: 0.6937919463087249\n",
      "Step: 596, Loss: 1.1884112358093262, Accuracy: 0.6937465103294249\n",
      "Step: 597, Loss: 1.3728766441345215, Accuracy: 0.693422519509476\n",
      "Step: 598, Loss: 1.2836085557937622, Accuracy: 0.6932387312186978\n",
      "Step: 599, Loss: 1.2119370698928833, Accuracy: 0.6931944444444444\n",
      "Step: 600, Loss: 1.0867900848388672, Accuracy: 0.6934276206322796\n",
      "Step: 601, Loss: 1.2355161905288696, Accuracy: 0.6933831672203765\n",
      "Step: 602, Loss: 1.367169976234436, Accuracy: 0.6930624654505252\n",
      "Step: 603, Loss: 1.5130332708358765, Accuracy: 0.6924668874172185\n",
      "Step: 604, Loss: 1.189245343208313, Accuracy: 0.6925619834710743\n",
      "Step: 605, Loss: 1.1478886604309082, Accuracy: 0.6925192519251925\n",
      "Step: 606, Loss: 1.0785483121871948, Accuracy: 0.6927512355848435\n",
      "Step: 607, Loss: 1.4566456079483032, Accuracy: 0.692297149122807\n",
      "Step: 608, Loss: 1.2085610628128052, Accuracy: 0.692391899288451\n",
      "Step: 609, Loss: 1.1612420082092285, Accuracy: 0.6926229508196722\n",
      "Step: 610, Loss: 1.1752305030822754, Accuracy: 0.6927168576104746\n",
      "Step: 611, Loss: 1.072528600692749, Accuracy: 0.6929466230936819\n",
      "Step: 612, Loss: 1.215966820716858, Accuracy: 0.6929037520391517\n",
      "Step: 613, Loss: 1.2674938440322876, Accuracy: 0.6928610206297503\n",
      "Step: 614, Loss: 1.3280507326126099, Accuracy: 0.6926829268292682\n",
      "Step: 615, Loss: 1.1479967832565308, Accuracy: 0.6929112554112554\n",
      "Step: 616, Loss: 1.3051214218139648, Accuracy: 0.692868719611021\n",
      "Step: 617, Loss: 1.3117936849594116, Accuracy: 0.6926914778856527\n",
      "Step: 618, Loss: 1.210317850112915, Accuracy: 0.6926494345718901\n",
      "Step: 619, Loss: 1.2736843824386597, Accuracy: 0.6924731182795699\n",
      "Step: 620, Loss: 1.3947664499282837, Accuracy: 0.6922973698336017\n",
      "Step: 621, Loss: 1.1658035516738892, Accuracy: 0.692524115755627\n",
      "Step: 622, Loss: 1.1537843942642212, Accuracy: 0.6926163723916533\n",
      "Step: 623, Loss: 1.4281773567199707, Accuracy: 0.6921741452991453\n",
      "Step: 624, Loss: 1.2015585899353027, Accuracy: 0.692\n",
      "Step: 625, Loss: 1.2879854440689087, Accuracy: 0.6918264110756124\n",
      "Step: 626, Loss: 1.2031437158584595, Accuracy: 0.6917862838915471\n",
      "Step: 627, Loss: 1.1671122312545776, Accuracy: 0.6918789808917197\n",
      "Step: 628, Loss: 1.190299391746521, Accuracy: 0.6918388977212506\n",
      "Step: 629, Loss: 1.2316824197769165, Accuracy: 0.6917989417989417\n",
      "Step: 630, Loss: 1.2138959169387817, Accuracy: 0.6918911780243\n",
      "Step: 631, Loss: 1.0531530380249023, Accuracy: 0.6921149789029536\n",
      "Step: 632, Loss: 1.1541197299957275, Accuracy: 0.6922064244339126\n",
      "Step: 633, Loss: 1.0674527883529663, Accuracy: 0.692429022082019\n",
      "Step: 634, Loss: 1.174865961074829, Accuracy: 0.6925196850393701\n",
      "Step: 635, Loss: 1.0963305234909058, Accuracy: 0.6927410901467506\n",
      "Step: 636, Loss: 1.1675456762313843, Accuracy: 0.6927001569858713\n",
      "Step: 637, Loss: 1.1679195165634155, Accuracy: 0.6927899686520376\n",
      "Step: 638, Loss: 1.0314973592758179, Accuracy: 0.6931403234220136\n",
      "Step: 639, Loss: 1.1163450479507446, Accuracy: 0.693359375\n",
      "Step: 640, Loss: 1.1774396896362305, Accuracy: 0.6934477379095164\n",
      "Step: 641, Loss: 1.0436272621154785, Accuracy: 0.6937954309449637\n",
      "Step: 642, Loss: 1.4061594009399414, Accuracy: 0.6934940383618455\n",
      "Step: 643, Loss: 1.1630817651748657, Accuracy: 0.6935817805383023\n",
      "Step: 644, Loss: 1.184812068939209, Accuracy: 0.6936692506459948\n",
      "Step: 645, Loss: 1.3818016052246094, Accuracy: 0.6934984520123839\n",
      "Step: 646, Loss: 1.2101854085922241, Accuracy: 0.6935857805255023\n",
      "Step: 647, Loss: 1.0720536708831787, Accuracy: 0.6938014403292181\n",
      "Step: 648, Loss: 1.1180959939956665, Accuracy: 0.6940164355418593\n",
      "Step: 649, Loss: 1.2399845123291016, Accuracy: 0.6941025641025641\n",
      "Step: 650, Loss: 1.310894250869751, Accuracy: 0.6939324116743472\n",
      "Step: 651, Loss: 1.1244837045669556, Accuracy: 0.6941462167689162\n",
      "Step: 652, Loss: 1.1636818647384644, Accuracy: 0.6942317508933129\n",
      "Step: 653, Loss: 1.322890281677246, Accuracy: 0.6939347604485219\n",
      "Step: 654, Loss: 1.3083875179290771, Accuracy: 0.693765903307888\n",
      "Step: 655, Loss: 1.2350027561187744, Accuracy: 0.693724593495935\n",
      "Step: 656, Loss: 1.0808134078979492, Accuracy: 0.6939370877727042\n",
      "Step: 657, Loss: 1.2113250494003296, Accuracy: 0.6938956433637284\n",
      "Step: 658, Loss: 0.9727486968040466, Accuracy: 0.6942336874051593\n",
      "Step: 659, Loss: 1.2383074760437012, Accuracy: 0.6941919191919191\n",
      "Step: 660, Loss: 1.3047887086868286, Accuracy: 0.6940242057488654\n",
      "Step: 661, Loss: 1.2645076513290405, Accuracy: 0.6938569989929506\n",
      "Step: 662, Loss: 1.1840739250183105, Accuracy: 0.6939416792357969\n",
      "Step: 663, Loss: 1.6075711250305176, Accuracy: 0.6932730923694779\n",
      "Step: 664, Loss: 1.4546798467636108, Accuracy: 0.6928571428571428\n",
      "Step: 665, Loss: 1.0804640054702759, Accuracy: 0.6930680680680681\n",
      "Step: 666, Loss: 0.9572155475616455, Accuracy: 0.6935282358820589\n",
      "Step: 667, Loss: 1.1846190690994263, Accuracy: 0.6934880239520959\n",
      "Step: 668, Loss: 1.4440044164657593, Accuracy: 0.693074240159442\n",
      "Step: 669, Loss: 1.4487628936767578, Accuracy: 0.6926616915422885\n",
      "Step: 670, Loss: 1.0242109298706055, Accuracy: 0.6929955290611028\n",
      "Step: 671, Loss: 1.0831702947616577, Accuracy: 0.6932043650793651\n",
      "Step: 672, Loss: 1.306887149810791, Accuracy: 0.6930411094601288\n",
      "Step: 673, Loss: 1.2840757369995117, Accuracy: 0.6928783382789317\n",
      "Step: 674, Loss: 1.1582322120666504, Accuracy: 0.6928395061728395\n",
      "Step: 675, Loss: 1.0869507789611816, Accuracy: 0.6930473372781065\n",
      "Step: 676, Loss: 0.9999541640281677, Accuracy: 0.6933776464795667\n",
      "Step: 677, Loss: 1.0703550577163696, Accuracy: 0.6935840707964602\n",
      "Step: 678, Loss: 1.0136114358901978, Accuracy: 0.6939126165930289\n",
      "Step: 679, Loss: 1.0714359283447266, Accuracy: 0.6941176470588235\n",
      "Step: 680, Loss: 1.1406047344207764, Accuracy: 0.6941997063142438\n",
      "Step: 681, Loss: 1.172652006149292, Accuracy: 0.6942815249266863\n",
      "Step: 682, Loss: 1.0441755056381226, Accuracy: 0.6944851146900928\n",
      "Step: 683, Loss: 1.2607052326202393, Accuracy: 0.6944444444444444\n",
      "Step: 684, Loss: 1.128487467765808, Accuracy: 0.694647201946472\n",
      "Step: 685, Loss: 1.235158920288086, Accuracy: 0.6946064139941691\n",
      "Step: 686, Loss: 1.1712443828582764, Accuracy: 0.6948083454633673\n",
      "Step: 687, Loss: 1.0006731748580933, Accuracy: 0.6951308139534884\n",
      "Step: 688, Loss: 1.0156441926956177, Accuracy: 0.6954523463957426\n",
      "Step: 689, Loss: 1.246451497077942, Accuracy: 0.6954106280193236\n",
      "Step: 690, Loss: 1.2766658067703247, Accuracy: 0.6952484322238301\n",
      "Step: 691, Loss: 1.2320996522903442, Accuracy: 0.6952071290944123\n",
      "Step: 692, Loss: 0.9563290476799011, Accuracy: 0.6956469456469456\n",
      "Step: 693, Loss: 1.1808534860610962, Accuracy: 0.6956051873198847\n",
      "Step: 694, Loss: 1.548189640045166, Accuracy: 0.6950839328537171\n",
      "Step: 695, Loss: 1.1164343357086182, Accuracy: 0.6951628352490421\n",
      "Step: 696, Loss: 1.2286243438720703, Accuracy: 0.6951219512195121\n",
      "Step: 697, Loss: 1.4568344354629517, Accuracy: 0.6947230181470869\n",
      "Step: 698, Loss: 1.1619433164596558, Accuracy: 0.6948020982355746\n",
      "Step: 699, Loss: 1.0002497434616089, Accuracy: 0.6951190476190476\n",
      "Step: 700, Loss: 1.1493922472000122, Accuracy: 0.6951973371374227\n",
      "Step: 701, Loss: 1.2370611429214478, Accuracy: 0.6951566951566952\n",
      "Step: 702, Loss: 1.2340837717056274, Accuracy: 0.6951161688003793\n",
      "Step: 703, Loss: 1.3342400789260864, Accuracy: 0.6949573863636364\n",
      "Step: 704, Loss: 1.2965024709701538, Accuracy: 0.6947990543735224\n",
      "Step: 705, Loss: 1.4614548683166504, Accuracy: 0.6944050991501416\n",
      "Step: 706, Loss: 1.0854095220565796, Accuracy: 0.6946016030174446\n",
      "Step: 707, Loss: 1.2525571584701538, Accuracy: 0.6945621468926554\n",
      "Step: 708, Loss: 1.1983715295791626, Accuracy: 0.6945228020686413\n",
      "Step: 709, Loss: 1.1190478801727295, Accuracy: 0.694718309859155\n",
      "Step: 710, Loss: 1.0754214525222778, Accuracy: 0.6949132676980778\n",
      "Step: 711, Loss: 1.3068512678146362, Accuracy: 0.6946395131086143\n",
      "Step: 712, Loss: 1.2762507200241089, Accuracy: 0.6944834034595605\n",
      "Step: 713, Loss: 1.1665254831314087, Accuracy: 0.6945611577964519\n",
      "Step: 714, Loss: 1.2514747381210327, Accuracy: 0.6945221445221446\n",
      "Step: 715, Loss: 1.146038293838501, Accuracy: 0.6945996275605214\n",
      "Step: 716, Loss: 1.2771340608596802, Accuracy: 0.694560669456067\n",
      "Step: 717, Loss: 1.2292755842208862, Accuracy: 0.6945218198700093\n",
      "Step: 718, Loss: 1.181764841079712, Accuracy: 0.6944830783495596\n",
      "Step: 719, Loss: 1.314448356628418, Accuracy: 0.6943287037037037\n",
      "Step: 720, Loss: 1.4368189573287964, Accuracy: 0.6939435968562182\n",
      "Step: 721, Loss: 1.15858793258667, Accuracy: 0.6940212373037857\n",
      "Step: 722, Loss: 1.1695719957351685, Accuracy: 0.694098662978331\n",
      "Step: 723, Loss: 1.5359028577804565, Accuracy: 0.6936003683241252\n",
      "Step: 724, Loss: 1.2087273597717285, Accuracy: 0.6935632183908046\n",
      "Step: 725, Loss: 1.203048586845398, Accuracy: 0.6935261707988981\n",
      "Step: 726, Loss: 1.2268744707107544, Accuracy: 0.6934892251260889\n",
      "Step: 727, Loss: 1.0992578268051147, Accuracy: 0.6936813186813187\n",
      "Step: 728, Loss: 1.1381999254226685, Accuracy: 0.693758573388203\n",
      "Step: 729, Loss: 1.3358527421951294, Accuracy: 0.693607305936073\n",
      "Step: 730, Loss: 1.0663145780563354, Accuracy: 0.6937984496124031\n",
      "Step: 731, Loss: 1.1670043468475342, Accuracy: 0.6937613843351548\n",
      "Step: 732, Loss: 1.2691805362701416, Accuracy: 0.6937244201909959\n",
      "Step: 733, Loss: 1.2306228876113892, Accuracy: 0.6936875567665758\n",
      "Step: 734, Loss: 1.1245697736740112, Accuracy: 0.693764172335601\n",
      "Step: 735, Loss: 1.1432452201843262, Accuracy: 0.6938405797101449\n",
      "Step: 736, Loss: 1.153963565826416, Accuracy: 0.6939167797376753\n",
      "Step: 737, Loss: 1.0426169633865356, Accuracy: 0.6941056910569106\n",
      "Step: 738, Loss: 1.3142927885055542, Accuracy: 0.6939557961208841\n",
      "Step: 739, Loss: 1.1510604619979858, Accuracy: 0.6940315315315315\n",
      "Step: 740, Loss: 1.183281421661377, Accuracy: 0.6941070625281152\n",
      "Step: 741, Loss: 1.1635594367980957, Accuracy: 0.6941823899371069\n",
      "Step: 742, Loss: 1.1362656354904175, Accuracy: 0.6943696724988784\n",
      "Step: 743, Loss: 1.2327706813812256, Accuracy: 0.6943324372759857\n",
      "Step: 744, Loss: 1.38179349899292, Accuracy: 0.6940715883668904\n",
      "Step: 745, Loss: 1.0796754360198975, Accuracy: 0.6942582663092046\n",
      "Step: 746, Loss: 1.2375961542129517, Accuracy: 0.6942213297634985\n",
      "Step: 747, Loss: 1.148510217666626, Accuracy: 0.6942959001782532\n",
      "Step: 748, Loss: 1.0637565851211548, Accuracy: 0.6944815309301291\n",
      "Step: 749, Loss: 1.0649219751358032, Accuracy: 0.6946666666666667\n",
      "Step: 750, Loss: 1.2508100271224976, Accuracy: 0.6946293830448291\n",
      "Step: 751, Loss: 1.095652461051941, Accuracy: 0.694813829787234\n",
      "Step: 752, Loss: 1.2261205911636353, Accuracy: 0.6947764497565294\n",
      "Step: 753, Loss: 1.2558361291885376, Accuracy: 0.6947391688771\n",
      "Step: 754, Loss: 1.080846905708313, Accuracy: 0.6949227373068433\n",
      "Step: 755, Loss: 1.132870078086853, Accuracy: 0.6949955908289241\n",
      "Step: 756, Loss: 1.3539390563964844, Accuracy: 0.6947380008806693\n",
      "Step: 757, Loss: 1.0983805656433105, Accuracy: 0.6949208443271768\n",
      "Step: 758, Loss: 1.2195698022842407, Accuracy: 0.6949934123847167\n",
      "Step: 759, Loss: 0.963111400604248, Accuracy: 0.6953947368421053\n",
      "Step: 760, Loss: 1.2320798635482788, Accuracy: 0.6953569864213753\n",
      "Step: 761, Loss: 1.2112687826156616, Accuracy: 0.6953193350831146\n",
      "Step: 762, Loss: 1.2177890539169312, Accuracy: 0.695391000436872\n",
      "Step: 763, Loss: 1.1519172191619873, Accuracy: 0.6954624781849913\n",
      "Step: 764, Loss: 1.1809841394424438, Accuracy: 0.6955337690631809\n",
      "Step: 765, Loss: 0.9970481991767883, Accuracy: 0.695822454308094\n",
      "Step: 766, Loss: 1.1699274778366089, Accuracy: 0.6958930899608866\n",
      "Step: 767, Loss: 1.21625816822052, Accuracy: 0.6958550347222222\n",
      "Step: 768, Loss: 1.024017333984375, Accuracy: 0.6961421759861292\n",
      "Step: 769, Loss: 1.0923407077789307, Accuracy: 0.6963203463203463\n",
      "Step: 770, Loss: 1.0104399919509888, Accuracy: 0.6966061392131431\n",
      "Step: 771, Loss: 1.2277792692184448, Accuracy: 0.6965673575129534\n",
      "Step: 772, Loss: 0.9997031092643738, Accuracy: 0.696852091418715\n",
      "Step: 773, Loss: 1.1089738607406616, Accuracy: 0.6969207579672696\n",
      "Step: 774, Loss: 1.3088289499282837, Accuracy: 0.6967741935483871\n",
      "Step: 775, Loss: 1.2040526866912842, Accuracy: 0.6967353951890034\n",
      "Step: 776, Loss: 0.9993734359741211, Accuracy: 0.6970184470184471\n",
      "Step: 777, Loss: 1.0029650926589966, Accuracy: 0.6973007712082262\n",
      "Step: 778, Loss: 1.3027278184890747, Accuracy: 0.6971544715447154\n",
      "Step: 779, Loss: 1.1995705366134644, Accuracy: 0.6971153846153846\n",
      "Step: 780, Loss: 1.403201699256897, Accuracy: 0.6968629961587708\n",
      "Step: 781, Loss: 1.2990509271621704, Accuracy: 0.6967178175618073\n",
      "Step: 782, Loss: 1.079646110534668, Accuracy: 0.696892294593444\n",
      "Step: 783, Loss: 1.0916862487792969, Accuracy: 0.6970663265306123\n",
      "Step: 784, Loss: 1.2025481462478638, Accuracy: 0.6970276008492569\n",
      "Step: 785, Loss: 1.0730801820755005, Accuracy: 0.6972010178117048\n",
      "Step: 786, Loss: 1.1575192213058472, Accuracy: 0.6972681067344345\n",
      "Step: 787, Loss: 1.2367514371871948, Accuracy: 0.6972292724196277\n",
      "Step: 788, Loss: 1.0971875190734863, Accuracy: 0.6974017743979721\n",
      "Step: 789, Loss: 1.0725215673446655, Accuracy: 0.6975738396624472\n",
      "Step: 790, Loss: 1.2084637880325317, Accuracy: 0.6975347661188369\n",
      "Step: 791, Loss: 1.347664713859558, Accuracy: 0.6973905723905723\n",
      "Step: 792, Loss: 1.2926013469696045, Accuracy: 0.6973518284993695\n",
      "Step: 793, Loss: 1.1912490129470825, Accuracy: 0.6973131821998321\n",
      "Step: 794, Loss: 0.924757182598114, Accuracy: 0.6976939203354298\n",
      "Step: 795, Loss: 1.3250017166137695, Accuracy: 0.6974455611390284\n",
      "Step: 796, Loss: 0.9895372986793518, Accuracy: 0.6977206189878712\n",
      "Step: 797, Loss: 1.422472357749939, Accuracy: 0.6974728487886382\n",
      "Step: 798, Loss: 1.4229063987731934, Accuracy: 0.6971214017521903\n",
      "Step: 799, Loss: 1.2980247735977173, Accuracy: 0.6969791666666667\n",
      "Step: 800, Loss: 1.2227610349655151, Accuracy: 0.6969413233458177\n",
      "Step: 801, Loss: 1.3155431747436523, Accuracy: 0.6967996674979219\n",
      "Step: 802, Loss: 1.2596293687820435, Accuracy: 0.6967621419676214\n",
      "Step: 803, Loss: 1.1105893850326538, Accuracy: 0.6969320066334992\n",
      "Step: 804, Loss: 1.1554399728775024, Accuracy: 0.6969979296066252\n",
      "Step: 805, Loss: 1.1898466348648071, Accuracy: 0.6969602977667494\n",
      "Step: 806, Loss: 1.156005620956421, Accuracy: 0.6970260223048327\n",
      "Step: 807, Loss: 1.2444514036178589, Accuracy: 0.6969884488448845\n",
      "Step: 808, Loss: 1.3266857862472534, Accuracy: 0.6968479604449939\n",
      "Step: 809, Loss: 1.0260710716247559, Accuracy: 0.6970164609053497\n",
      "Step: 810, Loss: 1.2197307348251343, Accuracy: 0.6969790382244143\n",
      "Step: 811, Loss: 1.1602672338485718, Accuracy: 0.6970443349753694\n",
      "Step: 812, Loss: 1.1531296968460083, Accuracy: 0.697109471094711\n",
      "Step: 813, Loss: 1.1770085096359253, Accuracy: 0.6970720720720721\n",
      "Step: 814, Loss: 1.3236135244369507, Accuracy: 0.6969325153374233\n",
      "Step: 815, Loss: 1.2465136051177979, Accuracy: 0.6968954248366013\n",
      "Step: 816, Loss: 1.065264344215393, Accuracy: 0.697062423500612\n",
      "Step: 817, Loss: 1.2093297243118286, Accuracy: 0.6970252648736757\n",
      "Step: 818, Loss: 1.3367503881454468, Accuracy: 0.6968864468864469\n",
      "Step: 819, Loss: 1.141229510307312, Accuracy: 0.6969512195121951\n",
      "Step: 820, Loss: 1.4210621118545532, Accuracy: 0.6966098254161591\n",
      "Step: 821, Loss: 1.055942416191101, Accuracy: 0.6967761557177615\n",
      "Step: 822, Loss: 1.332980990409851, Accuracy: 0.6966383151073309\n",
      "Step: 823, Loss: 1.268685221672058, Accuracy: 0.6966019417475728\n",
      "Step: 824, Loss: 1.1080509424209595, Accuracy: 0.6967676767676768\n",
      "Step: 825, Loss: 1.3319772481918335, Accuracy: 0.6966303470540759\n",
      "Step: 826, Loss: 1.4951454401016235, Accuracy: 0.6962918178153971\n",
      "Step: 827, Loss: 1.1892377138137817, Accuracy: 0.6963566827697263\n",
      "Step: 828, Loss: 0.9445877075195312, Accuracy: 0.6967229593888219\n",
      "Step: 829, Loss: 1.2305915355682373, Accuracy: 0.6966867469879519\n",
      "Step: 830, Loss: 1.365275502204895, Accuracy: 0.6964500601684718\n",
      "Step: 831, Loss: 1.498682975769043, Accuracy: 0.696113782051282\n",
      "Step: 832, Loss: 1.1668182611465454, Accuracy: 0.6961784713885554\n",
      "Step: 833, Loss: 1.1705964803695679, Accuracy: 0.6962430055955235\n",
      "Step: 834, Loss: 1.2857544422149658, Accuracy: 0.6962075848303393\n",
      "Step: 835, Loss: 1.2226793766021729, Accuracy: 0.6961722488038278\n",
      "Step: 836, Loss: 1.2904510498046875, Accuracy: 0.6961369972122661\n",
      "Step: 837, Loss: 1.150626301765442, Accuracy: 0.6963007159904535\n",
      "Step: 838, Loss: 1.143611192703247, Accuracy: 0.6963647199046484\n",
      "Step: 839, Loss: 1.1761119365692139, Accuracy: 0.6964285714285714\n",
      "Step: 840, Loss: 1.2987468242645264, Accuracy: 0.6962940943321443\n",
      "Step: 841, Loss: 1.2279170751571655, Accuracy: 0.6962589073634204\n",
      "Step: 842, Loss: 1.1976503133773804, Accuracy: 0.6963226571767497\n",
      "Step: 843, Loss: 1.2228258848190308, Accuracy: 0.6963862559241706\n",
      "Step: 844, Loss: 1.1300339698791504, Accuracy: 0.6964497041420118\n",
      "Step: 845, Loss: 1.2321341037750244, Accuracy: 0.696414499605989\n",
      "Step: 846, Loss: 1.3385857343673706, Accuracy: 0.6962809917355371\n",
      "Step: 847, Loss: 1.0785943269729614, Accuracy: 0.6964426100628931\n",
      "Step: 848, Loss: 1.1441174745559692, Accuracy: 0.6966038476639184\n",
      "Step: 849, Loss: 1.005225658416748, Accuracy: 0.6968627450980392\n",
      "Step: 850, Loss: 1.1930286884307861, Accuracy: 0.6969251860556208\n",
      "Step: 851, Loss: 1.1569936275482178, Accuracy: 0.6969874804381847\n",
      "Step: 852, Loss: 1.1634968519210815, Accuracy: 0.6970496287612349\n",
      "Step: 853, Loss: 1.246767520904541, Accuracy: 0.6970140515222483\n",
      "Step: 854, Loss: 1.3862882852554321, Accuracy: 0.6967836257309942\n",
      "Step: 855, Loss: 1.1815391778945923, Accuracy: 0.6968457943925234\n",
      "Step: 856, Loss: 1.0411100387573242, Accuracy: 0.6970050563982886\n",
      "Step: 857, Loss: 1.1332722902297974, Accuracy: 0.697066822066822\n",
      "Step: 858, Loss: 1.3334436416625977, Accuracy: 0.6969344198680636\n",
      "Step: 859, Loss: 1.264346718788147, Accuracy: 0.6968992248062016\n",
      "Step: 860, Loss: 1.2673109769821167, Accuracy: 0.6968641114982579\n",
      "Step: 861, Loss: 1.2948106527328491, Accuracy: 0.6967324052590874\n",
      "Step: 862, Loss: 1.2285833358764648, Accuracy: 0.6966975666280417\n",
      "Step: 863, Loss: 1.0471806526184082, Accuracy: 0.6968557098765432\n",
      "Step: 864, Loss: 1.1638261079788208, Accuracy: 0.6969171483622351\n",
      "Step: 865, Loss: 1.06686270236969, Accuracy: 0.6971709006928406\n",
      "Step: 866, Loss: 1.190364122390747, Accuracy: 0.6972318339100346\n",
      "Step: 867, Loss: 1.2973382472991943, Accuracy: 0.6971006144393241\n",
      "Step: 868, Loss: 1.0763479471206665, Accuracy: 0.6972573839662447\n",
      "Step: 869, Loss: 1.3105411529541016, Accuracy: 0.6971264367816092\n",
      "Step: 870, Loss: 1.4030600786209106, Accuracy: 0.6969001148105626\n",
      "Step: 871, Loss: 1.2468444108963013, Accuracy: 0.6968654434250765\n",
      "Step: 872, Loss: 0.9425327777862549, Accuracy: 0.6972126765941199\n",
      "Step: 873, Loss: 1.482785701751709, Accuracy: 0.6968916857360793\n",
      "Step: 874, Loss: 1.2374354600906372, Accuracy: 0.6968571428571428\n",
      "Step: 875, Loss: 1.3085474967956543, Accuracy: 0.6967275494672754\n",
      "Step: 876, Loss: 1.179967999458313, Accuracy: 0.6966932725199544\n",
      "Step: 877, Loss: 1.2933114767074585, Accuracy: 0.6965641609719059\n",
      "Step: 878, Loss: 1.1676143407821655, Accuracy: 0.6966249525976489\n",
      "Step: 879, Loss: 1.0542486906051636, Accuracy: 0.696780303030303\n",
      "Step: 880, Loss: 1.228143334388733, Accuracy: 0.6967461218312524\n",
      "Step: 881, Loss: 1.2531949281692505, Accuracy: 0.6967120181405896\n",
      "Step: 882, Loss: 1.2688850164413452, Accuracy: 0.6965836164590411\n",
      "Step: 883, Loss: 1.3914870023727417, Accuracy: 0.6963612368024132\n",
      "Step: 884, Loss: 1.2329351902008057, Accuracy: 0.6963276836158192\n",
      "Step: 885, Loss: 1.3884958028793335, Accuracy: 0.6961060948081265\n",
      "Step: 886, Loss: 1.4793481826782227, Accuracy: 0.6957910559939873\n",
      "Step: 887, Loss: 1.090063214302063, Accuracy: 0.6959459459459459\n",
      "Step: 888, Loss: 1.015764832496643, Accuracy: 0.6961942257217848\n",
      "Step: 889, Loss: 1.0886799097061157, Accuracy: 0.6963483146067416\n",
      "Step: 890, Loss: 1.396213412284851, Accuracy: 0.6962214739992518\n",
      "Step: 891, Loss: 1.3229707479476929, Accuracy: 0.6960949177877429\n",
      "Step: 892, Loss: 1.084053874015808, Accuracy: 0.6962486002239642\n",
      "Step: 893, Loss: 1.085463285446167, Accuracy: 0.6964019388516033\n",
      "Step: 894, Loss: 1.0430548191070557, Accuracy: 0.6966480446927374\n",
      "Step: 895, Loss: 1.221669316291809, Accuracy: 0.6966145833333334\n",
      "Step: 896, Loss: 1.1631537675857544, Accuracy: 0.6966740988480119\n",
      "Step: 897, Loss: 1.4734848737716675, Accuracy: 0.696362286562732\n",
      "Step: 898, Loss: 1.2377914190292358, Accuracy: 0.696329254727475\n",
      "Step: 899, Loss: 1.2746587991714478, Accuracy: 0.6962037037037037\n",
      "Step: 900, Loss: 1.240685224533081, Accuracy: 0.696078431372549\n",
      "Step: 901, Loss: 1.1986205577850342, Accuracy: 0.6961382113821138\n",
      "Step: 902, Loss: 1.087725043296814, Accuracy: 0.6961978589885567\n",
      "Step: 903, Loss: 1.152748703956604, Accuracy: 0.6962573746312685\n",
      "Step: 904, Loss: 1.1303938627243042, Accuracy: 0.6964088397790055\n",
      "Step: 905, Loss: 1.4461450576782227, Accuracy: 0.6961000735835173\n",
      "Step: 906, Loss: 0.9088185429573059, Accuracy: 0.6964351341418596\n",
      "Step: 907, Loss: 1.2222832441329956, Accuracy: 0.69640234948605\n",
      "Step: 908, Loss: 1.0627471208572388, Accuracy: 0.6965529886321965\n",
      "Step: 909, Loss: 1.0201226472854614, Accuracy: 0.6967948717948718\n",
      "Step: 910, Loss: 1.0072894096374512, Accuracy: 0.6970362239297475\n",
      "Step: 911, Loss: 1.2010339498519897, Accuracy: 0.6970942982456141\n",
      "Step: 912, Loss: 1.1488888263702393, Accuracy: 0.6971522453450164\n",
      "Step: 913, Loss: 1.2634344100952148, Accuracy: 0.6971188913202042\n",
      "Step: 914, Loss: 0.9708239436149597, Accuracy: 0.6974499089253188\n",
      "Step: 915, Loss: 1.2470930814743042, Accuracy: 0.6974163027656477\n",
      "Step: 916, Loss: 1.1596897840499878, Accuracy: 0.6974736459469284\n",
      "Step: 917, Loss: 1.309648036956787, Accuracy: 0.6973493100944081\n",
      "Step: 918, Loss: 1.1810511350631714, Accuracy: 0.6974066013783098\n",
      "Step: 919, Loss: 1.5279717445373535, Accuracy: 0.6970108695652174\n",
      "Step: 920, Loss: 1.0978405475616455, Accuracy: 0.6971588852696344\n",
      "Step: 921, Loss: 1.2481335401535034, Accuracy: 0.6971258134490239\n",
      "Step: 922, Loss: 1.285385012626648, Accuracy: 0.6970025279884435\n",
      "Step: 923, Loss: 1.3950767517089844, Accuracy: 0.6967893217893217\n",
      "Step: 924, Loss: 1.1669210195541382, Accuracy: 0.6968468468468468\n",
      "Step: 925, Loss: 1.2867599725723267, Accuracy: 0.6968142548596112\n",
      "Step: 926, Loss: 1.0169305801391602, Accuracy: 0.6970514203523912\n",
      "Step: 927, Loss: 1.1543177366256714, Accuracy: 0.6971084770114943\n",
      "Step: 928, Loss: 1.1847431659698486, Accuracy: 0.6971654108360243\n",
      "Step: 929, Loss: 1.0960296392440796, Accuracy: 0.6973118279569892\n",
      "Step: 930, Loss: 1.075007438659668, Accuracy: 0.6974579305406373\n",
      "Step: 931, Loss: 1.2729169130325317, Accuracy: 0.6973354792560801\n",
      "Step: 932, Loss: 1.0390355587005615, Accuracy: 0.6975705609146123\n",
      "Step: 933, Loss: 1.2927364110946655, Accuracy: 0.6974482512491078\n",
      "Step: 934, Loss: 1.2445979118347168, Accuracy: 0.6974153297682709\n",
      "Step: 935, Loss: 1.3049864768981934, Accuracy: 0.6973824786324786\n",
      "Step: 936, Loss: 1.089601755142212, Accuracy: 0.6974386339381003\n",
      "Step: 937, Loss: 1.0806782245635986, Accuracy: 0.6975835110163469\n",
      "Step: 938, Loss: 1.3993395566940308, Accuracy: 0.697373091941782\n",
      "Step: 939, Loss: 1.2726002931594849, Accuracy: 0.6973404255319149\n",
      "Step: 940, Loss: 1.1794406175613403, Accuracy: 0.6973078285511867\n",
      "Step: 941, Loss: 1.4541176557540894, Accuracy: 0.6970099079971691\n",
      "Step: 942, Loss: 1.3169175386428833, Accuracy: 0.6968893601979498\n",
      "Step: 943, Loss: 1.074931025505066, Accuracy: 0.6970338983050848\n",
      "Step: 944, Loss: 1.265809178352356, Accuracy: 0.6969135802469136\n",
      "Step: 945, Loss: 1.2487984895706177, Accuracy: 0.6968816067653277\n",
      "Step: 946, Loss: 1.2268288135528564, Accuracy: 0.6968497008095741\n",
      "Step: 947, Loss: 1.2066909074783325, Accuracy: 0.6969057665260197\n",
      "Step: 948, Loss: 1.145780324935913, Accuracy: 0.6969617140850017\n",
      "Step: 949, Loss: 1.2160084247589111, Accuracy: 0.6969298245614035\n",
      "Step: 950, Loss: 1.2920715808868408, Accuracy: 0.6968980021030494\n",
      "Step: 951, Loss: 1.2387281656265259, Accuracy: 0.6968662464985994\n",
      "Step: 952, Loss: 1.046187162399292, Accuracy: 0.69700944386149\n",
      "Step: 953, Loss: 1.3291341066360474, Accuracy: 0.6968902865129281\n",
      "Step: 954, Loss: 1.1213992834091187, Accuracy: 0.6969458987783596\n",
      "Step: 955, Loss: 1.088680386543274, Accuracy: 0.6970885634588564\n",
      "Step: 956, Loss: 1.112504005432129, Accuracy: 0.6971438523162661\n",
      "Step: 957, Loss: 1.2525194883346558, Accuracy: 0.6971120389700766\n",
      "Step: 958, Loss: 0.9796420931816101, Accuracy: 0.6973409801876955\n",
      "Step: 959, Loss: 0.9940440058708191, Accuracy: 0.6975694444444445\n",
      "Step: 960, Loss: 1.135361909866333, Accuracy: 0.6976240027748872\n",
      "Step: 961, Loss: 1.0725922584533691, Accuracy: 0.6977650727650727\n",
      "Step: 962, Loss: 1.3999518156051636, Accuracy: 0.6975597092419522\n",
      "Step: 963, Loss: 1.0292035341262817, Accuracy: 0.6977869986168741\n",
      "Step: 964, Loss: 1.3935333490371704, Accuracy: 0.6975820379965457\n",
      "Step: 965, Loss: 1.131585955619812, Accuracy: 0.6976363008971704\n",
      "Step: 966, Loss: 0.9270627498626709, Accuracy: 0.6979489831092727\n",
      "Step: 967, Loss: 1.2990185022354126, Accuracy: 0.6978305785123967\n",
      "Step: 968, Loss: 1.3478831052780151, Accuracy: 0.6977124183006536\n",
      "Step: 969, Loss: 1.1986050605773926, Accuracy: 0.697680412371134\n",
      "Step: 970, Loss: 1.2897142171859741, Accuracy: 0.6975626501888088\n",
      "Step: 971, Loss: 1.428804874420166, Accuracy: 0.6973593964334706\n",
      "Step: 972, Loss: 1.2862160205841064, Accuracy: 0.697327852004111\n",
      "Step: 973, Loss: 1.02183198928833, Accuracy: 0.6975530458590007\n",
      "Step: 974, Loss: 1.278502345085144, Accuracy: 0.6975213675213675\n",
      "Step: 975, Loss: 1.1797523498535156, Accuracy: 0.6975751366120219\n",
      "Step: 976, Loss: 0.995197057723999, Accuracy: 0.6977993858751279\n",
      "Step: 977, Loss: 1.2986444234848022, Accuracy: 0.6976823449216087\n",
      "Step: 978, Loss: 1.176093578338623, Accuracy: 0.6977357848144365\n",
      "Step: 979, Loss: 1.1337696313858032, Accuracy: 0.6977891156462585\n",
      "Step: 980, Loss: 1.5135998725891113, Accuracy: 0.6974176010873259\n",
      "Step: 981, Loss: 1.2099727392196655, Accuracy: 0.6973862864901561\n",
      "Step: 982, Loss: 1.1823549270629883, Accuracy: 0.6973550356052899\n",
      "Step: 983, Loss: 1.001438021659851, Accuracy: 0.6975779132791328\n",
      "Step: 984, Loss: 1.201581597328186, Accuracy: 0.6975465313028765\n",
      "Step: 985, Loss: 1.1436399221420288, Accuracy: 0.6975997295469912\n",
      "Step: 986, Loss: 1.5642567873001099, Accuracy: 0.6971462343802769\n",
      "Step: 987, Loss: 1.1331380605697632, Accuracy: 0.6972840755735492\n",
      "Step: 988, Loss: 1.2063614130020142, Accuracy: 0.6972531176272329\n",
      "Step: 989, Loss: 1.188045620918274, Accuracy: 0.6973063973063973\n",
      "Step: 990, Loss: 1.1933363676071167, Accuracy: 0.6973595694584594\n",
      "Step: 991, Loss: 1.111839771270752, Accuracy: 0.6974966397849462\n",
      "Step: 992, Loss: 1.307116150856018, Accuracy: 0.6973816717019133\n",
      "Step: 993, Loss: 1.1499747037887573, Accuracy: 0.6974346076458753\n",
      "Step: 994, Loss: 1.2460821866989136, Accuracy: 0.6974036850921274\n",
      "Step: 995, Loss: 1.120651125907898, Accuracy: 0.6974564926372155\n",
      "Step: 996, Loss: 1.238016963005066, Accuracy: 0.6974256101638248\n",
      "Step: 997, Loss: 1.1928504705429077, Accuracy: 0.6974782899131596\n",
      "Step: 998, Loss: 1.0889841318130493, Accuracy: 0.6976142809476142\n",
      "Step: 999, Loss: 1.1456247568130493, Accuracy: 0.69775\n",
      "Step: 1000, Loss: 1.305642008781433, Accuracy: 0.6976356976356977\n",
      "Step: 1001, Loss: 1.2490428686141968, Accuracy: 0.6976047904191617\n",
      "Step: 1002, Loss: 1.2322087287902832, Accuracy: 0.6975739448321702\n",
      "Step: 1003, Loss: 1.1751803159713745, Accuracy: 0.6976261620185923\n",
      "Step: 1004, Loss: 1.2747775316238403, Accuracy: 0.6975953565505805\n",
      "Step: 1005, Loss: 1.2439416646957397, Accuracy: 0.6975646123260437\n",
      "Step: 1006, Loss: 1.193642497062683, Accuracy: 0.697533929162529\n",
      "Step: 1007, Loss: 1.1431916952133179, Accuracy: 0.6975859788359788\n",
      "Step: 1008, Loss: 1.2367855310440063, Accuracy: 0.6975553353154939\n",
      "Step: 1009, Loss: 1.3073276281356812, Accuracy: 0.6974422442244225\n",
      "Step: 1010, Loss: 1.184108853340149, Accuracy: 0.6974118034948895\n",
      "Step: 1011, Loss: 1.3037973642349243, Accuracy: 0.6972990777338604\n",
      "Step: 1012, Loss: 1.160812497138977, Accuracy: 0.6973511023362948\n",
      "Step: 1013, Loss: 1.308487057685852, Accuracy: 0.6972386587771203\n",
      "Step: 1014, Loss: 1.0733962059020996, Accuracy: 0.6972906403940887\n",
      "Step: 1015, Loss: 1.1767706871032715, Accuracy: 0.697260498687664\n",
      "Step: 1016, Loss: 1.3257113695144653, Accuracy: 0.6970665355621107\n",
      "Step: 1017, Loss: 1.1458463668823242, Accuracy: 0.6971185330713818\n",
      "Step: 1018, Loss: 1.3188143968582153, Accuracy: 0.6970068694798822\n",
      "Step: 1019, Loss: 1.141160011291504, Accuracy: 0.6970588235294117\n",
      "Step: 1020, Loss: 1.3157069683074951, Accuracy: 0.6969474371531179\n",
      "Step: 1021, Loss: 1.2209843397140503, Accuracy: 0.696917808219178\n",
      "Step: 1022, Loss: 1.217857837677002, Accuracy: 0.6968882372108178\n",
      "Step: 1023, Loss: 1.251074194908142, Accuracy: 0.6968587239583334\n",
      "Step: 1024, Loss: 1.2326067686080933, Accuracy: 0.6968292682926829\n",
      "Step: 1025, Loss: 1.1225829124450684, Accuracy: 0.6968810916179338\n",
      "Step: 1026, Loss: 1.1908377408981323, Accuracy: 0.6968516715352159\n",
      "Step: 1027, Loss: 1.4533458948135376, Accuracy: 0.6965791180285343\n",
      "Step: 1028, Loss: 1.3062541484832764, Accuracy: 0.6964690638160026\n",
      "Step: 1029, Loss: 1.1680184602737427, Accuracy: 0.6965210355987055\n",
      "Step: 1030, Loss: 1.4731979370117188, Accuracy: 0.6963304235370191\n",
      "Step: 1031, Loss: 1.2339684963226318, Accuracy: 0.6963016795865633\n",
      "Step: 1032, Loss: 1.0769575834274292, Accuracy: 0.696434333656018\n",
      "Step: 1033, Loss: 1.2637094259262085, Accuracy: 0.6963249516441006\n",
      "Step: 1034, Loss: 1.3192332983016968, Accuracy: 0.6962157809983897\n",
      "Step: 1035, Loss: 1.2593032121658325, Accuracy: 0.6961872586872587\n",
      "Step: 1036, Loss: 1.082727074623108, Accuracy: 0.6963195114111218\n",
      "Step: 1037, Loss: 1.157362937927246, Accuracy: 0.6963712267180475\n",
      "Step: 1038, Loss: 1.0809887647628784, Accuracy: 0.696503047802374\n",
      "Step: 1039, Loss: 1.0536624193191528, Accuracy: 0.6966346153846154\n",
      "Step: 1040, Loss: 1.2973641157150269, Accuracy: 0.696525776496958\n",
      "Step: 1041, Loss: 1.3051960468292236, Accuracy: 0.6964171465131158\n",
      "Step: 1042, Loss: 1.1410154104232788, Accuracy: 0.6965484180249281\n",
      "Step: 1043, Loss: 1.1815060377120972, Accuracy: 0.6965996168582376\n",
      "Step: 1044, Loss: 1.2797638177871704, Accuracy: 0.6964912280701754\n",
      "Step: 1045, Loss: 1.0248080492019653, Accuracy: 0.6967017208413002\n",
      "Step: 1046, Loss: 1.1825376749038696, Accuracy: 0.6966730340655842\n",
      "Step: 1047, Loss: 1.0968400239944458, Accuracy: 0.6968034351145038\n",
      "Step: 1048, Loss: 1.6046022176742554, Accuracy: 0.6963775023832222\n",
      "Step: 1049, Loss: 1.302125096321106, Accuracy: 0.6962698412698413\n",
      "Step: 1050, Loss: 1.2575470209121704, Accuracy: 0.6962416745956232\n",
      "Step: 1051, Loss: 1.238240361213684, Accuracy: 0.6962135614702155\n",
      "Step: 1052, Loss: 1.3094333410263062, Accuracy: 0.6961063627730294\n",
      "Step: 1053, Loss: 1.1347746849060059, Accuracy: 0.696236559139785\n",
      "Step: 1054, Loss: 1.31179678440094, Accuracy: 0.696129541864139\n",
      "Step: 1055, Loss: 1.1062930822372437, Accuracy: 0.6962594696969697\n",
      "Step: 1056, Loss: 1.107677698135376, Accuracy: 0.696389151687165\n",
      "Step: 1057, Loss: 1.1819730997085571, Accuracy: 0.6964398235664776\n",
      "Step: 1058, Loss: 1.165108323097229, Accuracy: 0.6964903997481902\n",
      "Step: 1059, Loss: 1.2035695314407349, Accuracy: 0.6965408805031447\n",
      "Step: 1060, Loss: 1.279951810836792, Accuracy: 0.6964341815896953\n",
      "Step: 1061, Loss: 1.1271203756332397, Accuracy: 0.696563088512241\n",
      "Step: 1062, Loss: 1.3320534229278564, Accuracy: 0.6964565694575102\n",
      "Step: 1063, Loss: 1.2868882417678833, Accuracy: 0.6963502506265664\n",
      "Step: 1064, Loss: 1.2808877229690552, Accuracy: 0.6962441314553991\n",
      "Step: 1065, Loss: 1.3983154296875, Accuracy: 0.6960600375234521\n",
      "Step: 1066, Loss: 1.1599396467208862, Accuracy: 0.6961105904404874\n",
      "Step: 1067, Loss: 1.2290546894073486, Accuracy: 0.6960049937578028\n",
      "Step: 1068, Loss: 1.3614102602005005, Accuracy: 0.6958216401621453\n",
      "Step: 1069, Loss: 1.0455257892608643, Accuracy: 0.6960280373831775\n",
      "Step: 1070, Loss: 1.2037538290023804, Accuracy: 0.6960006224712108\n",
      "Step: 1071, Loss: 1.2131041288375854, Accuracy: 0.6959732587064676\n",
      "Step: 1072, Loss: 1.065955638885498, Accuracy: 0.6961012736874805\n",
      "Step: 1073, Loss: 1.2332029342651367, Accuracy: 0.6960738671632526\n",
      "Step: 1074, Loss: 1.3057924509048462, Accuracy: 0.695968992248062\n",
      "Step: 1075, Loss: 1.2428408861160278, Accuracy: 0.6959417596034696\n",
      "Step: 1076, Loss: 1.1709275245666504, Accuracy: 0.6959919529557412\n",
      "Step: 1077, Loss: 1.2867165803909302, Accuracy: 0.6959647495361782\n",
      "Step: 1078, Loss: 1.2106585502624512, Accuracy: 0.6958603645350633\n",
      "Step: 1079, Loss: 1.3635228872299194, Accuracy: 0.6957561728395062\n",
      "Step: 1080, Loss: 1.4192543029785156, Accuracy: 0.6954979956830095\n",
      "Step: 1081, Loss: 1.186522364616394, Accuracy: 0.6955483672211953\n",
      "Step: 1082, Loss: 1.229016661643982, Accuracy: 0.6955216989843028\n",
      "Step: 1083, Loss: 1.1697413921356201, Accuracy: 0.6955719557195572\n",
      "Step: 1084, Loss: 1.3340531587600708, Accuracy: 0.695468509984639\n",
      "Step: 1085, Loss: 1.3684884309768677, Accuracy: 0.6952885205647636\n",
      "Step: 1086, Loss: 1.132567286491394, Accuracy: 0.6953388531125422\n",
      "Step: 1087, Loss: 1.3499099016189575, Accuracy: 0.6951593137254902\n",
      "Step: 1088, Loss: 1.2233517169952393, Accuracy: 0.6951331496786042\n",
      "Step: 1089, Loss: 1.2293506860733032, Accuracy: 0.6951070336391437\n",
      "Step: 1090, Loss: 1.1853783130645752, Accuracy: 0.6951573479987779\n",
      "Step: 1091, Loss: 1.127875566482544, Accuracy: 0.6952838827838828\n",
      "Step: 1092, Loss: 1.288162112236023, Accuracy: 0.6951814577615126\n",
      "Step: 1093, Loss: 1.1661444902420044, Accuracy: 0.6952315661182206\n",
      "Step: 1094, Loss: 1.1751896142959595, Accuracy: 0.6952815829528158\n",
      "Step: 1095, Loss: 1.154128909111023, Accuracy: 0.695331508515815\n",
      "Step: 1096, Loss: 1.215865969657898, Accuracy: 0.6953053783044667\n",
      "Step: 1097, Loss: 1.0263943672180176, Accuracy: 0.6954310868245295\n",
      "Step: 1098, Loss: 1.0933440923690796, Accuracy: 0.6954807400667273\n",
      "Step: 1099, Loss: 1.163203477859497, Accuracy: 0.695530303030303\n",
      "Step: 1100, Loss: 1.1313227415084839, Accuracy: 0.6956554647290342\n",
      "Step: 1101, Loss: 1.2416189908981323, Accuracy: 0.6956291591046582\n",
      "Step: 1102, Loss: 1.2170734405517578, Accuracy: 0.6956029011786038\n",
      "Step: 1103, Loss: 1.2948166131973267, Accuracy: 0.6955012077294686\n",
      "Step: 1104, Loss: 1.0912655591964722, Accuracy: 0.6956259426847662\n",
      "Step: 1105, Loss: 1.0462957620620728, Accuracy: 0.6958257986738999\n",
      "Step: 1106, Loss: 1.1464496850967407, Accuracy: 0.695874736525143\n",
      "Step: 1107, Loss: 1.2511978149414062, Accuracy: 0.6958483754512635\n",
      "Step: 1108, Loss: 1.1970444917678833, Accuracy: 0.6958972046889089\n",
      "Step: 1109, Loss: 1.0727297067642212, Accuracy: 0.696021021021021\n",
      "Step: 1110, Loss: 1.0024354457855225, Accuracy: 0.6962196219621962\n",
      "Step: 1111, Loss: 1.3136191368103027, Accuracy: 0.6961181055155875\n",
      "Step: 1112, Loss: 1.073842167854309, Accuracy: 0.6962413896376161\n",
      "Step: 1113, Loss: 1.2696291208267212, Accuracy: 0.6961400359066428\n",
      "Step: 1114, Loss: 1.2387245893478394, Accuracy: 0.6961136023916293\n",
      "Step: 1115, Loss: 1.2409162521362305, Accuracy: 0.6960872162485066\n",
      "Step: 1116, Loss: 1.0927412509918213, Accuracy: 0.696210086541331\n",
      "Step: 1117, Loss: 1.3554550409317017, Accuracy: 0.6961091234347049\n",
      "Step: 1118, Loss: 1.2919563055038452, Accuracy: 0.6960083407804587\n",
      "Step: 1119, Loss: 1.1695327758789062, Accuracy: 0.6960565476190477\n",
      "Step: 1120, Loss: 1.3086923360824585, Accuracy: 0.6959559916741005\n",
      "Step: 1121, Loss: 1.0462557077407837, Accuracy: 0.696078431372549\n",
      "Step: 1122, Loss: 1.2025337219238281, Accuracy: 0.696126447016919\n",
      "Step: 1123, Loss: 1.0416263341903687, Accuracy: 0.6962485172004745\n",
      "Step: 1124, Loss: 1.1375242471694946, Accuracy: 0.6962962962962963\n",
      "Step: 1125, Loss: 1.2197647094726562, Accuracy: 0.6962699822380106\n",
      "Step: 1126, Loss: 1.1791596412658691, Accuracy: 0.6963176574977817\n",
      "Step: 1127, Loss: 1.1417531967163086, Accuracy: 0.6963652482269503\n",
      "Step: 1128, Loss: 1.3166474103927612, Accuracy: 0.6962651313847062\n",
      "Step: 1129, Loss: 1.2199897766113281, Accuracy: 0.6962389380530973\n",
      "Step: 1130, Loss: 1.197170376777649, Accuracy: 0.6962864721485411\n",
      "Step: 1131, Loss: 1.139580249786377, Accuracy: 0.696333922261484\n",
      "Step: 1132, Loss: 1.4265979528427124, Accuracy: 0.696087084436599\n",
      "Step: 1133, Loss: 1.2847107648849487, Accuracy: 0.696061140505585\n",
      "Step: 1134, Loss: 1.328468680381775, Accuracy: 0.6959618208516887\n",
      "Step: 1135, Loss: 1.1992348432540894, Accuracy: 0.6959360328638498\n",
      "Step: 1136, Loss: 1.1593066453933716, Accuracy: 0.6959835825271181\n",
      "Step: 1137, Loss: 1.0157281160354614, Accuracy: 0.6961775043936731\n",
      "Step: 1138, Loss: 1.396439552307129, Accuracy: 0.6960052677787533\n",
      "Step: 1139, Loss: 1.142378807067871, Accuracy: 0.6960526315789474\n",
      "Step: 1140, Loss: 1.1539300680160522, Accuracy: 0.6960999123575811\n",
      "Step: 1141, Loss: 1.254003882408142, Accuracy: 0.6960741389375364\n",
      "Step: 1142, Loss: 1.1581112146377563, Accuracy: 0.6961213181685623\n",
      "Step: 1143, Loss: 1.1710611581802368, Accuracy: 0.6960955710955711\n",
      "Step: 1144, Loss: 1.1564303636550903, Accuracy: 0.6961426491994177\n",
      "Step: 1145, Loss: 1.4302483797073364, Accuracy: 0.6959714950552647\n",
      "Step: 1146, Loss: 1.3835744857788086, Accuracy: 0.6958006393490265\n",
      "Step: 1147, Loss: 1.179756760597229, Accuracy: 0.6958478513356562\n",
      "Step: 1148, Loss: 1.1143735647201538, Accuracy: 0.6959675079779518\n",
      "Step: 1149, Loss: 1.1305557489395142, Accuracy: 0.6960869565217391\n",
      "Step: 1150, Loss: 0.9457094669342041, Accuracy: 0.6962785983203011\n",
      "Step: 1151, Loss: 1.1539252996444702, Accuracy: 0.6963252314814815\n",
      "Step: 1152, Loss: 1.039868950843811, Accuracy: 0.6964440589765828\n",
      "Step: 1153, Loss: 1.1272982358932495, Accuracy: 0.6965626805314847\n",
      "Step: 1154, Loss: 1.006752610206604, Accuracy: 0.6967532467532468\n",
      "Step: 1155, Loss: 1.1936944723129272, Accuracy: 0.6967993079584776\n",
      "Step: 1156, Loss: 1.213780403137207, Accuracy: 0.6967732641889945\n",
      "Step: 1157, Loss: 1.4657446146011353, Accuracy: 0.6965313759355211\n",
      "Step: 1158, Loss: 1.0429450273513794, Accuracy: 0.696649410411274\n",
      "Step: 1159, Loss: 1.1211271286010742, Accuracy: 0.6966954022988506\n",
      "Step: 1160, Loss: 1.2472203969955444, Accuracy: 0.6966695377548091\n",
      "Step: 1161, Loss: 0.9739026427268982, Accuracy: 0.6968588640275387\n",
      "Step: 1162, Loss: 1.30489981174469, Accuracy: 0.6967612496417311\n",
      "Step: 1163, Loss: 1.364806056022644, Accuracy: 0.6965922107674685\n",
      "Step: 1164, Loss: 1.5366309881210327, Accuracy: 0.696280400572246\n",
      "Step: 1165, Loss: 1.1630460023880005, Accuracy: 0.6963264722698685\n",
      "Step: 1166, Loss: 1.1165281534194946, Accuracy: 0.6963724650099972\n",
      "Step: 1167, Loss: 1.1654342412948608, Accuracy: 0.6964183789954338\n",
      "Step: 1168, Loss: 1.38481605052948, Accuracy: 0.6962503564299971\n",
      "Step: 1169, Loss: 1.1538199186325073, Accuracy: 0.6962962962962963\n",
      "Step: 1170, Loss: 1.139586091041565, Accuracy: 0.6963421576999715\n",
      "Step: 1171, Loss: 1.082822561264038, Accuracy: 0.6963879408418657\n",
      "Step: 1172, Loss: 1.1084805727005005, Accuracy: 0.6965046888320545\n",
      "Step: 1173, Loss: 1.3028287887573242, Accuracy: 0.6964082907438955\n",
      "Step: 1174, Loss: 1.2944532632827759, Accuracy: 0.6963829787234043\n",
      "Step: 1175, Loss: 1.136276364326477, Accuracy: 0.6964285714285714\n",
      "Step: 1176, Loss: 1.2998418807983398, Accuracy: 0.6963324837156613\n",
      "Step: 1177, Loss: 1.2239023447036743, Accuracy: 0.6963073005093379\n",
      "Step: 1178, Loss: 1.1450871229171753, Accuracy: 0.6963528413910093\n",
      "Step: 1179, Loss: 0.9907497763633728, Accuracy: 0.6966101694915254\n",
      "Step: 1180, Loss: 1.4235801696777344, Accuracy: 0.6964436917866215\n",
      "Step: 1181, Loss: 1.0759507417678833, Accuracy: 0.6965595036661026\n",
      "Step: 1182, Loss: 1.4695992469787598, Accuracy: 0.6963229078613694\n",
      "Step: 1183, Loss: 1.1501202583312988, Accuracy: 0.6964386261261262\n",
      "Step: 1184, Loss: 1.3846675157546997, Accuracy: 0.6962728551336146\n",
      "Step: 1185, Loss: 1.1650745868682861, Accuracy: 0.6963181562675661\n",
      "Step: 1186, Loss: 1.1189590692520142, Accuracy: 0.6963633810727323\n",
      "Step: 1187, Loss: 1.2989672422409058, Accuracy: 0.6962682379349046\n",
      "Step: 1188, Loss: 1.1221364736557007, Accuracy: 0.6963835155592936\n",
      "Step: 1189, Loss: 1.2292717695236206, Accuracy: 0.6963585434173669\n",
      "Step: 1190, Loss: 1.2686758041381836, Accuracy: 0.6963336132101875\n",
      "Step: 1191, Loss: 1.5652574300765991, Accuracy: 0.6960290827740492\n",
      "Step: 1192, Loss: 1.1116901636123657, Accuracy: 0.6960743224364347\n",
      "Step: 1193, Loss: 1.150224208831787, Accuracy: 0.6961194863204914\n",
      "Step: 1194, Loss: 1.2966853380203247, Accuracy: 0.6960251046025104\n",
      "Step: 1195, Loss: 1.4979066848754883, Accuracy: 0.6957915273132664\n",
      "Step: 1196, Loss: 1.3333169221878052, Accuracy: 0.6956975772765246\n",
      "Step: 1197, Loss: 1.2866829633712769, Accuracy: 0.6956037840845855\n",
      "Step: 1198, Loss: 1.251634955406189, Accuracy: 0.69557964970809\n",
      "Step: 1199, Loss: 1.3313401937484741, Accuracy: 0.6954861111111111\n",
      "Step: 1200, Loss: 1.0772215127944946, Accuracy: 0.695600888148765\n",
      "Step: 1201, Loss: 1.2240763902664185, Accuracy: 0.6955768164170827\n",
      "Step: 1202, Loss: 1.3040889501571655, Accuracy: 0.6954835134386257\n",
      "Step: 1203, Loss: 1.0558923482894897, Accuracy: 0.6955980066445183\n",
      "Step: 1204, Loss: 1.2520873546600342, Accuracy: 0.6955739972337482\n",
      "Step: 1205, Loss: 1.4326845407485962, Accuracy: 0.695411829740188\n",
      "Step: 1206, Loss: 1.251538872718811, Accuracy: 0.6953880143606739\n",
      "Step: 1207, Loss: 1.07927405834198, Accuracy: 0.6955022075055187\n",
      "Step: 1208, Loss: 1.2024985551834106, Accuracy: 0.6954783567686793\n",
      "Step: 1209, Loss: 1.1544328927993774, Accuracy: 0.6955234159779614\n",
      "Step: 1210, Loss: 1.3086165189743042, Accuracy: 0.6954307734654556\n",
      "Step: 1211, Loss: 1.080507516860962, Accuracy: 0.6955445544554455\n",
      "Step: 1212, Loss: 1.1431752443313599, Accuracy: 0.6955894476504534\n",
      "Step: 1213, Loss: 1.1374926567077637, Accuracy: 0.6956342668863262\n",
      "Step: 1214, Loss: 1.252651333808899, Accuracy: 0.6956104252400549\n",
      "Step: 1215, Loss: 1.1421456336975098, Accuracy: 0.6956551535087719\n",
      "Step: 1216, Loss: 1.1782031059265137, Accuracy: 0.6956313338811284\n",
      "Step: 1217, Loss: 1.43278968334198, Accuracy: 0.6954022988505747\n",
      "Step: 1218, Loss: 1.2323004007339478, Accuracy: 0.6953787257314739\n",
      "Step: 1219, Loss: 1.199955940246582, Accuracy: 0.6953551912568307\n",
      "Step: 1220, Loss: 1.3486016988754272, Accuracy: 0.6952634452634453\n",
      "Step: 1221, Loss: 1.1037518978118896, Accuracy: 0.6953082378614294\n",
      "Step: 1222, Loss: 1.1424763202667236, Accuracy: 0.6953529572090488\n",
      "Step: 1223, Loss: 1.2790035009384155, Accuracy: 0.6952614379084967\n",
      "Step: 1224, Loss: 1.2187808752059937, Accuracy: 0.6952380952380952\n",
      "Step: 1225, Loss: 1.3299182653427124, Accuracy: 0.6951468189233279\n",
      "Step: 1226, Loss: 1.114201307296753, Accuracy: 0.6951915240423798\n",
      "Step: 1227, Loss: 1.171284794807434, Accuracy: 0.6952361563517915\n",
      "Step: 1228, Loss: 1.0606433153152466, Accuracy: 0.6954163276376458\n",
      "Step: 1229, Loss: 1.5267853736877441, Accuracy: 0.695189701897019\n",
      "Step: 1230, Loss: 1.2995554208755493, Accuracy: 0.6950988356349851\n",
      "Step: 1231, Loss: 1.1477967500686646, Accuracy: 0.6951433982683982\n",
      "Step: 1232, Loss: 0.9978241324424744, Accuracy: 0.695323060286564\n",
      "Step: 1233, Loss: 1.0408776998519897, Accuracy: 0.6955024311183144\n",
      "Step: 1234, Loss: 1.2918728590011597, Accuracy: 0.6954116059379217\n",
      "Step: 1235, Loss: 0.9465005993843079, Accuracy: 0.6956580366774542\n",
      "Step: 1236, Loss: 1.141491413116455, Accuracy: 0.6957019671247642\n",
      "Step: 1237, Loss: 1.1165934801101685, Accuracy: 0.6957458266020463\n",
      "Step: 1238, Loss: 1.1826444864273071, Accuracy: 0.6957223567393059\n",
      "Step: 1239, Loss: 1.1212810277938843, Accuracy: 0.6957661290322581\n",
      "Step: 1240, Loss: 1.2374402284622192, Accuracy: 0.695675530486167\n",
      "Step: 1241, Loss: 1.1750575304031372, Accuracy: 0.6957192699946323\n",
      "Step: 1242, Loss: 1.2971738576889038, Accuracy: 0.6956288549208903\n",
      "Step: 1243, Loss: 1.228585124015808, Accuracy: 0.6956055734190782\n",
      "Step: 1244, Loss: 1.3839197158813477, Accuracy: 0.6954484605087015\n",
      "Step: 1245, Loss: 1.2819862365722656, Accuracy: 0.6954253611556982\n",
      "Step: 1246, Loss: 1.2565349340438843, Accuracy: 0.6954022988505747\n",
      "Step: 1247, Loss: 1.0663418769836426, Accuracy: 0.6955128205128205\n",
      "Step: 1248, Loss: 1.3254938125610352, Accuracy: 0.6954230050707233\n",
      "Step: 1249, Loss: 1.248132348060608, Accuracy: 0.6954\n",
      "Step: 1250, Loss: 1.154683232307434, Accuracy: 0.6954436450839329\n",
      "Step: 1251, Loss: 1.1205947399139404, Accuracy: 0.6955537806176784\n",
      "Step: 1252, Loss: 1.0917596817016602, Accuracy: 0.6956637403564778\n",
      "Step: 1253, Loss: 1.323763132095337, Accuracy: 0.6955741626794258\n",
      "Step: 1254, Loss: 1.290563941001892, Accuracy: 0.6955511288180611\n",
      "Step: 1255, Loss: 1.3011564016342163, Accuracy: 0.6955281316348195\n",
      "Step: 1256, Loss: 1.0714067220687866, Accuracy: 0.6956377618668788\n",
      "Step: 1257, Loss: 1.684381365776062, Accuracy: 0.695217276099629\n",
      "Step: 1258, Loss: 1.2939509153366089, Accuracy: 0.695128408790045\n",
      "Step: 1259, Loss: 1.2184828519821167, Accuracy: 0.6951058201058201\n",
      "Step: 1260, Loss: 1.062106728553772, Accuracy: 0.6952815226011102\n",
      "Step: 1261, Loss: 1.3311876058578491, Accuracy: 0.6951928156365558\n",
      "Step: 1262, Loss: 1.3224599361419678, Accuracy: 0.6951042491422539\n",
      "Step: 1263, Loss: 1.3201838731765747, Accuracy: 0.6950158227848101\n",
      "Step: 1264, Loss: 1.2245005369186401, Accuracy: 0.6949934123847167\n",
      "Step: 1265, Loss: 1.3902586698532104, Accuracy: 0.6948393891521853\n",
      "Step: 1266, Loss: 1.136374831199646, Accuracy: 0.694882925545909\n",
      "Step: 1267, Loss: 0.992506742477417, Accuracy: 0.6950578338590957\n",
      "Step: 1268, Loss: 1.1938018798828125, Accuracy: 0.6950354609929078\n",
      "Step: 1269, Loss: 1.1111575365066528, Accuracy: 0.6951443569553806\n",
      "Step: 1270, Loss: 1.3492335081100464, Accuracy: 0.6950563860477315\n",
      "Step: 1271, Loss: 1.2930055856704712, Accuracy: 0.6949685534591195\n",
      "Step: 1272, Loss: 1.0591832399368286, Accuracy: 0.6950772453521864\n",
      "Step: 1273, Loss: 1.2578908205032349, Accuracy: 0.695054945054945\n",
      "Step: 1274, Loss: 1.2436561584472656, Accuracy: 0.6950326797385621\n",
      "Step: 1275, Loss: 1.1182483434677124, Accuracy: 0.695141065830721\n",
      "Step: 1276, Loss: 1.3413177728652954, Accuracy: 0.6950535108326807\n",
      "Step: 1277, Loss: 1.2494287490844727, Accuracy: 0.6950312989045383\n",
      "Step: 1278, Loss: 1.3604568243026733, Accuracy: 0.6949439666406046\n",
      "Step: 1279, Loss: 1.1771259307861328, Accuracy: 0.6949869791666666\n",
      "Step: 1280, Loss: 1.1400253772735596, Accuracy: 0.6950299245381213\n",
      "Step: 1281, Loss: 1.1787092685699463, Accuracy: 0.6950728029121165\n",
      "Step: 1282, Loss: 1.4405865669250488, Accuracy: 0.6948558067030397\n",
      "Step: 1283, Loss: 1.0854586362838745, Accuracy: 0.694963655244029\n",
      "Step: 1284, Loss: 1.0411735773086548, Accuracy: 0.695071335927367\n",
      "Step: 1285, Loss: 1.0762265920639038, Accuracy: 0.6951788491446346\n",
      "Step: 1286, Loss: 1.2055753469467163, Accuracy: 0.6951566951566952\n",
      "Step: 1287, Loss: 1.2441155910491943, Accuracy: 0.6951345755693582\n",
      "Step: 1288, Loss: 1.1450835466384888, Accuracy: 0.6951771399017326\n",
      "Step: 1289, Loss: 1.1269980669021606, Accuracy: 0.6952842377260982\n",
      "Step: 1290, Loss: 1.1961110830307007, Accuracy: 0.6953266201910664\n",
      "Step: 1291, Loss: 1.1200937032699585, Accuracy: 0.6953689370485037\n",
      "Step: 1292, Loss: 1.147260308265686, Accuracy: 0.6954111884506317\n",
      "Step: 1293, Loss: 1.2838388681411743, Accuracy: 0.6953245749613601\n",
      "Step: 1294, Loss: 1.0782968997955322, Accuracy: 0.6954311454311455\n",
      "Step: 1295, Loss: 1.095477819442749, Accuracy: 0.6955375514403292\n",
      "Step: 1296, Loss: 1.133018136024475, Accuracy: 0.695579542534053\n",
      "Step: 1297, Loss: 1.1994808912277222, Accuracy: 0.6955572675911659\n",
      "Step: 1298, Loss: 1.2029781341552734, Accuracy: 0.6955991788555299\n",
      "Step: 1299, Loss: 1.207055926322937, Accuracy: 0.6955769230769231\n",
      "Step: 1300, Loss: 1.3577704429626465, Accuracy: 0.6954265949269792\n",
      "Step: 1301, Loss: 1.2274460792541504, Accuracy: 0.6954045058883769\n",
      "Step: 1302, Loss: 1.0954467058181763, Accuracy: 0.695510360706063\n",
      "Step: 1303, Loss: 1.1068626642227173, Accuracy: 0.6955521472392638\n",
      "Step: 1304, Loss: 1.2828618288040161, Accuracy: 0.6954661558109834\n",
      "Step: 1305, Loss: 1.4148141145706177, Accuracy: 0.6953164880040837\n",
      "Step: 1306, Loss: 1.223507285118103, Accuracy: 0.6952945677123182\n",
      "Step: 1307, Loss: 1.3126254081726074, Accuracy: 0.6952089704383282\n",
      "Step: 1308, Loss: 1.3295788764953613, Accuracy: 0.6951235039470334\n",
      "Step: 1309, Loss: 1.0166146755218506, Accuracy: 0.69529262086514\n",
      "Step: 1310, Loss: 1.1074166297912598, Accuracy: 0.6953979150775489\n",
      "Step: 1311, Loss: 1.349222183227539, Accuracy: 0.6952489837398373\n",
      "Step: 1312, Loss: 1.0874841213226318, Accuracy: 0.6953541507996953\n",
      "Step: 1313, Loss: 1.2457079887390137, Accuracy: 0.6953323186199899\n",
      "Step: 1314, Loss: 1.2265664339065552, Accuracy: 0.6953105196451204\n",
      "Step: 1315, Loss: 1.2115801572799683, Accuracy: 0.6953520770010132\n",
      "Step: 1316, Loss: 1.288151741027832, Accuracy: 0.69526702100734\n",
      "Step: 1317, Loss: 1.351739764213562, Accuracy: 0.6951820940819423\n",
      "Step: 1318, Loss: 1.2464581727981567, Accuracy: 0.6951604751074046\n",
      "Step: 1319, Loss: 1.0774435997009277, Accuracy: 0.6952651515151516\n",
      "Step: 1320, Loss: 1.317257285118103, Accuracy: 0.6951804188745899\n",
      "Step: 1321, Loss: 1.0806081295013428, Accuracy: 0.6952849218356026\n",
      "Step: 1322, Loss: 1.265966534614563, Accuracy: 0.6952632905013857\n",
      "Step: 1323, Loss: 1.1480035781860352, Accuracy: 0.6953046324269889\n",
      "Step: 1324, Loss: 1.1440125703811646, Accuracy: 0.6953459119496855\n",
      "Step: 1325, Loss: 1.1541402339935303, Accuracy: 0.6953871292106586\n",
      "Step: 1326, Loss: 1.2932049036026, Accuracy: 0.6953026877668927\n",
      "Step: 1327, Loss: 1.5130295753479004, Accuracy: 0.6950301204819277\n",
      "Step: 1328, Loss: 1.1941876411437988, Accuracy: 0.6950087785302232\n",
      "Step: 1329, Loss: 1.073866605758667, Accuracy: 0.6951127819548872\n",
      "Step: 1330, Loss: 1.3618792295455933, Accuracy: 0.6950288004007013\n",
      "Step: 1331, Loss: 1.0415397882461548, Accuracy: 0.6951951951951952\n",
      "Step: 1332, Loss: 1.0209518671035767, Accuracy: 0.6953613403350838\n",
      "Step: 1333, Loss: 1.3218156099319458, Accuracy: 0.6952148925537232\n",
      "Step: 1334, Loss: 1.060284972190857, Accuracy: 0.6953807740324595\n",
      "Step: 1335, Loss: 1.1564112901687622, Accuracy: 0.6954216566866267\n",
      "Step: 1336, Loss: 1.3970894813537598, Accuracy: 0.6952754923959112\n",
      "Step: 1337, Loss: 1.2104947566986084, Accuracy: 0.695254110612855\n",
      "Step: 1338, Loss: 1.1685128211975098, Accuracy: 0.6952949962658701\n",
      "Step: 1339, Loss: 1.3762420415878296, Accuracy: 0.6951492537313433\n",
      "Step: 1340, Loss: 1.2193063497543335, Accuracy: 0.6951280139199603\n",
      "Step: 1341, Loss: 1.1947507858276367, Accuracy: 0.6951689021361153\n",
      "Step: 1342, Loss: 1.3809494972229004, Accuracy: 0.695023579051874\n",
      "Step: 1343, Loss: 1.1451209783554077, Accuracy: 0.6951264880952381\n",
      "Step: 1344, Loss: 1.2917600870132446, Accuracy: 0.6950433705080545\n",
      "Step: 1345, Loss: 0.9922464489936829, Accuracy: 0.6952080237741456\n",
      "Step: 1346, Loss: 1.1805859804153442, Accuracy: 0.6952487008166296\n",
      "Step: 1347, Loss: 1.1560461521148682, Accuracy: 0.6952893175074184\n",
      "Step: 1348, Loss: 1.1706253290176392, Accuracy: 0.6953298739807264\n",
      "Step: 1349, Loss: 1.2461899518966675, Accuracy: 0.6953086419753086\n",
      "Step: 1350, Loss: 1.0195509195327759, Accuracy: 0.6954724895139403\n",
      "Step: 1351, Loss: 1.310602068901062, Accuracy: 0.6953895463510849\n",
      "Step: 1352, Loss: 1.3288425207138062, Accuracy: 0.6952451342695245\n",
      "Step: 1353, Loss: 1.1608186960220337, Accuracy: 0.6952855736090596\n",
      "Step: 1354, Loss: 1.3969560861587524, Accuracy: 0.6951414514145141\n",
      "Step: 1355, Loss: 1.0938360691070557, Accuracy: 0.6952433628318584\n",
      "Step: 1356, Loss: 1.0443090200424194, Accuracy: 0.6954065340211251\n",
      "Step: 1357, Loss: 1.3941006660461426, Accuracy: 0.6952626411389298\n",
      "Step: 1358, Loss: 1.2828890085220337, Accuracy: 0.6952415992151092\n",
      "Step: 1359, Loss: 1.376226782798767, Accuracy: 0.6950980392156862\n",
      "Step: 1360, Loss: 1.4214125871658325, Accuracy: 0.6949546901787901\n",
      "Step: 1361, Loss: 1.3414472341537476, Accuracy: 0.6948727361722956\n",
      "Step: 1362, Loss: 1.2221800088882446, Accuracy: 0.6948520420640744\n",
      "Step: 1363, Loss: 1.123717188835144, Accuracy: 0.6949535679374389\n",
      "Step: 1364, Loss: 1.068664312362671, Accuracy: 0.695054945054945\n",
      "Step: 1365, Loss: 1.1590394973754883, Accuracy: 0.695095168374817\n",
      "Step: 1366, Loss: 1.1543127298355103, Accuracy: 0.6951353328456474\n",
      "Step: 1367, Loss: 1.2218759059906006, Accuracy: 0.695114522417154\n",
      "Step: 1368, Loss: 1.1929479837417603, Accuracy: 0.6950937423910397\n",
      "Step: 1369, Loss: 1.250295877456665, Accuracy: 0.69507299270073\n",
      "Step: 1370, Loss: 1.2319186925888062, Accuracy: 0.6951130561633844\n",
      "Step: 1371, Loss: 1.112740159034729, Accuracy: 0.6952137998056366\n",
      "Step: 1372, Loss: 1.3117140531539917, Accuracy: 0.6951323136683661\n",
      "Step: 1373, Loss: 1.203797459602356, Accuracy: 0.6951722464822901\n",
      "Step: 1374, Loss: 1.1161078214645386, Accuracy: 0.6952727272727273\n",
      "Step: 1375, Loss: 1.0835093259811401, Accuracy: 0.6953730620155039\n",
      "Step: 1376, Loss: 1.2309153079986572, Accuracy: 0.6953522149600581\n",
      "Step: 1377, Loss: 1.0082484483718872, Accuracy: 0.6955128205128205\n",
      "Step: 1378, Loss: 1.0491199493408203, Accuracy: 0.6956127628716461\n",
      "Step: 1379, Loss: 1.3852344751358032, Accuracy: 0.6954710144927536\n",
      "Step: 1380, Loss: 1.3054356575012207, Accuracy: 0.695329471397538\n",
      "Step: 1381, Loss: 1.1747486591339111, Accuracy: 0.695369030390738\n",
      "Step: 1382, Loss: 1.16675865650177, Accuracy: 0.6954085321764281\n",
      "Step: 1383, Loss: 1.2231611013412476, Accuracy: 0.6953877649325626\n",
      "Step: 1384, Loss: 1.2399089336395264, Accuracy: 0.695367027677497\n",
      "Step: 1385, Loss: 1.2097948789596558, Accuracy: 0.6953463203463204\n",
      "Step: 1386, Loss: 1.1607407331466675, Accuracy: 0.6953857245854362\n",
      "Step: 1387, Loss: 1.100101351737976, Accuracy: 0.6954250720461095\n",
      "Step: 1388, Loss: 1.0392597913742065, Accuracy: 0.6955843532517398\n",
      "Step: 1389, Loss: 1.2819269895553589, Accuracy: 0.6955035971223021\n",
      "Step: 1390, Loss: 1.123520016670227, Accuracy: 0.6955427749820273\n",
      "Step: 1391, Loss: 1.1100767850875854, Accuracy: 0.6956417624521073\n",
      "Step: 1392, Loss: 1.313061237335205, Accuracy: 0.6955611390284757\n",
      "Step: 1393, Loss: 1.0087844133377075, Accuracy: 0.6957197513151602\n",
      "Step: 1394, Loss: 1.2493044137954712, Accuracy: 0.6956989247311828\n",
      "Step: 1395, Loss: 1.163440227508545, Accuracy: 0.6957378223495702\n",
      "Step: 1396, Loss: 1.2968900203704834, Accuracy: 0.6956573610116917\n",
      "Step: 1397, Loss: 1.4479409456253052, Accuracy: 0.6954577968526466\n",
      "Step: 1398, Loss: 1.268351435661316, Accuracy: 0.6954372170598047\n",
      "Step: 1399, Loss: 1.3493732213974, Accuracy: 0.6953571428571429\n",
      "Step: 1400, Loss: 1.1710675954818726, Accuracy: 0.6953961456102784\n",
      "Step: 1401, Loss: 1.4827972650527954, Accuracy: 0.6951973371374227\n",
      "Step: 1402, Loss: 1.4037681818008423, Accuracy: 0.6950582086006177\n",
      "Step: 1403, Loss: 1.2326160669326782, Accuracy: 0.6950379867046533\n",
      "Step: 1404, Loss: 1.3610330820083618, Accuracy: 0.6949584816132859\n",
      "Step: 1405, Loss: 1.4711874723434448, Accuracy: 0.694760550023708\n",
      "Step: 1406, Loss: 1.1253283023834229, Accuracy: 0.6948590381426202\n",
      "Step: 1407, Loss: 1.0281562805175781, Accuracy: 0.695016571969697\n",
      "Step: 1408, Loss: 1.4146093130111694, Accuracy: 0.6948781641826355\n",
      "Step: 1409, Loss: 1.1464060544967651, Accuracy: 0.6949172576832151\n",
      "Step: 1410, Loss: 1.1291704177856445, Accuracy: 0.6950153555398063\n",
      "Step: 1411, Loss: 1.2349928617477417, Accuracy: 0.6949952785646837\n",
      "Step: 1412, Loss: 1.1989846229553223, Accuracy: 0.695034206180703\n",
      "Step: 1413, Loss: 1.2823394536972046, Accuracy: 0.694955209806695\n",
      "Step: 1414, Loss: 1.2953535318374634, Accuracy: 0.6949352179034158\n",
      "Step: 1415, Loss: 1.3811259269714355, Accuracy: 0.6947975517890772\n",
      "Step: 1416, Loss: 1.1284486055374146, Accuracy: 0.6948365090566926\n",
      "Step: 1417, Loss: 1.1797224283218384, Accuracy: 0.6948166431593794\n",
      "Step: 1418, Loss: 1.2967851161956787, Accuracy: 0.6947380784590087\n",
      "Step: 1419, Loss: 1.0810579061508179, Accuracy: 0.6948356807511737\n",
      "Step: 1420, Loss: 1.0942741632461548, Accuracy: 0.6949331456720619\n",
      "Step: 1421, Loss: 1.1208337545394897, Accuracy: 0.6950304735114862\n",
      "Step: 1422, Loss: 1.1606417894363403, Accuracy: 0.6950691028343875\n",
      "Step: 1423, Loss: 1.2113934755325317, Accuracy: 0.6950491573033708\n",
      "Step: 1424, Loss: 1.1304124593734741, Accuracy: 0.6950877192982456\n",
      "Step: 1425, Loss: 1.4876580238342285, Accuracy: 0.6948924731182796\n",
      "Step: 1426, Loss: 1.2262758016586304, Accuracy: 0.6946391030133147\n",
      "Epoch: 6, Val_Accuracy: 0.2853582554517134\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd6e5a309584498eb2a00894119f279f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1427 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0, Loss: 1.2283217906951904, Accuracy: 0.6666666666666666\n",
      "Step: 1, Loss: 1.2300094366073608, Accuracy: 0.6666666666666666\n",
      "Step: 2, Loss: 1.124159336090088, Accuracy: 0.7222222222222222\n",
      "Step: 3, Loss: 1.401976466178894, Accuracy: 0.6666666666666666\n",
      "Step: 4, Loss: 1.2035797834396362, Accuracy: 0.6833333333333333\n",
      "Step: 5, Loss: 1.1496084928512573, Accuracy: 0.6944444444444444\n",
      "Step: 6, Loss: 1.3124873638153076, Accuracy: 0.6785714285714286\n",
      "Step: 7, Loss: 1.2981981039047241, Accuracy: 0.6666666666666666\n",
      "Step: 8, Loss: 1.2547653913497925, Accuracy: 0.6666666666666666\n",
      "Step: 9, Loss: 1.0964750051498413, Accuracy: 0.6833333333333333\n",
      "Step: 10, Loss: 1.231866478919983, Accuracy: 0.6818181818181818\n",
      "Step: 11, Loss: 1.2489761114120483, Accuracy: 0.6736111111111112\n",
      "Step: 12, Loss: 1.0767155885696411, Accuracy: 0.6858974358974359\n",
      "Step: 13, Loss: 1.3202800750732422, Accuracy: 0.6785714285714286\n",
      "Step: 14, Loss: 1.153103232383728, Accuracy: 0.6833333333333333\n",
      "Step: 15, Loss: 1.2873693704605103, Accuracy: 0.6770833333333334\n",
      "Step: 16, Loss: 1.2832149267196655, Accuracy: 0.6715686274509803\n",
      "Step: 17, Loss: 1.0309041738510132, Accuracy: 0.6851851851851852\n",
      "Step: 18, Loss: 1.1604892015457153, Accuracy: 0.6885964912280702\n",
      "Step: 19, Loss: 1.2397392988204956, Accuracy: 0.6875\n",
      "Step: 20, Loss: 1.1941996812820435, Accuracy: 0.6904761904761905\n",
      "Step: 21, Loss: 1.1734778881072998, Accuracy: 0.6931818181818182\n",
      "Step: 22, Loss: 1.1640900373458862, Accuracy: 0.6992753623188406\n",
      "Step: 23, Loss: 1.39496648311615, Accuracy: 0.6909722222222222\n",
      "Step: 24, Loss: 0.99204021692276, Accuracy: 0.7\n",
      "Step: 25, Loss: 1.2108474969863892, Accuracy: 0.6987179487179487\n",
      "Step: 26, Loss: 1.0483043193817139, Accuracy: 0.7037037037037037\n",
      "Step: 27, Loss: 1.1743460893630981, Accuracy: 0.7053571428571429\n",
      "Step: 28, Loss: 1.0316303968429565, Accuracy: 0.7097701149425287\n",
      "Step: 29, Loss: 1.081525206565857, Accuracy: 0.7138888888888889\n",
      "Step: 30, Loss: 1.280439853668213, Accuracy: 0.7123655913978495\n",
      "Step: 31, Loss: 1.3088263273239136, Accuracy: 0.7083333333333334\n",
      "Step: 32, Loss: 1.2391716241836548, Accuracy: 0.7095959595959596\n",
      "Step: 33, Loss: 1.103582501411438, Accuracy: 0.7132352941176471\n",
      "Step: 34, Loss: 1.2950531244277954, Accuracy: 0.7095238095238096\n",
      "Step: 35, Loss: 1.2188704013824463, Accuracy: 0.7106481481481481\n",
      "Step: 36, Loss: 1.1487287282943726, Accuracy: 0.7117117117117117\n",
      "Step: 37, Loss: 1.3364802598953247, Accuracy: 0.7105263157894737\n",
      "Step: 38, Loss: 1.2011833190917969, Accuracy: 0.7094017094017094\n",
      "Step: 39, Loss: 1.3202751874923706, Accuracy: 0.70625\n",
      "Step: 40, Loss: 1.0498772859573364, Accuracy: 0.709349593495935\n",
      "Step: 41, Loss: 1.0721397399902344, Accuracy: 0.7123015873015873\n",
      "Step: 42, Loss: 1.2740532159805298, Accuracy: 0.7112403100775194\n",
      "Step: 43, Loss: 0.9846432209014893, Accuracy: 0.7159090909090909\n",
      "Step: 44, Loss: 1.1525272130966187, Accuracy: 0.7166666666666667\n",
      "Step: 45, Loss: 1.2174042463302612, Accuracy: 0.7155797101449275\n",
      "Step: 46, Loss: 1.1232380867004395, Accuracy: 0.7163120567375887\n",
      "Step: 47, Loss: 1.1398370265960693, Accuracy: 0.7170138888888888\n",
      "Step: 48, Loss: 1.2795259952545166, Accuracy: 0.7159863945578231\n",
      "Step: 49, Loss: 1.1782933473587036, Accuracy: 0.7166666666666667\n",
      "Step: 50, Loss: 1.1924827098846436, Accuracy: 0.7173202614379085\n",
      "Step: 51, Loss: 1.1353394985198975, Accuracy: 0.717948717948718\n",
      "Step: 52, Loss: 1.2006754875183105, Accuracy: 0.7185534591194969\n",
      "Step: 53, Loss: 1.2063363790512085, Accuracy: 0.7175925925925926\n",
      "Step: 54, Loss: 1.2246123552322388, Accuracy: 0.7166666666666667\n",
      "Step: 55, Loss: 1.2500957250595093, Accuracy: 0.7157738095238095\n",
      "Step: 56, Loss: 1.0063916444778442, Accuracy: 0.7192982456140351\n",
      "Step: 57, Loss: 1.136682152748108, Accuracy: 0.7212643678160919\n",
      "Step: 58, Loss: 1.2314430475234985, Accuracy: 0.7203389830508474\n",
      "Step: 59, Loss: 1.1591297388076782, Accuracy: 0.7208333333333333\n",
      "Step: 60, Loss: 1.1333812475204468, Accuracy: 0.7226775956284153\n",
      "Step: 61, Loss: 1.406150221824646, Accuracy: 0.7190860215053764\n",
      "Step: 62, Loss: 1.1333125829696655, Accuracy: 0.7195767195767195\n",
      "Step: 63, Loss: 1.255037546157837, Accuracy: 0.7174479166666666\n",
      "Step: 64, Loss: 1.0494438409805298, Accuracy: 0.7192307692307692\n",
      "Step: 65, Loss: 1.3171873092651367, Accuracy: 0.7171717171717171\n",
      "Step: 66, Loss: 1.2978196144104004, Accuracy: 0.7151741293532339\n",
      "Step: 67, Loss: 1.2064261436462402, Accuracy: 0.7156862745098039\n",
      "Step: 68, Loss: 0.9879700541496277, Accuracy: 0.7185990338164251\n",
      "Step: 69, Loss: 1.1818037033081055, Accuracy: 0.719047619047619\n",
      "Step: 70, Loss: 1.125212550163269, Accuracy: 0.7206572769953051\n",
      "Step: 71, Loss: 1.3216832876205444, Accuracy: 0.7199074074074074\n",
      "Step: 72, Loss: 1.1436145305633545, Accuracy: 0.7203196347031964\n",
      "Step: 73, Loss: 1.1691086292266846, Accuracy: 0.7207207207207207\n",
      "Step: 74, Loss: 1.1510800123214722, Accuracy: 0.7211111111111111\n",
      "Step: 75, Loss: 1.1640983819961548, Accuracy: 0.7214912280701754\n",
      "Step: 76, Loss: 1.0668729543685913, Accuracy: 0.7229437229437229\n",
      "Step: 77, Loss: 1.2475991249084473, Accuracy: 0.7222222222222222\n",
      "Step: 78, Loss: 1.1229093074798584, Accuracy: 0.7236286919831224\n",
      "Step: 79, Loss: 1.211203932762146, Accuracy: 0.7229166666666667\n",
      "Step: 80, Loss: 1.1437770128250122, Accuracy: 0.7232510288065843\n",
      "Step: 81, Loss: 1.3116364479064941, Accuracy: 0.7215447154471545\n",
      "Step: 82, Loss: 1.2079370021820068, Accuracy: 0.7218875502008032\n",
      "Step: 83, Loss: 1.007550597190857, Accuracy: 0.7232142857142857\n",
      "Step: 84, Loss: 1.2755221128463745, Accuracy: 0.7225490196078431\n",
      "Step: 85, Loss: 1.2240933179855347, Accuracy: 0.7218992248062015\n",
      "Step: 86, Loss: 1.3145123720169067, Accuracy: 0.7203065134099617\n",
      "Step: 87, Loss: 1.2503734827041626, Accuracy: 0.7196969696969697\n",
      "Step: 88, Loss: 1.245064377784729, Accuracy: 0.7191011235955056\n",
      "Step: 89, Loss: 1.183502197265625, Accuracy: 0.7194444444444444\n",
      "Step: 90, Loss: 1.2388436794281006, Accuracy: 0.7188644688644689\n",
      "Step: 91, Loss: 1.3387203216552734, Accuracy: 0.717391304347826\n",
      "Step: 92, Loss: 1.2486587762832642, Accuracy: 0.7168458781362007\n",
      "Step: 93, Loss: 1.1650718450546265, Accuracy: 0.7163120567375887\n",
      "Step: 94, Loss: 1.2236697673797607, Accuracy: 0.7157894736842105\n",
      "Step: 95, Loss: 1.08242666721344, Accuracy: 0.7170138888888888\n",
      "Step: 96, Loss: 1.0879676342010498, Accuracy: 0.718213058419244\n",
      "Step: 97, Loss: 1.2550688982009888, Accuracy: 0.717687074829932\n",
      "Step: 98, Loss: 1.1016943454742432, Accuracy: 0.7188552188552189\n",
      "Step: 99, Loss: 1.1177729368209839, Accuracy: 0.72\n",
      "Step: 100, Loss: 1.4583120346069336, Accuracy: 0.716996699669967\n",
      "Step: 101, Loss: 1.3219997882843018, Accuracy: 0.7156862745098039\n",
      "Step: 102, Loss: 1.3639302253723145, Accuracy: 0.7135922330097088\n",
      "Step: 103, Loss: 1.2395944595336914, Accuracy: 0.7131410256410257\n",
      "Step: 104, Loss: 1.312601923942566, Accuracy: 0.7119047619047619\n",
      "Step: 105, Loss: 1.238277792930603, Accuracy: 0.7114779874213837\n",
      "Step: 106, Loss: 1.1258537769317627, Accuracy: 0.7118380062305296\n",
      "Step: 107, Loss: 1.072022795677185, Accuracy: 0.7129629629629629\n",
      "Step: 108, Loss: 1.0794854164123535, Accuracy: 0.7140672782874617\n",
      "Step: 109, Loss: 1.5887104272842407, Accuracy: 0.7106060606060606\n",
      "Step: 110, Loss: 1.0139580965042114, Accuracy: 0.7124624624624625\n",
      "Step: 111, Loss: 1.0244568586349487, Accuracy: 0.7135416666666666\n",
      "Step: 112, Loss: 1.226094365119934, Accuracy: 0.7131268436578171\n",
      "Step: 113, Loss: 0.9920259118080139, Accuracy: 0.7149122807017544\n",
      "Step: 114, Loss: 1.2295656204223633, Accuracy: 0.7144927536231884\n",
      "Step: 115, Loss: 1.2740072011947632, Accuracy: 0.7140804597701149\n",
      "Step: 116, Loss: 1.2894006967544556, Accuracy: 0.7129629629629629\n",
      "Step: 117, Loss: 1.1462470293045044, Accuracy: 0.713276836158192\n",
      "Step: 118, Loss: 1.1913453340530396, Accuracy: 0.7128851540616247\n",
      "Step: 119, Loss: 1.0953596830368042, Accuracy: 0.7131944444444445\n",
      "Step: 120, Loss: 1.305999755859375, Accuracy: 0.7121212121212122\n",
      "Step: 121, Loss: 1.3540822267532349, Accuracy: 0.7103825136612022\n",
      "Step: 122, Loss: 1.4633455276489258, Accuracy: 0.7079945799457995\n",
      "Step: 123, Loss: 1.1554800271987915, Accuracy: 0.7083333333333334\n",
      "Step: 124, Loss: 1.2634423971176147, Accuracy: 0.708\n",
      "Step: 125, Loss: 1.2923914194107056, Accuracy: 0.707010582010582\n",
      "Step: 126, Loss: 0.9843800663948059, Accuracy: 0.7086614173228346\n",
      "Step: 127, Loss: 1.0937691926956177, Accuracy: 0.7096354166666666\n",
      "Step: 128, Loss: 1.1625080108642578, Accuracy: 0.7099483204134367\n",
      "Step: 129, Loss: 1.1910184621810913, Accuracy: 0.7096153846153846\n",
      "Step: 130, Loss: 1.421098232269287, Accuracy: 0.7080152671755725\n",
      "Step: 131, Loss: 1.3277746438980103, Accuracy: 0.7070707070707071\n",
      "Step: 132, Loss: 1.2455288171768188, Accuracy: 0.706766917293233\n",
      "Step: 133, Loss: 1.0505155324935913, Accuracy: 0.7077114427860697\n",
      "Step: 134, Loss: 1.2199440002441406, Accuracy: 0.7074074074074074\n",
      "Step: 135, Loss: 1.221981406211853, Accuracy: 0.7071078431372549\n",
      "Step: 136, Loss: 1.0805584192276, Accuracy: 0.708029197080292\n",
      "Step: 137, Loss: 1.1041699647903442, Accuracy: 0.7083333333333334\n",
      "Step: 138, Loss: 1.1633483171463013, Accuracy: 0.7086330935251799\n",
      "Step: 139, Loss: 1.1570054292678833, Accuracy: 0.7089285714285715\n",
      "Step: 140, Loss: 1.3892723321914673, Accuracy: 0.706855791962175\n",
      "Step: 141, Loss: 1.177512764930725, Accuracy: 0.7071596244131455\n",
      "Step: 142, Loss: 1.231601595878601, Accuracy: 0.7068764568764568\n",
      "Step: 143, Loss: 1.1109583377838135, Accuracy: 0.7077546296296297\n",
      "Step: 144, Loss: 1.2776803970336914, Accuracy: 0.707471264367816\n",
      "Step: 145, Loss: 1.1063064336776733, Accuracy: 0.7083333333333334\n",
      "Step: 146, Loss: 1.1185017824172974, Accuracy: 0.7091836734693877\n",
      "Step: 147, Loss: 1.0135935544967651, Accuracy: 0.7105855855855856\n",
      "Step: 148, Loss: 1.2437708377838135, Accuracy: 0.7102908277404921\n",
      "Step: 149, Loss: 1.2122029066085815, Accuracy: 0.7105555555555556\n",
      "Step: 150, Loss: 1.0750775337219238, Accuracy: 0.7113686534216336\n",
      "Step: 151, Loss: 1.0443507432937622, Accuracy: 0.7127192982456141\n",
      "Step: 152, Loss: 1.3043142557144165, Accuracy: 0.7118736383442266\n",
      "Step: 153, Loss: 1.1156704425811768, Accuracy: 0.7126623376623377\n",
      "Step: 154, Loss: 1.2459217309951782, Accuracy: 0.7123655913978495\n",
      "Step: 155, Loss: 1.1451178789138794, Accuracy: 0.7126068376068376\n",
      "Step: 156, Loss: 1.255940318107605, Accuracy: 0.7123142250530785\n",
      "Step: 157, Loss: 1.249047875404358, Accuracy: 0.7120253164556962\n",
      "Step: 158, Loss: 1.3080110549926758, Accuracy: 0.7112159329140462\n",
      "Step: 159, Loss: 1.3752855062484741, Accuracy: 0.7098958333333333\n",
      "Step: 160, Loss: 1.2073050737380981, Accuracy: 0.7096273291925466\n",
      "Step: 161, Loss: 1.0941756963729858, Accuracy: 0.7103909465020576\n",
      "Step: 162, Loss: 1.0692962408065796, Accuracy: 0.7111451942740287\n",
      "Step: 163, Loss: 1.1094688177108765, Accuracy: 0.711890243902439\n",
      "Step: 164, Loss: 1.3029041290283203, Accuracy: 0.7111111111111111\n",
      "Step: 165, Loss: 1.160406470298767, Accuracy: 0.7113453815261044\n",
      "Step: 166, Loss: 1.2532681226730347, Accuracy: 0.7110778443113772\n",
      "Step: 167, Loss: 1.057745337486267, Accuracy: 0.7123015873015873\n",
      "Step: 168, Loss: 1.0808466672897339, Accuracy: 0.7130177514792899\n",
      "Step: 169, Loss: 1.168667197227478, Accuracy: 0.7132352941176471\n",
      "Step: 170, Loss: 1.2877999544143677, Accuracy: 0.7129629629629629\n",
      "Step: 171, Loss: 1.082210898399353, Accuracy: 0.7136627906976745\n",
      "Step: 172, Loss: 1.057477355003357, Accuracy: 0.714354527938343\n",
      "Step: 173, Loss: 1.2156082391738892, Accuracy: 0.7140804597701149\n",
      "Step: 174, Loss: 1.2680662870407104, Accuracy: 0.7133333333333334\n",
      "Step: 175, Loss: 1.3177961111068726, Accuracy: 0.712594696969697\n",
      "Step: 176, Loss: 1.3043586015701294, Accuracy: 0.711864406779661\n",
      "Step: 177, Loss: 1.064469337463379, Accuracy: 0.7125468164794008\n",
      "Step: 178, Loss: 1.0281250476837158, Accuracy: 0.7132216014897579\n",
      "Step: 179, Loss: 1.165822982788086, Accuracy: 0.7134259259259259\n",
      "Step: 180, Loss: 1.2863458395004272, Accuracy: 0.712707182320442\n",
      "Step: 181, Loss: 1.1917649507522583, Accuracy: 0.7129120879120879\n",
      "Step: 182, Loss: 1.2474781274795532, Accuracy: 0.7126593806921676\n",
      "Step: 183, Loss: 1.3523648977279663, Accuracy: 0.7119565217391305\n",
      "Step: 184, Loss: 1.1602593660354614, Accuracy: 0.7121621621621622\n",
      "Step: 185, Loss: 1.3325732946395874, Accuracy: 0.7114695340501792\n",
      "Step: 186, Loss: 1.2848443984985352, Accuracy: 0.7107843137254902\n",
      "Step: 187, Loss: 1.030286431312561, Accuracy: 0.7118794326241135\n",
      "Step: 188, Loss: 1.2399022579193115, Accuracy: 0.7116402116402116\n",
      "Step: 189, Loss: 1.2223843336105347, Accuracy: 0.7118421052631579\n",
      "Step: 190, Loss: 0.9794254302978516, Accuracy: 0.712914485165794\n",
      "Step: 191, Loss: 1.1596951484680176, Accuracy: 0.7131076388888888\n",
      "Step: 192, Loss: 1.535414695739746, Accuracy: 0.711139896373057\n",
      "Step: 193, Loss: 0.9128827452659607, Accuracy: 0.7126288659793815\n",
      "Step: 194, Loss: 1.146981120109558, Accuracy: 0.7128205128205128\n",
      "Step: 195, Loss: 1.1974000930786133, Accuracy: 0.7130102040816326\n",
      "Step: 196, Loss: 1.1769644021987915, Accuracy: 0.7131979695431472\n",
      "Step: 197, Loss: 1.2750722169876099, Accuracy: 0.7125420875420876\n",
      "Step: 198, Loss: 1.3773117065429688, Accuracy: 0.7114740368509213\n",
      "Step: 199, Loss: 1.0443007946014404, Accuracy: 0.7120833333333333\n",
      "Step: 200, Loss: 1.3457026481628418, Accuracy: 0.711028192371476\n",
      "Step: 201, Loss: 1.2990347146987915, Accuracy: 0.7103960396039604\n",
      "Step: 202, Loss: 1.339342474937439, Accuracy: 0.7097701149425287\n",
      "Step: 203, Loss: 1.1750086545944214, Accuracy: 0.7099673202614379\n",
      "Step: 204, Loss: 1.112880825996399, Accuracy: 0.7105691056910569\n",
      "Step: 205, Loss: 1.2407220602035522, Accuracy: 0.7103559870550162\n",
      "Step: 206, Loss: 1.4365053176879883, Accuracy: 0.7093397745571659\n",
      "Step: 207, Loss: 1.4097065925598145, Accuracy: 0.7083333333333334\n",
      "Step: 208, Loss: 1.0767987966537476, Accuracy: 0.7089314194577353\n",
      "Step: 209, Loss: 1.1587592363357544, Accuracy: 0.7091269841269842\n",
      "Step: 210, Loss: 1.1949282884597778, Accuracy: 0.7089257503949447\n",
      "Step: 211, Loss: 1.3508104085922241, Accuracy: 0.7079402515723271\n",
      "Step: 212, Loss: 1.3246556520462036, Accuracy: 0.7073552425665102\n",
      "Step: 213, Loss: 1.197858452796936, Accuracy: 0.7071651090342679\n",
      "Step: 214, Loss: 1.0297808647155762, Accuracy: 0.708139534883721\n",
      "Step: 215, Loss: 1.1191750764846802, Accuracy: 0.7087191358024691\n",
      "Step: 216, Loss: 1.1201714277267456, Accuracy: 0.7089093701996928\n",
      "Step: 217, Loss: 1.274547815322876, Accuracy: 0.7083333333333334\n",
      "Step: 218, Loss: 1.178394079208374, Accuracy: 0.7081430745814308\n",
      "Step: 219, Loss: 1.337125301361084, Accuracy: 0.7075757575757575\n",
      "Step: 220, Loss: 1.3884276151657104, Accuracy: 0.7066365007541479\n",
      "Step: 221, Loss: 1.393884301185608, Accuracy: 0.7057057057057057\n",
      "Step: 222, Loss: 1.0729947090148926, Accuracy: 0.7059043348281017\n",
      "Step: 223, Loss: 1.3927974700927734, Accuracy: 0.7049851190476191\n",
      "Step: 224, Loss: 1.2421292066574097, Accuracy: 0.7048148148148148\n",
      "Step: 225, Loss: 1.270647406578064, Accuracy: 0.7042772861356932\n",
      "Step: 226, Loss: 1.250029444694519, Accuracy: 0.7041116005873715\n",
      "Step: 227, Loss: 1.5465787649154663, Accuracy: 0.702485380116959\n",
      "Step: 228, Loss: 1.0910427570343018, Accuracy: 0.7034206695778749\n",
      "Step: 229, Loss: 1.0690644979476929, Accuracy: 0.7039855072463768\n",
      "Step: 230, Loss: 1.1482974290847778, Accuracy: 0.7041847041847041\n",
      "Step: 231, Loss: 1.2428730726242065, Accuracy: 0.7036637931034483\n",
      "Step: 232, Loss: 1.3298234939575195, Accuracy: 0.7031473533619457\n",
      "Step: 233, Loss: 1.2168432474136353, Accuracy: 0.7033475783475783\n",
      "Step: 234, Loss: 1.5064435005187988, Accuracy: 0.7021276595744681\n",
      "Step: 235, Loss: 1.3678911924362183, Accuracy: 0.701271186440678\n",
      "Step: 236, Loss: 1.2453669309616089, Accuracy: 0.7011251758087201\n",
      "Step: 237, Loss: 1.1844490766525269, Accuracy: 0.7013305322128851\n",
      "Step: 238, Loss: 1.324843168258667, Accuracy: 0.700836820083682\n",
      "Step: 239, Loss: 1.5717869997024536, Accuracy: 0.6993055555555555\n",
      "Step: 240, Loss: 1.3156489133834839, Accuracy: 0.6984785615491009\n",
      "Step: 241, Loss: 1.2761127948760986, Accuracy: 0.6983471074380165\n",
      "Step: 242, Loss: 1.2845183610916138, Accuracy: 0.6982167352537723\n",
      "Step: 243, Loss: 1.5521682500839233, Accuracy: 0.6970628415300546\n",
      "Step: 244, Loss: 1.5577473640441895, Accuracy: 0.6955782312925171\n",
      "Step: 245, Loss: 1.2182092666625977, Accuracy: 0.6954607046070461\n",
      "Step: 246, Loss: 1.0723096132278442, Accuracy: 0.6960188933873145\n",
      "Step: 247, Loss: 0.9495871663093567, Accuracy: 0.697244623655914\n",
      "Step: 248, Loss: 1.1195095777511597, Accuracy: 0.6977911646586346\n",
      "Step: 249, Loss: 1.1358293294906616, Accuracy: 0.698\n",
      "Step: 250, Loss: 1.0096824169158936, Accuracy: 0.6988711819389111\n",
      "Step: 251, Loss: 1.149308681488037, Accuracy: 0.6990740740740741\n",
      "Step: 252, Loss: 1.1788898706436157, Accuracy: 0.6992753623188406\n",
      "Step: 253, Loss: 1.158420205116272, Accuracy: 0.699475065616798\n",
      "Step: 254, Loss: 1.0729788541793823, Accuracy: 0.7\n",
      "Step: 255, Loss: 1.3014155626296997, Accuracy: 0.6995442708333334\n",
      "Step: 256, Loss: 1.1974881887435913, Accuracy: 0.6994163424124513\n",
      "Step: 257, Loss: 1.1337865591049194, Accuracy: 0.6996124031007752\n",
      "Step: 258, Loss: 1.2116585969924927, Accuracy: 0.6994851994851995\n",
      "Step: 259, Loss: 1.0551632642745972, Accuracy: 0.7\n",
      "Step: 260, Loss: 1.2701427936553955, Accuracy: 0.6995530012771393\n",
      "Step: 261, Loss: 1.0327595472335815, Accuracy: 0.700381679389313\n",
      "Step: 262, Loss: 1.2482191324234009, Accuracy: 0.7002534854245881\n",
      "Step: 263, Loss: 1.0971473455429077, Accuracy: 0.7007575757575758\n",
      "Step: 264, Loss: 1.2995375394821167, Accuracy: 0.700314465408805\n",
      "Step: 265, Loss: 1.0825049877166748, Accuracy: 0.7008145363408521\n",
      "Step: 266, Loss: 1.3298319578170776, Accuracy: 0.7000624219725343\n",
      "Step: 267, Loss: 1.0831546783447266, Accuracy: 0.7005597014925373\n",
      "Step: 268, Loss: 1.1469334363937378, Accuracy: 0.7007434944237918\n",
      "Step: 269, Loss: 1.1910605430603027, Accuracy: 0.7006172839506173\n",
      "Step: 270, Loss: 1.255387544631958, Accuracy: 0.7004920049200491\n",
      "Step: 271, Loss: 1.2000726461410522, Accuracy: 0.7003676470588235\n",
      "Step: 272, Loss: 1.046118974685669, Accuracy: 0.7008547008547008\n",
      "Step: 273, Loss: 1.1626710891723633, Accuracy: 0.7010340632603407\n",
      "Step: 274, Loss: 1.0672181844711304, Accuracy: 0.7015151515151515\n",
      "Step: 275, Loss: 1.1362074613571167, Accuracy: 0.7016908212560387\n",
      "Step: 276, Loss: 1.1438794136047363, Accuracy: 0.7018652226233454\n",
      "Step: 277, Loss: 1.2239381074905396, Accuracy: 0.7017386091127098\n",
      "Step: 278, Loss: 1.2097535133361816, Accuracy: 0.7016129032258065\n",
      "Step: 279, Loss: 1.4363656044006348, Accuracy: 0.7008928571428571\n",
      "Step: 280, Loss: 1.1521726846694946, Accuracy: 0.701067615658363\n",
      "Step: 281, Loss: 1.0377298593521118, Accuracy: 0.7018321513002365\n",
      "Step: 282, Loss: 1.555193543434143, Accuracy: 0.700530035335689\n",
      "Step: 283, Loss: 1.2670689821243286, Accuracy: 0.7001173708920188\n",
      "Step: 284, Loss: 1.2344478368759155, Accuracy: 0.7\n",
      "Step: 285, Loss: 1.0792796611785889, Accuracy: 0.7004662004662005\n",
      "Step: 286, Loss: 1.1650035381317139, Accuracy: 0.7006387921022067\n",
      "Step: 287, Loss: 1.163796305656433, Accuracy: 0.7008101851851852\n",
      "Step: 288, Loss: 1.307369351387024, Accuracy: 0.7004036908881199\n",
      "Step: 289, Loss: 1.130540370941162, Accuracy: 0.7005747126436782\n",
      "Step: 290, Loss: 1.2128785848617554, Accuracy: 0.7007445589919816\n",
      "Step: 291, Loss: 1.2380167245864868, Accuracy: 0.7006278538812786\n",
      "Step: 292, Loss: 1.1143594980239868, Accuracy: 0.7010807736063709\n",
      "Step: 293, Loss: 1.2447513341903687, Accuracy: 0.7009637188208617\n",
      "Step: 294, Loss: 1.427804946899414, Accuracy: 0.7\n",
      "Step: 295, Loss: 1.1120717525482178, Accuracy: 0.700168918918919\n",
      "Step: 296, Loss: 1.2199071645736694, Accuracy: 0.7000561167227833\n",
      "Step: 297, Loss: 1.1445865631103516, Accuracy: 0.7002237136465325\n",
      "Step: 298, Loss: 1.3688594102859497, Accuracy: 0.6995540691192865\n",
      "Step: 299, Loss: 1.0766078233718872, Accuracy: 0.7\n",
      "Step: 300, Loss: 1.0025360584259033, Accuracy: 0.7007198228128461\n",
      "Step: 301, Loss: 1.1959306001663208, Accuracy: 0.7008830022075055\n",
      "Step: 302, Loss: 1.1806665658950806, Accuracy: 0.7010451045104511\n",
      "Step: 303, Loss: 1.304862141609192, Accuracy: 0.7003837719298246\n",
      "Step: 304, Loss: 1.1677151918411255, Accuracy: 0.7005464480874317\n",
      "Step: 305, Loss: 1.16163170337677, Accuracy: 0.7007080610021786\n",
      "Step: 306, Loss: 1.4020017385482788, Accuracy: 0.7000542888165038\n",
      "Step: 307, Loss: 1.2334049940109253, Accuracy: 0.6999458874458875\n",
      "Step: 308, Loss: 1.131829857826233, Accuracy: 0.7001078748651565\n",
      "Step: 309, Loss: 1.0662373304367065, Accuracy: 0.7005376344086022\n",
      "Step: 310, Loss: 1.1063792705535889, Accuracy: 0.7009646302250804\n",
      "Step: 311, Loss: 1.2871778011322021, Accuracy: 0.7008547008547008\n",
      "Step: 312, Loss: 1.133724331855774, Accuracy: 0.7010117145899893\n",
      "Step: 313, Loss: 1.0284720659255981, Accuracy: 0.7016985138004246\n",
      "Step: 314, Loss: 1.5283392667770386, Accuracy: 0.7005291005291006\n",
      "Step: 315, Loss: 1.1407877206802368, Accuracy: 0.7006856540084389\n",
      "Step: 316, Loss: 1.0344191789627075, Accuracy: 0.7013669821240799\n",
      "Step: 317, Loss: 1.2208741903305054, Accuracy: 0.7012578616352201\n",
      "Step: 318, Loss: 1.3243449926376343, Accuracy: 0.7008881922675027\n",
      "Step: 319, Loss: 1.5425206422805786, Accuracy: 0.6997395833333333\n",
      "Step: 320, Loss: 1.304353952407837, Accuracy: 0.6993769470404985\n",
      "Step: 321, Loss: 1.1680864095687866, Accuracy: 0.6995341614906833\n",
      "Step: 322, Loss: 1.2966817617416382, Accuracy: 0.6991744066047472\n",
      "Step: 323, Loss: 1.147478699684143, Accuracy: 0.6993312757201646\n",
      "Step: 324, Loss: 1.0920754671096802, Accuracy: 0.6997435897435897\n",
      "Step: 325, Loss: 1.1558568477630615, Accuracy: 0.6998977505112475\n",
      "Step: 326, Loss: 1.1788748502731323, Accuracy: 0.7000509683995922\n",
      "Step: 327, Loss: 1.1101996898651123, Accuracy: 0.7004573170731707\n",
      "Step: 328, Loss: 1.198838233947754, Accuracy: 0.7006079027355623\n",
      "Step: 329, Loss: 1.218082070350647, Accuracy: 0.7005050505050505\n",
      "Step: 330, Loss: 1.1155117750167847, Accuracy: 0.7009063444108762\n",
      "Step: 331, Loss: 1.3396292924880981, Accuracy: 0.7005522088353414\n",
      "Step: 332, Loss: 1.2470080852508545, Accuracy: 0.7004504504504504\n",
      "Step: 333, Loss: 1.5344001054763794, Accuracy: 0.6993512974051896\n",
      "Step: 334, Loss: 1.0832170248031616, Accuracy: 0.6997512437810945\n",
      "Step: 335, Loss: 0.9968592524528503, Accuracy: 0.7003968253968254\n",
      "Step: 336, Loss: 1.0980528593063354, Accuracy: 0.7007912957467853\n",
      "Step: 337, Loss: 1.0807029008865356, Accuracy: 0.7011834319526628\n",
      "Step: 338, Loss: 1.0637253522872925, Accuracy: 0.7015732546705998\n",
      "Step: 339, Loss: 1.4812384843826294, Accuracy: 0.700735294117647\n",
      "Step: 340, Loss: 1.0865141153335571, Accuracy: 0.7011241446725318\n",
      "Step: 341, Loss: 1.224784255027771, Accuracy: 0.7010233918128655\n",
      "Step: 342, Loss: 1.090969204902649, Accuracy: 0.7014091350826045\n",
      "Step: 343, Loss: 1.4669431447982788, Accuracy: 0.7005813953488372\n",
      "Step: 344, Loss: 1.0777517557144165, Accuracy: 0.7009661835748793\n",
      "Step: 345, Loss: 1.2904475927352905, Accuracy: 0.700626204238921\n",
      "Step: 346, Loss: 1.1626225709915161, Accuracy: 0.7007684918347743\n",
      "Step: 347, Loss: 1.3968712091445923, Accuracy: 0.7001915708812261\n",
      "Step: 348, Loss: 1.0718004703521729, Accuracy: 0.7005730659025788\n",
      "Step: 349, Loss: 1.099981427192688, Accuracy: 0.700952380952381\n",
      "Step: 350, Loss: 1.1062004566192627, Accuracy: 0.701329534662868\n",
      "Step: 351, Loss: 1.0691370964050293, Accuracy: 0.7017045454545454\n",
      "Step: 352, Loss: 1.1062302589416504, Accuracy: 0.7020774315391879\n",
      "Step: 353, Loss: 1.3941529989242554, Accuracy: 0.7015065913370998\n",
      "Step: 354, Loss: 1.4852056503295898, Accuracy: 0.7007042253521126\n",
      "Step: 355, Loss: 1.068750262260437, Accuracy: 0.7010767790262172\n",
      "Step: 356, Loss: 1.2372487783432007, Accuracy: 0.7012138188608776\n",
      "Step: 357, Loss: 1.2779282331466675, Accuracy: 0.7008845437616388\n",
      "Step: 358, Loss: 1.4623980522155762, Accuracy: 0.7000928505106778\n",
      "Step: 359, Loss: 1.055508017539978, Accuracy: 0.7006944444444444\n",
      "Step: 360, Loss: 1.1295087337493896, Accuracy: 0.701061865189289\n",
      "Step: 361, Loss: 1.2379302978515625, Accuracy: 0.7009668508287292\n",
      "Step: 362, Loss: 1.384871482849121, Accuracy: 0.7004132231404959\n",
      "Step: 363, Loss: 1.2134196758270264, Accuracy: 0.7003205128205128\n",
      "Step: 364, Loss: 1.3398417234420776, Accuracy: 0.7\n",
      "Step: 365, Loss: 1.2506767511367798, Accuracy: 0.6999089253187614\n",
      "Step: 366, Loss: 1.2649502754211426, Accuracy: 0.6995912806539509\n",
      "Step: 367, Loss: 1.1050281524658203, Accuracy: 0.6997282608695652\n",
      "Step: 368, Loss: 1.2163540124893188, Accuracy: 0.6994128274616079\n",
      "Step: 369, Loss: 1.2296799421310425, Accuracy: 0.6993243243243243\n",
      "Step: 370, Loss: 1.3125882148742676, Accuracy: 0.6990116801437556\n",
      "Step: 371, Loss: 1.093580961227417, Accuracy: 0.6991487455197133\n",
      "Step: 372, Loss: 1.1178622245788574, Accuracy: 0.6992850759606791\n",
      "Step: 373, Loss: 1.2651773691177368, Accuracy: 0.6991978609625669\n",
      "Step: 374, Loss: 1.183764100074768, Accuracy: 0.6993333333333334\n",
      "Step: 375, Loss: 1.1435366868972778, Accuracy: 0.699468085106383\n",
      "Step: 376, Loss: 1.3846794366836548, Accuracy: 0.6989389920424404\n",
      "Step: 377, Loss: 1.1714142560958862, Accuracy: 0.6990740740740741\n",
      "Step: 378, Loss: 1.2210122346878052, Accuracy: 0.6989885664028144\n",
      "Step: 379, Loss: 1.3435839414596558, Accuracy: 0.6986842105263158\n",
      "Step: 380, Loss: 1.0777846574783325, Accuracy: 0.6990376202974629\n",
      "Step: 381, Loss: 1.1527591943740845, Accuracy: 0.6991710296684118\n",
      "Step: 382, Loss: 1.174639105796814, Accuracy: 0.6993037423846823\n",
      "Step: 383, Loss: 1.3317065238952637, Accuracy: 0.6990017361111112\n",
      "Step: 384, Loss: 1.1475766897201538, Accuracy: 0.6991341991341992\n",
      "Step: 385, Loss: 1.0561957359313965, Accuracy: 0.6996977547495682\n",
      "Step: 386, Loss: 1.2441121339797974, Accuracy: 0.6996124031007752\n",
      "Step: 387, Loss: 1.1778169870376587, Accuracy: 0.6995274914089347\n",
      "Step: 388, Loss: 1.1378202438354492, Accuracy: 0.6996572407883462\n",
      "Step: 389, Loss: 1.2029460668563843, Accuracy: 0.6995726495726495\n",
      "Step: 390, Loss: 1.2267051935195923, Accuracy: 0.6994884910485933\n",
      "Step: 391, Loss: 1.253513216972351, Accuracy: 0.6994047619047619\n",
      "Step: 392, Loss: 1.4617148637771606, Accuracy: 0.6986853265479219\n",
      "Step: 393, Loss: 1.238838791847229, Accuracy: 0.6986040609137056\n",
      "Step: 394, Loss: 1.0821963548660278, Accuracy: 0.6989451476793249\n",
      "Step: 395, Loss: 1.3284939527511597, Accuracy: 0.6986531986531986\n",
      "Step: 396, Loss: 1.1175802946090698, Accuracy: 0.6987825356842989\n",
      "Step: 397, Loss: 1.012449860572815, Accuracy: 0.6993299832495813\n",
      "Step: 398, Loss: 1.2110527753829956, Accuracy: 0.6992481203007519\n",
      "Step: 399, Loss: 1.0876836776733398, Accuracy: 0.6995833333333333\n",
      "Step: 400, Loss: 1.479476809501648, Accuracy: 0.6988778054862843\n",
      "Step: 401, Loss: 1.2465715408325195, Accuracy: 0.6987976782752903\n",
      "Step: 402, Loss: 1.2546948194503784, Accuracy: 0.6987179487179487\n",
      "Step: 403, Loss: 1.2420003414154053, Accuracy: 0.6986386138613861\n",
      "Step: 404, Loss: 1.0773621797561646, Accuracy: 0.6989711934156378\n",
      "Step: 405, Loss: 1.0247288942337036, Accuracy: 0.6995073891625616\n",
      "Step: 406, Loss: 1.2249929904937744, Accuracy: 0.6994266994266994\n",
      "Step: 407, Loss: 1.1344317197799683, Accuracy: 0.6995506535947712\n",
      "Step: 408, Loss: 1.056104063987732, Accuracy: 0.6996740016299918\n",
      "Step: 409, Loss: 1.268399715423584, Accuracy: 0.6995934959349593\n",
      "Step: 410, Loss: 1.24172842502594, Accuracy: 0.6993106244931062\n",
      "Step: 411, Loss: 1.3584028482437134, Accuracy: 0.698826860841424\n",
      "Step: 412, Loss: 1.0850023031234741, Accuracy: 0.6991525423728814\n",
      "Step: 413, Loss: 1.21216881275177, Accuracy: 0.6990740740740741\n",
      "Step: 414, Loss: 1.1457599401474, Accuracy: 0.6993975903614458\n",
      "Step: 415, Loss: 1.1590596437454224, Accuracy: 0.6995192307692307\n",
      "Step: 416, Loss: 1.1657589673995972, Accuracy: 0.6994404476418865\n",
      "Step: 417, Loss: 1.058592438697815, Accuracy: 0.6997607655502392\n",
      "Step: 418, Loss: 1.3844375610351562, Accuracy: 0.6992840095465394\n",
      "Step: 419, Loss: 1.1884233951568604, Accuracy: 0.6994047619047619\n",
      "Step: 420, Loss: 1.2288918495178223, Accuracy: 0.6993269992082344\n",
      "Step: 421, Loss: 1.1300249099731445, Accuracy: 0.6994470774091627\n",
      "Step: 422, Loss: 1.4400367736816406, Accuracy: 0.6987785657998424\n",
      "Step: 423, Loss: 1.2333935499191284, Accuracy: 0.6987028301886793\n",
      "Step: 424, Loss: 1.231284737586975, Accuracy: 0.6986274509803921\n",
      "Step: 425, Loss: 1.1710013151168823, Accuracy: 0.6987480438184663\n",
      "Step: 426, Loss: 1.256988286972046, Accuracy: 0.6984777517564403\n",
      "Step: 427, Loss: 1.0804495811462402, Accuracy: 0.6987928348909658\n",
      "Step: 428, Loss: 1.241385817527771, Accuracy: 0.6987179487179487\n",
      "Step: 429, Loss: 1.3233503103256226, Accuracy: 0.6984496124031008\n",
      "Step: 430, Loss: 1.0057733058929443, Accuracy: 0.6989559164733179\n",
      "Step: 431, Loss: 1.1892945766448975, Accuracy: 0.6990740740740741\n",
      "Step: 432, Loss: 1.3684929609298706, Accuracy: 0.6988067744418783\n",
      "Step: 433, Loss: 1.0529779195785522, Accuracy: 0.6991167434715821\n",
      "Step: 434, Loss: 1.0606809854507446, Accuracy: 0.6994252873563218\n",
      "Step: 435, Loss: 1.380759835243225, Accuracy: 0.6989678899082569\n",
      "Step: 436, Loss: 1.0491987466812134, Accuracy: 0.6992753623188406\n",
      "Step: 437, Loss: 1.2275265455245972, Accuracy: 0.6992009132420092\n",
      "Step: 438, Loss: 1.2080285549163818, Accuracy: 0.6991268033409264\n",
      "Step: 439, Loss: 1.0695940256118774, Accuracy: 0.6994318181818182\n",
      "Step: 440, Loss: 1.214861273765564, Accuracy: 0.6993575207860923\n",
      "Step: 441, Loss: 1.1699191331863403, Accuracy: 0.69947209653092\n",
      "Step: 442, Loss: 1.2510316371917725, Accuracy: 0.699398043641836\n",
      "Step: 443, Loss: 1.244462013244629, Accuracy: 0.6993243243243243\n",
      "Step: 444, Loss: 1.0192588567733765, Accuracy: 0.699812734082397\n",
      "Step: 445, Loss: 1.164157748222351, Accuracy: 0.6999252615844545\n",
      "Step: 446, Loss: 1.120222568511963, Accuracy: 0.7000372856077554\n",
      "Step: 447, Loss: 1.1393120288848877, Accuracy: 0.7001488095238095\n",
      "Step: 448, Loss: 1.201159954071045, Accuracy: 0.7002598366740905\n",
      "Step: 449, Loss: 1.2327967882156372, Accuracy: 0.7001851851851851\n",
      "Step: 450, Loss: 1.2686938047409058, Accuracy: 0.700110864745011\n",
      "Step: 451, Loss: 1.072922945022583, Accuracy: 0.700405604719764\n",
      "Step: 452, Loss: 1.4259287118911743, Accuracy: 0.6999632082413539\n",
      "Step: 453, Loss: 1.4650044441223145, Accuracy: 0.6993392070484582\n",
      "Step: 454, Loss: 1.2230844497680664, Accuracy: 0.6992673992673992\n",
      "Step: 455, Loss: 1.0659008026123047, Accuracy: 0.6995614035087719\n",
      "Step: 456, Loss: 1.2345366477966309, Accuracy: 0.6994894237782641\n",
      "Step: 457, Loss: 1.0182623863220215, Accuracy: 0.6999636098981077\n",
      "Step: 458, Loss: 1.2192331552505493, Accuracy: 0.6998910675381264\n",
      "Step: 459, Loss: 1.1572405099868774, Accuracy: 0.7\n",
      "Step: 460, Loss: 1.368404507637024, Accuracy: 0.6995661605206074\n",
      "Step: 461, Loss: 1.093140721321106, Accuracy: 0.6998556998556998\n",
      "Step: 462, Loss: 1.493013858795166, Accuracy: 0.699244060475162\n",
      "Step: 463, Loss: 1.1280869245529175, Accuracy: 0.6993534482758621\n",
      "Step: 464, Loss: 1.264949917793274, Accuracy: 0.6992831541218638\n",
      "Step: 465, Loss: 1.2122596502304077, Accuracy: 0.6992131616595136\n",
      "Step: 466, Loss: 1.4823869466781616, Accuracy: 0.698429693076374\n",
      "Step: 467, Loss: 1.1308659315109253, Accuracy: 0.698539886039886\n",
      "Step: 468, Loss: 1.1069501638412476, Accuracy: 0.6986496090973703\n",
      "Step: 469, Loss: 1.142358660697937, Accuracy: 0.698936170212766\n",
      "Step: 470, Loss: 1.2310962677001953, Accuracy: 0.6988676574663836\n",
      "Step: 471, Loss: 1.172876000404358, Accuracy: 0.698975988700565\n",
      "Step: 472, Loss: 1.1512399911880493, Accuracy: 0.6990838618745595\n",
      "Step: 473, Loss: 1.1381759643554688, Accuracy: 0.6991912798874824\n",
      "Step: 474, Loss: 1.2124314308166504, Accuracy: 0.6991228070175438\n",
      "Step: 475, Loss: 1.233260154724121, Accuracy: 0.6990546218487395\n",
      "Step: 476, Loss: 1.0429121255874634, Accuracy: 0.6995108315863033\n",
      "Step: 477, Loss: 1.1261208057403564, Accuracy: 0.6996164574616457\n",
      "Step: 478, Loss: 1.139039397239685, Accuracy: 0.6997216423103688\n",
      "Step: 479, Loss: 1.0333404541015625, Accuracy: 0.7001736111111111\n",
      "Step: 480, Loss: 1.2559558153152466, Accuracy: 0.6999306999306999\n",
      "Step: 481, Loss: 1.020005226135254, Accuracy: 0.7003803596127247\n",
      "Step: 482, Loss: 1.1629976034164429, Accuracy: 0.7004830917874396\n",
      "Step: 483, Loss: 1.2269049882888794, Accuracy: 0.7004132231404959\n",
      "Step: 484, Loss: 1.2280058860778809, Accuracy: 0.7003436426116838\n",
      "Step: 485, Loss: 1.3321270942687988, Accuracy: 0.7001028806584362\n",
      "Step: 486, Loss: 1.2383774518966675, Accuracy: 0.7000342231348392\n",
      "Step: 487, Loss: 1.142472267150879, Accuracy: 0.7003073770491803\n",
      "Step: 488, Loss: 1.2178369760513306, Accuracy: 0.7002385821404227\n",
      "Step: 489, Loss: 1.0198591947555542, Accuracy: 0.7006802721088435\n",
      "Step: 490, Loss: 1.2252591848373413, Accuracy: 0.7006109979633401\n",
      "Step: 491, Loss: 1.229063630104065, Accuracy: 0.7005420054200542\n",
      "Step: 492, Loss: 1.05535888671875, Accuracy: 0.7008113590263692\n",
      "Step: 493, Loss: 1.0786547660827637, Accuracy: 0.7010796221322537\n",
      "Step: 494, Loss: 1.1724008321762085, Accuracy: 0.7011784511784511\n",
      "Step: 495, Loss: 1.2263574600219727, Accuracy: 0.7011088709677419\n",
      "Step: 496, Loss: 1.1675832271575928, Accuracy: 0.7010395707578806\n",
      "Step: 497, Loss: 1.0050703287124634, Accuracy: 0.7014725568942436\n",
      "Step: 498, Loss: 1.3721407651901245, Accuracy: 0.7010688042752171\n",
      "Step: 499, Loss: 1.1721938848495483, Accuracy: 0.7011666666666667\n",
      "Step: 500, Loss: 1.1643561124801636, Accuracy: 0.7012641383898869\n",
      "Step: 501, Loss: 1.1927446126937866, Accuracy: 0.7013612217795485\n",
      "Step: 502, Loss: 1.198095679283142, Accuracy: 0.7012922465208747\n",
      "Step: 503, Loss: 1.0195822715759277, Accuracy: 0.7017195767195767\n",
      "Step: 504, Loss: 1.2526359558105469, Accuracy: 0.7016501650165017\n",
      "Step: 505, Loss: 1.023105502128601, Accuracy: 0.7020750988142292\n",
      "Step: 506, Loss: 1.153009295463562, Accuracy: 0.7021696252465484\n",
      "Step: 507, Loss: 1.2352027893066406, Accuracy: 0.7020997375328084\n",
      "Step: 508, Loss: 1.1523354053497314, Accuracy: 0.7021938441388343\n",
      "Step: 509, Loss: 1.1895946264266968, Accuracy: 0.7022875816993464\n",
      "Step: 510, Loss: 1.0975157022476196, Accuracy: 0.7025440313111546\n",
      "Step: 511, Loss: 1.1836442947387695, Accuracy: 0.7024739583333334\n",
      "Step: 512, Loss: 1.1186577081680298, Accuracy: 0.702729044834308\n",
      "Step: 513, Loss: 1.3515619039535522, Accuracy: 0.7024967574578469\n",
      "Step: 514, Loss: 1.3503538370132446, Accuracy: 0.7021035598705502\n",
      "Step: 515, Loss: 1.2385737895965576, Accuracy: 0.7020348837209303\n",
      "Step: 516, Loss: 1.2016692161560059, Accuracy: 0.7019664732430689\n",
      "Step: 517, Loss: 1.248336672782898, Accuracy: 0.7018983268983269\n",
      "Step: 518, Loss: 1.1773892641067505, Accuracy: 0.7019910083493899\n",
      "Step: 519, Loss: 1.1356794834136963, Accuracy: 0.7020833333333333\n",
      "Step: 520, Loss: 1.1749683618545532, Accuracy: 0.7021753039027511\n",
      "Step: 521, Loss: 1.2270413637161255, Accuracy: 0.7021072796934866\n",
      "Step: 522, Loss: 1.2432082891464233, Accuracy: 0.7020395156150414\n",
      "Step: 523, Loss: 1.4724321365356445, Accuracy: 0.7014949109414759\n",
      "Step: 524, Loss: 1.1500563621520996, Accuracy: 0.7015873015873015\n",
      "Step: 525, Loss: 1.2335948944091797, Accuracy: 0.7015209125475285\n",
      "Step: 526, Loss: 1.3836936950683594, Accuracy: 0.7011385199240987\n",
      "Step: 527, Loss: 1.1601459980010986, Accuracy: 0.7012310606060606\n",
      "Step: 528, Loss: 0.991556704044342, Accuracy: 0.701638311279143\n",
      "Step: 529, Loss: 1.244477391242981, Accuracy: 0.7015723270440252\n",
      "Step: 530, Loss: 1.148703694343567, Accuracy: 0.7016635279347144\n",
      "Step: 531, Loss: 1.2192462682724, Accuracy: 0.7015977443609023\n",
      "Step: 532, Loss: 1.0841422080993652, Accuracy: 0.7018449030644153\n",
      "Step: 533, Loss: 1.0719634294509888, Accuracy: 0.7020911360799001\n",
      "Step: 534, Loss: 1.1653867959976196, Accuracy: 0.7021806853582554\n",
      "Step: 535, Loss: 1.4391342401504517, Accuracy: 0.7016480099502488\n",
      "Step: 536, Loss: 1.5186527967453003, Accuracy: 0.7009621353196772\n",
      "Step: 537, Loss: 1.1613367795944214, Accuracy: 0.7010532837670385\n",
      "Step: 538, Loss: 1.1943970918655396, Accuracy: 0.7011440940012369\n",
      "Step: 539, Loss: 1.1224550008773804, Accuracy: 0.7012345679012346\n",
      "Step: 540, Loss: 1.3190001249313354, Accuracy: 0.7010166358595195\n",
      "Step: 541, Loss: 1.0803372859954834, Accuracy: 0.701260762607626\n",
      "Step: 542, Loss: 1.3335213661193848, Accuracy: 0.7010435850214856\n",
      "Step: 543, Loss: 1.1411020755767822, Accuracy: 0.7011335784313726\n",
      "Step: 544, Loss: 1.3318095207214355, Accuracy: 0.7009174311926606\n",
      "Step: 545, Loss: 1.131037712097168, Accuracy: 0.701007326007326\n",
      "Step: 546, Loss: 1.2179397344589233, Accuracy: 0.7009445460085314\n",
      "Step: 547, Loss: 1.241902470588684, Accuracy: 0.70088199513382\n",
      "Step: 548, Loss: 1.1298052072525024, Accuracy: 0.7009714632665452\n",
      "Step: 549, Loss: 1.127528429031372, Accuracy: 0.701060606060606\n",
      "Step: 550, Loss: 1.1622567176818848, Accuracy: 0.7011494252873564\n",
      "Step: 551, Loss: 1.1289933919906616, Accuracy: 0.701237922705314\n",
      "Step: 552, Loss: 1.1328200101852417, Accuracy: 0.7014767932489452\n",
      "Step: 553, Loss: 1.1011611223220825, Accuracy: 0.7017148014440433\n",
      "Step: 554, Loss: 1.2150232791900635, Accuracy: 0.7016516516516517\n",
      "Step: 555, Loss: 1.20103919506073, Accuracy: 0.7017386091127098\n",
      "Step: 556, Loss: 1.2340649366378784, Accuracy: 0.7016756433273489\n",
      "Step: 557, Loss: 1.308465600013733, Accuracy: 0.7014635603345281\n",
      "Step: 558, Loss: 1.4161258935928345, Accuracy: 0.7009540846750149\n",
      "Step: 559, Loss: 1.0850508213043213, Accuracy: 0.7011904761904761\n",
      "Step: 560, Loss: 1.2849210500717163, Accuracy: 0.7009803921568627\n",
      "Step: 561, Loss: 1.237798810005188, Accuracy: 0.7009193357058126\n",
      "Step: 562, Loss: 1.3011513948440552, Accuracy: 0.7007104795737122\n",
      "Step: 563, Loss: 1.222819209098816, Accuracy: 0.7006501182033097\n",
      "Step: 564, Loss: 1.208807110786438, Accuracy: 0.700589970501475\n",
      "Step: 565, Loss: 1.1677300930023193, Accuracy: 0.7006772673733804\n",
      "Step: 566, Loss: 1.2261308431625366, Accuracy: 0.7006172839506173\n",
      "Step: 567, Loss: 1.089698314666748, Accuracy: 0.7008509389671361\n",
      "Step: 568, Loss: 1.262245535850525, Accuracy: 0.7007908611599297\n",
      "Step: 569, Loss: 1.2435742616653442, Accuracy: 0.7007309941520468\n",
      "Step: 570, Loss: 1.3311406373977661, Accuracy: 0.7005253940455342\n",
      "Step: 571, Loss: 1.5758048295974731, Accuracy: 0.6998834498834499\n",
      "Step: 572, Loss: 1.232560634613037, Accuracy: 0.699825479930192\n",
      "Step: 573, Loss: 1.1362160444259644, Accuracy: 0.6999128919860628\n",
      "Step: 574, Loss: 1.5583100318908691, Accuracy: 0.6992753623188406\n",
      "Step: 575, Loss: 1.3703078031539917, Accuracy: 0.6989293981481481\n",
      "Step: 576, Loss: 1.4653558731079102, Accuracy: 0.6984402079722704\n",
      "Step: 577, Loss: 1.310043215751648, Accuracy: 0.6982410611303345\n",
      "Step: 578, Loss: 1.1131895780563354, Accuracy: 0.6983304548071387\n",
      "Step: 579, Loss: 1.2889647483825684, Accuracy: 0.698132183908046\n",
      "Step: 580, Loss: 1.0180071592330933, Accuracy: 0.6985083189902467\n",
      "Step: 581, Loss: 1.0817824602127075, Accuracy: 0.6987399770904925\n",
      "Step: 582, Loss: 1.2607734203338623, Accuracy: 0.6986849628359062\n",
      "Step: 583, Loss: 1.1755292415618896, Accuracy: 0.6987728310502284\n",
      "Step: 584, Loss: 1.1226239204406738, Accuracy: 0.6988603988603989\n",
      "Step: 585, Loss: 1.048326849937439, Accuracy: 0.699089874857793\n",
      "Step: 586, Loss: 1.1705573797225952, Accuracy: 0.6991766042021579\n",
      "Step: 587, Loss: 1.3885774612426758, Accuracy: 0.6988378684807256\n",
      "Step: 588, Loss: 1.4683724641799927, Accuracy: 0.6983588002263724\n",
      "Step: 589, Loss: 1.1574798822402954, Accuracy: 0.6983050847457627\n",
      "Step: 590, Loss: 1.15657377243042, Accuracy: 0.6983925549915397\n",
      "Step: 591, Loss: 1.2739654779434204, Accuracy: 0.6981981981981982\n",
      "Step: 592, Loss: 1.1110464334487915, Accuracy: 0.6984260820685778\n",
      "Step: 593, Loss: 1.2338428497314453, Accuracy: 0.6983726150392817\n",
      "Step: 594, Loss: 1.1224733591079712, Accuracy: 0.6984593837535014\n",
      "Step: 595, Loss: 1.041218638420105, Accuracy: 0.6988255033557047\n",
      "Step: 596, Loss: 1.2155475616455078, Accuracy: 0.6989112227805695\n",
      "Step: 597, Loss: 1.0846928358078003, Accuracy: 0.6991360089186176\n",
      "Step: 598, Loss: 1.4463080167770386, Accuracy: 0.6986644407345576\n",
      "Step: 599, Loss: 1.0755139589309692, Accuracy: 0.6990277777777778\n",
      "Step: 600, Loss: 1.142068862915039, Accuracy: 0.6991125901275652\n",
      "Step: 601, Loss: 1.2433322668075562, Accuracy: 0.6990586932447398\n",
      "Step: 602, Loss: 1.357773780822754, Accuracy: 0.6987285793255943\n",
      "Step: 603, Loss: 1.1048423051834106, Accuracy: 0.6988134657836644\n",
      "Step: 604, Loss: 1.191555142402649, Accuracy: 0.6987603305785124\n",
      "Step: 605, Loss: 1.2490581274032593, Accuracy: 0.6987073707370737\n",
      "Step: 606, Loss: 1.0875462293624878, Accuracy: 0.6989291598023064\n",
      "Step: 607, Loss: 1.187041163444519, Accuracy: 0.6988760964912281\n",
      "Step: 608, Loss: 1.2319413423538208, Accuracy: 0.6988232074438971\n",
      "Step: 609, Loss: 1.149046540260315, Accuracy: 0.6989071038251367\n",
      "Step: 610, Loss: 1.2765780687332153, Accuracy: 0.6988543371522095\n",
      "Step: 611, Loss: 1.2406519651412964, Accuracy: 0.69880174291939\n",
      "Step: 612, Loss: 1.298085331916809, Accuracy: 0.6986133768352365\n",
      "Step: 613, Loss: 1.3936386108398438, Accuracy: 0.6981541802388708\n",
      "Step: 614, Loss: 1.1594318151474, Accuracy: 0.6982384823848239\n",
      "Step: 615, Loss: 1.093614101409912, Accuracy: 0.6983225108225108\n",
      "Step: 616, Loss: 1.2317701578140259, Accuracy: 0.6982712047541869\n",
      "Step: 617, Loss: 1.2263811826705933, Accuracy: 0.698220064724919\n",
      "Step: 618, Loss: 1.2774666547775269, Accuracy: 0.6981690899299946\n",
      "Step: 619, Loss: 1.1397024393081665, Accuracy: 0.698252688172043\n",
      "Step: 620, Loss: 1.192356824874878, Accuracy: 0.6983360171765969\n",
      "Step: 621, Loss: 1.189562439918518, Accuracy: 0.6984190782422294\n",
      "Step: 622, Loss: 1.2695482969284058, Accuracy: 0.6983681112894596\n",
      "Step: 623, Loss: 1.3316066265106201, Accuracy: 0.6981837606837606\n",
      "Step: 624, Loss: 1.457201361656189, Accuracy: 0.6977333333333333\n",
      "Step: 625, Loss: 1.0825018882751465, Accuracy: 0.6979499467518637\n",
      "Step: 626, Loss: 1.1633213758468628, Accuracy: 0.6980329611908559\n",
      "Step: 627, Loss: 1.2407230138778687, Accuracy: 0.6979830148619958\n",
      "Step: 628, Loss: 1.0639448165893555, Accuracy: 0.6983306836248012\n",
      "Step: 629, Loss: 1.1650997400283813, Accuracy: 0.6984126984126984\n",
      "Step: 630, Loss: 0.9418165683746338, Accuracy: 0.6988906497622821\n",
      "Step: 631, Loss: 1.2581578493118286, Accuracy: 0.698707805907173\n",
      "Step: 632, Loss: 1.2133793830871582, Accuracy: 0.6986571879936809\n",
      "Step: 633, Loss: 1.2071248292922974, Accuracy: 0.6986067297581493\n",
      "Step: 634, Loss: 1.1927415132522583, Accuracy: 0.6986876640419948\n",
      "Step: 635, Loss: 1.1591320037841797, Accuracy: 0.6987683438155137\n",
      "Step: 636, Loss: 1.0968509912490845, Accuracy: 0.6989795918367347\n",
      "Step: 637, Loss: 1.2019621133804321, Accuracy: 0.6989289446185998\n",
      "Step: 638, Loss: 1.1648615598678589, Accuracy: 0.6990088680229525\n",
      "Step: 639, Loss: 1.0973478555679321, Accuracy: 0.69921875\n",
      "Step: 640, Loss: 1.3913205862045288, Accuracy: 0.6989079563182528\n",
      "Step: 641, Loss: 1.3949475288391113, Accuracy: 0.6985981308411215\n",
      "Step: 642, Loss: 1.570416808128357, Accuracy: 0.6980300673924313\n",
      "Step: 643, Loss: 1.4464755058288574, Accuracy: 0.6975931677018633\n",
      "Step: 644, Loss: 1.0909661054611206, Accuracy: 0.6978036175710595\n",
      "Step: 645, Loss: 1.2500154972076416, Accuracy: 0.6977554179566563\n",
      "Step: 646, Loss: 1.118748426437378, Accuracy: 0.6979649665121072\n",
      "Step: 647, Loss: 1.3343042135238647, Accuracy: 0.6977880658436214\n",
      "Step: 648, Loss: 1.3815984725952148, Accuracy: 0.6974833076527992\n",
      "Step: 649, Loss: 1.2123230695724487, Accuracy: 0.6974358974358974\n",
      "Step: 650, Loss: 1.3417211771011353, Accuracy: 0.6972606246799795\n",
      "Step: 651, Loss: 1.229592204093933, Accuracy: 0.6972137014314929\n",
      "Step: 652, Loss: 1.3451091051101685, Accuracy: 0.6970393057682491\n",
      "Step: 653, Loss: 1.0212339162826538, Accuracy: 0.6973751274209989\n",
      "Step: 654, Loss: 1.0583014488220215, Accuracy: 0.6975826972010178\n",
      "Step: 655, Loss: 1.0921027660369873, Accuracy: 0.6977896341463414\n",
      "Step: 656, Loss: 1.2173354625701904, Accuracy: 0.6977422628107559\n",
      "Step: 657, Loss: 1.232775092124939, Accuracy: 0.6976950354609929\n",
      "Step: 658, Loss: 1.1187852621078491, Accuracy: 0.6977744056651493\n",
      "Step: 659, Loss: 1.2967374324798584, Accuracy: 0.6977272727272728\n",
      "Step: 660, Loss: 1.3404535055160522, Accuracy: 0.6975542107917297\n",
      "Step: 661, Loss: 0.9109641909599304, Accuracy: 0.6980110775427996\n",
      "Step: 662, Loss: 1.0779075622558594, Accuracy: 0.6982151835093011\n",
      "Step: 663, Loss: 1.1529077291488647, Accuracy: 0.6982931726907631\n",
      "Step: 664, Loss: 1.1497007608413696, Accuracy: 0.6983709273182958\n",
      "Step: 665, Loss: 1.2075632810592651, Accuracy: 0.6983233233233234\n",
      "Step: 666, Loss: 1.233902096748352, Accuracy: 0.6982758620689655\n",
      "Step: 667, Loss: 1.328733205795288, Accuracy: 0.6981037924151696\n",
      "Step: 668, Loss: 1.2034189701080322, Accuracy: 0.6980568011958147\n",
      "Step: 669, Loss: 1.3078231811523438, Accuracy: 0.6978855721393035\n",
      "Step: 670, Loss: 1.3159856796264648, Accuracy: 0.6977148534525583\n",
      "Step: 671, Loss: 1.157962441444397, Accuracy: 0.6977926587301587\n",
      "Step: 672, Loss: 1.1525447368621826, Accuracy: 0.6978702327885091\n",
      "Step: 673, Loss: 0.9974179267883301, Accuracy: 0.6981948565776459\n",
      "Step: 674, Loss: 1.2375389337539673, Accuracy: 0.6981481481481482\n",
      "Step: 675, Loss: 1.3072093725204468, Accuracy: 0.6979783037475346\n",
      "Step: 676, Loss: 1.2285749912261963, Accuracy: 0.6979320531757754\n",
      "Step: 677, Loss: 1.259912371635437, Accuracy: 0.6977630285152409\n",
      "Step: 678, Loss: 1.2410526275634766, Accuracy: 0.6977172312223858\n",
      "Step: 679, Loss: 1.1517385244369507, Accuracy: 0.6977941176470588\n",
      "Step: 680, Loss: 1.0242987871170044, Accuracy: 0.6981155163974547\n",
      "Step: 681, Loss: 1.246644139289856, Accuracy: 0.698069403714565\n",
      "Step: 682, Loss: 1.1578019857406616, Accuracy: 0.6981454367984382\n",
      "Step: 683, Loss: 1.2122459411621094, Accuracy: 0.6980994152046783\n",
      "Step: 684, Loss: 1.3574687242507935, Accuracy: 0.6978102189781021\n",
      "Step: 685, Loss: 1.3928104639053345, Accuracy: 0.6975218658892128\n",
      "Step: 686, Loss: 1.3261297941207886, Accuracy: 0.6973556525958272\n",
      "Step: 687, Loss: 1.0119730234146118, Accuracy: 0.6976744186046512\n",
      "Step: 688, Loss: 1.1419085264205933, Accuracy: 0.6977503628447025\n",
      "Step: 689, Loss: 1.0820978879928589, Accuracy: 0.6979468599033817\n",
      "Step: 690, Loss: 1.3176194429397583, Accuracy: 0.6977809937288953\n",
      "Step: 691, Loss: 1.41692054271698, Accuracy: 0.697495183044316\n",
      "Step: 692, Loss: 1.1407418251037598, Accuracy: 0.6975709475709476\n",
      "Step: 693, Loss: 1.3880705833435059, Accuracy: 0.6972862632084534\n",
      "Step: 694, Loss: 1.0781025886535645, Accuracy: 0.6976019184652278\n",
      "Step: 695, Loss: 1.152281403541565, Accuracy: 0.6977969348659003\n",
      "Step: 696, Loss: 1.1291037797927856, Accuracy: 0.6978718316594931\n",
      "Step: 697, Loss: 1.3933868408203125, Accuracy: 0.6975883476599809\n",
      "Step: 698, Loss: 1.140102744102478, Accuracy: 0.6977825464949928\n",
      "Step: 699, Loss: 0.9484992027282715, Accuracy: 0.6982142857142857\n",
      "Step: 700, Loss: 1.1857281923294067, Accuracy: 0.6981692819781264\n",
      "Step: 701, Loss: 1.2843612432479858, Accuracy: 0.698005698005698\n",
      "Step: 702, Loss: 1.4171141386032104, Accuracy: 0.6976055002370792\n",
      "Step: 703, Loss: 1.1075127124786377, Accuracy: 0.6977982954545454\n",
      "Step: 704, Loss: 1.1646188497543335, Accuracy: 0.6978723404255319\n",
      "Step: 705, Loss: 1.2161197662353516, Accuracy: 0.6978281397544853\n",
      "Step: 706, Loss: 1.2396252155303955, Accuracy: 0.6977840641206978\n",
      "Step: 707, Loss: 1.0736829042434692, Accuracy: 0.6979755178907722\n",
      "Step: 708, Loss: 1.2022761106491089, Accuracy: 0.6979313587212036\n",
      "Step: 709, Loss: 1.2847933769226074, Accuracy: 0.6977699530516432\n",
      "Step: 710, Loss: 1.1878734827041626, Accuracy: 0.6978434130332865\n",
      "Step: 711, Loss: 1.1924740076065063, Accuracy: 0.6979166666666666\n",
      "Step: 712, Loss: 1.0647937059402466, Accuracy: 0.698223468910706\n",
      "Step: 713, Loss: 1.0867377519607544, Accuracy: 0.6984126984126984\n",
      "Step: 714, Loss: 0.9727216362953186, Accuracy: 0.6987179487179487\n",
      "Step: 715, Loss: 1.1657651662826538, Accuracy: 0.6987895716945997\n",
      "Step: 716, Loss: 1.0056766271591187, Accuracy: 0.6990934449093444\n",
      "Step: 717, Loss: 1.2193477153778076, Accuracy: 0.6990482822655525\n",
      "Step: 718, Loss: 1.2832099199295044, Accuracy: 0.6988873435326843\n",
      "Step: 719, Loss: 0.9562061429023743, Accuracy: 0.6991898148148148\n",
      "Step: 720, Loss: 1.2140717506408691, Accuracy: 0.6991447064262598\n",
      "Step: 721, Loss: 1.1152249574661255, Accuracy: 0.6992151431209603\n",
      "Step: 722, Loss: 1.388210415840149, Accuracy: 0.6989396035039188\n",
      "Step: 723, Loss: 1.0797523260116577, Accuracy: 0.6991252302025782\n",
      "Step: 724, Loss: 1.265251874923706, Accuracy: 0.6990804597701149\n",
      "Step: 725, Loss: 1.2290350198745728, Accuracy: 0.6990358126721763\n",
      "Step: 726, Loss: 1.192643404006958, Accuracy: 0.6991059147180193\n",
      "Step: 727, Loss: 1.030888319015503, Accuracy: 0.699290293040293\n",
      "Step: 728, Loss: 1.1334422826766968, Accuracy: 0.6994741655235482\n",
      "Step: 729, Loss: 1.1416465044021606, Accuracy: 0.6996575342465754\n",
      "Step: 730, Loss: 1.3391846418380737, Accuracy: 0.6994984040127679\n",
      "Step: 731, Loss: 1.2250827550888062, Accuracy: 0.6994535519125683\n",
      "Step: 732, Loss: 1.0656284093856812, Accuracy: 0.6996361982719418\n",
      "Step: 733, Loss: 1.2328904867172241, Accuracy: 0.6995912806539509\n",
      "Step: 734, Loss: 1.1835492849349976, Accuracy: 0.6996598639455782\n",
      "Step: 735, Loss: 1.1716270446777344, Accuracy: 0.6997282608695652\n",
      "Step: 736, Loss: 1.0674341917037964, Accuracy: 0.6999095431931253\n",
      "Step: 737, Loss: 1.212946891784668, Accuracy: 0.6998644986449865\n",
      "Step: 738, Loss: 1.1696454286575317, Accuracy: 0.6999323410013532\n",
      "Step: 739, Loss: 1.2929373979568481, Accuracy: 0.6997747747747748\n",
      "Step: 740, Loss: 1.0125283002853394, Accuracy: 0.7000674763832658\n",
      "Step: 741, Loss: 1.2821340560913086, Accuracy: 0.6999101527403414\n",
      "Step: 742, Loss: 1.170298457145691, Accuracy: 0.6999775684163302\n",
      "Step: 743, Loss: 1.1673541069030762, Accuracy: 0.6999327956989247\n",
      "Step: 744, Loss: 1.0466461181640625, Accuracy: 0.7001118568232663\n",
      "Step: 745, Loss: 1.2222124338150024, Accuracy: 0.7000670241286864\n",
      "Step: 746, Loss: 0.9353463053703308, Accuracy: 0.7004685408299867\n",
      "Step: 747, Loss: 1.2244083881378174, Accuracy: 0.7004233511586453\n",
      "Step: 748, Loss: 1.1947569847106934, Accuracy: 0.7004895416110369\n",
      "Step: 749, Loss: 1.2055455446243286, Accuracy: 0.7005555555555556\n",
      "Step: 750, Loss: 1.104053258895874, Accuracy: 0.7006213936972925\n",
      "Step: 751, Loss: 1.0273314714431763, Accuracy: 0.7009086879432624\n",
      "Step: 752, Loss: 1.2509030103683472, Accuracy: 0.7007525453740593\n",
      "Step: 753, Loss: 1.109513759613037, Accuracy: 0.7008178603006189\n",
      "Step: 754, Loss: 1.2990511655807495, Accuracy: 0.7006622516556291\n",
      "Step: 755, Loss: 1.0252333879470825, Accuracy: 0.7009479717813051\n",
      "Step: 756, Loss: 1.3938840627670288, Accuracy: 0.7006825187142228\n",
      "Step: 757, Loss: 1.1237717866897583, Accuracy: 0.7007475813544415\n",
      "Step: 758, Loss: 1.2562761306762695, Accuracy: 0.7007026789635485\n",
      "Step: 759, Loss: 1.4102140665054321, Accuracy: 0.7004385964912281\n",
      "Step: 760, Loss: 1.1149288415908813, Accuracy: 0.7005037231712659\n",
      "Step: 761, Loss: 1.224141001701355, Accuracy: 0.7004593175853019\n",
      "Step: 762, Loss: 1.0710426568984985, Accuracy: 0.7006334643949322\n",
      "Step: 763, Loss: 1.1694178581237793, Accuracy: 0.7006980802792321\n",
      "Step: 764, Loss: 1.136290431022644, Accuracy: 0.7007625272331155\n",
      "Step: 765, Loss: 1.1759544610977173, Accuracy: 0.7008268059181897\n",
      "Step: 766, Loss: 1.2366441488265991, Accuracy: 0.7007822685788787\n",
      "Step: 767, Loss: 1.1188122034072876, Accuracy: 0.7008463541666666\n",
      "Step: 768, Loss: 1.3381948471069336, Accuracy: 0.700693541395752\n",
      "Step: 769, Loss: 1.2086292505264282, Accuracy: 0.7007575757575758\n",
      "Step: 770, Loss: 1.2974404096603394, Accuracy: 0.7006052745352356\n",
      "Step: 771, Loss: 1.222023844718933, Accuracy: 0.7005613126079447\n",
      "Step: 772, Loss: 1.3835231065750122, Accuracy: 0.7003018542475204\n",
      "Step: 773, Loss: 1.014711618423462, Accuracy: 0.700473729543497\n",
      "Step: 774, Loss: 1.2849431037902832, Accuracy: 0.7003225806451613\n",
      "Step: 775, Loss: 1.4951566457748413, Accuracy: 0.6999570446735395\n",
      "Step: 776, Loss: 1.1480613946914673, Accuracy: 0.70002145002145\n",
      "Step: 777, Loss: 1.22337806224823, Accuracy: 0.6999785775492716\n",
      "Step: 778, Loss: 1.2282928228378296, Accuracy: 0.6999358151476252\n",
      "Step: 779, Loss: 0.936714231967926, Accuracy: 0.7003205128205128\n",
      "Step: 780, Loss: 1.0686765909194946, Accuracy: 0.7004908237302604\n",
      "Step: 781, Loss: 1.1705034971237183, Accuracy: 0.7005541346973572\n",
      "Step: 782, Loss: 1.3274335861206055, Accuracy: 0.7004044274159217\n",
      "Step: 783, Loss: 1.2442779541015625, Accuracy: 0.7003613945578231\n",
      "Step: 784, Loss: 1.358554720878601, Accuracy: 0.7001061571125266\n",
      "Step: 785, Loss: 1.1259369850158691, Accuracy: 0.700275657336726\n",
      "Step: 786, Loss: 1.1689177751541138, Accuracy: 0.7003388394747988\n",
      "Step: 787, Loss: 1.078856110572815, Accuracy: 0.700507614213198\n",
      "Step: 788, Loss: 1.1627424955368042, Accuracy: 0.7005703422053232\n",
      "Step: 789, Loss: 1.1555272340774536, Accuracy: 0.700632911392405\n",
      "Step: 790, Loss: 1.5699620246887207, Accuracy: 0.7001685630004214\n",
      "Step: 791, Loss: 1.2259786128997803, Accuracy: 0.7001262626262627\n",
      "Step: 792, Loss: 1.2311166524887085, Accuracy: 0.700084068936528\n",
      "Step: 793, Loss: 1.2426081895828247, Accuracy: 0.7000419815281276\n",
      "Step: 794, Loss: 1.2093480825424194, Accuracy: 0.700104821802935\n",
      "Step: 795, Loss: 1.3095651865005493, Accuracy: 0.7000628140703518\n",
      "Step: 796, Loss: 1.0204998254776, Accuracy: 0.7003345880384776\n",
      "Step: 797, Loss: 1.131763219833374, Accuracy: 0.7005012531328321\n",
      "Step: 798, Loss: 1.1191986799240112, Accuracy: 0.7005632040050063\n",
      "Step: 799, Loss: 1.2410625219345093, Accuracy: 0.7005208333333334\n",
      "Step: 800, Loss: 1.2429333925247192, Accuracy: 0.7004785684560966\n",
      "Step: 801, Loss: 1.1923555135726929, Accuracy: 0.7005403158769742\n",
      "Step: 802, Loss: 1.0520528554916382, Accuracy: 0.7007056870070568\n",
      "Step: 803, Loss: 1.023976445198059, Accuracy: 0.7009742951907131\n",
      "Step: 804, Loss: 1.2873426675796509, Accuracy: 0.7008281573498965\n",
      "Step: 805, Loss: 1.2212525606155396, Accuracy: 0.7007857733664186\n",
      "Step: 806, Loss: 1.323673129081726, Accuracy: 0.7006402313093762\n",
      "Step: 807, Loss: 1.3870776891708374, Accuracy: 0.7003919141914191\n",
      "Step: 808, Loss: 1.0682694911956787, Accuracy: 0.7005562422744128\n",
      "Step: 809, Loss: 1.1495476961135864, Accuracy: 0.7006172839506173\n",
      "Step: 810, Loss: 1.4718360900878906, Accuracy: 0.7002671598849157\n",
      "Step: 811, Loss: 1.1734203100204468, Accuracy: 0.7003284072249589\n",
      "Step: 812, Loss: 1.1758921146392822, Accuracy: 0.700389503895039\n",
      "Step: 813, Loss: 1.2354060411453247, Accuracy: 0.7003480753480753\n",
      "Step: 814, Loss: 1.1885281801223755, Accuracy: 0.7003067484662576\n",
      "Step: 815, Loss: 1.1932157278060913, Accuracy: 0.700265522875817\n",
      "Step: 816, Loss: 1.2697334289550781, Accuracy: 0.7001223990208079\n",
      "Step: 817, Loss: 1.0852352380752563, Accuracy: 0.7002852485737572\n",
      "Step: 818, Loss: 1.2832123041152954, Accuracy: 0.7001424501424501\n",
      "Step: 819, Loss: 1.0048249959945679, Accuracy: 0.7004065040650407\n",
      "Step: 820, Loss: 1.142570972442627, Accuracy: 0.700466910272026\n",
      "Step: 821, Loss: 1.1171627044677734, Accuracy: 0.7006285482562855\n",
      "Step: 822, Loss: 0.9966312050819397, Accuracy: 0.7008910490076954\n",
      "Step: 823, Loss: 1.1867839097976685, Accuracy: 0.700950647249191\n",
      "Step: 824, Loss: 1.0974117517471313, Accuracy: 0.7011111111111111\n",
      "Step: 825, Loss: 1.3648724555969238, Accuracy: 0.7008676351896691\n",
      "Step: 826, Loss: 1.2896612882614136, Accuracy: 0.7008262797259169\n",
      "Step: 827, Loss: 1.084481954574585, Accuracy: 0.7009863123993558\n",
      "Step: 828, Loss: 1.4235721826553345, Accuracy: 0.7007438681141938\n",
      "Step: 829, Loss: 1.3082396984100342, Accuracy: 0.7006024096385542\n",
      "Step: 830, Loss: 1.2518255710601807, Accuracy: 0.7005615724027277\n",
      "Step: 831, Loss: 1.4304817914962769, Accuracy: 0.7002203525641025\n",
      "Step: 832, Loss: 1.1711324453353882, Accuracy: 0.7001800720288115\n",
      "Step: 833, Loss: 1.389890193939209, Accuracy: 0.6999400479616307\n",
      "Step: 834, Loss: 0.9968669414520264, Accuracy: 0.7001996007984032\n",
      "Step: 835, Loss: 1.3537620306015015, Accuracy: 0.6999601275917066\n",
      "Step: 836, Loss: 1.2685651779174805, Accuracy: 0.6999203504579848\n",
      "Step: 837, Loss: 1.2156122922897339, Accuracy: 0.6999801113762928\n",
      "Step: 838, Loss: 1.1181527376174927, Accuracy: 0.7001390544298768\n",
      "Step: 839, Loss: 1.1514884233474731, Accuracy: 0.7001984126984127\n",
      "Step: 840, Loss: 1.3199490308761597, Accuracy: 0.7000594530321046\n",
      "Step: 841, Loss: 1.435858130455017, Accuracy: 0.69972288202692\n",
      "Step: 842, Loss: 1.227373480796814, Accuracy: 0.6996836694345591\n",
      "Step: 843, Loss: 1.2644315958023071, Accuracy: 0.6996445497630331\n",
      "Step: 844, Loss: 1.3196438550949097, Accuracy: 0.6995069033530572\n",
      "Step: 845, Loss: 1.2365394830703735, Accuracy: 0.699468085106383\n",
      "Step: 846, Loss: 1.1951347589492798, Accuracy: 0.6994293585202677\n",
      "Step: 847, Loss: 1.2258248329162598, Accuracy: 0.6993907232704403\n",
      "Step: 848, Loss: 1.215250015258789, Accuracy: 0.6994503337259521\n",
      "Step: 849, Loss: 1.254967451095581, Accuracy: 0.6994117647058824\n",
      "Step: 850, Loss: 1.0744892358779907, Accuracy: 0.6995691343517431\n",
      "Step: 851, Loss: 1.1773284673690796, Accuracy: 0.6996283255086072\n",
      "Step: 852, Loss: 1.2126022577285767, Accuracy: 0.6995896834701055\n",
      "Step: 853, Loss: 1.0888978242874146, Accuracy: 0.699648711943794\n",
      "Step: 854, Loss: 1.0331417322158813, Accuracy: 0.6999025341130605\n",
      "Step: 855, Loss: 1.3321229219436646, Accuracy: 0.6997663551401869\n",
      "Step: 856, Loss: 1.326728343963623, Accuracy: 0.6996304939712175\n",
      "Step: 857, Loss: 1.0631567239761353, Accuracy: 0.6997863247863247\n",
      "Step: 858, Loss: 1.0804758071899414, Accuracy: 0.6999417927823051\n",
      "Step: 859, Loss: 1.302403450012207, Accuracy: 0.6998062015503876\n",
      "Step: 860, Loss: 1.2379778623580933, Accuracy: 0.6997677119628339\n",
      "Step: 861, Loss: 1.260229229927063, Accuracy: 0.6997293116782676\n",
      "Step: 862, Loss: 1.4690837860107422, Accuracy: 0.6994013132483584\n",
      "Step: 863, Loss: 1.2467433214187622, Accuracy: 0.6993634259259259\n",
      "Step: 864, Loss: 1.2262908220291138, Accuracy: 0.6993256262042389\n",
      "Step: 865, Loss: 1.0872844457626343, Accuracy: 0.6994803695150116\n",
      "Step: 866, Loss: 1.2710148096084595, Accuracy: 0.699442522106882\n",
      "Step: 867, Loss: 1.149368166923523, Accuracy: 0.6995007680491552\n",
      "Step: 868, Loss: 1.120402455329895, Accuracy: 0.6995588799386268\n",
      "Step: 869, Loss: 1.2005773782730103, Accuracy: 0.6996168582375479\n",
      "Step: 870, Loss: 1.0787694454193115, Accuracy: 0.6997703788748565\n",
      "Step: 871, Loss: 1.2564789056777954, Accuracy: 0.6997324159021406\n",
      "Step: 872, Loss: 1.100617527961731, Accuracy: 0.699885452462772\n",
      "Step: 873, Loss: 1.1751540899276733, Accuracy: 0.6999427917620137\n",
      "Step: 874, Loss: 1.2253433465957642, Accuracy: 0.7\n",
      "Step: 875, Loss: 1.1453839540481567, Accuracy: 0.7000570776255708\n",
      "Step: 876, Loss: 1.5017913579940796, Accuracy: 0.6996389205625237\n",
      "Step: 877, Loss: 1.1464735269546509, Accuracy: 0.699791192103265\n",
      "Step: 878, Loss: 1.1259729862213135, Accuracy: 0.699943117178612\n",
      "Step: 879, Loss: 1.2001219987869263, Accuracy: 0.6999053030303031\n",
      "Step: 880, Loss: 1.1214796304702759, Accuracy: 0.7000567536889898\n",
      "Step: 881, Loss: 1.2968199253082275, Accuracy: 0.6999244142101285\n",
      "Step: 882, Loss: 1.1133331060409546, Accuracy: 0.7000755001887504\n",
      "Step: 883, Loss: 1.1604889631271362, Accuracy: 0.7001319758672699\n",
      "Step: 884, Loss: 1.2149585485458374, Accuracy: 0.7000941619585688\n",
      "Step: 885, Loss: 1.2279736995697021, Accuracy: 0.7000564334085779\n",
      "Step: 886, Loss: 1.1170345544815063, Accuracy: 0.700206689214581\n",
      "Step: 887, Loss: 1.1699665784835815, Accuracy: 0.7002627627627628\n",
      "Step: 888, Loss: 1.276044249534607, Accuracy: 0.7001312335958005\n",
      "Step: 889, Loss: 1.2509056329727173, Accuracy: 0.7000936329588014\n",
      "Step: 890, Loss: 1.1960524320602417, Accuracy: 0.7000561167227833\n",
      "Step: 891, Loss: 1.0187112092971802, Accuracy: 0.7002989536621823\n",
      "Step: 892, Loss: 1.4010275602340698, Accuracy: 0.7000746547219112\n",
      "Step: 893, Loss: 1.1796780824661255, Accuracy: 0.7001304996271439\n",
      "Step: 894, Loss: 1.0101250410079956, Accuracy: 0.7003724394785847\n",
      "Step: 895, Loss: 0.9187266826629639, Accuracy: 0.7007068452380952\n",
      "Step: 896, Loss: 1.2304080724716187, Accuracy: 0.7006688963210702\n",
      "Step: 897, Loss: 1.1109334230422974, Accuracy: 0.7008166295471417\n",
      "Step: 898, Loss: 1.0197631120681763, Accuracy: 0.7010567296996663\n",
      "Step: 899, Loss: 1.3119453191757202, Accuracy: 0.700925925925926\n",
      "Step: 900, Loss: 0.956612765789032, Accuracy: 0.701165371809101\n",
      "Step: 901, Loss: 1.2312650680541992, Accuracy: 0.7011271249076128\n",
      "Step: 902, Loss: 1.4431997537612915, Accuracy: 0.700812107788852\n",
      "Step: 903, Loss: 1.0757688283920288, Accuracy: 0.7009587020648967\n",
      "Step: 904, Loss: 1.3169517517089844, Accuracy: 0.700828729281768\n",
      "Step: 905, Loss: 1.302798867225647, Accuracy: 0.7006990434142752\n",
      "Step: 906, Loss: 1.2433871030807495, Accuracy: 0.7006615214994487\n",
      "Step: 907, Loss: 1.1682062149047852, Accuracy: 0.700715859030837\n",
      "Step: 908, Loss: 1.1241788864135742, Accuracy: 0.7008617528419508\n",
      "Step: 909, Loss: 1.042859673500061, Accuracy: 0.701007326007326\n",
      "Step: 910, Loss: 1.0727587938308716, Accuracy: 0.7011525795828759\n",
      "Step: 911, Loss: 1.233445405960083, Accuracy: 0.7011147660818714\n",
      "Step: 912, Loss: 1.08108651638031, Accuracy: 0.701350857977364\n",
      "Step: 913, Loss: 1.1392134428024292, Accuracy: 0.7014040846097739\n",
      "Step: 914, Loss: 1.1003302335739136, Accuracy: 0.7015482695810564\n",
      "Step: 915, Loss: 1.0994983911514282, Accuracy: 0.7016921397379913\n",
      "Step: 916, Loss: 1.1564899682998657, Accuracy: 0.7017448200654307\n",
      "Step: 917, Loss: 1.1111336946487427, Accuracy: 0.701797385620915\n",
      "Step: 918, Loss: 1.162413239479065, Accuracy: 0.7018498367791077\n",
      "Step: 919, Loss: 1.23590886592865, Accuracy: 0.7018115942028985\n",
      "Step: 920, Loss: 1.3658376932144165, Accuracy: 0.7015924719507781\n",
      "Step: 921, Loss: 1.23241126537323, Accuracy: 0.7015545914678236\n",
      "Step: 922, Loss: 1.2642182111740112, Accuracy: 0.7015167930660888\n",
      "Step: 923, Loss: 1.3426553010940552, Accuracy: 0.7013888888888888\n",
      "Step: 924, Loss: 1.0991111993789673, Accuracy: 0.7015315315315316\n",
      "Step: 925, Loss: 1.3142673969268799, Accuracy: 0.7014038876889849\n",
      "Step: 926, Loss: 1.2515822649002075, Accuracy: 0.7013664149586479\n",
      "Step: 927, Loss: 1.376271367073059, Accuracy: 0.7011494252873564\n",
      "Step: 928, Loss: 1.0699764490127563, Accuracy: 0.701291711517761\n",
      "Step: 929, Loss: 1.2112305164337158, Accuracy: 0.7013440860215053\n",
      "Step: 930, Loss: 1.1099886894226074, Accuracy: 0.7014858575008951\n",
      "Step: 931, Loss: 1.2393718957901, Accuracy: 0.7014484978540773\n",
      "Step: 932, Loss: 1.1360629796981812, Accuracy: 0.7015005359056806\n",
      "Step: 933, Loss: 1.187274694442749, Accuracy: 0.7014632405424697\n",
      "Step: 934, Loss: 1.0213559865951538, Accuracy: 0.7016934046345811\n",
      "Step: 935, Loss: 1.156969428062439, Accuracy: 0.7017450142450142\n",
      "Step: 936, Loss: 0.9570605754852295, Accuracy: 0.7020633226609747\n",
      "Step: 937, Loss: 1.387900471687317, Accuracy: 0.7018479033404407\n",
      "Step: 938, Loss: 1.1525177955627441, Accuracy: 0.7018991835285765\n",
      "Step: 939, Loss: 1.1709500551223755, Accuracy: 0.701950354609929\n",
      "Step: 940, Loss: 1.3211431503295898, Accuracy: 0.7018243003896564\n",
      "Step: 941, Loss: 1.3388985395431519, Accuracy: 0.7016985138004246\n",
      "Step: 942, Loss: 1.4903689622879028, Accuracy: 0.7013962530929657\n",
      "Step: 943, Loss: 1.29507315158844, Accuracy: 0.701271186440678\n",
      "Step: 944, Loss: 1.041949987411499, Accuracy: 0.7014109347442681\n",
      "Step: 945, Loss: 1.288454294204712, Accuracy: 0.7012861169837914\n",
      "Step: 946, Loss: 1.0644935369491577, Accuracy: 0.7014255543822597\n",
      "Step: 947, Loss: 1.2635375261306763, Accuracy: 0.7013888888888888\n",
      "Step: 948, Loss: 1.281209945678711, Accuracy: 0.7013523006673692\n",
      "Step: 949, Loss: 1.271607756614685, Accuracy: 0.7012280701754386\n",
      "Step: 950, Loss: 1.4344574213027954, Accuracy: 0.7010164738871364\n",
      "Step: 951, Loss: 1.1143137216567993, Accuracy: 0.7010679271708683\n",
      "Step: 952, Loss: 1.3606890439987183, Accuracy: 0.7008569429870584\n",
      "Step: 953, Loss: 1.1506613492965698, Accuracy: 0.7009084556254368\n",
      "Step: 954, Loss: 1.1681264638900757, Accuracy: 0.7009598603839442\n",
      "Step: 955, Loss: 1.1045594215393066, Accuracy: 0.7010983263598326\n",
      "Step: 956, Loss: 1.3926843404769897, Accuracy: 0.7008881922675027\n",
      "Step: 957, Loss: 1.1005661487579346, Accuracy: 0.7009394572025052\n",
      "Step: 958, Loss: 1.277803659439087, Accuracy: 0.7009037191518943\n",
      "Step: 959, Loss: 1.233622670173645, Accuracy: 0.7008680555555555\n",
      "Step: 960, Loss: 1.2356758117675781, Accuracy: 0.7008324661810614\n",
      "Step: 961, Loss: 1.1439449787139893, Accuracy: 0.7007969507969508\n",
      "Step: 962, Loss: 1.1129049062728882, Accuracy: 0.7009345794392523\n",
      "Step: 963, Loss: 1.2054075002670288, Accuracy: 0.7008990318118948\n",
      "Step: 964, Loss: 1.2105990648269653, Accuracy: 0.7008635578583765\n",
      "Step: 965, Loss: 1.1312998533248901, Accuracy: 0.7009144237405107\n",
      "Step: 966, Loss: 1.012513518333435, Accuracy: 0.7011375387797312\n",
      "Step: 967, Loss: 1.0798639059066772, Accuracy: 0.7012741046831956\n",
      "Step: 968, Loss: 1.3563176393508911, Accuracy: 0.7010663914688683\n",
      "Step: 969, Loss: 1.1421644687652588, Accuracy: 0.7011168384879725\n",
      "Step: 970, Loss: 1.1196860074996948, Accuracy: 0.7011671815997254\n",
      "Step: 971, Loss: 1.2981425523757935, Accuracy: 0.7010459533607681\n",
      "Step: 972, Loss: 1.164993166923523, Accuracy: 0.7010962658444673\n",
      "Step: 973, Loss: 1.2135883569717407, Accuracy: 0.7011464750171116\n",
      "Step: 974, Loss: 1.1750664710998535, Accuracy: 0.7011965811965812\n",
      "Step: 975, Loss: 1.177289366722107, Accuracy: 0.7012465846994536\n",
      "Step: 976, Loss: 1.1629722118377686, Accuracy: 0.7012964858410099\n",
      "Step: 977, Loss: 1.0831407308578491, Accuracy: 0.7014314928425358\n",
      "Step: 978, Loss: 1.1499520540237427, Accuracy: 0.7014811031664965\n",
      "Step: 979, Loss: 1.2376821041107178, Accuracy: 0.7014455782312925\n",
      "Step: 980, Loss: 1.3371424674987793, Accuracy: 0.7013251783893986\n",
      "Step: 981, Loss: 1.1496751308441162, Accuracy: 0.7013747454175153\n",
      "Step: 982, Loss: 1.306926965713501, Accuracy: 0.7012546625974907\n",
      "Step: 983, Loss: 1.2009814977645874, Accuracy: 0.7012195121951219\n",
      "Step: 984, Loss: 1.321856141090393, Accuracy: 0.7010998307952623\n",
      "Step: 985, Loss: 1.125829815864563, Accuracy: 0.7011494252873564\n",
      "Step: 986, Loss: 1.0085114240646362, Accuracy: 0.7013677811550152\n",
      "Step: 987, Loss: 1.079529881477356, Accuracy: 0.7015013495276653\n",
      "Step: 988, Loss: 1.2130804061889648, Accuracy: 0.7014661274014156\n",
      "Step: 989, Loss: 1.3372763395309448, Accuracy: 0.7012626262626263\n",
      "Step: 990, Loss: 1.390372395515442, Accuracy: 0.7010595358224017\n",
      "Step: 991, Loss: 1.1032286882400513, Accuracy: 0.7011088709677419\n",
      "Step: 992, Loss: 1.3204947710037231, Accuracy: 0.7009063444108762\n",
      "Step: 993, Loss: 1.266885757446289, Accuracy: 0.7007880617035547\n",
      "Step: 994, Loss: 1.2328234910964966, Accuracy: 0.7007537688442211\n",
      "Step: 995, Loss: 1.2414828538894653, Accuracy: 0.7007195448460509\n",
      "Step: 996, Loss: 1.4393733739852905, Accuracy: 0.7004346372450685\n",
      "Step: 997, Loss: 1.1883896589279175, Accuracy: 0.7004843019372078\n",
      "Step: 998, Loss: 1.1407994031906128, Accuracy: 0.7005338672005339\n",
      "Step: 999, Loss: 1.1509308815002441, Accuracy: 0.7005833333333333\n",
      "Step: 1000, Loss: 1.0981184244155884, Accuracy: 0.7007159507159507\n",
      "Step: 1001, Loss: 1.2959507703781128, Accuracy: 0.7005988023952096\n",
      "Step: 1002, Loss: 1.045101284980774, Accuracy: 0.7008142239946826\n",
      "Step: 1003, Loss: 1.2733144760131836, Accuracy: 0.7006972111553785\n",
      "Step: 1004, Loss: 1.3238781690597534, Accuracy: 0.7005804311774461\n",
      "Step: 1005, Loss: 1.1939691305160522, Accuracy: 0.7006295559973492\n",
      "Step: 1006, Loss: 1.1444956064224243, Accuracy: 0.7006785832505793\n",
      "Step: 1007, Loss: 1.376334547996521, Accuracy: 0.7004794973544973\n",
      "Step: 1008, Loss: 1.1162151098251343, Accuracy: 0.7006111661711265\n",
      "Step: 1009, Loss: 1.0166720151901245, Accuracy: 0.7008250825082508\n",
      "Step: 1010, Loss: 0.978659451007843, Accuracy: 0.7010385756676558\n",
      "Step: 1011, Loss: 1.124499797821045, Accuracy: 0.70116930171278\n",
      "Step: 1012, Loss: 1.186653733253479, Accuracy: 0.7012175057584732\n",
      "Step: 1013, Loss: 1.2637956142425537, Accuracy: 0.7011834319526628\n",
      "Step: 1014, Loss: 1.5587830543518066, Accuracy: 0.7008210180623974\n",
      "Step: 1015, Loss: 1.2652801275253296, Accuracy: 0.7007053805774278\n",
      "Step: 1016, Loss: 1.1647933721542358, Accuracy: 0.700671910848902\n",
      "Step: 1017, Loss: 1.2577675580978394, Accuracy: 0.7005566470203013\n",
      "Step: 1018, Loss: 1.212702989578247, Accuracy: 0.7006051684658161\n",
      "Step: 1019, Loss: 1.0319463014602661, Accuracy: 0.7008169934640522\n",
      "Step: 1020, Loss: 1.3349112272262573, Accuracy: 0.7007019262161279\n",
      "Step: 1021, Loss: 1.2565568685531616, Accuracy: 0.700587084148728\n",
      "Step: 1022, Loss: 1.2333464622497559, Accuracy: 0.7005539263603779\n",
      "Step: 1023, Loss: 1.2543216943740845, Accuracy: 0.7005208333333334\n",
      "Step: 1024, Loss: 0.9937075972557068, Accuracy: 0.7007317073170731\n",
      "Step: 1025, Loss: 1.1446782350540161, Accuracy: 0.7008609486679662\n",
      "Step: 1026, Loss: 1.0667005777359009, Accuracy: 0.7009899383317105\n",
      "Step: 1027, Loss: 1.2276391983032227, Accuracy: 0.7009565499351491\n",
      "Step: 1028, Loss: 1.2936500310897827, Accuracy: 0.7008422416585682\n",
      "Step: 1029, Loss: 1.3630656003952026, Accuracy: 0.7006472491909385\n",
      "Step: 1030, Loss: 1.3570761680603027, Accuracy: 0.7004526349822179\n",
      "Step: 1031, Loss: 1.2523237466812134, Accuracy: 0.7004198966408268\n",
      "Step: 1032, Loss: 1.0757144689559937, Accuracy: 0.7005485640529203\n",
      "Step: 1033, Loss: 1.0930365324020386, Accuracy: 0.7006769825918762\n",
      "Step: 1034, Loss: 1.1630139350891113, Accuracy: 0.7007246376811594\n",
      "Step: 1035, Loss: 1.0529786348342896, Accuracy: 0.7008526383526383\n",
      "Step: 1036, Loss: 1.301007866859436, Accuracy: 0.70073931211829\n",
      "Step: 1037, Loss: 1.2413606643676758, Accuracy: 0.7007064868336544\n",
      "Step: 1038, Loss: 1.2803945541381836, Accuracy: 0.7005935194096888\n",
      "Step: 1039, Loss: 1.191956877708435, Accuracy: 0.7005608974358974\n",
      "Step: 1040, Loss: 1.31570565700531, Accuracy: 0.7004482869036183\n",
      "Step: 1041, Loss: 1.2005912065505981, Accuracy: 0.7004158669225847\n",
      "Step: 1042, Loss: 1.1894828081130981, Accuracy: 0.7003835091083414\n",
      "Step: 1043, Loss: 1.1010010242462158, Accuracy: 0.7005108556832694\n",
      "Step: 1044, Loss: 1.337308406829834, Accuracy: 0.7003189792663477\n",
      "Step: 1045, Loss: 1.2235536575317383, Accuracy: 0.7002868068833652\n",
      "Step: 1046, Loss: 1.1102547645568848, Accuracy: 0.7004138809296402\n",
      "Step: 1047, Loss: 1.1909388303756714, Accuracy: 0.7004611959287532\n",
      "Step: 1048, Loss: 0.9821243286132812, Accuracy: 0.7006673021925643\n",
      "Step: 1049, Loss: 1.2120893001556396, Accuracy: 0.7007142857142857\n",
      "Step: 1050, Loss: 1.2368286848068237, Accuracy: 0.7006818902632413\n",
      "Step: 1051, Loss: 1.3438128232955933, Accuracy: 0.7005703422053232\n",
      "Step: 1052, Loss: 1.204679250717163, Accuracy: 0.7006172839506173\n",
      "Step: 1053, Loss: 1.227013111114502, Accuracy: 0.7006641366223909\n",
      "Step: 1054, Loss: 1.3966765403747559, Accuracy: 0.7004739336492891\n",
      "Step: 1055, Loss: 1.0859169960021973, Accuracy: 0.7005997474747475\n",
      "Step: 1056, Loss: 1.1778994798660278, Accuracy: 0.7006464837590666\n",
      "Step: 1057, Loss: 1.2734060287475586, Accuracy: 0.7006143667296786\n",
      "Step: 1058, Loss: 1.3506299257278442, Accuracy: 0.7004249291784702\n",
      "Step: 1059, Loss: 1.1288028955459595, Accuracy: 0.7004716981132075\n",
      "Step: 1060, Loss: 1.0830234289169312, Accuracy: 0.7005969211435752\n",
      "Step: 1061, Loss: 1.2099761962890625, Accuracy: 0.7005649717514124\n",
      "Step: 1062, Loss: 1.1469316482543945, Accuracy: 0.7006114769520225\n",
      "Step: 1063, Loss: 1.0743674039840698, Accuracy: 0.7007362155388471\n",
      "Step: 1064, Loss: 1.2302993535995483, Accuracy: 0.7007042253521126\n",
      "Step: 1065, Loss: 1.2353544235229492, Accuracy: 0.7006722951844903\n",
      "Step: 1066, Loss: 1.2945401668548584, Accuracy: 0.7005623242736645\n",
      "Step: 1067, Loss: 1.1403461694717407, Accuracy: 0.7006086142322098\n",
      "Step: 1068, Loss: 1.095987319946289, Accuracy: 0.7007327720611163\n",
      "Step: 1069, Loss: 1.1916428804397583, Accuracy: 0.7007009345794393\n",
      "Step: 1070, Loss: 1.3380180597305298, Accuracy: 0.7005913476501712\n",
      "Step: 1071, Loss: 1.311470627784729, Accuracy: 0.7004819651741293\n",
      "Step: 1072, Loss: 1.3552961349487305, Accuracy: 0.7003727865796832\n",
      "Step: 1073, Loss: 1.4155100584030151, Accuracy: 0.7001086281812539\n",
      "Step: 1074, Loss: 1.5295549631118774, Accuracy: 0.6997674418604651\n",
      "Step: 1075, Loss: 1.3117495775222778, Accuracy: 0.6996592317224287\n",
      "Step: 1076, Loss: 1.2046064138412476, Accuracy: 0.6996285979572887\n",
      "Step: 1077, Loss: 1.059238314628601, Accuracy: 0.6997526283240569\n",
      "Step: 1078, Loss: 1.1554616689682007, Accuracy: 0.6997991967871486\n",
      "Step: 1079, Loss: 1.302577257156372, Accuracy: 0.6996913580246914\n",
      "Step: 1080, Loss: 1.2656644582748413, Accuracy: 0.6996608078939254\n",
      "Step: 1081, Loss: 1.2926684617996216, Accuracy: 0.6995532963647566\n",
      "Step: 1082, Loss: 1.2682281732559204, Accuracy: 0.6994459833795014\n",
      "Step: 1083, Loss: 1.2696596384048462, Accuracy: 0.6994157441574416\n",
      "Step: 1084, Loss: 1.1036006212234497, Accuracy: 0.6995391705069124\n",
      "Step: 1085, Loss: 1.10233736038208, Accuracy: 0.6996623695518723\n",
      "Step: 1086, Loss: 1.2670499086380005, Accuracy: 0.6996320147194113\n",
      "Step: 1087, Loss: 1.3312655687332153, Accuracy: 0.6995251225490197\n",
      "Step: 1088, Loss: 1.1658107042312622, Accuracy: 0.6995714722987451\n",
      "Step: 1089, Loss: 1.2365299463272095, Accuracy: 0.6995412844036697\n",
      "Step: 1090, Loss: 1.060972809791565, Accuracy: 0.6997402994194928\n",
      "Step: 1091, Loss: 1.3070231676101685, Accuracy: 0.6996336996336996\n",
      "Step: 1092, Loss: 1.2821717262268066, Accuracy: 0.6995272949069838\n",
      "Step: 1093, Loss: 1.2864086627960205, Accuracy: 0.6994210847044485\n",
      "Step: 1094, Loss: 1.059165596961975, Accuracy: 0.6995433789954338\n",
      "Step: 1095, Loss: 1.1341432332992554, Accuracy: 0.6995894160583942\n",
      "Step: 1096, Loss: 1.1931099891662598, Accuracy: 0.6995594044363416\n",
      "Step: 1097, Loss: 1.2821155786514282, Accuracy: 0.6994535519125683\n",
      "Step: 1098, Loss: 1.2204478979110718, Accuracy: 0.6994237185319988\n",
      "Step: 1099, Loss: 1.1014097929000854, Accuracy: 0.699469696969697\n",
      "Step: 1100, Loss: 1.176684021949768, Accuracy: 0.6994399031183772\n",
      "Step: 1101, Loss: 1.2215322256088257, Accuracy: 0.699410163339383\n",
      "Step: 1102, Loss: 1.0700472593307495, Accuracy: 0.6995315805379269\n",
      "Step: 1103, Loss: 0.9948568940162659, Accuracy: 0.6997282608695652\n",
      "Step: 1104, Loss: 1.1541788578033447, Accuracy: 0.6997737556561086\n",
      "Step: 1105, Loss: 1.2104902267456055, Accuracy: 0.6997438215792646\n",
      "Step: 1106, Loss: 1.0137481689453125, Accuracy: 0.6999397771755496\n",
      "Step: 1107, Loss: 1.2318638563156128, Accuracy: 0.6999097472924187\n",
      "Step: 1108, Loss: 1.0783616304397583, Accuracy: 0.7000300571085062\n",
      "Step: 1109, Loss: 1.1546919345855713, Accuracy: 0.700075075075075\n",
      "Step: 1110, Loss: 1.0515127182006836, Accuracy: 0.7002700270027002\n",
      "Step: 1111, Loss: 1.2632091045379639, Accuracy: 0.7002398081534772\n",
      "Step: 1112, Loss: 1.2407351732254028, Accuracy: 0.70020964360587\n",
      "Step: 1113, Loss: 1.170939564704895, Accuracy: 0.7001795332136446\n",
      "Step: 1114, Loss: 1.1385953426361084, Accuracy: 0.7002242152466368\n",
      "Step: 1115, Loss: 1.2107259035110474, Accuracy: 0.7001941457586619\n",
      "Step: 1116, Loss: 1.3068315982818604, Accuracy: 0.7000895255147717\n",
      "Step: 1117, Loss: 1.1544420719146729, Accuracy: 0.700134168157424\n",
      "Step: 1118, Loss: 1.2492882013320923, Accuracy: 0.7000297885016383\n",
      "Step: 1119, Loss: 1.1286855936050415, Accuracy: 0.7000744047619047\n",
      "Step: 1120, Loss: 1.3335981369018555, Accuracy: 0.6999702646446625\n",
      "Step: 1121, Loss: 1.2299435138702393, Accuracy: 0.6999405822935235\n",
      "Step: 1122, Loss: 1.1878036260604858, Accuracy: 0.6999851588008311\n",
      "Step: 1123, Loss: 1.151816964149475, Accuracy: 0.70002965599051\n",
      "Step: 1124, Loss: 1.5542831420898438, Accuracy: 0.6997037037037037\n",
      "Step: 1125, Loss: 1.1751526594161987, Accuracy: 0.6997483718176436\n",
      "Step: 1126, Loss: 1.2779639959335327, Accuracy: 0.6996450754214729\n",
      "Step: 1127, Loss: 1.151509404182434, Accuracy: 0.6996897163120568\n",
      "Step: 1128, Loss: 1.7216659784317017, Accuracy: 0.6992175966932389\n",
      "Step: 1129, Loss: 1.2206555604934692, Accuracy: 0.699188790560472\n",
      "Step: 1130, Loss: 1.1405861377716064, Accuracy: 0.6992337164750958\n",
      "Step: 1131, Loss: 1.1031426191329956, Accuracy: 0.6992785630153121\n",
      "Step: 1132, Loss: 1.2637008428573608, Accuracy: 0.6991762283024419\n",
      "Step: 1133, Loss: 1.1501233577728271, Accuracy: 0.6992210464432687\n",
      "Step: 1134, Loss: 1.3866815567016602, Accuracy: 0.6990455212922173\n",
      "Step: 1135, Loss: 1.0080066919326782, Accuracy: 0.6992370892018779\n",
      "Step: 1136, Loss: 1.3120254278182983, Accuracy: 0.6991351509821166\n",
      "Step: 1137, Loss: 1.2025208473205566, Accuracy: 0.6991066198008201\n",
      "Step: 1138, Loss: 1.1389950513839722, Accuracy: 0.6991513023119695\n",
      "Step: 1139, Loss: 1.2077088356018066, Accuracy: 0.6991228070175438\n",
      "Step: 1140, Loss: 1.0814803838729858, Accuracy: 0.6992404323692667\n",
      "Step: 1141, Loss: 1.139333724975586, Accuracy: 0.6992848803269118\n",
      "Step: 1142, Loss: 1.2511539459228516, Accuracy: 0.6992563429571304\n",
      "Step: 1143, Loss: 1.3052810430526733, Accuracy: 0.6991550116550117\n",
      "Step: 1144, Loss: 1.2109800577163696, Accuracy: 0.6991266375545852\n",
      "Step: 1145, Loss: 1.1730302572250366, Accuracy: 0.6991710296684118\n",
      "Step: 1146, Loss: 1.2008095979690552, Accuracy: 0.6992153443766347\n",
      "Step: 1147, Loss: 1.091488003730774, Accuracy: 0.6992595818815331\n",
      "Step: 1148, Loss: 1.18925940990448, Accuracy: 0.6993037423846823\n",
      "Step: 1149, Loss: 1.2619463205337524, Accuracy: 0.6992028985507246\n",
      "Step: 1150, Loss: 1.232306718826294, Accuracy: 0.6991746307558645\n",
      "Step: 1151, Loss: 1.12050461769104, Accuracy: 0.69921875\n",
      "Step: 1152, Loss: 1.350267767906189, Accuracy: 0.6990459670424979\n",
      "Step: 1153, Loss: 1.2631480693817139, Accuracy: 0.6989456961294049\n",
      "Step: 1154, Loss: 1.0694674253463745, Accuracy: 0.699062049062049\n",
      "Step: 1155, Loss: 1.3350764513015747, Accuracy: 0.698961937716263\n",
      "Step: 1156, Loss: 1.0746313333511353, Accuracy: 0.6990780754825698\n",
      "Step: 1157, Loss: 1.2681292295455933, Accuracy: 0.6990500863557858\n",
      "Step: 1158, Loss: 1.4249682426452637, Accuracy: 0.6988783433994823\n",
      "Step: 1159, Loss: 1.1671851873397827, Accuracy: 0.6989224137931035\n",
      "Step: 1160, Loss: 1.2296769618988037, Accuracy: 0.6988946310651737\n",
      "Step: 1161, Loss: 1.476906418800354, Accuracy: 0.6986517498565691\n",
      "Step: 1162, Loss: 1.19426429271698, Accuracy: 0.6986242476354256\n",
      "Step: 1163, Loss: 1.017970323562622, Accuracy: 0.69881156930126\n",
      "Step: 1164, Loss: 1.124003291130066, Accuracy: 0.6989270386266094\n",
      "Step: 1165, Loss: 1.1665432453155518, Accuracy: 0.6989708404802745\n",
      "Step: 1166, Loss: 1.3644927740097046, Accuracy: 0.6988003427592117\n",
      "Step: 1167, Loss: 1.0068871974945068, Accuracy: 0.6989868721461188\n",
      "Step: 1168, Loss: 1.0481749773025513, Accuracy: 0.6991017964071856\n",
      "Step: 1169, Loss: 1.1836658716201782, Accuracy: 0.6991452991452991\n",
      "Step: 1170, Loss: 1.089659333229065, Accuracy: 0.6992598918303444\n",
      "Step: 1171, Loss: 1.036805510520935, Accuracy: 0.6994453924914675\n",
      "Step: 1172, Loss: 1.2140634059906006, Accuracy: 0.6994174481386758\n",
      "Step: 1173, Loss: 1.1451020240783691, Accuracy: 0.6995315161839863\n",
      "Step: 1174, Loss: 1.4179047346115112, Accuracy: 0.6993617021276596\n",
      "Step: 1175, Loss: 1.1210641860961914, Accuracy: 0.6994047619047619\n",
      "Step: 1176, Loss: 1.0135207176208496, Accuracy: 0.6995893514585103\n",
      "Step: 1177, Loss: 1.1051321029663086, Accuracy: 0.6997028862478778\n",
      "Step: 1178, Loss: 1.2597304582595825, Accuracy: 0.6996748657054\n",
      "Step: 1179, Loss: 1.2889453172683716, Accuracy: 0.6995762711864407\n",
      "Step: 1180, Loss: 1.1373616456985474, Accuracy: 0.699618966977138\n",
      "Step: 1181, Loss: 1.2286561727523804, Accuracy: 0.6995910885504795\n",
      "Step: 1182, Loss: 1.0115928649902344, Accuracy: 0.699774584389969\n",
      "Step: 1183, Loss: 1.073170781135559, Accuracy: 0.6998873873873874\n",
      "Step: 1184, Loss: 1.1969914436340332, Accuracy: 0.699929676511955\n",
      "Step: 1185, Loss: 1.1531190872192383, Accuracy: 0.6999718943226532\n",
      "Step: 1186, Loss: 0.9107489585876465, Accuracy: 0.7002246559955069\n",
      "Step: 1187, Loss: 1.1624387502670288, Accuracy: 0.7002665544332211\n",
      "Step: 1188, Loss: 1.2400285005569458, Accuracy: 0.7002382954864032\n",
      "Step: 1189, Loss: 1.0155175924301147, Accuracy: 0.7004201680672268\n",
      "Step: 1190, Loss: 1.1481133699417114, Accuracy: 0.7004617968094039\n",
      "Step: 1191, Loss: 1.0730186700820923, Accuracy: 0.7005732662192393\n",
      "Step: 1192, Loss: 1.1842350959777832, Accuracy: 0.7006146968426935\n",
      "Step: 1193, Loss: 1.3763060569763184, Accuracy: 0.7004466778336125\n",
      "Step: 1194, Loss: 0.9845283031463623, Accuracy: 0.7006276150627615\n",
      "Step: 1195, Loss: 1.5535030364990234, Accuracy: 0.7003205128205128\n",
      "Step: 1196, Loss: 1.1497526168823242, Accuracy: 0.7003620161514899\n",
      "Step: 1197, Loss: 1.3022655248641968, Accuracy: 0.7002643294379521\n",
      "Step: 1198, Loss: 1.0892904996871948, Accuracy: 0.7003058103975535\n",
      "Step: 1199, Loss: 1.192601203918457, Accuracy: 0.7003472222222222\n",
      "Step: 1200, Loss: 1.080145239830017, Accuracy: 0.7004579517069109\n",
      "Step: 1201, Loss: 1.2785238027572632, Accuracy: 0.7004298391569607\n",
      "Step: 1202, Loss: 1.3136506080627441, Accuracy: 0.700332502078138\n",
      "Step: 1203, Loss: 1.3537737131118774, Accuracy: 0.7001661129568106\n",
      "Step: 1204, Loss: 0.9839334487915039, Accuracy: 0.7003457814661134\n",
      "Step: 1205, Loss: 1.3073610067367554, Accuracy: 0.7002487562189055\n",
      "Step: 1206, Loss: 1.3626359701156616, Accuracy: 0.700082850041425\n",
      "Step: 1207, Loss: 1.0789259672164917, Accuracy: 0.7001931567328918\n",
      "Step: 1208, Loss: 1.0695252418518066, Accuracy: 0.7003032809484422\n",
      "Step: 1209, Loss: 1.0755488872528076, Accuracy: 0.7004132231404959\n",
      "Step: 1210, Loss: 1.0766929388046265, Accuracy: 0.700522983759978\n",
      "Step: 1211, Loss: 1.147850513458252, Accuracy: 0.7005638063806381\n",
      "Step: 1212, Loss: 1.0879175662994385, Accuracy: 0.7006732618851332\n",
      "Step: 1213, Loss: 1.0741307735443115, Accuracy: 0.7007825370675453\n",
      "Step: 1214, Loss: 1.362733244895935, Accuracy: 0.7006172839506173\n",
      "Step: 1215, Loss: 1.1555923223495483, Accuracy: 0.7006578947368421\n",
      "Step: 1216, Loss: 1.2294385433197021, Accuracy: 0.7006299643933169\n",
      "Step: 1217, Loss: 1.226269245147705, Accuracy: 0.7006020799124247\n",
      "Step: 1218, Loss: 1.1348670721054077, Accuracy: 0.7006426032266886\n",
      "Step: 1219, Loss: 1.3906539678573608, Accuracy: 0.7004781420765027\n",
      "Step: 1220, Loss: 1.156925082206726, Accuracy: 0.7005187005187005\n",
      "Step: 1221, Loss: 1.3844741582870483, Accuracy: 0.700354609929078\n",
      "Step: 1222, Loss: 1.3731884956359863, Accuracy: 0.7001907876805669\n",
      "Step: 1223, Loss: 1.1468197107315063, Accuracy: 0.7002314814814815\n",
      "Step: 1224, Loss: 1.1961218118667603, Accuracy: 0.7002721088435374\n",
      "Step: 1225, Loss: 1.2091562747955322, Accuracy: 0.7002446982055465\n",
      "Step: 1226, Loss: 1.1397751569747925, Accuracy: 0.7002852485737572\n",
      "Step: 1227, Loss: 1.4441481828689575, Accuracy: 0.7001221498371335\n",
      "Step: 1228, Loss: 1.3522838354110718, Accuracy: 0.6999593165174939\n",
      "Step: 1229, Loss: 1.0324679613113403, Accuracy: 0.7001355013550136\n",
      "Step: 1230, Loss: 1.1825464963912964, Accuracy: 0.7001083130246412\n",
      "Step: 1231, Loss: 1.2441521883010864, Accuracy: 0.7000811688311688\n",
      "Step: 1232, Loss: 1.1407389640808105, Accuracy: 0.7001216545012166\n",
      "Step: 1233, Loss: 1.0642637014389038, Accuracy: 0.7002296056185845\n",
      "Step: 1234, Loss: 1.2459105253219604, Accuracy: 0.7002024291497976\n",
      "Step: 1235, Loss: 1.138096809387207, Accuracy: 0.7002427184466019\n",
      "Step: 1236, Loss: 1.0765398740768433, Accuracy: 0.7003503098895176\n",
      "Step: 1237, Loss: 1.084752082824707, Accuracy: 0.7004577275175013\n",
      "Step: 1238, Loss: 1.053861379623413, Accuracy: 0.7004977132095777\n",
      "Step: 1239, Loss: 1.2502940893173218, Accuracy: 0.7004704301075269\n",
      "Step: 1240, Loss: 1.2167755365371704, Accuracy: 0.7004431909750202\n",
      "Step: 1241, Loss: 1.0600039958953857, Accuracy: 0.7006172839506173\n",
      "Step: 1242, Loss: 1.151046872138977, Accuracy: 0.7006570126039152\n",
      "Step: 1243, Loss: 1.209877848625183, Accuracy: 0.7006296891747053\n",
      "Step: 1244, Loss: 1.1680803298950195, Accuracy: 0.700669344042838\n",
      "Step: 1245, Loss: 1.072839379310608, Accuracy: 0.7007758159443552\n",
      "Step: 1246, Loss: 1.138888955116272, Accuracy: 0.7008152900294039\n",
      "Step: 1247, Loss: 1.2358897924423218, Accuracy: 0.7007879273504274\n",
      "Step: 1248, Loss: 1.3227347135543823, Accuracy: 0.7006938884440886\n",
      "Step: 1249, Loss: 1.286253809928894, Accuracy: 0.7006\n",
      "Step: 1250, Loss: 1.2464932203292847, Accuracy: 0.7005728750333067\n",
      "Step: 1251, Loss: 1.1936285495758057, Accuracy: 0.7005457933972311\n",
      "Step: 1252, Loss: 1.2996526956558228, Accuracy: 0.7004522479382814\n",
      "Step: 1253, Loss: 1.215716004371643, Accuracy: 0.700491759702286\n",
      "Step: 1254, Loss: 1.1547385454177856, Accuracy: 0.700531208499336\n",
      "Step: 1255, Loss: 1.097801923751831, Accuracy: 0.7006369426751592\n",
      "Step: 1256, Loss: 1.48088800907135, Accuracy: 0.7004110315566163\n",
      "Step: 1257, Loss: 1.0097609758377075, Accuracy: 0.7005829358770536\n",
      "Step: 1258, Loss: 1.0374754667282104, Accuracy: 0.7007545671167593\n",
      "Step: 1259, Loss: 1.3031827211380005, Accuracy: 0.7006613756613757\n",
      "Step: 1260, Loss: 1.157470464706421, Accuracy: 0.700700502246894\n",
      "Step: 1261, Loss: 1.1549021005630493, Accuracy: 0.7007395668251453\n",
      "Step: 1262, Loss: 1.187027931213379, Accuracy: 0.7007785695434151\n",
      "Step: 1263, Loss: 1.0566409826278687, Accuracy: 0.7009493670886076\n",
      "Step: 1264, Loss: 1.1075907945632935, Accuracy: 0.7009881422924901\n",
      "Step: 1265, Loss: 1.249590516090393, Accuracy: 0.7009610321221695\n",
      "Step: 1266, Loss: 1.0192660093307495, Accuracy: 0.7011312812417785\n",
      "Step: 1267, Loss: 1.3384294509887695, Accuracy: 0.7010383806519453\n",
      "Step: 1268, Loss: 1.2259608507156372, Accuracy: 0.7010112949829261\n",
      "Step: 1269, Loss: 1.2932311296463013, Accuracy: 0.7009186351706037\n",
      "Step: 1270, Loss: 1.3262746334075928, Accuracy: 0.7008261211644374\n",
      "Step: 1271, Loss: 1.2242845296859741, Accuracy: 0.7007992662473794\n",
      "Step: 1272, Loss: 1.3890498876571655, Accuracy: 0.7006415291961247\n",
      "Step: 1273, Loss: 1.22117018699646, Accuracy: 0.7006148613291471\n",
      "Step: 1274, Loss: 1.303737998008728, Accuracy: 0.7005228758169935\n",
      "Step: 1275, Loss: 1.4244791269302368, Accuracy: 0.7003657262277951\n",
      "Step: 1276, Loss: 1.1143416166305542, Accuracy: 0.700404594100757\n",
      "Step: 1277, Loss: 1.0650748014450073, Accuracy: 0.7005086071987481\n",
      "Step: 1278, Loss: 1.23885977268219, Accuracy: 0.7004821475110764\n",
      "Step: 1279, Loss: 1.0615397691726685, Accuracy: 0.7005859375\n",
      "Step: 1280, Loss: 1.3147958517074585, Accuracy: 0.7004944054124382\n",
      "Step: 1281, Loss: 1.4836983680725098, Accuracy: 0.7002730109204368\n",
      "Step: 1282, Loss: 1.275099754333496, Accuracy: 0.7001818654195895\n",
      "Step: 1283, Loss: 1.401879906654358, Accuracy: 0.7000259605399792\n",
      "Step: 1284, Loss: 1.3388785123825073, Accuracy: 0.699935149156939\n",
      "Step: 1285, Loss: 1.2353954315185547, Accuracy: 0.6999092794193883\n",
      "Step: 1286, Loss: 1.3144606351852417, Accuracy: 0.6998186998186998\n",
      "Step: 1287, Loss: 1.3609027862548828, Accuracy: 0.6996635610766045\n",
      "Step: 1288, Loss: 1.0724513530731201, Accuracy: 0.6997672614429791\n",
      "Step: 1289, Loss: 1.317015290260315, Accuracy: 0.6996770025839794\n",
      "Step: 1290, Loss: 1.1543381214141846, Accuracy: 0.699715982442551\n",
      "Step: 1291, Loss: 1.249320387840271, Accuracy: 0.6996904024767802\n",
      "Step: 1292, Loss: 1.197951316833496, Accuracy: 0.6997293116782676\n",
      "Step: 1293, Loss: 1.227800726890564, Accuracy: 0.6997037609479649\n",
      "Step: 1294, Loss: 1.5947366952896118, Accuracy: 0.6993564993564994\n",
      "Step: 1295, Loss: 1.1186400651931763, Accuracy: 0.6993955761316872\n",
      "Step: 1296, Loss: 1.1130744218826294, Accuracy: 0.6994345926497044\n",
      "Step: 1297, Loss: 1.3580212593078613, Accuracy: 0.6993451463790447\n",
      "Step: 1298, Loss: 1.1597836017608643, Accuracy: 0.6993841416474211\n",
      "Step: 1299, Loss: 1.147024154663086, Accuracy: 0.6994230769230769\n",
      "Step: 1300, Loss: 1.1646274328231812, Accuracy: 0.6994619523443505\n",
      "Step: 1301, Loss: 1.0071104764938354, Accuracy: 0.6996287762416795\n",
      "Step: 1302, Loss: 1.2558990716934204, Accuracy: 0.6995395241749808\n",
      "Step: 1303, Loss: 1.1183756589889526, Accuracy: 0.6995782208588958\n",
      "Step: 1304, Loss: 1.2933627367019653, Accuracy: 0.6994891443167305\n",
      "Step: 1305, Loss: 1.1105211973190308, Accuracy: 0.6995916283818274\n",
      "Step: 1306, Loss: 1.2436131238937378, Accuracy: 0.6995664371333843\n",
      "Step: 1307, Loss: 1.296636939048767, Accuracy: 0.6994775739041794\n",
      "Step: 1308, Loss: 1.1619389057159424, Accuracy: 0.6995161701044054\n",
      "Step: 1309, Loss: 1.3439064025878906, Accuracy: 0.6994274809160306\n",
      "Step: 1310, Loss: 1.044228434562683, Accuracy: 0.6995296211543351\n",
      "Step: 1311, Loss: 1.364040732383728, Accuracy: 0.6993775406504065\n",
      "Step: 1312, Loss: 1.0021382570266724, Accuracy: 0.6995430312261995\n",
      "Step: 1313, Loss: 1.220776915550232, Accuracy: 0.6995814307458144\n",
      "Step: 1314, Loss: 1.0566478967666626, Accuracy: 0.699683143219265\n",
      "Step: 1315, Loss: 1.197891354560852, Accuracy: 0.6997213779128673\n",
      "Step: 1316, Loss: 1.2282078266143799, Accuracy: 0.6996962794229309\n",
      "Step: 1317, Loss: 1.3769043684005737, Accuracy: 0.6995447647951442\n",
      "Step: 1318, Loss: 1.0852832794189453, Accuracy: 0.6996461966135962\n",
      "Step: 1319, Loss: 1.1278046369552612, Accuracy: 0.6996843434343434\n",
      "Step: 1320, Loss: 1.2749215364456177, Accuracy: 0.699596265455463\n",
      "Step: 1321, Loss: 1.1442593336105347, Accuracy: 0.6996343923348461\n",
      "Step: 1322, Loss: 1.1481627225875854, Accuracy: 0.6996724615772235\n",
      "Step: 1323, Loss: 1.1675457954406738, Accuracy: 0.6997104733131924\n",
      "Step: 1324, Loss: 1.3045916557312012, Accuracy: 0.699622641509434\n",
      "Step: 1325, Loss: 1.2561006546020508, Accuracy: 0.6995977878330819\n",
      "Step: 1326, Loss: 1.1542831659317017, Accuracy: 0.6995729716151721\n",
      "Step: 1327, Loss: 1.2986887693405151, Accuracy: 0.6994854417670683\n",
      "Step: 1328, Loss: 1.1143344640731812, Accuracy: 0.6995861550037622\n",
      "Step: 1329, Loss: 1.0639530420303345, Accuracy: 0.6996867167919799\n",
      "Step: 1330, Loss: 1.3893219232559204, Accuracy: 0.6995366892061107\n",
      "Step: 1331, Loss: 1.2248566150665283, Accuracy: 0.699512012012012\n",
      "Step: 1332, Loss: 1.1694180965423584, Accuracy: 0.6994873718429607\n",
      "Step: 1333, Loss: 1.2213010787963867, Accuracy: 0.6994627686156921\n",
      "Step: 1334, Loss: 1.14275324344635, Accuracy: 0.6995006242197254\n",
      "Step: 1335, Loss: 1.0744189023971558, Accuracy: 0.6996007984031936\n",
      "Step: 1336, Loss: 1.2297970056533813, Accuracy: 0.6995761655447519\n",
      "Step: 1337, Loss: 1.354894995689392, Accuracy: 0.6994270054808172\n",
      "Step: 1338, Loss: 1.2873023748397827, Accuracy: 0.6993403037092357\n",
      "Step: 1339, Loss: 1.474319577217102, Accuracy: 0.6991293532338309\n",
      "Step: 1340, Loss: 0.9800105094909668, Accuracy: 0.6992915734526473\n",
      "Step: 1341, Loss: 0.9967801570892334, Accuracy: 0.6994535519125683\n",
      "Step: 1342, Loss: 1.233765959739685, Accuracy: 0.6994911888806156\n",
      "Step: 1343, Loss: 1.1444612741470337, Accuracy: 0.6995287698412699\n",
      "Step: 1344, Loss: 1.2524267435073853, Accuracy: 0.6995043370508055\n",
      "Step: 1345, Loss: 1.4274808168411255, Accuracy: 0.6993561168895492\n",
      "Step: 1346, Loss: 1.2009241580963135, Accuracy: 0.6993318485523385\n",
      "Step: 1347, Loss: 1.3141506910324097, Accuracy: 0.6992457962413452\n",
      "Step: 1348, Loss: 1.4318143129348755, Accuracy: 0.6990363232023721\n",
      "Step: 1349, Loss: 1.3261784315109253, Accuracy: 0.6989506172839506\n",
      "Step: 1350, Loss: 1.6936031579971313, Accuracy: 0.6986183074265976\n",
      "Step: 1351, Loss: 1.3371158838272095, Accuracy: 0.6985330374753451\n",
      "Step: 1352, Loss: 1.0972801446914673, Accuracy: 0.6986326681448632\n",
      "Step: 1353, Loss: 1.0817447900772095, Accuracy: 0.6987321516494338\n",
      "Step: 1354, Loss: 1.3687806129455566, Accuracy: 0.6985854858548586\n",
      "Step: 1355, Loss: 1.2966694831848145, Accuracy: 0.6985004916420846\n",
      "Step: 1356, Loss: 1.3128775358200073, Accuracy: 0.698415622697126\n",
      "Step: 1357, Loss: 1.1060703992843628, Accuracy: 0.698514972999509\n",
      "Step: 1358, Loss: 1.202940583229065, Accuracy: 0.6984915378955114\n",
      "Step: 1359, Loss: 1.357039451599121, Accuracy: 0.6983455882352941\n",
      "Step: 1360, Loss: 1.2601810693740845, Accuracy: 0.6983223120254715\n",
      "Step: 1361, Loss: 1.2038339376449585, Accuracy: 0.6983602545276554\n",
      "Step: 1362, Loss: 1.0967086553573608, Accuracy: 0.698459280997799\n",
      "Step: 1363, Loss: 1.1983641386032104, Accuracy: 0.6984970674486803\n",
      "Step: 1364, Loss: 1.0332063436508179, Accuracy: 0.6985958485958486\n",
      "Step: 1365, Loss: 1.5284878015518188, Accuracy: 0.6983284529038556\n",
      "Step: 1366, Loss: 1.0743619203567505, Accuracy: 0.6984272128749086\n",
      "Step: 1367, Loss: 1.1308082342147827, Accuracy: 0.698525828460039\n",
      "Step: 1368, Loss: 1.2109352350234985, Accuracy: 0.6985025566106647\n",
      "Step: 1369, Loss: 1.1004129648208618, Accuracy: 0.6986009732360098\n",
      "Step: 1370, Loss: 1.0135008096694946, Accuracy: 0.6987600291757841\n",
      "Step: 1371, Loss: 1.1517764329910278, Accuracy: 0.6987973760932945\n",
      "Step: 1372, Loss: 1.252682089805603, Accuracy: 0.6987739742655984\n",
      "Step: 1373, Loss: 1.2754701375961304, Accuracy: 0.6986899563318777\n",
      "Step: 1374, Loss: 1.1178488731384277, Accuracy: 0.6987272727272728\n",
      "Step: 1375, Loss: 1.2987658977508545, Accuracy: 0.6986434108527132\n",
      "Step: 1376, Loss: 1.0169659852981567, Accuracy: 0.69880174291939\n",
      "Step: 1377, Loss: 1.1228313446044922, Accuracy: 0.6988993710691824\n",
      "Step: 1378, Loss: 1.3931373357772827, Accuracy: 0.6987551365723954\n",
      "Step: 1379, Loss: 1.1536325216293335, Accuracy: 0.6987922705314009\n",
      "Step: 1380, Loss: 1.1426798105239868, Accuracy: 0.6987690079652426\n",
      "Step: 1381, Loss: 1.1074309349060059, Accuracy: 0.698866377231066\n",
      "Step: 1382, Loss: 1.2363966703414917, Accuracy: 0.6988430947216197\n",
      "Step: 1383, Loss: 1.2027413845062256, Accuracy: 0.6988198458574181\n",
      "Step: 1384, Loss: 1.0081394910812378, Accuracy: 0.6989771359807461\n",
      "Step: 1385, Loss: 1.1489638090133667, Accuracy: 0.699013949013949\n",
      "Step: 1386, Loss: 1.2116690874099731, Accuracy: 0.6989906272530642\n",
      "Step: 1387, Loss: 1.104690432548523, Accuracy: 0.6990874159462056\n",
      "Step: 1388, Loss: 1.2869583368301392, Accuracy: 0.6990040796736261\n",
      "Step: 1389, Loss: 1.2388192415237427, Accuracy: 0.6989808153477218\n",
      "Step: 1390, Loss: 1.1475703716278076, Accuracy: 0.6990174934100167\n",
      "Step: 1391, Loss: 1.2324390411376953, Accuracy: 0.6989942528735632\n",
      "Step: 1392, Loss: 1.251668095588684, Accuracy: 0.6990308686288585\n",
      "Step: 1393, Loss: 1.3091179132461548, Accuracy: 0.6989478718316595\n",
      "Step: 1394, Loss: 1.3140864372253418, Accuracy: 0.6988649940262843\n",
      "Step: 1395, Loss: 1.30740225315094, Accuracy: 0.6987822349570201\n",
      "Step: 1396, Loss: 1.0741981267929077, Accuracy: 0.69887854927225\n",
      "Step: 1397, Loss: 1.3164089918136597, Accuracy: 0.698795898903195\n",
      "Step: 1398, Loss: 1.249721646308899, Accuracy: 0.6987729330474148\n",
      "Step: 1399, Loss: 1.1608673334121704, Accuracy: 0.6988095238095238\n",
      "Step: 1400, Loss: 1.1144596338272095, Accuracy: 0.698905543659291\n",
      "Step: 1401, Loss: 1.0257941484451294, Accuracy: 0.6990608654303376\n",
      "Step: 1402, Loss: 1.0858125686645508, Accuracy: 0.6991565692563554\n",
      "Step: 1403, Loss: 1.2841325998306274, Accuracy: 0.6990740740740741\n",
      "Step: 1404, Loss: 1.1758652925491333, Accuracy: 0.6991103202846976\n",
      "Step: 1405, Loss: 1.308580994606018, Accuracy: 0.6990279753437648\n",
      "Step: 1406, Loss: 1.0912771224975586, Accuracy: 0.6991234304667141\n",
      "Step: 1407, Loss: 1.2625104188919067, Accuracy: 0.6990411931818182\n",
      "Step: 1408, Loss: 1.3496185541152954, Accuracy: 0.6989590726283416\n",
      "Step: 1409, Loss: 1.1762101650238037, Accuracy: 0.6989952718676123\n",
      "Step: 1410, Loss: 1.1296440362930298, Accuracy: 0.6990314197968344\n",
      "Step: 1411, Loss: 1.0823793411254883, Accuracy: 0.6991265344664778\n",
      "Step: 1412, Loss: 1.2356888055801392, Accuracy: 0.699103562160887\n",
      "Step: 1413, Loss: 1.3870372772216797, Accuracy: 0.6989627534181989\n",
      "Step: 1414, Loss: 1.176697850227356, Accuracy: 0.6989988221436985\n",
      "Step: 1415, Loss: 1.1660345792770386, Accuracy: 0.6990348399246704\n",
      "Step: 1416, Loss: 1.1461135149002075, Accuracy: 0.699070806868972\n",
      "Step: 1417, Loss: 1.2731661796569824, Accuracy: 0.6989891866478608\n",
      "Step: 1418, Loss: 1.2240060567855835, Accuracy: 0.6990251350716467\n",
      "Step: 1419, Loss: 1.0631922483444214, Accuracy: 0.6991197183098592\n",
      "Step: 1420, Loss: 1.1823222637176514, Accuracy: 0.699155524278677\n",
      "Step: 1421, Loss: 0.9893561005592346, Accuracy: 0.6993084857008908\n",
      "Step: 1422, Loss: 1.379000186920166, Accuracy: 0.6991684235183884\n",
      "Step: 1423, Loss: 1.257188320159912, Accuracy: 0.6991455992509363\n",
      "Step: 1424, Loss: 1.4879279136657715, Accuracy: 0.6989473684210527\n",
      "Step: 1425, Loss: 1.204694151878357, Accuracy: 0.6989247311827957\n",
      "Step: 1426, Loss: 1.2348551750183105, Accuracy: 0.6986685353889278\n",
      "Epoch: 7, Val_Accuracy: 0.3716510903426791\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6562f3098cf04049aff230b05b34ca12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1427 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0, Loss: 1.2590357065200806, Accuracy: 0.6666666666666666\n",
      "Step: 1, Loss: 1.2605406045913696, Accuracy: 0.6666666666666666\n",
      "Step: 2, Loss: 1.242194652557373, Accuracy: 0.6666666666666666\n",
      "Step: 3, Loss: 1.2192707061767578, Accuracy: 0.6666666666666666\n",
      "Step: 4, Loss: 1.2694450616836548, Accuracy: 0.6666666666666666\n",
      "Step: 5, Loss: 1.197769045829773, Accuracy: 0.6805555555555556\n",
      "Step: 6, Loss: 0.972682535648346, Accuracy: 0.7261904761904762\n",
      "Step: 7, Loss: 1.2203296422958374, Accuracy: 0.71875\n",
      "Step: 8, Loss: 1.1715563535690308, Accuracy: 0.7129629629629629\n",
      "Step: 9, Loss: 1.1510672569274902, Accuracy: 0.7166666666666667\n",
      "Step: 10, Loss: 0.969387948513031, Accuracy: 0.7424242424242424\n",
      "Step: 11, Loss: 1.2534576654434204, Accuracy: 0.7361111111111112\n",
      "Step: 12, Loss: 1.1673613786697388, Accuracy: 0.7371794871794872\n",
      "Step: 13, Loss: 1.2661255598068237, Accuracy: 0.7321428571428571\n",
      "Step: 14, Loss: 1.0375102758407593, Accuracy: 0.7388888888888889\n",
      "Step: 15, Loss: 1.2148884534835815, Accuracy: 0.734375\n",
      "Step: 16, Loss: 1.214218020439148, Accuracy: 0.7303921568627451\n",
      "Step: 17, Loss: 1.1988272666931152, Accuracy: 0.7268518518518519\n",
      "Step: 18, Loss: 1.3034950494766235, Accuracy: 0.7192982456140351\n",
      "Step: 19, Loss: 1.1550642251968384, Accuracy: 0.7208333333333333\n",
      "Step: 20, Loss: 1.0961716175079346, Accuracy: 0.7261904761904762\n",
      "Step: 21, Loss: 1.1603821516036987, Accuracy: 0.7272727272727273\n",
      "Step: 22, Loss: 1.0481414794921875, Accuracy: 0.7318840579710145\n",
      "Step: 23, Loss: 1.3092842102050781, Accuracy: 0.7256944444444444\n",
      "Step: 24, Loss: 1.2740459442138672, Accuracy: 0.72\n",
      "Step: 25, Loss: 1.3046460151672363, Accuracy: 0.7147435897435898\n",
      "Step: 26, Loss: 1.189057469367981, Accuracy: 0.7129629629629629\n",
      "Step: 27, Loss: 1.221300482749939, Accuracy: 0.7113095238095238\n",
      "Step: 28, Loss: 1.1940821409225464, Accuracy: 0.7126436781609196\n",
      "Step: 29, Loss: 1.075594425201416, Accuracy: 0.7166666666666667\n",
      "Step: 30, Loss: 1.1538535356521606, Accuracy: 0.717741935483871\n",
      "Step: 31, Loss: 1.3174916505813599, Accuracy: 0.7135416666666666\n",
      "Step: 32, Loss: 1.0937319993972778, Accuracy: 0.7171717171717171\n",
      "Step: 33, Loss: 1.1203899383544922, Accuracy: 0.7181372549019608\n",
      "Step: 34, Loss: 0.9851446151733398, Accuracy: 0.7238095238095238\n",
      "Step: 35, Loss: 1.2351818084716797, Accuracy: 0.7222222222222222\n",
      "Step: 36, Loss: 1.3846229314804077, Accuracy: 0.7162162162162162\n",
      "Step: 37, Loss: 1.1853314638137817, Accuracy: 0.7171052631578947\n",
      "Step: 38, Loss: 1.2348966598510742, Accuracy: 0.7158119658119658\n",
      "Step: 39, Loss: 1.2327240705490112, Accuracy: 0.7166666666666667\n",
      "Step: 40, Loss: 1.0957531929016113, Accuracy: 0.7195121951219512\n",
      "Step: 41, Loss: 1.1990362405776978, Accuracy: 0.7202380952380952\n",
      "Step: 42, Loss: 1.3130792379379272, Accuracy: 0.7170542635658915\n",
      "Step: 43, Loss: 1.1881030797958374, Accuracy: 0.7178030303030303\n",
      "Step: 44, Loss: 1.37838876247406, Accuracy: 0.7129629629629629\n",
      "Step: 45, Loss: 1.2205902338027954, Accuracy: 0.7119565217391305\n",
      "Step: 46, Loss: 1.393198013305664, Accuracy: 0.7056737588652482\n",
      "Step: 47, Loss: 1.2254806756973267, Accuracy: 0.7048611111111112\n",
      "Step: 48, Loss: 0.9779044985771179, Accuracy: 0.7091836734693877\n",
      "Step: 49, Loss: 1.2358310222625732, Accuracy: 0.7083333333333334\n",
      "Step: 50, Loss: 1.1401433944702148, Accuracy: 0.7091503267973857\n",
      "Step: 51, Loss: 1.302013874053955, Accuracy: 0.7067307692307693\n",
      "Step: 52, Loss: 1.0803700685501099, Accuracy: 0.7091194968553459\n",
      "Step: 53, Loss: 1.1427996158599854, Accuracy: 0.7098765432098766\n",
      "Step: 54, Loss: 1.33958899974823, Accuracy: 0.706060606060606\n",
      "Step: 55, Loss: 1.11033296585083, Accuracy: 0.7083333333333334\n",
      "Step: 56, Loss: 1.190355658531189, Accuracy: 0.7076023391812866\n",
      "Step: 57, Loss: 1.2914708852767944, Accuracy: 0.7068965517241379\n",
      "Step: 58, Loss: 1.1246894598007202, Accuracy: 0.7090395480225988\n",
      "Step: 59, Loss: 1.0947259664535522, Accuracy: 0.7097222222222223\n",
      "Step: 60, Loss: 0.9452245831489563, Accuracy: 0.7144808743169399\n",
      "Step: 61, Loss: 1.1863497495651245, Accuracy: 0.7150537634408602\n",
      "Step: 62, Loss: 0.9743394255638123, Accuracy: 0.7182539682539683\n",
      "Step: 63, Loss: 1.041752815246582, Accuracy: 0.7200520833333334\n",
      "Step: 64, Loss: 1.1851086616516113, Accuracy: 0.7192307692307692\n",
      "Step: 65, Loss: 1.1006219387054443, Accuracy: 0.7209595959595959\n",
      "Step: 66, Loss: 1.2419918775558472, Accuracy: 0.7201492537313433\n",
      "Step: 67, Loss: 1.2195165157318115, Accuracy: 0.7193627450980392\n",
      "Step: 68, Loss: 1.1420310735702515, Accuracy: 0.7198067632850241\n",
      "Step: 69, Loss: 1.3481162786483765, Accuracy: 0.7178571428571429\n",
      "Step: 70, Loss: 1.1764291524887085, Accuracy: 0.7183098591549296\n",
      "Step: 71, Loss: 1.3671678304672241, Accuracy: 0.7164351851851852\n",
      "Step: 72, Loss: 1.346049189567566, Accuracy: 0.7134703196347032\n",
      "Step: 73, Loss: 1.429093360900879, Accuracy: 0.7105855855855856\n",
      "Step: 74, Loss: 1.2084881067276, Accuracy: 0.71\n",
      "Step: 75, Loss: 1.0048843622207642, Accuracy: 0.7127192982456141\n",
      "Step: 76, Loss: 1.476385235786438, Accuracy: 0.7088744588744589\n",
      "Step: 77, Loss: 1.1942733526229858, Accuracy: 0.7094017094017094\n",
      "Step: 78, Loss: 1.0131080150604248, Accuracy: 0.7120253164556962\n",
      "Step: 79, Loss: 1.0808969736099243, Accuracy: 0.7135416666666666\n",
      "Step: 80, Loss: 1.2206624746322632, Accuracy: 0.7129629629629629\n",
      "Step: 81, Loss: 1.112336277961731, Accuracy: 0.7134146341463414\n",
      "Step: 82, Loss: 1.058329701423645, Accuracy: 0.714859437751004\n",
      "Step: 83, Loss: 1.0963696241378784, Accuracy: 0.7162698412698413\n",
      "Step: 84, Loss: 1.1070154905319214, Accuracy: 0.7176470588235294\n",
      "Step: 85, Loss: 1.476805329322815, Accuracy: 0.7141472868217055\n",
      "Step: 86, Loss: 1.365565299987793, Accuracy: 0.7116858237547893\n",
      "Step: 87, Loss: 1.3286716938018799, Accuracy: 0.7102272727272727\n",
      "Step: 88, Loss: 1.1832351684570312, Accuracy: 0.7106741573033708\n",
      "Step: 89, Loss: 1.1934505701065063, Accuracy: 0.7101851851851851\n",
      "Step: 90, Loss: 1.1933958530426025, Accuracy: 0.7106227106227107\n",
      "Step: 91, Loss: 1.1533037424087524, Accuracy: 0.7110507246376812\n",
      "Step: 92, Loss: 1.3387088775634766, Accuracy: 0.7096774193548387\n",
      "Step: 93, Loss: 1.1750061511993408, Accuracy: 0.7101063829787234\n",
      "Step: 94, Loss: 1.3698225021362305, Accuracy: 0.7078947368421052\n",
      "Step: 95, Loss: 1.0598403215408325, Accuracy: 0.7092013888888888\n",
      "Step: 96, Loss: 1.2090469598770142, Accuracy: 0.7096219931271478\n",
      "Step: 97, Loss: 1.2939324378967285, Accuracy: 0.7083333333333334\n",
      "Step: 98, Loss: 1.0825458765029907, Accuracy: 0.7095959595959596\n",
      "Step: 99, Loss: 1.1399186849594116, Accuracy: 0.71\n",
      "Step: 100, Loss: 1.4036849737167358, Accuracy: 0.7079207920792079\n",
      "Step: 101, Loss: 1.1629217863082886, Accuracy: 0.7083333333333334\n",
      "Step: 102, Loss: 1.4548569917678833, Accuracy: 0.7055016181229773\n",
      "Step: 103, Loss: 1.1901202201843262, Accuracy: 0.7051282051282052\n",
      "Step: 104, Loss: 1.1365437507629395, Accuracy: 0.7055555555555556\n",
      "Step: 105, Loss: 1.205762505531311, Accuracy: 0.7059748427672956\n",
      "Step: 106, Loss: 1.3049201965332031, Accuracy: 0.7048286604361371\n",
      "Step: 107, Loss: 1.0444787740707397, Accuracy: 0.7060185185185185\n",
      "Step: 108, Loss: 1.1676708459854126, Accuracy: 0.7064220183486238\n",
      "Step: 109, Loss: 1.4002488851547241, Accuracy: 0.7037878787878787\n",
      "Step: 110, Loss: 1.1542565822601318, Accuracy: 0.7042042042042042\n",
      "Step: 111, Loss: 1.0844035148620605, Accuracy: 0.7053571428571429\n",
      "Step: 112, Loss: 1.4426556825637817, Accuracy: 0.7035398230088495\n",
      "Step: 113, Loss: 1.1133387088775635, Accuracy: 0.7046783625730995\n",
      "Step: 114, Loss: 1.0985994338989258, Accuracy: 0.7057971014492753\n",
      "Step: 115, Loss: 0.9514060616493225, Accuracy: 0.7076149425287356\n",
      "Step: 116, Loss: 1.1962546110153198, Accuracy: 0.7072649572649573\n",
      "Step: 117, Loss: 1.2574180364608765, Accuracy: 0.7069209039548022\n",
      "Step: 118, Loss: 1.3268297910690308, Accuracy: 0.7051820728291317\n",
      "Step: 119, Loss: 1.1655834913253784, Accuracy: 0.7055555555555556\n",
      "Step: 120, Loss: 1.173694133758545, Accuracy: 0.7059228650137741\n",
      "Step: 121, Loss: 1.2257417440414429, Accuracy: 0.7056010928961749\n",
      "Step: 122, Loss: 1.234183669090271, Accuracy: 0.7052845528455285\n",
      "Step: 123, Loss: 1.2833425998687744, Accuracy: 0.7043010752688172\n",
      "Step: 124, Loss: 0.9954553246498108, Accuracy: 0.706\n",
      "Step: 125, Loss: 1.316335916519165, Accuracy: 0.705026455026455\n",
      "Step: 126, Loss: 1.2160125970840454, Accuracy: 0.7047244094488189\n",
      "Step: 127, Loss: 1.3367692232131958, Accuracy: 0.7044270833333334\n",
      "Step: 128, Loss: 1.3381905555725098, Accuracy: 0.7034883720930233\n",
      "Step: 129, Loss: 1.5085501670837402, Accuracy: 0.7006410256410256\n",
      "Step: 130, Loss: 1.0265406370162964, Accuracy: 0.7022900763358778\n",
      "Step: 131, Loss: 1.2958921194076538, Accuracy: 0.7013888888888888\n",
      "Step: 132, Loss: 1.2788792848587036, Accuracy: 0.7011278195488722\n",
      "Step: 133, Loss: 1.2984756231307983, Accuracy: 0.7002487562189055\n",
      "Step: 134, Loss: 1.4650784730911255, Accuracy: 0.6981481481481482\n",
      "Step: 135, Loss: 1.0583113431930542, Accuracy: 0.6997549019607843\n",
      "Step: 136, Loss: 1.0759786367416382, Accuracy: 0.7007299270072993\n",
      "Step: 137, Loss: 1.0054928064346313, Accuracy: 0.7022946859903382\n",
      "Step: 138, Loss: 1.1039222478866577, Accuracy: 0.7032374100719424\n",
      "Step: 139, Loss: 1.0099244117736816, Accuracy: 0.7047619047619048\n",
      "Step: 140, Loss: 1.1697728633880615, Accuracy: 0.7050827423167849\n",
      "Step: 141, Loss: 1.2326058149337769, Accuracy: 0.70481220657277\n",
      "Step: 142, Loss: 1.1654316186904907, Accuracy: 0.7051282051282052\n",
      "Step: 143, Loss: 1.052937388420105, Accuracy: 0.7060185185185185\n",
      "Step: 144, Loss: 1.0105010271072388, Accuracy: 0.707471264367816\n",
      "Step: 145, Loss: 1.1796118021011353, Accuracy: 0.7071917808219178\n",
      "Step: 146, Loss: 1.0622538328170776, Accuracy: 0.7086167800453514\n",
      "Step: 147, Loss: 1.2193630933761597, Accuracy: 0.7083333333333334\n",
      "Step: 148, Loss: 1.1207650899887085, Accuracy: 0.7086129753914989\n",
      "Step: 149, Loss: 1.096211314201355, Accuracy: 0.7094444444444444\n",
      "Step: 150, Loss: 1.2066622972488403, Accuracy: 0.7091611479028698\n",
      "Step: 151, Loss: 1.2245080471038818, Accuracy: 0.7088815789473685\n",
      "Step: 152, Loss: 1.1639951467514038, Accuracy: 0.7091503267973857\n",
      "Step: 153, Loss: 1.2292585372924805, Accuracy: 0.7094155844155844\n",
      "Step: 154, Loss: 1.0005439519882202, Accuracy: 0.710752688172043\n",
      "Step: 155, Loss: 1.3933907747268677, Accuracy: 0.7094017094017094\n",
      "Step: 156, Loss: 1.1281912326812744, Accuracy: 0.7096602972399151\n",
      "Step: 157, Loss: 1.219275712966919, Accuracy: 0.7093881856540084\n",
      "Step: 158, Loss: 1.1799544095993042, Accuracy: 0.709643605870021\n",
      "Step: 159, Loss: 1.3941950798034668, Accuracy: 0.7083333333333334\n",
      "Step: 160, Loss: 1.0016411542892456, Accuracy: 0.7096273291925466\n",
      "Step: 161, Loss: 1.2428499460220337, Accuracy: 0.7093621399176955\n",
      "Step: 162, Loss: 0.9622089266777039, Accuracy: 0.7111451942740287\n",
      "Step: 163, Loss: 1.1289417743682861, Accuracy: 0.711890243902439\n",
      "Step: 164, Loss: 1.2556244134902954, Accuracy: 0.7111111111111111\n",
      "Step: 165, Loss: 1.1205962896347046, Accuracy: 0.7113453815261044\n",
      "Step: 166, Loss: 1.1265486478805542, Accuracy: 0.7115768463073853\n",
      "Step: 167, Loss: 1.0810213088989258, Accuracy: 0.7123015873015873\n",
      "Step: 168, Loss: 1.3216251134872437, Accuracy: 0.7115384615384616\n",
      "Step: 169, Loss: 1.2260288000106812, Accuracy: 0.7112745098039216\n",
      "Step: 170, Loss: 1.3282054662704468, Accuracy: 0.7100389863547758\n",
      "Step: 171, Loss: 1.291054129600525, Accuracy: 0.7093023255813954\n",
      "Step: 172, Loss: 1.0052653551101685, Accuracy: 0.7105009633911368\n",
      "Step: 173, Loss: 1.3031023740768433, Accuracy: 0.7097701149425287\n",
      "Step: 174, Loss: 1.1747654676437378, Accuracy: 0.71\n",
      "Step: 175, Loss: 1.240343451499939, Accuracy: 0.7097537878787878\n",
      "Step: 176, Loss: 1.0960302352905273, Accuracy: 0.71045197740113\n",
      "Step: 177, Loss: 1.177697777748108, Accuracy: 0.7106741573033708\n",
      "Step: 178, Loss: 1.3890328407287598, Accuracy: 0.7094972067039106\n",
      "Step: 179, Loss: 1.1100490093231201, Accuracy: 0.7101851851851851\n",
      "Step: 180, Loss: 1.2435551881790161, Accuracy: 0.7099447513812155\n",
      "Step: 181, Loss: 1.069921612739563, Accuracy: 0.7106227106227107\n",
      "Step: 182, Loss: 1.1825352907180786, Accuracy: 0.7108378870673953\n",
      "Step: 183, Loss: 1.168384075164795, Accuracy: 0.7110507246376812\n",
      "Step: 184, Loss: 1.0545276403427124, Accuracy: 0.7117117117117117\n",
      "Step: 185, Loss: 1.0329822301864624, Accuracy: 0.7128136200716846\n",
      "Step: 186, Loss: 1.3253551721572876, Accuracy: 0.7121212121212122\n",
      "Step: 187, Loss: 1.0897096395492554, Accuracy: 0.7127659574468085\n",
      "Step: 188, Loss: 1.1446857452392578, Accuracy: 0.7129629629629629\n",
      "Step: 189, Loss: 1.1379728317260742, Accuracy: 0.7131578947368421\n",
      "Step: 190, Loss: 1.0601710081100464, Accuracy: 0.7137870855148342\n",
      "Step: 191, Loss: 1.166437029838562, Accuracy: 0.7139756944444444\n",
      "Step: 192, Loss: 1.305621862411499, Accuracy: 0.7132987910189983\n",
      "Step: 193, Loss: 1.0009765625, Accuracy: 0.7143470790378007\n",
      "Step: 194, Loss: 1.2446571588516235, Accuracy: 0.7141025641025641\n",
      "Step: 195, Loss: 1.2217772006988525, Accuracy: 0.7138605442176871\n",
      "Step: 196, Loss: 1.0715972185134888, Accuracy: 0.7144670050761421\n",
      "Step: 197, Loss: 1.2630165815353394, Accuracy: 0.7142255892255892\n",
      "Step: 198, Loss: 1.230959177017212, Accuracy: 0.7139865996649917\n",
      "Step: 199, Loss: 1.1784394979476929, Accuracy: 0.71375\n",
      "Step: 200, Loss: 1.1885098218917847, Accuracy: 0.7139303482587065\n",
      "Step: 201, Loss: 1.0864613056182861, Accuracy: 0.7145214521452146\n",
      "Step: 202, Loss: 1.4334665536880493, Accuracy: 0.7130541871921182\n",
      "Step: 203, Loss: 1.1851409673690796, Accuracy: 0.7132352941176471\n",
      "Step: 204, Loss: 1.0621590614318848, Accuracy: 0.7138211382113822\n",
      "Step: 205, Loss: 1.4113389253616333, Accuracy: 0.7127831715210357\n",
      "Step: 206, Loss: 1.0953694581985474, Accuracy: 0.7133655394524959\n",
      "Step: 207, Loss: 1.0783065557479858, Accuracy: 0.7139423076923077\n",
      "Step: 208, Loss: 1.132745623588562, Accuracy: 0.7141148325358851\n",
      "Step: 209, Loss: 1.1707526445388794, Accuracy: 0.7142857142857143\n",
      "Step: 210, Loss: 1.0892425775527954, Accuracy: 0.7148499210110585\n",
      "Step: 211, Loss: 1.3479474782943726, Accuracy: 0.7142295597484277\n",
      "Step: 212, Loss: 1.3196483850479126, Accuracy: 0.7136150234741784\n",
      "Step: 213, Loss: 1.2508928775787354, Accuracy: 0.7133956386292835\n",
      "Step: 214, Loss: 1.2272032499313354, Accuracy: 0.7131782945736435\n",
      "Step: 215, Loss: 1.0945439338684082, Accuracy: 0.7133487654320988\n",
      "Step: 216, Loss: 1.2132164239883423, Accuracy: 0.7131336405529954\n",
      "Step: 217, Loss: 1.3000119924545288, Accuracy: 0.7125382262996942\n",
      "Step: 218, Loss: 1.162176489830017, Accuracy: 0.7127092846270928\n",
      "Step: 219, Loss: 1.3221503496170044, Accuracy: 0.7121212121212122\n",
      "Step: 220, Loss: 1.1625970602035522, Accuracy: 0.7122926093514329\n",
      "Step: 221, Loss: 1.147085428237915, Accuracy: 0.7124624624624625\n",
      "Step: 222, Loss: 1.0742038488388062, Accuracy: 0.7130044843049327\n",
      "Step: 223, Loss: 1.24736487865448, Accuracy: 0.7127976190476191\n",
      "Step: 224, Loss: 0.9910343289375305, Accuracy: 0.7137037037037037\n",
      "Step: 225, Loss: 1.0686434507369995, Accuracy: 0.7142330383480826\n",
      "Step: 226, Loss: 1.4227617979049683, Accuracy: 0.7132892804698973\n",
      "Step: 227, Loss: 1.3263601064682007, Accuracy: 0.7130847953216374\n",
      "Step: 228, Loss: 1.3095016479492188, Accuracy: 0.7125181950509462\n",
      "Step: 229, Loss: 1.2067216634750366, Accuracy: 0.7123188405797102\n",
      "Step: 230, Loss: 1.2544628381729126, Accuracy: 0.7121212121212122\n",
      "Step: 231, Loss: 1.2189230918884277, Accuracy: 0.7119252873563219\n",
      "Step: 232, Loss: 1.044356107711792, Accuracy: 0.7124463519313304\n",
      "Step: 233, Loss: 1.0915707349777222, Accuracy: 0.7126068376068376\n",
      "Step: 234, Loss: 0.993908166885376, Accuracy: 0.7134751773049646\n",
      "Step: 235, Loss: 1.3863617181777954, Accuracy: 0.7125706214689266\n",
      "Step: 236, Loss: 1.306962013244629, Accuracy: 0.7120253164556962\n",
      "Step: 237, Loss: 1.3579062223434448, Accuracy: 0.7111344537815126\n",
      "Step: 238, Loss: 1.0893131494522095, Accuracy: 0.7116457461645747\n",
      "Step: 239, Loss: 1.3311411142349243, Accuracy: 0.7111111111111111\n",
      "Step: 240, Loss: 1.1786091327667236, Accuracy: 0.7112724757952974\n",
      "Step: 241, Loss: 1.3441801071166992, Accuracy: 0.7107438016528925\n",
      "Step: 242, Loss: 1.459879755973816, Accuracy: 0.7095336076817559\n",
      "Step: 243, Loss: 1.0367252826690674, Accuracy: 0.7100409836065574\n",
      "Step: 244, Loss: 1.159498929977417, Accuracy: 0.710204081632653\n",
      "Step: 245, Loss: 1.1264134645462036, Accuracy: 0.7107046070460704\n",
      "Step: 246, Loss: 1.2192224264144897, Accuracy: 0.7105263157894737\n",
      "Step: 247, Loss: 1.4327493906021118, Accuracy: 0.7093413978494624\n",
      "Step: 248, Loss: 1.230193018913269, Accuracy: 0.7091700133868809\n",
      "Step: 249, Loss: 1.0257667303085327, Accuracy: 0.7096666666666667\n",
      "Step: 250, Loss: 1.0874550342559814, Accuracy: 0.7101593625498008\n",
      "Step: 251, Loss: 1.077952265739441, Accuracy: 0.7106481481481481\n",
      "Step: 252, Loss: 1.010260820388794, Accuracy: 0.7114624505928854\n",
      "Step: 253, Loss: 1.3374866247177124, Accuracy: 0.7109580052493438\n",
      "Step: 254, Loss: 1.1712303161621094, Accuracy: 0.7111111111111111\n",
      "Step: 255, Loss: 1.0813289880752563, Accuracy: 0.7115885416666666\n",
      "Step: 256, Loss: 1.137407898902893, Accuracy: 0.7117380025940337\n",
      "Step: 257, Loss: 1.1292556524276733, Accuracy: 0.7122093023255814\n",
      "Step: 258, Loss: 1.1821781396865845, Accuracy: 0.7123552123552124\n",
      "Step: 259, Loss: 1.225396752357483, Accuracy: 0.7121794871794872\n",
      "Step: 260, Loss: 1.3142329454421997, Accuracy: 0.7116858237547893\n",
      "Step: 261, Loss: 1.0617040395736694, Accuracy: 0.712468193384224\n",
      "Step: 262, Loss: 1.08534836769104, Accuracy: 0.7126108998732573\n",
      "Step: 263, Loss: 1.4495290517807007, Accuracy: 0.711489898989899\n",
      "Step: 264, Loss: 1.0744786262512207, Accuracy: 0.7122641509433962\n",
      "Step: 265, Loss: 0.9780142903327942, Accuracy: 0.7130325814536341\n",
      "Step: 266, Loss: 0.9405934810638428, Accuracy: 0.714107365792759\n",
      "Step: 267, Loss: 1.1752511262893677, Accuracy: 0.7142412935323383\n",
      "Step: 268, Loss: 1.125326156616211, Accuracy: 0.7143742255266419\n",
      "Step: 269, Loss: 1.222663164138794, Accuracy: 0.7141975308641976\n",
      "Step: 270, Loss: 1.256532907485962, Accuracy: 0.7140221402214022\n",
      "Step: 271, Loss: 1.0410219430923462, Accuracy: 0.7147671568627451\n",
      "Step: 272, Loss: 1.1445096731185913, Accuracy: 0.7148962148962149\n",
      "Step: 273, Loss: 1.2983546257019043, Accuracy: 0.7144160583941606\n",
      "Step: 274, Loss: 1.0256073474884033, Accuracy: 0.7151515151515152\n",
      "Step: 275, Loss: 1.5313897132873535, Accuracy: 0.7140700483091788\n",
      "Step: 276, Loss: 1.1115208864212036, Accuracy: 0.7145006016847172\n",
      "Step: 277, Loss: 1.2440608739852905, Accuracy: 0.7143285371702638\n",
      "Step: 278, Loss: 1.0681899785995483, Accuracy: 0.7147550776583035\n",
      "Step: 279, Loss: 1.251473307609558, Accuracy: 0.7145833333333333\n",
      "Step: 280, Loss: 1.0071521997451782, Accuracy: 0.7153024911032029\n",
      "Step: 281, Loss: 1.1995033025741577, Accuracy: 0.7151300236406619\n",
      "Step: 282, Loss: 1.083234429359436, Accuracy: 0.715547703180212\n",
      "Step: 283, Loss: 1.2930850982666016, Accuracy: 0.7150821596244131\n",
      "Step: 284, Loss: 1.2266370058059692, Accuracy: 0.7149122807017544\n",
      "Step: 285, Loss: 1.354648470878601, Accuracy: 0.7141608391608392\n",
      "Step: 286, Loss: 1.1106222867965698, Accuracy: 0.7145760743321719\n",
      "Step: 287, Loss: 1.2695186138153076, Accuracy: 0.7141203703703703\n",
      "Step: 288, Loss: 1.1797007322311401, Accuracy: 0.7139561707035755\n",
      "Step: 289, Loss: 1.1292939186096191, Accuracy: 0.714367816091954\n",
      "Step: 290, Loss: 1.2291293144226074, Accuracy: 0.7142038946162658\n",
      "Step: 291, Loss: 1.144012689590454, Accuracy: 0.7146118721461188\n",
      "Step: 292, Loss: 1.1575627326965332, Accuracy: 0.7147326507394767\n",
      "Step: 293, Loss: 1.0713460445404053, Accuracy: 0.7151360544217688\n",
      "Step: 294, Loss: 1.007900357246399, Accuracy: 0.715819209039548\n",
      "Step: 295, Loss: 1.4071818590164185, Accuracy: 0.7148085585585585\n",
      "Step: 296, Loss: 1.2295517921447754, Accuracy: 0.7146464646464646\n",
      "Step: 297, Loss: 1.1664235591888428, Accuracy: 0.714765100671141\n",
      "Step: 298, Loss: 1.2261837720870972, Accuracy: 0.7146042363433668\n",
      "Step: 299, Loss: 1.2890875339508057, Accuracy: 0.7144444444444444\n",
      "Step: 300, Loss: 1.3504300117492676, Accuracy: 0.7140088593576965\n",
      "Step: 301, Loss: 1.1879281997680664, Accuracy: 0.7138520971302428\n",
      "Step: 302, Loss: 1.1653432846069336, Accuracy: 0.713971397139714\n",
      "Step: 303, Loss: 1.398416519165039, Accuracy: 0.7132675438596491\n",
      "Step: 304, Loss: 1.2963879108428955, Accuracy: 0.7128415300546448\n",
      "Step: 305, Loss: 1.1578997373580933, Accuracy: 0.7129629629629629\n",
      "Step: 306, Loss: 1.1394463777542114, Accuracy: 0.7130836047774158\n",
      "Step: 307, Loss: 1.2136539220809937, Accuracy: 0.7132034632034632\n",
      "Step: 308, Loss: 1.1988279819488525, Accuracy: 0.7133225458468176\n",
      "Step: 309, Loss: 1.0693609714508057, Accuracy: 0.7137096774193549\n",
      "Step: 310, Loss: 1.1997257471084595, Accuracy: 0.7135584137191854\n",
      "Step: 311, Loss: 1.5478371381759644, Accuracy: 0.7123397435897436\n",
      "Step: 312, Loss: 1.250909447669983, Accuracy: 0.7121938232161874\n",
      "Step: 313, Loss: 1.238751769065857, Accuracy: 0.7120488322717622\n",
      "Step: 314, Loss: 1.15115487575531, Accuracy: 0.7121693121693121\n",
      "Step: 315, Loss: 1.181746482849121, Accuracy: 0.7122890295358649\n",
      "Step: 316, Loss: 1.3656033277511597, Accuracy: 0.7116193480546793\n",
      "Step: 317, Loss: 1.1087511777877808, Accuracy: 0.7120020964360587\n",
      "Step: 318, Loss: 1.0512280464172363, Accuracy: 0.7123824451410659\n",
      "Step: 319, Loss: 1.237250566482544, Accuracy: 0.7122395833333334\n",
      "Step: 320, Loss: 1.2766934633255005, Accuracy: 0.7118380062305296\n",
      "Step: 321, Loss: 0.9957088828086853, Accuracy: 0.7124741200828157\n",
      "Step: 322, Loss: 1.328351616859436, Accuracy: 0.7118163054695562\n",
      "Step: 323, Loss: 1.496413230895996, Accuracy: 0.7109053497942387\n",
      "Step: 324, Loss: 1.217463731765747, Accuracy: 0.7107692307692308\n",
      "Step: 325, Loss: 1.167840838432312, Accuracy: 0.7108895705521472\n",
      "Step: 326, Loss: 1.5141576528549194, Accuracy: 0.7099898063200816\n",
      "Step: 327, Loss: 1.229960560798645, Accuracy: 0.7098577235772358\n",
      "Step: 328, Loss: 1.0642930269241333, Accuracy: 0.7102330293819655\n",
      "Step: 329, Loss: 1.2579127550125122, Accuracy: 0.7098484848484848\n",
      "Step: 330, Loss: 1.2615548372268677, Accuracy: 0.709718026183283\n",
      "Step: 331, Loss: 1.0721180438995361, Accuracy: 0.7100903614457831\n",
      "Step: 332, Loss: 1.1669267416000366, Accuracy: 0.7102102102102102\n",
      "Step: 333, Loss: 1.2574657201766968, Accuracy: 0.7103293413173652\n",
      "Step: 334, Loss: 1.2175657749176025, Accuracy: 0.7101990049751243\n",
      "Step: 335, Loss: 1.0353857278823853, Accuracy: 0.7105654761904762\n",
      "Step: 336, Loss: 1.3833619356155396, Accuracy: 0.7099406528189911\n",
      "Step: 337, Loss: 1.2399431467056274, Accuracy: 0.7098126232741617\n",
      "Step: 338, Loss: 1.1456060409545898, Accuracy: 0.7099311701081613\n",
      "Step: 339, Loss: 1.2077546119689941, Accuracy: 0.7098039215686275\n",
      "Step: 340, Loss: 1.1823749542236328, Accuracy: 0.7096774193548387\n",
      "Step: 341, Loss: 1.130793571472168, Accuracy: 0.7097953216374269\n",
      "Step: 342, Loss: 1.433201789855957, Accuracy: 0.7091836734693877\n",
      "Step: 343, Loss: 1.2737849950790405, Accuracy: 0.7088178294573644\n",
      "Step: 344, Loss: 1.2625908851623535, Accuracy: 0.7084541062801932\n",
      "Step: 345, Loss: 1.0668174028396606, Accuracy: 0.7088150289017341\n",
      "Step: 346, Loss: 1.1571719646453857, Accuracy: 0.7089337175792507\n",
      "Step: 347, Loss: 1.1770949363708496, Accuracy: 0.709051724137931\n",
      "Step: 348, Loss: 1.2908450365066528, Accuracy: 0.708691499522445\n",
      "Step: 349, Loss: 1.239579439163208, Accuracy: 0.7088095238095238\n",
      "Step: 350, Loss: 1.149888277053833, Accuracy: 0.7089268755935423\n",
      "Step: 351, Loss: 1.4272394180297852, Accuracy: 0.7083333333333334\n",
      "Step: 352, Loss: 1.258623719215393, Accuracy: 0.7082152974504249\n",
      "Step: 353, Loss: 1.1210627555847168, Accuracy: 0.7083333333333334\n",
      "Step: 354, Loss: 1.2275240421295166, Accuracy: 0.7082159624413146\n",
      "Step: 355, Loss: 1.146299958229065, Accuracy: 0.7083333333333334\n",
      "Step: 356, Loss: 1.1329416036605835, Accuracy: 0.7084500466853408\n",
      "Step: 357, Loss: 1.1931965351104736, Accuracy: 0.7083333333333334\n",
      "Step: 358, Loss: 1.1667178869247437, Accuracy: 0.7084493964716806\n",
      "Step: 359, Loss: 1.3405622243881226, Accuracy: 0.7081018518518518\n",
      "Step: 360, Loss: 1.1990712881088257, Accuracy: 0.7082179132040628\n",
      "Step: 361, Loss: 1.4019564390182495, Accuracy: 0.7076427255985267\n",
      "Step: 362, Loss: 1.2080323696136475, Accuracy: 0.7077594123048668\n",
      "Step: 363, Loss: 1.3636159896850586, Accuracy: 0.7071886446886447\n",
      "Step: 364, Loss: 1.319275975227356, Accuracy: 0.7068493150684931\n",
      "Step: 365, Loss: 1.2404570579528809, Accuracy: 0.7067395264116576\n",
      "Step: 366, Loss: 1.328762412071228, Accuracy: 0.7064032697547684\n",
      "Step: 367, Loss: 1.1895090341567993, Accuracy: 0.7065217391304348\n",
      "Step: 368, Loss: 1.0405174493789673, Accuracy: 0.7068654019873533\n",
      "Step: 369, Loss: 1.2992712259292603, Accuracy: 0.7065315315315316\n",
      "Step: 370, Loss: 1.23018217086792, Accuracy: 0.7064240790655885\n",
      "Step: 371, Loss: 1.3288753032684326, Accuracy: 0.7058691756272402\n",
      "Step: 372, Loss: 1.3318698406219482, Accuracy: 0.7055406613047364\n",
      "Step: 373, Loss: 1.145048975944519, Accuracy: 0.7056595365418895\n",
      "Step: 374, Loss: 0.9892817139625549, Accuracy: 0.7062222222222222\n",
      "Step: 375, Loss: 1.3498584032058716, Accuracy: 0.7056737588652482\n",
      "Step: 376, Loss: 1.192975401878357, Accuracy: 0.7057913351016799\n",
      "Step: 377, Loss: 1.234366536140442, Accuracy: 0.7056878306878307\n",
      "Step: 378, Loss: 1.1386593580245972, Accuracy: 0.7058047493403694\n",
      "Step: 379, Loss: 1.2857149839401245, Accuracy: 0.7054824561403509\n",
      "Step: 380, Loss: 1.323823094367981, Accuracy: 0.705161854768154\n",
      "Step: 381, Loss: 1.0631321668624878, Accuracy: 0.7054973821989529\n",
      "Step: 382, Loss: 0.9982615113258362, Accuracy: 0.7060487380330722\n",
      "Step: 383, Loss: 1.2109946012496948, Accuracy: 0.7059461805555556\n",
      "Step: 384, Loss: 1.113365650177002, Accuracy: 0.706060606060606\n",
      "Step: 385, Loss: 1.2363202571868896, Accuracy: 0.7059585492227979\n",
      "Step: 386, Loss: 1.1516121625900269, Accuracy: 0.7062876830318691\n",
      "Step: 387, Loss: 1.1935739517211914, Accuracy: 0.7061855670103093\n",
      "Step: 388, Loss: 1.1877535581588745, Accuracy: 0.7060839760068551\n",
      "Step: 389, Loss: 1.2213858366012573, Accuracy: 0.7061965811965812\n",
      "Step: 390, Loss: 1.0880361795425415, Accuracy: 0.7065217391304348\n",
      "Step: 391, Loss: 1.3128595352172852, Accuracy: 0.7062074829931972\n",
      "Step: 392, Loss: 0.9358993172645569, Accuracy: 0.7069550466497031\n",
      "Step: 393, Loss: 1.2336899042129517, Accuracy: 0.7068527918781726\n",
      "Step: 394, Loss: 1.2829999923706055, Accuracy: 0.7067510548523207\n",
      "Step: 395, Loss: 1.109897494316101, Accuracy: 0.7070707070707071\n",
      "Step: 396, Loss: 1.1802455186843872, Accuracy: 0.7071788413098237\n",
      "Step: 397, Loss: 0.9354069828987122, Accuracy: 0.7079145728643216\n",
      "Step: 398, Loss: 1.1311912536621094, Accuracy: 0.7080200501253133\n",
      "Step: 399, Loss: 1.2125355005264282, Accuracy: 0.7079166666666666\n",
      "Step: 400, Loss: 1.2155704498291016, Accuracy: 0.7078137988362427\n",
      "Step: 401, Loss: 1.3334589004516602, Accuracy: 0.7075041459369817\n",
      "Step: 402, Loss: 1.2608352899551392, Accuracy: 0.707196029776675\n",
      "Step: 403, Loss: 1.3136969804763794, Accuracy: 0.7068894389438944\n",
      "Step: 404, Loss: 1.5371073484420776, Accuracy: 0.7059670781893004\n",
      "Step: 405, Loss: 1.267396330833435, Accuracy: 0.7058702791461412\n",
      "Step: 406, Loss: 1.3194500207901, Accuracy: 0.7055692055692055\n",
      "Step: 407, Loss: 1.2401522397994995, Accuracy: 0.7054738562091504\n",
      "Step: 408, Loss: 1.090234637260437, Accuracy: 0.7057864710676447\n",
      "Step: 409, Loss: 1.3931702375411987, Accuracy: 0.7052845528455285\n",
      "Step: 410, Loss: 1.2842706441879272, Accuracy: 0.7049878345498783\n",
      "Step: 411, Loss: 0.9588667750358582, Accuracy: 0.7057038834951457\n",
      "Step: 412, Loss: 1.1102311611175537, Accuracy: 0.7060129136400323\n",
      "Step: 413, Loss: 1.169059157371521, Accuracy: 0.7061191626409018\n",
      "Step: 414, Loss: 1.190010905265808, Accuracy: 0.7062248995983936\n",
      "Step: 415, Loss: 1.3285471200942993, Accuracy: 0.7057291666666666\n",
      "Step: 416, Loss: 1.1354111433029175, Accuracy: 0.7058353317346123\n",
      "Step: 417, Loss: 1.1515275239944458, Accuracy: 0.7059409888357256\n",
      "Step: 418, Loss: 1.185874342918396, Accuracy: 0.7058472553699284\n",
      "Step: 419, Loss: 1.1537437438964844, Accuracy: 0.705952380952381\n",
      "Step: 420, Loss: 1.085267424583435, Accuracy: 0.7062549485352335\n",
      "Step: 421, Loss: 1.2579920291900635, Accuracy: 0.7061611374407583\n",
      "Step: 422, Loss: 1.238837718963623, Accuracy: 0.7060677698975572\n",
      "Step: 423, Loss: 0.9765527844429016, Accuracy: 0.706564465408805\n",
      "Step: 424, Loss: 1.3252507448196411, Accuracy: 0.706078431372549\n",
      "Step: 425, Loss: 1.1052707433700562, Accuracy: 0.706377151799687\n",
      "Step: 426, Loss: 1.0749682188034058, Accuracy: 0.7066744730679156\n",
      "Step: 427, Loss: 1.2720969915390015, Accuracy: 0.7065809968847352\n",
      "Step: 428, Loss: 1.3262743949890137, Accuracy: 0.7062937062937062\n",
      "Step: 429, Loss: 1.2915557622909546, Accuracy: 0.7060077519379845\n",
      "Step: 430, Loss: 0.984364926815033, Accuracy: 0.7064965197215777\n",
      "Step: 431, Loss: 1.2198293209075928, Accuracy: 0.7062114197530864\n",
      "Step: 432, Loss: 1.1522111892700195, Accuracy: 0.7063125481139338\n",
      "Step: 433, Loss: 1.235296368598938, Accuracy: 0.7060291858678955\n",
      "Step: 434, Loss: 1.354864478111267, Accuracy: 0.7055555555555556\n",
      "Step: 435, Loss: 1.0890048742294312, Accuracy: 0.705848623853211\n",
      "Step: 436, Loss: 1.014914870262146, Accuracy: 0.7063310450038138\n",
      "Step: 437, Loss: 1.0685725212097168, Accuracy: 0.70662100456621\n",
      "Step: 438, Loss: 1.178082823753357, Accuracy: 0.7065299924069856\n",
      "Step: 439, Loss: 1.1937364339828491, Accuracy: 0.7066287878787879\n",
      "Step: 440, Loss: 1.1791048049926758, Accuracy: 0.7065381708238851\n",
      "Step: 441, Loss: 1.0894789695739746, Accuracy: 0.7068250377073907\n",
      "Step: 442, Loss: 1.3805913925170898, Accuracy: 0.7063581640331076\n",
      "Step: 443, Loss: 1.0566352605819702, Accuracy: 0.7066441441441441\n",
      "Step: 444, Loss: 1.2197290658950806, Accuracy: 0.7065543071161049\n",
      "Step: 445, Loss: 1.1676770448684692, Accuracy: 0.7066517189835575\n",
      "Step: 446, Loss: 1.1081315279006958, Accuracy: 0.7069351230425056\n",
      "Step: 447, Loss: 1.2646734714508057, Accuracy: 0.7066592261904762\n",
      "Step: 448, Loss: 1.1234813928604126, Accuracy: 0.7069413511507052\n",
      "Step: 449, Loss: 1.1972030401229858, Accuracy: 0.707037037037037\n",
      "Step: 450, Loss: 1.2702473402023315, Accuracy: 0.7069475240206947\n",
      "Step: 451, Loss: 1.6251319646835327, Accuracy: 0.7059365781710915\n",
      "Step: 452, Loss: 1.2791098356246948, Accuracy: 0.7056659308314938\n",
      "Step: 453, Loss: 1.046988606452942, Accuracy: 0.7059471365638766\n",
      "Step: 454, Loss: 1.1898897886276245, Accuracy: 0.7060439560439561\n",
      "Step: 455, Loss: 1.1728960275650024, Accuracy: 0.706140350877193\n",
      "Step: 456, Loss: 1.31411874294281, Accuracy: 0.7058716265499635\n",
      "Step: 457, Loss: 1.1625007390975952, Accuracy: 0.7059679767103348\n",
      "Step: 458, Loss: 1.1461658477783203, Accuracy: 0.7060639070442992\n",
      "Step: 459, Loss: 1.3477811813354492, Accuracy: 0.7057971014492753\n",
      "Step: 460, Loss: 1.0819441080093384, Accuracy: 0.7060737527114967\n",
      "Step: 461, Loss: 1.314252495765686, Accuracy: 0.7058080808080808\n",
      "Step: 462, Loss: 1.2567564249038696, Accuracy: 0.7057235421166307\n",
      "Step: 463, Loss: 1.3177560567855835, Accuracy: 0.7054597701149425\n",
      "Step: 464, Loss: 1.0519267320632935, Accuracy: 0.7057347670250896\n",
      "Step: 465, Loss: 1.1632853746414185, Accuracy: 0.705829756795422\n",
      "Step: 466, Loss: 1.3747915029525757, Accuracy: 0.7053890078515346\n",
      "Step: 467, Loss: 1.184723138809204, Accuracy: 0.7053062678062678\n",
      "Step: 468, Loss: 1.203431487083435, Accuracy: 0.7054015636105189\n",
      "Step: 469, Loss: 1.1139906644821167, Accuracy: 0.7054964539007093\n",
      "Step: 470, Loss: 1.2207527160644531, Accuracy: 0.7054140127388535\n",
      "Step: 471, Loss: 1.072566270828247, Accuracy: 0.7056850282485876\n",
      "Step: 472, Loss: 1.3681493997573853, Accuracy: 0.7052501761804087\n",
      "Step: 473, Loss: 1.2336515188217163, Accuracy: 0.705168776371308\n",
      "Step: 474, Loss: 1.0464285612106323, Accuracy: 0.7054385964912281\n",
      "Step: 475, Loss: 1.3218374252319336, Accuracy: 0.7051820728291317\n",
      "Step: 476, Loss: 1.2361375093460083, Accuracy: 0.7051013277428372\n",
      "Step: 477, Loss: 1.2610089778900146, Accuracy: 0.7050209205020921\n",
      "Step: 478, Loss: 1.0820764303207397, Accuracy: 0.7052887961029923\n",
      "Step: 479, Loss: 1.0248743295669556, Accuracy: 0.7055555555555556\n",
      "Step: 480, Loss: 1.1151103973388672, Accuracy: 0.7058212058212058\n",
      "Step: 481, Loss: 1.2638342380523682, Accuracy: 0.705567081604426\n",
      "Step: 482, Loss: 1.3933244943618774, Accuracy: 0.7051414768806074\n",
      "Step: 483, Loss: 1.2526341676712036, Accuracy: 0.7050619834710744\n",
      "Step: 484, Loss: 1.1215643882751465, Accuracy: 0.7051546391752578\n",
      "Step: 485, Loss: 1.3063538074493408, Accuracy: 0.7049039780521262\n",
      "Step: 486, Loss: 1.195907473564148, Accuracy: 0.7049965776865161\n",
      "Step: 487, Loss: 1.1569868326187134, Accuracy: 0.7050887978142076\n",
      "Step: 488, Loss: 1.1916558742523193, Accuracy: 0.7051806407634629\n",
      "Step: 489, Loss: 1.08674156665802, Accuracy: 0.7054421768707483\n",
      "Step: 490, Loss: 1.1500118970870972, Accuracy: 0.7055329260013578\n",
      "Step: 491, Loss: 0.9894374012947083, Accuracy: 0.7059620596205962\n",
      "Step: 492, Loss: 1.3422216176986694, Accuracy: 0.7057133198106829\n",
      "Step: 493, Loss: 1.391035556793213, Accuracy: 0.7052968960863698\n",
      "Step: 494, Loss: 1.1873267889022827, Accuracy: 0.7052188552188552\n",
      "Step: 495, Loss: 1.1198691129684448, Accuracy: 0.7054771505376344\n",
      "Step: 496, Loss: 1.3014179468154907, Accuracy: 0.7052313883299799\n",
      "Step: 497, Loss: 1.3710070848464966, Accuracy: 0.7048192771084337\n",
      "Step: 498, Loss: 1.1622847318649292, Accuracy: 0.7049098196392786\n",
      "Step: 499, Loss: 1.2210019826889038, Accuracy: 0.7048333333333333\n",
      "Step: 500, Loss: 1.1600390672683716, Accuracy: 0.7049234863606121\n",
      "Step: 501, Loss: 1.448306679725647, Accuracy: 0.7043492695883135\n",
      "Step: 502, Loss: 1.1235692501068115, Accuracy: 0.7046056991385024\n",
      "Step: 503, Loss: 1.1534719467163086, Accuracy: 0.7046957671957672\n",
      "Step: 504, Loss: 1.069812297821045, Accuracy: 0.7049504950495049\n",
      "Step: 505, Loss: 1.3483905792236328, Accuracy: 0.7047101449275363\n",
      "Step: 506, Loss: 1.1528323888778687, Accuracy: 0.7047994740302432\n",
      "Step: 507, Loss: 1.1209605932235718, Accuracy: 0.7050524934383202\n",
      "Step: 508, Loss: 1.0284069776535034, Accuracy: 0.7053045186640472\n",
      "Step: 509, Loss: 1.1783089637756348, Accuracy: 0.7053921568627451\n",
      "Step: 510, Loss: 1.282572627067566, Accuracy: 0.7051532941943901\n",
      "Step: 511, Loss: 1.3126578330993652, Accuracy: 0.7049153645833334\n",
      "Step: 512, Loss: 1.3216224908828735, Accuracy: 0.7046783625730995\n",
      "Step: 513, Loss: 1.1446062326431274, Accuracy: 0.7047665369649806\n",
      "Step: 514, Loss: 1.3322395086288452, Accuracy: 0.7045307443365696\n",
      "Step: 515, Loss: 1.164473295211792, Accuracy: 0.7046188630490956\n",
      "Step: 516, Loss: 1.2338981628417969, Accuracy: 0.7045454545454546\n",
      "Step: 517, Loss: 1.0128731727600098, Accuracy: 0.704954954954955\n",
      "Step: 518, Loss: 1.0751545429229736, Accuracy: 0.7053628773281952\n",
      "Step: 519, Loss: 1.0711466073989868, Accuracy: 0.7056089743589744\n",
      "Step: 520, Loss: 0.939676821231842, Accuracy: 0.7061740243122201\n",
      "Step: 521, Loss: 1.1992340087890625, Accuracy: 0.7060983397190294\n",
      "Step: 522, Loss: 1.2202190160751343, Accuracy: 0.7060229445506692\n",
      "Step: 523, Loss: 1.2711743116378784, Accuracy: 0.7057888040712468\n",
      "Step: 524, Loss: 1.0738903284072876, Accuracy: 0.7061904761904761\n",
      "Step: 525, Loss: 1.3083221912384033, Accuracy: 0.70595690747782\n",
      "Step: 526, Loss: 1.292167067527771, Accuracy: 0.7057242251739405\n",
      "Step: 527, Loss: 1.3748035430908203, Accuracy: 0.7053345959595959\n",
      "Step: 528, Loss: 1.1886149644851685, Accuracy: 0.705419029615627\n",
      "Step: 529, Loss: 1.2562669515609741, Accuracy: 0.7053459119496855\n",
      "Step: 530, Loss: 1.237191081047058, Accuracy: 0.7052730696798494\n",
      "Step: 531, Loss: 1.0742671489715576, Accuracy: 0.7055137844611529\n",
      "Step: 532, Loss: 1.2675429582595825, Accuracy: 0.7052845528455285\n",
      "Step: 533, Loss: 0.9902580380439758, Accuracy: 0.7058364544319601\n",
      "Step: 534, Loss: 1.3165693283081055, Accuracy: 0.705607476635514\n",
      "Step: 535, Loss: 0.9638872146606445, Accuracy: 0.7061567164179104\n",
      "Step: 536, Loss: 1.2172974348068237, Accuracy: 0.7060831781502173\n",
      "Step: 537, Loss: 1.0834640264511108, Accuracy: 0.7063197026022305\n",
      "Step: 538, Loss: 1.42341947555542, Accuracy: 0.7059369202226345\n",
      "Step: 539, Loss: 1.1696789264678955, Accuracy: 0.7060185185185185\n",
      "Step: 540, Loss: 1.1889467239379883, Accuracy: 0.7060998151571165\n",
      "Step: 541, Loss: 1.0075048208236694, Accuracy: 0.7064883148831488\n",
      "Step: 542, Loss: 1.1743463277816772, Accuracy: 0.7065684468999386\n",
      "Step: 543, Loss: 1.0818984508514404, Accuracy: 0.7068014705882353\n",
      "Step: 544, Loss: 1.198614478111267, Accuracy: 0.7068807339449541\n",
      "Step: 545, Loss: 1.1574970483779907, Accuracy: 0.706959706959707\n",
      "Step: 546, Loss: 1.1249629259109497, Accuracy: 0.7070383912248629\n",
      "Step: 547, Loss: 0.9907334446907043, Accuracy: 0.7074209245742092\n",
      "Step: 548, Loss: 1.3587416410446167, Accuracy: 0.7070431086824529\n",
      "Step: 549, Loss: 0.9624607563018799, Accuracy: 0.7075757575757575\n",
      "Step: 550, Loss: 1.1922860145568848, Accuracy: 0.7075015124016939\n",
      "Step: 551, Loss: 1.0786644220352173, Accuracy: 0.7077294685990339\n",
      "Step: 552, Loss: 1.143849492073059, Accuracy: 0.7078059071729957\n",
      "Step: 553, Loss: 1.1679543256759644, Accuracy: 0.7078820697954272\n",
      "Step: 554, Loss: 1.010509967803955, Accuracy: 0.7082582582582583\n",
      "Step: 555, Loss: 1.3639111518859863, Accuracy: 0.7080335731414868\n",
      "Step: 556, Loss: 1.0109914541244507, Accuracy: 0.708557749850389\n",
      "Step: 557, Loss: 1.0785151720046997, Accuracy: 0.7087813620071685\n",
      "Step: 558, Loss: 1.0918179750442505, Accuracy: 0.7090041741204532\n",
      "Step: 559, Loss: 0.9855628609657288, Accuracy: 0.7095238095238096\n",
      "Step: 560, Loss: 1.1571990251541138, Accuracy: 0.7095959595959596\n",
      "Step: 561, Loss: 1.382941722869873, Accuracy: 0.7092230130486358\n",
      "Step: 562, Loss: 1.2301393747329712, Accuracy: 0.7091474245115453\n",
      "Step: 563, Loss: 1.243262529373169, Accuracy: 0.7090721040189125\n",
      "Step: 564, Loss: 1.1953095197677612, Accuracy: 0.7091445427728613\n",
      "Step: 565, Loss: 1.3778138160705566, Accuracy: 0.7087750294464076\n",
      "Step: 566, Loss: 1.0292208194732666, Accuracy: 0.708994708994709\n",
      "Step: 567, Loss: 1.1666170358657837, Accuracy: 0.7090669014084507\n",
      "Step: 568, Loss: 1.063184380531311, Accuracy: 0.7092852958406561\n",
      "Step: 569, Loss: 1.1485000848770142, Accuracy: 0.7093567251461989\n",
      "Step: 570, Loss: 1.2355948686599731, Accuracy: 0.7091360186806772\n",
      "Step: 571, Loss: 1.2699710130691528, Accuracy: 0.708916083916084\n",
      "Step: 572, Loss: 1.3148761987686157, Accuracy: 0.7086969168121\n",
      "Step: 573, Loss: 1.092649221420288, Accuracy: 0.7089140534262486\n",
      "Step: 574, Loss: 1.2434238195419312, Accuracy: 0.7088405797101449\n",
      "Step: 575, Loss: 1.2871856689453125, Accuracy: 0.7086226851851852\n",
      "Step: 576, Loss: 1.1470218896865845, Accuracy: 0.7086943963027152\n",
      "Step: 577, Loss: 1.2396727800369263, Accuracy: 0.7086216839677048\n",
      "Step: 578, Loss: 1.1649311780929565, Accuracy: 0.7086931491076569\n",
      "Step: 579, Loss: 1.367324709892273, Accuracy: 0.7083333333333334\n",
      "Step: 580, Loss: 0.9919874668121338, Accuracy: 0.7086919104991394\n",
      "Step: 581, Loss: 1.1653451919555664, Accuracy: 0.7087628865979382\n",
      "Step: 582, Loss: 1.031836986541748, Accuracy: 0.7091194968553459\n",
      "Step: 583, Loss: 1.1028292179107666, Accuracy: 0.709332191780822\n",
      "Step: 584, Loss: 1.2419021129608154, Accuracy: 0.7092592592592593\n",
      "Step: 585, Loss: 1.2457115650177002, Accuracy: 0.7091865756541524\n",
      "Step: 586, Loss: 1.655049204826355, Accuracy: 0.7084043157296991\n",
      "Step: 587, Loss: 1.400755524635315, Accuracy: 0.7080498866213152\n",
      "Step: 588, Loss: 1.1734771728515625, Accuracy: 0.7081211092246746\n",
      "Step: 589, Loss: 1.117425799369812, Accuracy: 0.7081920903954803\n",
      "Step: 590, Loss: 1.1201653480529785, Accuracy: 0.708262831359278\n",
      "Step: 591, Loss: 1.0792949199676514, Accuracy: 0.7084740990990991\n",
      "Step: 592, Loss: 1.01094651222229, Accuracy: 0.7088251826869028\n",
      "Step: 593, Loss: 1.076520562171936, Accuracy: 0.7091750841750841\n",
      "Step: 594, Loss: 1.1107195615768433, Accuracy: 0.7092436974789916\n",
      "Step: 595, Loss: 1.1521497964859009, Accuracy: 0.7093120805369127\n",
      "Step: 596, Loss: 1.1533606052398682, Accuracy: 0.7093802345058626\n",
      "Step: 597, Loss: 1.1853864192962646, Accuracy: 0.7094481605351171\n",
      "Step: 598, Loss: 1.3432384729385376, Accuracy: 0.7092376182526433\n",
      "Step: 599, Loss: 1.212669014930725, Accuracy: 0.7091666666666666\n",
      "Step: 600, Loss: 1.3371015787124634, Accuracy: 0.7089572933998891\n",
      "Step: 601, Loss: 1.0309311151504517, Accuracy: 0.7093023255813954\n",
      "Step: 602, Loss: 1.000369668006897, Accuracy: 0.7096462133775566\n",
      "Step: 603, Loss: 1.0053486824035645, Accuracy: 0.7099889624724062\n",
      "Step: 604, Loss: 1.2775256633758545, Accuracy: 0.7097796143250689\n",
      "Step: 605, Loss: 1.222561240196228, Accuracy: 0.7097084708470847\n",
      "Step: 606, Loss: 1.1705296039581299, Accuracy: 0.7097748489840747\n",
      "Step: 607, Loss: 1.5198745727539062, Accuracy: 0.7091557017543859\n",
      "Step: 608, Loss: 1.1243616342544556, Accuracy: 0.7093596059113301\n",
      "Step: 609, Loss: 1.1634529829025269, Accuracy: 0.7094262295081967\n",
      "Step: 610, Loss: 1.1676884889602661, Accuracy: 0.70949263502455\n",
      "Step: 611, Loss: 1.0532580614089966, Accuracy: 0.7096949891067538\n",
      "Step: 612, Loss: 0.9194834232330322, Accuracy: 0.710168569874932\n",
      "Step: 613, Loss: 1.1160792112350464, Accuracy: 0.7102334419109664\n",
      "Step: 614, Loss: 1.3028517961502075, Accuracy: 0.7100271002710027\n",
      "Step: 615, Loss: 1.4486161470413208, Accuracy: 0.7095508658008658\n",
      "Step: 616, Loss: 1.2490774393081665, Accuracy: 0.7094813614262561\n",
      "Step: 617, Loss: 1.3148154020309448, Accuracy: 0.709277238403452\n",
      "Step: 618, Loss: 0.9980656504631042, Accuracy: 0.7096122778675282\n",
      "Step: 619, Loss: 1.1738779544830322, Accuracy: 0.7096774193548387\n",
      "Step: 620, Loss: 1.2035306692123413, Accuracy: 0.7096081588835212\n",
      "Step: 621, Loss: 1.226830244064331, Accuracy: 0.7095391211146839\n",
      "Step: 622, Loss: 1.2410950660705566, Accuracy: 0.709470304975923\n",
      "Step: 623, Loss: 1.1040164232254028, Accuracy: 0.7096688034188035\n",
      "Step: 624, Loss: 1.2328022718429565, Accuracy: 0.7096\n",
      "Step: 625, Loss: 1.169608473777771, Accuracy: 0.7096645367412141\n",
      "Step: 626, Loss: 1.1299960613250732, Accuracy: 0.7098617756512493\n",
      "Step: 627, Loss: 1.32847261428833, Accuracy: 0.7096602972399151\n",
      "Step: 628, Loss: 1.1124540567398071, Accuracy: 0.7098569157392687\n",
      "Step: 629, Loss: 1.3236690759658813, Accuracy: 0.7096560846560847\n",
      "Step: 630, Loss: 1.111162781715393, Accuracy: 0.7098520866349709\n",
      "Step: 631, Loss: 1.315895438194275, Accuracy: 0.7096518987341772\n",
      "Step: 632, Loss: 1.1667885780334473, Accuracy: 0.7097156398104265\n",
      "Step: 633, Loss: 1.2892481088638306, Accuracy: 0.7096477392218717\n",
      "Step: 634, Loss: 1.1664129495620728, Accuracy: 0.7097112860892388\n",
      "Step: 635, Loss: 1.151338815689087, Accuracy: 0.7097746331236897\n",
      "Step: 636, Loss: 1.063659429550171, Accuracy: 0.7099686028257457\n",
      "Step: 637, Loss: 1.2561763525009155, Accuracy: 0.7099007314524556\n",
      "Step: 638, Loss: 1.067181944847107, Accuracy: 0.710093896713615\n",
      "Step: 639, Loss: 1.2397104501724243, Accuracy: 0.7100260416666667\n",
      "Step: 640, Loss: 1.2499109506607056, Accuracy: 0.7099583983359334\n",
      "Step: 641, Loss: 1.1209499835968018, Accuracy: 0.7101505711318795\n",
      "Step: 642, Loss: 1.2537301778793335, Accuracy: 0.710082944530845\n",
      "Step: 643, Loss: 1.0606787204742432, Accuracy: 0.7102743271221532\n",
      "Step: 644, Loss: 1.026491403579712, Accuracy: 0.710594315245478\n",
      "Step: 645, Loss: 1.323556661605835, Accuracy: 0.7103973168214655\n",
      "Step: 646, Loss: 1.2510294914245605, Accuracy: 0.7103297269448737\n",
      "Step: 647, Loss: 1.3481616973876953, Accuracy: 0.710133744855967\n",
      "Step: 648, Loss: 1.126682162284851, Accuracy: 0.7101951720595788\n",
      "Step: 649, Loss: 1.0705803632736206, Accuracy: 0.7103846153846154\n",
      "Step: 650, Loss: 1.0785073041915894, Accuracy: 0.7105734767025089\n",
      "Step: 651, Loss: 1.2183223962783813, Accuracy: 0.7105061349693251\n",
      "Step: 652, Loss: 1.0073827505111694, Accuracy: 0.7108218478815722\n",
      "Step: 653, Loss: 1.1652328968048096, Accuracy: 0.710881753312946\n",
      "Step: 654, Loss: 1.0576913356781006, Accuracy: 0.7110687022900763\n",
      "Step: 655, Loss: 1.1805411577224731, Accuracy: 0.7110010162601627\n",
      "Step: 656, Loss: 1.0905853509902954, Accuracy: 0.7111872146118722\n",
      "Step: 657, Loss: 1.280809998512268, Accuracy: 0.7111195542046606\n",
      "Step: 658, Loss: 1.3329054117202759, Accuracy: 0.7107991906929692\n",
      "Step: 659, Loss: 1.1544712781906128, Accuracy: 0.7108585858585859\n",
      "Step: 660, Loss: 1.0196963548660278, Accuracy: 0.7111699445284921\n",
      "Step: 661, Loss: 1.307094693183899, Accuracy: 0.7109768378650554\n",
      "Step: 662, Loss: 1.2222107648849487, Accuracy: 0.7109100050276521\n",
      "Step: 663, Loss: 1.2302732467651367, Accuracy: 0.7108433734939759\n",
      "Step: 664, Loss: 1.2659271955490112, Accuracy: 0.7107769423558897\n",
      "Step: 665, Loss: 1.3494669198989868, Accuracy: 0.7104604604604604\n",
      "Step: 666, Loss: 1.1709210872650146, Accuracy: 0.710519740129935\n",
      "Step: 667, Loss: 1.2682616710662842, Accuracy: 0.7104540918163673\n",
      "Step: 668, Loss: 1.3976854085922241, Accuracy: 0.7101395117090185\n",
      "Step: 669, Loss: 1.040429949760437, Accuracy: 0.7104477611940299\n",
      "Step: 670, Loss: 1.3102765083312988, Accuracy: 0.7102583209140586\n",
      "Step: 671, Loss: 1.0404791831970215, Accuracy: 0.7104414682539683\n",
      "Step: 672, Loss: 1.2324496507644653, Accuracy: 0.7103764239722635\n",
      "Step: 673, Loss: 1.0875244140625, Accuracy: 0.7105588526211671\n",
      "Step: 674, Loss: 1.15188729763031, Accuracy: 0.7106172839506173\n",
      "Step: 675, Loss: 1.1151063442230225, Accuracy: 0.7107988165680473\n",
      "Step: 676, Loss: 1.2074425220489502, Accuracy: 0.7107336287543082\n",
      "Step: 677, Loss: 1.0873346328735352, Accuracy: 0.7109144542772862\n",
      "Step: 678, Loss: 1.0118879079818726, Accuracy: 0.7112174766813942\n",
      "Step: 679, Loss: 1.0746814012527466, Accuracy: 0.7113970588235294\n",
      "Step: 680, Loss: 1.3902760744094849, Accuracy: 0.7110866372980911\n",
      "Step: 681, Loss: 1.203511118888855, Accuracy: 0.7110215053763441\n",
      "Step: 682, Loss: 1.3002384901046753, Accuracy: 0.7108345534407028\n",
      "Step: 683, Loss: 1.1269549131393433, Accuracy: 0.7108918128654971\n",
      "Step: 684, Loss: 1.2267823219299316, Accuracy: 0.7108272506082725\n",
      "Step: 685, Loss: 1.0867286920547485, Accuracy: 0.7110058309037901\n",
      "Step: 686, Loss: 0.9874032139778137, Accuracy: 0.7113051916545367\n",
      "Step: 687, Loss: 1.0037102699279785, Accuracy: 0.7116036821705426\n",
      "Step: 688, Loss: 1.2201958894729614, Accuracy: 0.7115384615384616\n",
      "Step: 689, Loss: 1.252144455909729, Accuracy: 0.711352657004831\n",
      "Step: 690, Loss: 1.3124767541885376, Accuracy: 0.7111673902556681\n",
      "Step: 691, Loss: 1.0337144136428833, Accuracy: 0.7114643545279383\n",
      "Step: 692, Loss: 1.3109086751937866, Accuracy: 0.7112794612794613\n",
      "Step: 693, Loss: 1.0911349058151245, Accuracy: 0.7114553314121037\n",
      "Step: 694, Loss: 1.1426597833633423, Accuracy: 0.7115107913669064\n",
      "Step: 695, Loss: 1.554822564125061, Accuracy: 0.7109674329501916\n",
      "Step: 696, Loss: 1.1191750764846802, Accuracy: 0.7110234337637494\n",
      "Step: 697, Loss: 1.0553622245788574, Accuracy: 0.7111986628462273\n",
      "Step: 698, Loss: 1.0728048086166382, Accuracy: 0.7113733905579399\n",
      "Step: 699, Loss: 1.3222007751464844, Accuracy: 0.7111904761904762\n",
      "Step: 700, Loss: 0.9282898306846619, Accuracy: 0.7116024726581075\n",
      "Step: 701, Loss: 1.1455464363098145, Accuracy: 0.7116571699905033\n",
      "Step: 702, Loss: 1.1997359991073608, Accuracy: 0.7115931721194879\n",
      "Step: 703, Loss: 1.1230287551879883, Accuracy: 0.7116477272727273\n",
      "Step: 704, Loss: 1.3643394708633423, Accuracy: 0.7113475177304964\n",
      "Step: 705, Loss: 1.2472621202468872, Accuracy: 0.7112842304060434\n",
      "Step: 706, Loss: 1.2717617750167847, Accuracy: 0.7111032531824611\n",
      "Step: 707, Loss: 1.1812587976455688, Accuracy: 0.7111581920903954\n",
      "Step: 708, Loss: 1.2664188146591187, Accuracy: 0.7110954395862717\n",
      "Step: 709, Loss: 1.2936514616012573, Accuracy: 0.7110328638497653\n",
      "Step: 710, Loss: 1.0625275373458862, Accuracy: 0.7112048757618378\n",
      "Step: 711, Loss: 1.119116187095642, Accuracy: 0.711376404494382\n",
      "Step: 712, Loss: 1.33739173412323, Accuracy: 0.7110799438990182\n",
      "Step: 713, Loss: 1.2411562204360962, Accuracy: 0.7110177404295052\n",
      "Step: 714, Loss: 1.0769649744033813, Accuracy: 0.7111888111888112\n",
      "Step: 715, Loss: 1.1214267015457153, Accuracy: 0.7113594040968343\n",
      "Step: 716, Loss: 1.2171026468276978, Accuracy: 0.7112970711297071\n",
      "Step: 717, Loss: 1.0436381101608276, Accuracy: 0.7115831012070566\n",
      "Step: 718, Loss: 1.2274715900421143, Accuracy: 0.7115206305053314\n",
      "Step: 719, Loss: 0.9300026297569275, Accuracy: 0.7119212962962963\n",
      "Step: 720, Loss: 1.1668764352798462, Accuracy: 0.7119741100323624\n",
      "Step: 721, Loss: 1.1539517641067505, Accuracy: 0.7120267774699908\n",
      "Step: 722, Loss: 1.3032275438308716, Accuracy: 0.7118487782388198\n",
      "Step: 723, Loss: 1.1942834854125977, Accuracy: 0.7117863720073665\n",
      "Step: 724, Loss: 1.0410481691360474, Accuracy: 0.7119540229885057\n",
      "Step: 725, Loss: 1.3346487283706665, Accuracy: 0.7117768595041323\n",
      "Step: 726, Loss: 1.0455485582351685, Accuracy: 0.7119440623567171\n",
      "Step: 727, Loss: 1.1212657690048218, Accuracy: 0.711996336996337\n",
      "Step: 728, Loss: 1.1791468858718872, Accuracy: 0.7119341563786008\n",
      "Step: 729, Loss: 1.0569404363632202, Accuracy: 0.7121004566210045\n",
      "Step: 730, Loss: 1.2441667318344116, Accuracy: 0.7120383036935705\n",
      "Step: 731, Loss: 1.120138168334961, Accuracy: 0.7120901639344263\n",
      "Step: 732, Loss: 1.113188624382019, Accuracy: 0.7121418826739427\n",
      "Step: 733, Loss: 1.161461591720581, Accuracy: 0.7121934604904632\n",
      "Step: 734, Loss: 1.191891074180603, Accuracy: 0.7121315192743765\n",
      "Step: 735, Loss: 1.196048617362976, Accuracy: 0.7120697463768116\n",
      "Step: 736, Loss: 1.0787097215652466, Accuracy: 0.7122342831298055\n",
      "Step: 737, Loss: 1.248801589012146, Accuracy: 0.712059620596206\n",
      "Step: 738, Loss: 1.2706598043441772, Accuracy: 0.7119981957600361\n",
      "Step: 739, Loss: 1.178824782371521, Accuracy: 0.7120495495495496\n",
      "Step: 740, Loss: 1.0173543691635132, Accuracy: 0.7123256860098965\n",
      "Step: 741, Loss: 1.0130914449691772, Accuracy: 0.7126010781671159\n",
      "Step: 742, Loss: 1.2489997148513794, Accuracy: 0.7125392552714221\n",
      "Step: 743, Loss: 1.430992603302002, Accuracy: 0.712141577060932\n",
      "Step: 744, Loss: 1.251224398612976, Accuracy: 0.7120805369127516\n",
      "Step: 745, Loss: 1.1288937330245972, Accuracy: 0.7121313672922251\n",
      "Step: 746, Loss: 1.1702502965927124, Accuracy: 0.7121820615796519\n",
      "Step: 747, Loss: 1.2338052988052368, Accuracy: 0.7121212121212122\n",
      "Step: 748, Loss: 1.2153050899505615, Accuracy: 0.7120605251446372\n",
      "Step: 749, Loss: 1.0395596027374268, Accuracy: 0.7122222222222222\n",
      "Step: 750, Loss: 1.093488335609436, Accuracy: 0.7123834886817576\n",
      "Step: 751, Loss: 1.1715903282165527, Accuracy: 0.712322695035461\n",
      "Step: 752, Loss: 1.2853916883468628, Accuracy: 0.7122620628596724\n",
      "Step: 753, Loss: 1.104801058769226, Accuracy: 0.712422634836428\n",
      "Step: 754, Loss: 1.1060385704040527, Accuracy: 0.7125827814569536\n",
      "Step: 755, Loss: 1.1721493005752563, Accuracy: 0.7125220458553791\n",
      "Step: 756, Loss: 1.1605181694030762, Accuracy: 0.7124614707177455\n",
      "Step: 757, Loss: 1.2121661901474, Accuracy: 0.7125109938434476\n",
      "Step: 758, Loss: 1.2869267463684082, Accuracy: 0.7123407992973211\n",
      "Step: 759, Loss: 1.1568403244018555, Accuracy: 0.7123903508771929\n",
      "Step: 760, Loss: 1.1549594402313232, Accuracy: 0.7124397722295226\n",
      "Step: 761, Loss: 1.1647851467132568, Accuracy: 0.7124890638670166\n",
      "Step: 762, Loss: 1.1977629661560059, Accuracy: 0.7125382262996942\n",
      "Step: 763, Loss: 1.099709153175354, Accuracy: 0.712696335078534\n",
      "Step: 764, Loss: 1.2274315357208252, Accuracy: 0.712636165577342\n",
      "Step: 765, Loss: 1.1639741659164429, Accuracy: 0.7126849434290687\n",
      "Step: 766, Loss: 1.2067739963531494, Accuracy: 0.7126249456757932\n",
      "Step: 767, Loss: 1.3275336027145386, Accuracy: 0.7124565972222222\n",
      "Step: 768, Loss: 1.1044620275497437, Accuracy: 0.7126137841352406\n",
      "Step: 769, Loss: 1.252913475036621, Accuracy: 0.7124458874458874\n",
      "Step: 770, Loss: 1.2631455659866333, Accuracy: 0.7123865110246433\n",
      "Step: 771, Loss: 1.3624343872070312, Accuracy: 0.7122193436960277\n",
      "Step: 772, Loss: 1.0716818571090698, Accuracy: 0.7123760241483398\n",
      "Step: 773, Loss: 1.3689508438110352, Accuracy: 0.7121016365202412\n",
      "Step: 774, Loss: 1.2373815774917603, Accuracy: 0.7120430107526882\n",
      "Step: 775, Loss: 1.2779361009597778, Accuracy: 0.711877147766323\n",
      "Step: 776, Loss: 0.9866185784339905, Accuracy: 0.7121407121407122\n",
      "Step: 777, Loss: 1.2794358730316162, Accuracy: 0.711975149957155\n",
      "Step: 778, Loss: 1.2249224185943604, Accuracy: 0.7119169875909286\n",
      "Step: 779, Loss: 1.2088918685913086, Accuracy: 0.7118589743589744\n",
      "Step: 780, Loss: 1.0760618448257446, Accuracy: 0.7120145113102859\n",
      "Step: 781, Loss: 1.3064464330673218, Accuracy: 0.711849957374254\n",
      "Step: 782, Loss: 0.9701299667358398, Accuracy: 0.7122179650915283\n",
      "Step: 783, Loss: 1.4981637001037598, Accuracy: 0.7118409863945578\n",
      "Step: 784, Loss: 1.256428599357605, Accuracy: 0.7117834394904459\n",
      "Step: 785, Loss: 1.4402860403060913, Accuracy: 0.7114079728583546\n",
      "Step: 786, Loss: 1.0967427492141724, Accuracy: 0.711457009741635\n",
      "Step: 787, Loss: 1.074496865272522, Accuracy: 0.7116116751269036\n",
      "Step: 788, Loss: 1.2676411867141724, Accuracy: 0.7114490916772286\n",
      "Step: 789, Loss: 1.3620601892471313, Accuracy: 0.7111814345991562\n",
      "Step: 790, Loss: 1.1731352806091309, Accuracy: 0.7112305099030762\n",
      "Step: 791, Loss: 1.2646801471710205, Accuracy: 0.7111742424242424\n",
      "Step: 792, Loss: 1.0978749990463257, Accuracy: 0.7113282891971416\n",
      "Step: 793, Loss: 1.2590374946594238, Accuracy: 0.711272040302267\n",
      "Step: 794, Loss: 1.237246036529541, Accuracy: 0.7112159329140462\n",
      "Step: 795, Loss: 1.17697012424469, Accuracy: 0.7111599664991625\n",
      "Step: 796, Loss: 1.2478834390640259, Accuracy: 0.7111041405269761\n",
      "Step: 797, Loss: 1.1701654195785522, Accuracy: 0.7111528822055138\n",
      "Step: 798, Loss: 1.3550833463668823, Accuracy: 0.7108886107634543\n",
      "Step: 799, Loss: 1.3754229545593262, Accuracy: 0.710625\n",
      "Step: 800, Loss: 1.1101369857788086, Accuracy: 0.7106741573033708\n",
      "Step: 801, Loss: 1.3350329399108887, Accuracy: 0.7105153782211139\n",
      "Step: 802, Loss: 1.1366229057312012, Accuracy: 0.7105645496056455\n",
      "Step: 803, Loss: 1.1649774312973022, Accuracy: 0.7106135986733002\n",
      "Step: 804, Loss: 1.0654518604278564, Accuracy: 0.7107660455486542\n",
      "Step: 805, Loss: 1.2380062341690063, Accuracy: 0.7107113316790736\n",
      "Step: 806, Loss: 1.3770915269851685, Accuracy: 0.7105534902932672\n",
      "Step: 807, Loss: 1.31219482421875, Accuracy: 0.7103960396039604\n",
      "Step: 808, Loss: 1.1070700883865356, Accuracy: 0.7104449938195303\n",
      "Step: 809, Loss: 1.3228998184204102, Accuracy: 0.7102880658436214\n",
      "Step: 810, Loss: 1.2307878732681274, Accuracy: 0.7102342786683107\n",
      "Step: 811, Loss: 1.118728518486023, Accuracy: 0.7102832512315271\n",
      "Step: 812, Loss: 1.2803648710250854, Accuracy: 0.7102296022960229\n",
      "Step: 813, Loss: 1.4724087715148926, Accuracy: 0.7098689598689598\n",
      "Step: 814, Loss: 1.3173173666000366, Accuracy: 0.7097137014314928\n",
      "Step: 815, Loss: 1.195129632949829, Accuracy: 0.7097630718954249\n",
      "Step: 816, Loss: 1.1396750211715698, Accuracy: 0.709812321501428\n",
      "Step: 817, Loss: 1.2905491590499878, Accuracy: 0.7096577017114915\n",
      "Step: 818, Loss: 1.1545981168746948, Accuracy: 0.7096052096052096\n",
      "Step: 819, Loss: 1.1783075332641602, Accuracy: 0.7096544715447154\n",
      "Step: 820, Loss: 1.2236756086349487, Accuracy: 0.7096021112464475\n",
      "Step: 821, Loss: 1.1690707206726074, Accuracy: 0.7096512570965126\n",
      "Step: 822, Loss: 1.3048826456069946, Accuracy: 0.7094977723774808\n",
      "Step: 823, Loss: 1.1236563920974731, Accuracy: 0.7096480582524272\n",
      "Step: 824, Loss: 1.0642284154891968, Accuracy: 0.7097979797979798\n",
      "Step: 825, Loss: 1.2032142877578735, Accuracy: 0.7097457627118644\n",
      "Step: 826, Loss: 1.2279983758926392, Accuracy: 0.7096936719064894\n",
      "Step: 827, Loss: 1.2588964700698853, Accuracy: 0.7096417069243156\n",
      "Step: 828, Loss: 1.1796356439590454, Accuracy: 0.7096903900281464\n",
      "Step: 829, Loss: 1.0741263628005981, Accuracy: 0.7099397590361446\n",
      "Step: 830, Loss: 1.1531285047531128, Accuracy: 0.7099879663056559\n",
      "Step: 831, Loss: 1.1051197052001953, Accuracy: 0.710136217948718\n",
      "Step: 832, Loss: 1.1337281465530396, Accuracy: 0.7101840736294518\n",
      "Step: 833, Loss: 1.2220499515533447, Accuracy: 0.7101318944844125\n",
      "Step: 834, Loss: 1.099021315574646, Accuracy: 0.7102794411177644\n",
      "Step: 835, Loss: 1.2208248376846313, Accuracy: 0.7102272727272727\n",
      "Step: 836, Loss: 1.3007408380508423, Accuracy: 0.7100756670649144\n",
      "Step: 837, Loss: 1.1502130031585693, Accuracy: 0.7101233094669849\n",
      "Step: 838, Loss: 1.2625783681869507, Accuracy: 0.7100715137067938\n",
      "Step: 839, Loss: 1.1394988298416138, Accuracy: 0.7101190476190476\n",
      "Step: 840, Loss: 1.238141417503357, Accuracy: 0.7100673801030519\n",
      "Step: 841, Loss: 1.1981626749038696, Accuracy: 0.7100158353127475\n",
      "Step: 842, Loss: 1.464665412902832, Accuracy: 0.7096678529062871\n",
      "Step: 843, Loss: 1.38906991481781, Accuracy: 0.7094194312796208\n",
      "Step: 844, Loss: 1.2381422519683838, Accuracy: 0.7093688362919132\n",
      "Step: 845, Loss: 1.1763142347335815, Accuracy: 0.7094168636721828\n",
      "Step: 846, Loss: 1.4003163576126099, Accuracy: 0.7091696182605274\n",
      "Step: 847, Loss: 0.9521057605743408, Accuracy: 0.7095125786163522\n",
      "Step: 848, Loss: 1.0917909145355225, Accuracy: 0.709658421672556\n",
      "Step: 849, Loss: 1.3514294624328613, Accuracy: 0.7094117647058824\n",
      "Step: 850, Loss: 1.1473960876464844, Accuracy: 0.7094594594594594\n",
      "Step: 851, Loss: 1.1123725175857544, Accuracy: 0.7096048513302035\n",
      "Step: 852, Loss: 1.419774055480957, Accuracy: 0.7092614302461899\n",
      "Step: 853, Loss: 1.0701860189437866, Accuracy: 0.7094067135050741\n",
      "Step: 854, Loss: 1.308089017868042, Accuracy: 0.7092592592592593\n",
      "Step: 855, Loss: 1.369528889656067, Accuracy: 0.7091121495327103\n",
      "Step: 856, Loss: 1.249225378036499, Accuracy: 0.7090626215480358\n",
      "Step: 857, Loss: 1.2479841709136963, Accuracy: 0.709013209013209\n",
      "Step: 858, Loss: 1.4075450897216797, Accuracy: 0.7087698874660457\n",
      "Step: 859, Loss: 1.2084612846374512, Accuracy: 0.7087209302325581\n",
      "Step: 860, Loss: 0.9911481738090515, Accuracy: 0.7089624467673248\n",
      "Step: 861, Loss: 1.2183517217636108, Accuracy: 0.7089133797370456\n",
      "Step: 862, Loss: 1.252759337425232, Accuracy: 0.708864426419467\n",
      "Step: 863, Loss: 1.3066320419311523, Accuracy: 0.7087191358024691\n",
      "Step: 864, Loss: 1.1844353675842285, Accuracy: 0.708766859344894\n",
      "Step: 865, Loss: 1.4088443517684937, Accuracy: 0.7085257890685143\n",
      "Step: 866, Loss: 1.1646767854690552, Accuracy: 0.7085736255286428\n",
      "Step: 867, Loss: 1.2651320695877075, Accuracy: 0.7085253456221198\n",
      "Step: 868, Loss: 1.2246090173721313, Accuracy: 0.7084771768316072\n",
      "Step: 869, Loss: 1.3601230382919312, Accuracy: 0.7083333333333334\n",
      "Step: 870, Loss: 1.0742214918136597, Accuracy: 0.7084768465365481\n",
      "Step: 871, Loss: 1.02968168258667, Accuracy: 0.7087155963302753\n",
      "Step: 872, Loss: 1.0949276685714722, Accuracy: 0.7088583428789614\n",
      "Step: 873, Loss: 1.308522343635559, Accuracy: 0.7087147215865751\n",
      "Step: 874, Loss: 1.1907144784927368, Accuracy: 0.7086666666666667\n",
      "Step: 875, Loss: 1.3676670789718628, Accuracy: 0.7084284627092846\n",
      "Step: 876, Loss: 1.271067500114441, Accuracy: 0.7083808437856328\n",
      "Step: 877, Loss: 1.2332347631454468, Accuracy: 0.7084282460136674\n",
      "Step: 878, Loss: 1.2267799377441406, Accuracy: 0.70838073568449\n",
      "Step: 879, Loss: 1.1714946031570435, Accuracy: 0.7084280303030303\n",
      "Step: 880, Loss: 1.2062331438064575, Accuracy: 0.7084752175558078\n",
      "Step: 881, Loss: 1.2361286878585815, Accuracy: 0.7084278155706727\n",
      "Step: 882, Loss: 1.0749486684799194, Accuracy: 0.7085692714231786\n",
      "Step: 883, Loss: 1.2369834184646606, Accuracy: 0.7085218702865762\n",
      "Step: 884, Loss: 1.1011790037155151, Accuracy: 0.7086629001883239\n",
      "Step: 885, Loss: 1.4260491132736206, Accuracy: 0.7083333333333334\n",
      "Step: 886, Loss: 1.0870341062545776, Accuracy: 0.7084742577978204\n",
      "Step: 887, Loss: 1.2271760702133179, Accuracy: 0.7084271771771772\n",
      "Step: 888, Loss: 1.4070854187011719, Accuracy: 0.7081927259092613\n",
      "Step: 889, Loss: 1.3062149286270142, Accuracy: 0.7080524344569289\n",
      "Step: 890, Loss: 1.2796103954315186, Accuracy: 0.7080059857837636\n",
      "Step: 891, Loss: 1.4522448778152466, Accuracy: 0.7077727952167414\n",
      "Step: 892, Loss: 1.14181387424469, Accuracy: 0.7078200821201941\n",
      "Step: 893, Loss: 1.1621581315994263, Accuracy: 0.7078672632363907\n",
      "Step: 894, Loss: 1.2477318048477173, Accuracy: 0.7078212290502793\n",
      "Step: 895, Loss: 1.216823697090149, Accuracy: 0.7077752976190477\n",
      "Step: 896, Loss: 1.2259291410446167, Accuracy: 0.7078223708658491\n",
      "Step: 897, Loss: 1.3224287033081055, Accuracy: 0.7076837416481069\n",
      "Step: 898, Loss: 1.1617941856384277, Accuracy: 0.7077308120133482\n",
      "Step: 899, Loss: 1.160704255104065, Accuracy: 0.7076851851851852\n",
      "Step: 900, Loss: 0.9926414489746094, Accuracy: 0.7079171291157973\n",
      "Step: 901, Loss: 1.2839971780776978, Accuracy: 0.7077790096082779\n",
      "Step: 902, Loss: 0.9188929200172424, Accuracy: 0.7081026208933185\n",
      "Step: 903, Loss: 1.3324462175369263, Accuracy: 0.7079646017699115\n",
      "Step: 904, Loss: 1.0737584829330444, Accuracy: 0.708011049723757\n",
      "Step: 905, Loss: 1.0693471431732178, Accuracy: 0.7081493745401031\n",
      "Step: 906, Loss: 1.1262598037719727, Accuracy: 0.7081955163542815\n",
      "Step: 907, Loss: 1.1607493162155151, Accuracy: 0.708241556534508\n",
      "Step: 908, Loss: 1.1120800971984863, Accuracy: 0.7082874954162083\n",
      "Step: 909, Loss: 1.1842072010040283, Accuracy: 0.7082417582417583\n",
      "Step: 910, Loss: 1.234573483467102, Accuracy: 0.7081961214782291\n",
      "Step: 911, Loss: 1.13910973072052, Accuracy: 0.7082419590643275\n",
      "Step: 912, Loss: 1.0287500619888306, Accuracy: 0.7084702446148229\n",
      "Step: 913, Loss: 1.1870594024658203, Accuracy: 0.7085156819839533\n",
      "Step: 914, Loss: 1.4475351572036743, Accuracy: 0.7081967213114754\n",
      "Step: 915, Loss: 1.1678649187088013, Accuracy: 0.7082423580786026\n",
      "Step: 916, Loss: 1.3237359523773193, Accuracy: 0.7080152671755725\n",
      "Step: 917, Loss: 1.071341872215271, Accuracy: 0.7081517792302106\n",
      "Step: 918, Loss: 1.1348122358322144, Accuracy: 0.7081973159231049\n",
      "Step: 919, Loss: 0.9909864068031311, Accuracy: 0.7084239130434783\n",
      "Step: 920, Loss: 1.1906377077102661, Accuracy: 0.7083785740137531\n",
      "Step: 921, Loss: 1.0186636447906494, Accuracy: 0.7086044830079538\n",
      "Step: 922, Loss: 1.4215726852416992, Accuracy: 0.7083784759841097\n",
      "Step: 923, Loss: 1.0742344856262207, Accuracy: 0.7085137085137085\n",
      "Step: 924, Loss: 1.1919993162155151, Accuracy: 0.7085585585585585\n",
      "Step: 925, Loss: 1.1500215530395508, Accuracy: 0.7086033117350612\n",
      "Step: 926, Loss: 1.5190099477767944, Accuracy: 0.7081984897518878\n",
      "Step: 927, Loss: 0.9531806111335754, Accuracy: 0.7084231321839081\n",
      "Step: 928, Loss: 1.1696382761001587, Accuracy: 0.7084678866164335\n",
      "Step: 929, Loss: 1.0140509605407715, Accuracy: 0.7086917562724014\n",
      "Step: 930, Loss: 1.377049446105957, Accuracy: 0.7084675975653419\n",
      "Step: 931, Loss: 1.197421908378601, Accuracy: 0.7085121602288984\n",
      "Step: 932, Loss: 1.304718017578125, Accuracy: 0.7082886745266167\n",
      "Step: 933, Loss: 1.2321451902389526, Accuracy: 0.7082441113490364\n",
      "Step: 934, Loss: 1.3181363344192505, Accuracy: 0.7081105169340464\n",
      "Step: 935, Loss: 1.0661181211471558, Accuracy: 0.708244301994302\n",
      "Step: 936, Loss: 1.1804448366165161, Accuracy: 0.7082888651725364\n",
      "Step: 937, Loss: 1.3471976518630981, Accuracy: 0.7081556503198294\n",
      "Step: 938, Loss: 1.0435675382614136, Accuracy: 0.7083777067802627\n",
      "Step: 939, Loss: 1.2635605335235596, Accuracy: 0.7083333333333334\n",
      "Step: 940, Loss: 1.0817270278930664, Accuracy: 0.7084661707403471\n",
      "Step: 941, Loss: 1.1041409969329834, Accuracy: 0.7085987261146497\n",
      "Step: 942, Loss: 1.1517072916030884, Accuracy: 0.70864262990456\n",
      "Step: 943, Loss: 1.4293700456619263, Accuracy: 0.7083333333333334\n",
      "Step: 944, Loss: 1.0017439126968384, Accuracy: 0.7085537918871252\n",
      "Step: 945, Loss: 1.2614295482635498, Accuracy: 0.7085095137420718\n",
      "Step: 946, Loss: 1.2925575971603394, Accuracy: 0.7083773319253784\n",
      "Step: 947, Loss: 1.3011913299560547, Accuracy: 0.708245428973277\n",
      "Step: 948, Loss: 1.268778920173645, Accuracy: 0.7082016157358623\n",
      "Step: 949, Loss: 1.3182034492492676, Accuracy: 0.7080701754385965\n",
      "Step: 950, Loss: 1.3329044580459595, Accuracy: 0.7079390115667719\n",
      "Step: 951, Loss: 1.083177924156189, Accuracy: 0.7080707282913166\n",
      "Step: 952, Loss: 1.172507882118225, Accuracy: 0.7081147254284715\n",
      "Step: 953, Loss: 1.408979058265686, Accuracy: 0.7078965758211041\n",
      "Step: 954, Loss: 1.2709540128707886, Accuracy: 0.7078534031413612\n",
      "Step: 955, Loss: 1.147063970565796, Accuracy: 0.707897489539749\n",
      "Step: 956, Loss: 1.4732760190963745, Accuracy: 0.7075931731104145\n",
      "Step: 957, Loss: 1.1476473808288574, Accuracy: 0.7077244258872651\n",
      "Step: 958, Loss: 1.187293529510498, Accuracy: 0.7077685088633994\n",
      "Step: 959, Loss: 1.4353208541870117, Accuracy: 0.7074652777777778\n",
      "Step: 960, Loss: 1.0948585271835327, Accuracy: 0.7075962539021852\n",
      "Step: 961, Loss: 1.3263972997665405, Accuracy: 0.7074670824670825\n",
      "Step: 962, Loss: 1.1796740293502808, Accuracy: 0.7075112495673244\n",
      "Step: 963, Loss: 1.1557039022445679, Accuracy: 0.7075553250345782\n",
      "Step: 964, Loss: 1.369356632232666, Accuracy: 0.7073402417962004\n",
      "Step: 965, Loss: 1.3063185214996338, Accuracy: 0.7072118702553485\n",
      "Step: 966, Loss: 1.1507700681686401, Accuracy: 0.7072561185798001\n",
      "Step: 967, Loss: 1.1518408060073853, Accuracy: 0.7073002754820936\n",
      "Step: 968, Loss: 1.2304879426956177, Accuracy: 0.7072583419332645\n",
      "Step: 969, Loss: 1.0781296491622925, Accuracy: 0.7073883161512028\n",
      "Step: 970, Loss: 1.2237712144851685, Accuracy: 0.7073463783041538\n",
      "Step: 971, Loss: 1.263670563697815, Accuracy: 0.7073045267489712\n",
      "Step: 972, Loss: 1.062984824180603, Accuracy: 0.7074340527577938\n",
      "Step: 973, Loss: 1.230901837348938, Accuracy: 0.7073921971252567\n",
      "Step: 974, Loss: 1.2879959344863892, Accuracy: 0.7072649572649573\n",
      "Step: 975, Loss: 1.2112771272659302, Accuracy: 0.7072233606557377\n",
      "Step: 976, Loss: 1.1646889448165894, Accuracy: 0.7072671443193449\n",
      "Step: 977, Loss: 1.4241122007369995, Accuracy: 0.7070552147239264\n",
      "Step: 978, Loss: 1.1751675605773926, Accuracy: 0.7070139598229486\n",
      "Step: 979, Loss: 1.3016691207885742, Accuracy: 0.7068877551020408\n",
      "Step: 980, Loss: 1.0604106187820435, Accuracy: 0.7071015970098539\n",
      "Step: 981, Loss: 1.2255512475967407, Accuracy: 0.7070604209097081\n",
      "Step: 982, Loss: 1.2948790788650513, Accuracy: 0.7069345540861309\n",
      "Step: 983, Loss: 1.3940378427505493, Accuracy: 0.7067242547425474\n",
      "Step: 984, Loss: 0.9136988520622253, Accuracy: 0.7070219966159053\n",
      "Step: 985, Loss: 1.2087081670761108, Accuracy: 0.7070655848546316\n",
      "Step: 986, Loss: 1.4497199058532715, Accuracy: 0.7067713610266801\n",
      "Step: 987, Loss: 1.351455569267273, Accuracy: 0.7065620782726046\n",
      "Step: 988, Loss: 1.1340177059173584, Accuracy: 0.7066902595214021\n",
      "Step: 989, Loss: 1.1813102960586548, Accuracy: 0.7067340067340068\n",
      "Step: 990, Loss: 1.2120667695999146, Accuracy: 0.7066935755129499\n",
      "Step: 991, Loss: 1.2163519859313965, Accuracy: 0.7066532258064516\n",
      "Step: 992, Loss: 1.088262677192688, Accuracy: 0.706780798925814\n",
      "Step: 993, Loss: 1.2627522945404053, Accuracy: 0.7066566063044937\n",
      "Step: 994, Loss: 1.3162740468978882, Accuracy: 0.7065326633165829\n",
      "Step: 995, Loss: 1.269487977027893, Accuracy: 0.706408969210174\n",
      "Step: 996, Loss: 1.381431221961975, Accuracy: 0.7062019391507857\n",
      "Step: 997, Loss: 1.274046778678894, Accuracy: 0.7060788243152972\n",
      "Step: 998, Loss: 1.2309348583221436, Accuracy: 0.7060393727060393\n",
      "Step: 999, Loss: 1.112489938735962, Accuracy: 0.7061666666666667\n",
      "Step: 1000, Loss: 1.3432730436325073, Accuracy: 0.7059607059607059\n",
      "Step: 1001, Loss: 1.1915277242660522, Accuracy: 0.7059214903526281\n",
      "Step: 1002, Loss: 1.459825873374939, Accuracy: 0.7056331006979063\n",
      "Step: 1003, Loss: 1.1564091444015503, Accuracy: 0.7056772908366534\n",
      "Step: 1004, Loss: 1.214086651802063, Accuracy: 0.7056384742951907\n",
      "Step: 1005, Loss: 1.3106731176376343, Accuracy: 0.7055168986083499\n",
      "Step: 1006, Loss: 1.0409032106399536, Accuracy: 0.7056438265475008\n",
      "Step: 1007, Loss: 1.3048001527786255, Accuracy: 0.7055224867724867\n",
      "Step: 1008, Loss: 1.185577392578125, Accuracy: 0.7054839775355137\n",
      "Step: 1009, Loss: 1.1650336980819702, Accuracy: 0.7055280528052805\n",
      "Step: 1010, Loss: 1.2067930698394775, Accuracy: 0.7054896142433235\n",
      "Step: 1011, Loss: 1.120208501815796, Accuracy: 0.7056159420289855\n",
      "Step: 1012, Loss: 1.3067008256912231, Accuracy: 0.7054952286936492\n",
      "Step: 1013, Loss: 1.1784175634384155, Accuracy: 0.7055391190006575\n",
      "Step: 1014, Loss: 1.2265571355819702, Accuracy: 0.7055008210180624\n",
      "Step: 1015, Loss: 1.1351491212844849, Accuracy: 0.7055446194225722\n",
      "Step: 1016, Loss: 1.2706243991851807, Accuracy: 0.7054244509996722\n",
      "Step: 1017, Loss: 1.2264392375946045, Accuracy: 0.7053863785199738\n",
      "Step: 1018, Loss: 1.2621861696243286, Accuracy: 0.7053483807654564\n",
      "Step: 1019, Loss: 1.1712502241134644, Accuracy: 0.7053921568627451\n",
      "Step: 1020, Loss: 1.075986385345459, Accuracy: 0.7055174665360757\n",
      "Step: 1021, Loss: 1.2834542989730835, Accuracy: 0.7053979125896934\n",
      "Step: 1022, Loss: 1.1771527528762817, Accuracy: 0.7054415118931248\n",
      "Step: 1023, Loss: 0.9110367298126221, Accuracy: 0.7057291666666666\n",
      "Step: 1024, Loss: 1.0884040594100952, Accuracy: 0.7058536585365853\n",
      "Step: 1025, Loss: 1.0025557279586792, Accuracy: 0.7060591293047433\n",
      "Step: 1026, Loss: 1.2259814739227295, Accuracy: 0.7060207724764687\n",
      "Step: 1027, Loss: 1.3462613821029663, Accuracy: 0.7058203631647212\n",
      "Step: 1028, Loss: 1.3225314617156982, Accuracy: 0.7057013281503077\n",
      "Step: 1029, Loss: 1.404420018196106, Accuracy: 0.7055016181229773\n",
      "Step: 1030, Loss: 1.0717111825942993, Accuracy: 0.7056256062075654\n",
      "Step: 1031, Loss: 1.0832831859588623, Accuracy: 0.7058301033591732\n",
      "Step: 1032, Loss: 1.2110086679458618, Accuracy: 0.7057921910293643\n",
      "Step: 1033, Loss: 1.0693072080612183, Accuracy: 0.7059961315280464\n",
      "Step: 1034, Loss: 1.2182884216308594, Accuracy: 0.7059581320450886\n",
      "Step: 1035, Loss: 1.2275408506393433, Accuracy: 0.7059202059202059\n",
      "Step: 1036, Loss: 1.2249523401260376, Accuracy: 0.7058823529411765\n",
      "Step: 1037, Loss: 1.1159136295318604, Accuracy: 0.7059248554913294\n",
      "Step: 1038, Loss: 1.1852490901947021, Accuracy: 0.7058870709015078\n",
      "Step: 1039, Loss: 1.1565295457839966, Accuracy: 0.7059294871794872\n",
      "Step: 1040, Loss: 1.1122090816497803, Accuracy: 0.7059718219660582\n",
      "Step: 1041, Loss: 1.3066960573196411, Accuracy: 0.7058541266794626\n",
      "Step: 1042, Loss: 1.2125447988510132, Accuracy: 0.7058165548098434\n",
      "Step: 1043, Loss: 1.3946418762207031, Accuracy: 0.7056194125159643\n",
      "Step: 1044, Loss: 1.142593502998352, Accuracy: 0.7056618819776714\n",
      "Step: 1045, Loss: 1.0887173414230347, Accuracy: 0.7057839388145315\n",
      "Step: 1046, Loss: 1.0720282793045044, Accuracy: 0.7059057624960203\n",
      "Step: 1047, Loss: 1.1471179723739624, Accuracy: 0.7060273536895675\n",
      "Step: 1048, Loss: 1.1332625150680542, Accuracy: 0.7060692723228471\n",
      "Step: 1049, Loss: 1.1111396551132202, Accuracy: 0.7061904761904761\n",
      "Step: 1050, Loss: 1.0227097272872925, Accuracy: 0.7063907389787504\n",
      "Step: 1051, Loss: 1.1981446743011475, Accuracy: 0.7064321926489227\n",
      "Step: 1052, Loss: 1.1176929473876953, Accuracy: 0.7065527065527065\n",
      "Step: 1053, Loss: 1.2165004014968872, Accuracy: 0.7065148640101202\n",
      "Step: 1054, Loss: 1.219310998916626, Accuracy: 0.7065560821484992\n",
      "Step: 1055, Loss: 1.4746028184890747, Accuracy: 0.7062815656565656\n",
      "Step: 1056, Loss: 1.1601827144622803, Accuracy: 0.706322926521602\n",
      "Step: 1057, Loss: 1.313125729560852, Accuracy: 0.7062066792690611\n",
      "Step: 1058, Loss: 1.1564141511917114, Accuracy: 0.7062480327352849\n",
      "Step: 1059, Loss: 1.226676106452942, Accuracy: 0.7062106918238994\n",
      "Step: 1060, Loss: 1.39287531375885, Accuracy: 0.7060163367891926\n",
      "Step: 1061, Loss: 1.0898027420043945, Accuracy: 0.7061362209667295\n",
      "Step: 1062, Loss: 1.1855612993240356, Accuracy: 0.7061774851050486\n",
      "Step: 1063, Loss: 0.9858139157295227, Accuracy: 0.706375313283208\n",
      "Step: 1064, Loss: 1.072588324546814, Accuracy: 0.7064945226917058\n",
      "Step: 1065, Loss: 1.1916512250900269, Accuracy: 0.7065353345841151\n",
      "Step: 1066, Loss: 1.2570933103561401, Accuracy: 0.7064979693845673\n",
      "Step: 1067, Loss: 1.1093189716339111, Accuracy: 0.7066167290886392\n",
      "Step: 1068, Loss: 1.32318115234375, Accuracy: 0.7065014031805426\n",
      "Step: 1069, Loss: 1.0836068391799927, Accuracy: 0.7066199376947041\n",
      "Step: 1070, Loss: 1.348528265953064, Accuracy: 0.7064270152505446\n",
      "Step: 1071, Loss: 1.3432954549789429, Accuracy: 0.7063121890547264\n",
      "Step: 1072, Loss: 1.1377471685409546, Accuracy: 0.7063529046287667\n",
      "Step: 1073, Loss: 1.0361664295196533, Accuracy: 0.7064711359404097\n",
      "Step: 1074, Loss: 1.144541621208191, Accuracy: 0.7065116279069767\n",
      "Step: 1075, Loss: 1.2616016864776611, Accuracy: 0.7064745972738538\n",
      "Step: 1076, Loss: 1.1511026620864868, Accuracy: 0.7065150108325596\n",
      "Step: 1077, Loss: 1.3215845823287964, Accuracy: 0.7064007421150278\n",
      "Step: 1078, Loss: 1.330858588218689, Accuracy: 0.7062866852023478\n",
      "Step: 1079, Loss: 1.3381177186965942, Accuracy: 0.7061728395061728\n",
      "Step: 1080, Loss: 1.2444863319396973, Accuracy: 0.70613629355535\n",
      "Step: 1081, Loss: 1.4191848039627075, Accuracy: 0.7059457794208256\n",
      "Step: 1082, Loss: 1.2362533807754517, Accuracy: 0.7059095106186519\n",
      "Step: 1083, Loss: 1.0420619249343872, Accuracy: 0.7060270602706027\n",
      "Step: 1084, Loss: 1.176069736480713, Accuracy: 0.7060675883256529\n",
      "Step: 1085, Loss: 1.3587666749954224, Accuracy: 0.7059545733578882\n",
      "Step: 1086, Loss: 1.164952278137207, Accuracy: 0.7059950935295921\n",
      "Step: 1087, Loss: 1.1446353197097778, Accuracy: 0.7060355392156863\n",
      "Step: 1088, Loss: 1.3755873441696167, Accuracy: 0.7058463422099786\n",
      "Step: 1089, Loss: 1.3771005868911743, Accuracy: 0.7056574923547401\n",
      "Step: 1090, Loss: 1.0878015756607056, Accuracy: 0.7057745187901008\n",
      "Step: 1091, Loss: 1.1777515411376953, Accuracy: 0.7058150183150184\n",
      "Step: 1092, Loss: 1.259456992149353, Accuracy: 0.7057792009759073\n",
      "Step: 1093, Loss: 1.0917247533798218, Accuracy: 0.7058957952468007\n",
      "Step: 1094, Loss: 1.1604591608047485, Accuracy: 0.7059360730593607\n",
      "Step: 1095, Loss: 1.16281259059906, Accuracy: 0.7059762773722628\n",
      "Step: 1096, Loss: 1.0535515546798706, Accuracy: 0.7060923731388635\n",
      "Step: 1097, Loss: 1.3824228048324585, Accuracy: 0.7059046751669702\n",
      "Step: 1098, Loss: 1.2333136796951294, Accuracy: 0.7057931452835912\n",
      "Step: 1099, Loss: 1.217311978340149, Accuracy: 0.7058333333333333\n",
      "Step: 1100, Loss: 1.1548863649368286, Accuracy: 0.7058734483802603\n",
      "Step: 1101, Loss: 1.2206145524978638, Accuracy: 0.7059134906231095\n",
      "Step: 1102, Loss: 1.2493011951446533, Accuracy: 0.7058779087337564\n",
      "Step: 1103, Loss: 1.0604257583618164, Accuracy: 0.7059933574879227\n",
      "Step: 1104, Loss: 1.1642634868621826, Accuracy: 0.7060331825037708\n",
      "Step: 1105, Loss: 1.043112874031067, Accuracy: 0.7062236286919831\n",
      "Step: 1106, Loss: 1.2598994970321655, Accuracy: 0.7061878952122854\n",
      "Step: 1107, Loss: 1.1870274543762207, Accuracy: 0.7062274368231047\n",
      "Step: 1108, Loss: 1.1601481437683105, Accuracy: 0.7062669071235347\n",
      "Step: 1109, Loss: 1.1513131856918335, Accuracy: 0.7063063063063063\n",
      "Step: 1110, Loss: 1.0897252559661865, Accuracy: 0.7064206420642064\n",
      "Step: 1111, Loss: 1.020168423652649, Accuracy: 0.7066097122302158\n",
      "Step: 1112, Loss: 1.0949631929397583, Accuracy: 0.7067235699311171\n",
      "Step: 1113, Loss: 1.3049708604812622, Accuracy: 0.7066128067025733\n",
      "Step: 1114, Loss: 1.222317099571228, Accuracy: 0.7065022421524664\n",
      "Step: 1115, Loss: 1.2382985353469849, Accuracy: 0.7064665471923537\n",
      "Step: 1116, Loss: 1.3050483465194702, Accuracy: 0.7063563115487914\n",
      "Step: 1117, Loss: 1.233048677444458, Accuracy: 0.7063208109719737\n",
      "Step: 1118, Loss: 0.9602258205413818, Accuracy: 0.7065832588620793\n",
      "Step: 1119, Loss: 1.022408127784729, Accuracy: 0.7066964285714286\n",
      "Step: 1120, Loss: 1.4036483764648438, Accuracy: 0.7065120428189117\n",
      "Step: 1121, Loss: 1.3842483758926392, Accuracy: 0.7063279857397504\n",
      "Step: 1122, Loss: 1.008018970489502, Accuracy: 0.7065152864351439\n",
      "Step: 1123, Loss: 1.1090975999832153, Accuracy: 0.7066281138790036\n",
      "Step: 1124, Loss: 1.1811248064041138, Accuracy: 0.7066666666666667\n",
      "Step: 1125, Loss: 1.2634135484695435, Accuracy: 0.7066311426879811\n",
      "Step: 1126, Loss: 1.07948899269104, Accuracy: 0.7067435669920142\n",
      "Step: 1127, Loss: 1.018449306488037, Accuracy: 0.7069296690307328\n",
      "Step: 1128, Loss: 1.2437092065811157, Accuracy: 0.7068940064954237\n",
      "Step: 1129, Loss: 1.2174347639083862, Accuracy: 0.706858407079646\n",
      "Step: 1130, Loss: 1.255322813987732, Accuracy: 0.706822870615974\n",
      "Step: 1131, Loss: 0.9428160190582275, Accuracy: 0.7070818610129564\n",
      "Step: 1132, Loss: 1.3100489377975464, Accuracy: 0.706972639011474\n",
      "Step: 1133, Loss: 1.0589394569396973, Accuracy: 0.7071575543797766\n",
      "Step: 1134, Loss: 1.0694040060043335, Accuracy: 0.7072687224669604\n",
      "Step: 1135, Loss: 1.3976473808288574, Accuracy: 0.7070862676056338\n",
      "Step: 1136, Loss: 1.224805235862732, Accuracy: 0.7070507182644385\n",
      "Step: 1137, Loss: 1.1714733839035034, Accuracy: 0.7070884592852958\n",
      "Step: 1138, Loss: 1.2284644842147827, Accuracy: 0.7070529704419081\n",
      "Step: 1139, Loss: 1.2180479764938354, Accuracy: 0.7070906432748538\n",
      "Step: 1140, Loss: 1.1517791748046875, Accuracy: 0.7071282500730354\n",
      "Step: 1141, Loss: 1.442857265472412, Accuracy: 0.7068739054290718\n",
      "Step: 1142, Loss: 1.2793532609939575, Accuracy: 0.7067658209390493\n",
      "Step: 1143, Loss: 1.1946816444396973, Accuracy: 0.7068036130536131\n",
      "Step: 1144, Loss: 1.2609848976135254, Accuracy: 0.7067685589519651\n",
      "Step: 1145, Loss: 1.2847224473953247, Accuracy: 0.7066608493310064\n",
      "Step: 1146, Loss: 1.1951266527175903, Accuracy: 0.7066259808195292\n",
      "Step: 1147, Loss: 1.1522618532180786, Accuracy: 0.706663763066202\n",
      "Step: 1148, Loss: 1.1388417482376099, Accuracy: 0.7067014795474326\n",
      "Step: 1149, Loss: 1.1962323188781738, Accuracy: 0.7067391304347826\n",
      "Step: 1150, Loss: 1.1570583581924438, Accuracy: 0.7067767158992181\n",
      "Step: 1151, Loss: 1.1863054037094116, Accuracy: 0.7068142361111112\n",
      "Step: 1152, Loss: 1.049655795097351, Accuracy: 0.706923966464296\n",
      "Step: 1153, Loss: 1.074471116065979, Accuracy: 0.7070335066435587\n",
      "Step: 1154, Loss: 1.1826545000076294, Accuracy: 0.7070707070707071\n",
      "Step: 1155, Loss: 1.1641929149627686, Accuracy: 0.7071078431372549\n",
      "Step: 1156, Loss: 1.0771374702453613, Accuracy: 0.7072169403630078\n",
      "Step: 1157, Loss: 1.2774802446365356, Accuracy: 0.7071099597006333\n",
      "Step: 1158, Loss: 1.0211440324783325, Accuracy: 0.707290767903365\n",
      "Step: 1159, Loss: 1.1780601739883423, Accuracy: 0.7073275862068965\n",
      "Step: 1160, Loss: 0.9955129623413086, Accuracy: 0.7075078954923916\n",
      "Step: 1161, Loss: 1.1970081329345703, Accuracy: 0.7074727481353987\n",
      "Step: 1162, Loss: 1.27194344997406, Accuracy: 0.7074376612209802\n",
      "Step: 1163, Loss: 1.1612703800201416, Accuracy: 0.7074742268041238\n",
      "Step: 1164, Loss: 1.1525808572769165, Accuracy: 0.707510729613734\n",
      "Step: 1165, Loss: 1.1948062181472778, Accuracy: 0.7075471698113207\n",
      "Step: 1166, Loss: 1.2320764064788818, Accuracy: 0.707512139388746\n",
      "Step: 1167, Loss: 1.4402507543563843, Accuracy: 0.7072631278538812\n",
      "Step: 1168, Loss: 1.2457964420318604, Accuracy: 0.7072284003421728\n",
      "Step: 1169, Loss: 1.0154398679733276, Accuracy: 0.7074074074074074\n",
      "Step: 1170, Loss: 1.2724695205688477, Accuracy: 0.7073014517506405\n",
      "Step: 1171, Loss: 1.4164628982543945, Accuracy: 0.7071245733788396\n",
      "Step: 1172, Loss: 1.000105381011963, Accuracy: 0.7073032111395283\n",
      "Step: 1173, Loss: 1.2460225820541382, Accuracy: 0.7072685973878479\n",
      "Step: 1174, Loss: 1.3956832885742188, Accuracy: 0.7070921985815602\n",
      "Step: 1175, Loss: 1.2502756118774414, Accuracy: 0.7070578231292517\n",
      "Step: 1176, Loss: 1.1776106357574463, Accuracy: 0.7070943075615973\n",
      "Step: 1177, Loss: 1.0910159349441528, Accuracy: 0.7072014714204867\n",
      "Step: 1178, Loss: 1.3236734867095947, Accuracy: 0.7070964093864858\n",
      "Step: 1179, Loss: 1.0761550664901733, Accuracy: 0.7072033898305085\n",
      "Step: 1180, Loss: 1.258747935295105, Accuracy: 0.7071690657634773\n",
      "Step: 1181, Loss: 1.2298555374145508, Accuracy: 0.7071347997743936\n",
      "Step: 1182, Loss: 1.4073961973190308, Accuracy: 0.706959706959707\n",
      "Step: 1183, Loss: 1.3484677076339722, Accuracy: 0.7067849099099099\n",
      "Step: 1184, Loss: 1.0825278759002686, Accuracy: 0.7068917018284107\n",
      "Step: 1185, Loss: 1.2616628408432007, Accuracy: 0.7068577852726251\n",
      "Step: 1186, Loss: 1.1004911661148071, Accuracy: 0.7069643358607133\n",
      "Step: 1187, Loss: 1.1605417728424072, Accuracy: 0.7070005611672279\n",
      "Step: 1188, Loss: 1.2871772050857544, Accuracy: 0.7068965517241379\n",
      "Step: 1189, Loss: 1.26068913936615, Accuracy: 0.7068627450980393\n",
      "Step: 1190, Loss: 1.306616187095642, Accuracy: 0.7067590260285475\n",
      "Step: 1191, Loss: 1.045845866203308, Accuracy: 0.7069351230425056\n",
      "Step: 1192, Loss: 1.19150972366333, Accuracy: 0.7069013690975132\n",
      "Step: 1193, Loss: 1.2155896425247192, Accuracy: 0.7068676716917923\n",
      "Step: 1194, Loss: 1.2864551544189453, Accuracy: 0.7067642956764296\n",
      "Step: 1195, Loss: 1.2892879247665405, Accuracy: 0.7066610925306578\n",
      "Step: 1196, Loss: 1.168835997581482, Accuracy: 0.7066276803118908\n",
      "Step: 1197, Loss: 1.1238423585891724, Accuracy: 0.7066638842515304\n",
      "Step: 1198, Loss: 0.9998274445533752, Accuracy: 0.7068390325271059\n",
      "Step: 1199, Loss: 1.1130479574203491, Accuracy: 0.706875\n",
      "Step: 1200, Loss: 1.2284443378448486, Accuracy: 0.7068415209547599\n",
      "Step: 1201, Loss: 1.1606162786483765, Accuracy: 0.70687742651137\n",
      "Step: 1202, Loss: 1.3755027055740356, Accuracy: 0.7067054585757828\n",
      "Step: 1203, Loss: 0.9455336928367615, Accuracy: 0.7069490586932448\n",
      "Step: 1204, Loss: 1.3183330297470093, Accuracy: 0.7068464730290457\n",
      "Step: 1205, Loss: 0.9160224795341492, Accuracy: 0.707089552238806\n",
      "Step: 1206, Loss: 1.2550128698349, Accuracy: 0.7070560618613643\n",
      "Step: 1207, Loss: 1.3478307723999023, Accuracy: 0.706953642384106\n",
      "Step: 1208, Loss: 1.1365432739257812, Accuracy: 0.706989247311828\n",
      "Step: 1209, Loss: 1.1927340030670166, Accuracy: 0.7070247933884297\n",
      "Step: 1210, Loss: 1.0905557870864868, Accuracy: 0.7070602807597027\n",
      "Step: 1211, Loss: 1.3512128591537476, Accuracy: 0.7068894389438944\n",
      "Step: 1212, Loss: 1.3080439567565918, Accuracy: 0.7067875790052213\n",
      "Step: 1213, Loss: 0.9866423010826111, Accuracy: 0.7069604612850082\n",
      "Step: 1214, Loss: 1.2461421489715576, Accuracy: 0.7069272976680384\n",
      "Step: 1215, Loss: 1.14642333984375, Accuracy: 0.7069627192982456\n",
      "Step: 1216, Loss: 0.9924521446228027, Accuracy: 0.7071350314982197\n",
      "Step: 1217, Loss: 1.1216100454330444, Accuracy: 0.7072386425834701\n",
      "Step: 1218, Loss: 1.2771104574203491, Accuracy: 0.7072053595843588\n",
      "Step: 1219, Loss: 1.2441933155059814, Accuracy: 0.7071038251366121\n",
      "Step: 1220, Loss: 1.3259390592575073, Accuracy: 0.7069342069342069\n",
      "Step: 1221, Loss: 1.46453857421875, Accuracy: 0.706696672122204\n",
      "Step: 1222, Loss: 1.1361056566238403, Accuracy: 0.7067320795857182\n",
      "Step: 1223, Loss: 1.341911792755127, Accuracy: 0.7066312636165577\n",
      "Step: 1224, Loss: 1.3501757383346558, Accuracy: 0.7064625850340136\n",
      "Step: 1225, Loss: 1.2305361032485962, Accuracy: 0.7064980967917346\n",
      "Step: 1226, Loss: 1.1548153162002563, Accuracy: 0.70653355066558\n",
      "Step: 1227, Loss: 1.302423119544983, Accuracy: 0.7064332247557004\n",
      "Step: 1228, Loss: 1.3168548345565796, Accuracy: 0.7063330621101166\n",
      "Step: 1229, Loss: 1.1273061037063599, Accuracy: 0.7064363143631436\n",
      "Step: 1230, Loss: 1.3206113576889038, Accuracy: 0.7063363119415109\n",
      "Step: 1231, Loss: 1.1375070810317993, Accuracy: 0.7063717532467533\n",
      "Step: 1232, Loss: 1.279629111289978, Accuracy: 0.706271965396053\n",
      "Step: 1233, Loss: 1.2456265687942505, Accuracy: 0.706172339276067\n",
      "Step: 1234, Loss: 1.2227932214736938, Accuracy: 0.706140350877193\n",
      "Step: 1235, Loss: 1.299460530281067, Accuracy: 0.7060409924487594\n",
      "Step: 1236, Loss: 1.2150436639785767, Accuracy: 0.7060091619509566\n",
      "Step: 1237, Loss: 1.1487164497375488, Accuracy: 0.7060446957458266\n",
      "Step: 1238, Loss: 0.9918067455291748, Accuracy: 0.7062146892655368\n",
      "Step: 1239, Loss: 1.1356428861618042, Accuracy: 0.70625\n",
      "Step: 1240, Loss: 1.1033377647399902, Accuracy: 0.7062852538275585\n",
      "Step: 1241, Loss: 1.2300196886062622, Accuracy: 0.7062533548040795\n",
      "Step: 1242, Loss: 1.147664189338684, Accuracy: 0.7062885492089032\n",
      "Step: 1243, Loss: 1.1838740110397339, Accuracy: 0.7062566988210075\n",
      "Step: 1244, Loss: 1.3489526510238647, Accuracy: 0.7061579651941098\n",
      "Step: 1245, Loss: 1.331953525543213, Accuracy: 0.7059925093632958\n",
      "Step: 1246, Loss: 1.2262223958969116, Accuracy: 0.7058941459502807\n",
      "Step: 1247, Loss: 0.9128174781799316, Accuracy: 0.7061298076923077\n",
      "Step: 1248, Loss: 1.0767687559127808, Accuracy: 0.7062316519882573\n",
      "Step: 1249, Loss: 1.1964877843856812, Accuracy: 0.7062666666666667\n",
      "Step: 1250, Loss: 1.4100934267044067, Accuracy: 0.7061017852384759\n",
      "Step: 1251, Loss: 1.0555672645568848, Accuracy: 0.7062034078807242\n",
      "Step: 1252, Loss: 1.4481626749038696, Accuracy: 0.7059723330673051\n",
      "Step: 1253, Loss: 1.1968753337860107, Accuracy: 0.7060074428495481\n",
      "Step: 1254, Loss: 1.1359041929244995, Accuracy: 0.7060424966799469\n",
      "Step: 1255, Loss: 1.256472110748291, Accuracy: 0.7060111464968153\n",
      "Step: 1256, Loss: 1.228772521018982, Accuracy: 0.7059798461946434\n",
      "Step: 1257, Loss: 1.389288067817688, Accuracy: 0.7057498675145734\n",
      "Step: 1258, Loss: 1.242467999458313, Accuracy: 0.7057188244638603\n",
      "Step: 1259, Loss: 1.2206188440322876, Accuracy: 0.7057539682539683\n",
      "Step: 1260, Loss: 1.3171933889389038, Accuracy: 0.7056568860692572\n",
      "Step: 1261, Loss: 1.418676733970642, Accuracy: 0.7054939249867934\n",
      "Step: 1262, Loss: 1.3266066312789917, Accuracy: 0.7053972024280812\n",
      "Step: 1263, Loss: 1.0842703580856323, Accuracy: 0.705498417721519\n",
      "Step: 1264, Loss: 1.0932681560516357, Accuracy: 0.7055994729907773\n",
      "Step: 1265, Loss: 1.3147121667861938, Accuracy: 0.7055028962611901\n",
      "Step: 1266, Loss: 1.1514915227890015, Accuracy: 0.7055380163114969\n",
      "Step: 1267, Loss: 1.2334996461868286, Accuracy: 0.7055073606729758\n",
      "Step: 1268, Loss: 1.3276361227035522, Accuracy: 0.705411084843709\n",
      "Step: 1269, Loss: 1.0983834266662598, Accuracy: 0.705511811023622\n",
      "Step: 1270, Loss: 1.3549232482910156, Accuracy: 0.7053501180173092\n",
      "Step: 1271, Loss: 1.2321528196334839, Accuracy: 0.7053197064989518\n",
      "Step: 1272, Loss: 1.2169605493545532, Accuracy: 0.7053548049227546\n",
      "Step: 1273, Loss: 0.9743759036064148, Accuracy: 0.7055860805860806\n",
      "Step: 1274, Loss: 1.128456711769104, Accuracy: 0.7056209150326798\n",
      "Step: 1275, Loss: 1.2107082605361938, Accuracy: 0.7055903866248694\n",
      "Step: 1276, Loss: 0.986636221408844, Accuracy: 0.7057556773688332\n",
      "Step: 1277, Loss: 1.1618708372116089, Accuracy: 0.7057902973395931\n",
      "Step: 1278, Loss: 1.1784862279891968, Accuracy: 0.705824863174355\n",
      "Step: 1279, Loss: 1.1040079593658447, Accuracy: 0.7059244791666667\n",
      "Step: 1280, Loss: 1.0762062072753906, Accuracy: 0.706023939630497\n",
      "Step: 1281, Loss: 1.1030324697494507, Accuracy: 0.7060582423296932\n",
      "Step: 1282, Loss: 1.3893104791641235, Accuracy: 0.7058976357495453\n",
      "Step: 1283, Loss: 1.250313401222229, Accuracy: 0.7058670820353063\n",
      "Step: 1284, Loss: 1.0676592588424683, Accuracy: 0.7059662775616083\n",
      "Step: 1285, Loss: 1.0888763666152954, Accuracy: 0.7060653188180405\n",
      "Step: 1286, Loss: 1.1617343425750732, Accuracy: 0.7060994560994561\n",
      "Step: 1287, Loss: 1.072096586227417, Accuracy: 0.7061982401656315\n",
      "Step: 1288, Loss: 1.2258952856063843, Accuracy: 0.706167571761055\n",
      "Step: 1289, Loss: 1.2922694683074951, Accuracy: 0.7060723514211886\n",
      "Step: 1290, Loss: 1.1792322397232056, Accuracy: 0.7061063774851536\n",
      "Step: 1291, Loss: 1.0681101083755493, Accuracy: 0.7062048503611971\n",
      "Step: 1292, Loss: 1.164585828781128, Accuracy: 0.7062387213199278\n",
      "Step: 1293, Loss: 1.0801749229431152, Accuracy: 0.7063369397217929\n",
      "Step: 1294, Loss: 1.1332429647445679, Accuracy: 0.7063706563706563\n",
      "Step: 1295, Loss: 1.121767520904541, Accuracy: 0.706468621399177\n",
      "Step: 1296, Loss: 1.3633474111557007, Accuracy: 0.7063094320226163\n",
      "Step: 1297, Loss: 1.360467553138733, Accuracy: 0.7062146892655368\n",
      "Step: 1298, Loss: 1.1520925760269165, Accuracy: 0.7062483962022068\n",
      "Step: 1299, Loss: 1.1739791631698608, Accuracy: 0.7062820512820512\n",
      "Step: 1300, Loss: 1.1607412099838257, Accuracy: 0.7063156546246477\n",
      "Step: 1301, Loss: 1.2340816259384155, Accuracy: 0.7062852022529442\n",
      "Step: 1302, Loss: 1.054770827293396, Accuracy: 0.7063827065745715\n",
      "Step: 1303, Loss: 1.2865740060806274, Accuracy: 0.7063522494887525\n",
      "Step: 1304, Loss: 1.3147858381271362, Accuracy: 0.7062579821200511\n",
      "Step: 1305, Loss: 1.5104690790176392, Accuracy: 0.7060362429811128\n",
      "Step: 1306, Loss: 1.1240534782409668, Accuracy: 0.7061336393777098\n",
      "Step: 1307, Loss: 1.200116753578186, Accuracy: 0.7061671763506626\n",
      "Step: 1308, Loss: 1.2761893272399902, Accuracy: 0.7060733384262796\n",
      "Step: 1309, Loss: 1.2730021476745605, Accuracy: 0.7060432569974555\n",
      "Step: 1310, Loss: 1.1007136106491089, Accuracy: 0.706140350877193\n",
      "Step: 1311, Loss: 0.9337422847747803, Accuracy: 0.7063643292682927\n",
      "Step: 1312, Loss: 1.4298478364944458, Accuracy: 0.7062071591774562\n",
      "Step: 1313, Loss: 1.1063861846923828, Accuracy: 0.7062404870624048\n",
      "Step: 1314, Loss: 1.446283221244812, Accuracy: 0.706020278833967\n",
      "Step: 1315, Loss: 0.9963733553886414, Accuracy: 0.7061803444782169\n",
      "Step: 1316, Loss: 1.2432373762130737, Accuracy: 0.7061503416856492\n",
      "Step: 1317, Loss: 1.3165115118026733, Accuracy: 0.7060571573090542\n",
      "Step: 1318, Loss: 1.179594874382019, Accuracy: 0.7060904725802376\n",
      "Step: 1319, Loss: 1.2252906560897827, Accuracy: 0.706060606060606\n",
      "Step: 1320, Loss: 1.1722365617752075, Accuracy: 0.7060938682816048\n",
      "Step: 1321, Loss: 1.2238048315048218, Accuracy: 0.7060640443772063\n",
      "Step: 1322, Loss: 1.0046042203903198, Accuracy: 0.7062232300327539\n",
      "Step: 1323, Loss: 1.2282260656356812, Accuracy: 0.7061933534743202\n",
      "Step: 1324, Loss: 1.1489295959472656, Accuracy: 0.7062264150943396\n",
      "Step: 1325, Loss: 0.9060229659080505, Accuracy: 0.706447963800905\n",
      "Step: 1326, Loss: 1.3791509866714478, Accuracy: 0.7062923888470234\n",
      "Step: 1327, Loss: 1.4342468976974487, Accuracy: 0.7061370481927711\n",
      "Step: 1328, Loss: 1.1435484886169434, Accuracy: 0.7061700526711814\n",
      "Step: 1329, Loss: 1.066044807434082, Accuracy: 0.706265664160401\n",
      "Step: 1330, Loss: 1.1805535554885864, Accuracy: 0.7062985224142249\n",
      "Step: 1331, Loss: 1.2625700235366821, Accuracy: 0.7062062062062062\n",
      "Step: 1332, Loss: 1.1635783910751343, Accuracy: 0.7062390597649413\n",
      "Step: 1333, Loss: 1.2897568941116333, Accuracy: 0.7061469265367316\n",
      "Step: 1334, Loss: 1.2747130393981934, Accuracy: 0.7060549313358302\n",
      "Step: 1335, Loss: 1.0932756662368774, Accuracy: 0.7061501996007984\n",
      "Step: 1336, Loss: 1.2675305604934692, Accuracy: 0.706120668162553\n",
      "Step: 1337, Loss: 1.3402332067489624, Accuracy: 0.706028898854011\n",
      "Step: 1338, Loss: 1.2135435342788696, Accuracy: 0.7060617376151357\n",
      "Step: 1339, Loss: 1.3555580377578735, Accuracy: 0.7059079601990049\n",
      "Step: 1340, Loss: 1.1226729154586792, Accuracy: 0.7060029828486204\n",
      "Step: 1341, Loss: 1.169142723083496, Accuracy: 0.7060357675111774\n",
      "Step: 1342, Loss: 1.1959902048110962, Accuracy: 0.7060064532141971\n",
      "Step: 1343, Loss: 1.1549690961837769, Accuracy: 0.7060391865079365\n",
      "Step: 1344, Loss: 1.3575224876403809, Accuracy: 0.7058859975216852\n",
      "Step: 1345, Loss: 1.4859333038330078, Accuracy: 0.7056711243189698\n",
      "Step: 1346, Loss: 1.294009804725647, Accuracy: 0.7055803019054689\n",
      "Step: 1347, Loss: 1.1422330141067505, Accuracy: 0.7056132542037586\n",
      "Step: 1348, Loss: 1.330403447151184, Accuracy: 0.705522609340252\n",
      "Step: 1349, Loss: 1.1763343811035156, Accuracy: 0.7055555555555556\n",
      "Step: 1350, Loss: 1.2381391525268555, Accuracy: 0.7055267702936097\n",
      "Step: 1351, Loss: 1.0449703931808472, Accuracy: 0.7056213017751479\n",
      "Step: 1352, Loss: 1.1179217100143433, Accuracy: 0.7057156935205716\n",
      "Step: 1353, Loss: 1.13567054271698, Accuracy: 0.7057483998030527\n",
      "Step: 1354, Loss: 1.1265217065811157, Accuracy: 0.7058425584255843\n",
      "Step: 1355, Loss: 1.1935559511184692, Accuracy: 0.7058751229105211\n",
      "Step: 1356, Loss: 1.071083903312683, Accuracy: 0.7059690493736183\n",
      "Step: 1357, Loss: 1.296107530593872, Accuracy: 0.7058787432498773\n",
      "Step: 1358, Loss: 1.1602550745010376, Accuracy: 0.7059112092224675\n",
      "Step: 1359, Loss: 1.1419281959533691, Accuracy: 0.7059436274509804\n",
      "Step: 1360, Loss: 1.1624833345413208, Accuracy: 0.7059759980406564\n",
      "Step: 1361, Loss: 1.5727814435958862, Accuracy: 0.705702398433676\n",
      "Step: 1362, Loss: 1.1098581552505493, Accuracy: 0.7057960381511372\n",
      "Step: 1363, Loss: 1.2248517274856567, Accuracy: 0.7057673509286413\n",
      "Step: 1364, Loss: 1.3871136903762817, Accuracy: 0.7056166056166057\n",
      "Step: 1365, Loss: 1.2579251527786255, Accuracy: 0.7055270863836017\n",
      "Step: 1366, Loss: 1.2715736627578735, Accuracy: 0.7054376981224092\n",
      "Step: 1367, Loss: 1.1447429656982422, Accuracy: 0.7054702729044834\n",
      "Step: 1368, Loss: 1.0496340990066528, Accuracy: 0.705563671779888\n",
      "Step: 1369, Loss: 1.1891893148422241, Accuracy: 0.7055352798053528\n",
      "Step: 1370, Loss: 1.146384596824646, Accuracy: 0.7055677121322635\n",
      "Step: 1371, Loss: 1.1572564840316772, Accuracy: 0.7056000971817298\n",
      "Step: 1372, Loss: 1.3154258728027344, Accuracy: 0.7055110463704782\n",
      "Step: 1373, Loss: 1.372691035270691, Accuracy: 0.70536147501213\n",
      "Step: 1374, Loss: 1.096408486366272, Accuracy: 0.7054545454545454\n",
      "Step: 1375, Loss: 0.9382569193840027, Accuracy: 0.7056686046511628\n",
      "Step: 1376, Loss: 1.2046390771865845, Accuracy: 0.7056402808036795\n",
      "Step: 1377, Loss: 1.2816916704177856, Accuracy: 0.7055515239477503\n",
      "Step: 1378, Loss: 1.1081072092056274, Accuracy: 0.7056441866086536\n",
      "Step: 1379, Loss: 1.261247992515564, Accuracy: 0.7055555555555556\n",
      "Step: 1380, Loss: 1.2058284282684326, Accuracy: 0.705527395607048\n",
      "Step: 1381, Loss: 1.171289324760437, Accuracy: 0.7055595754944525\n",
      "Step: 1382, Loss: 1.2628685235977173, Accuracy: 0.705531453362256\n",
      "Step: 1383, Loss: 1.166164755821228, Accuracy: 0.7055635838150289\n",
      "Step: 1384, Loss: 1.24504554271698, Accuracy: 0.7055354993983153\n",
      "Step: 1385, Loss: 1.0597835779190063, Accuracy: 0.7056277056277056\n",
      "Step: 1386, Loss: 1.5039058923721313, Accuracy: 0.7053592886325403\n",
      "Step: 1387, Loss: 1.2337409257888794, Accuracy: 0.7053314121037464\n",
      "Step: 1388, Loss: 1.08286714553833, Accuracy: 0.7054235661147108\n",
      "Step: 1389, Loss: 0.9365000128746033, Accuracy: 0.7056354916067147\n",
      "Step: 1390, Loss: 1.0286647081375122, Accuracy: 0.7057872034507549\n",
      "Step: 1391, Loss: 1.340328335762024, Accuracy: 0.7056992337164751\n",
      "Step: 1392, Loss: 1.4965182542800903, Accuracy: 0.705491744436468\n",
      "Step: 1393, Loss: 1.3317948579788208, Accuracy: 0.7054041128646581\n",
      "Step: 1394, Loss: 1.1017539501190186, Accuracy: 0.7054360812425329\n",
      "Step: 1395, Loss: 1.3541483879089355, Accuracy: 0.7052889207258835\n",
      "Step: 1396, Loss: 1.2888330221176147, Accuracy: 0.7052612741589119\n",
      "Step: 1397, Loss: 1.2000141143798828, Accuracy: 0.7052932761087267\n",
      "Step: 1398, Loss: 1.2298749685287476, Accuracy: 0.7052656659518703\n",
      "Step: 1399, Loss: 1.084693431854248, Accuracy: 0.7053571428571429\n",
      "Step: 1400, Loss: 1.1160553693771362, Accuracy: 0.7053890078515346\n",
      "Step: 1401, Loss: 1.01042640209198, Accuracy: 0.7055397051830719\n",
      "Step: 1402, Loss: 1.3311882019042969, Accuracy: 0.7054526015680684\n",
      "Step: 1403, Loss: 1.2485512495040894, Accuracy: 0.7054249762583096\n",
      "Step: 1404, Loss: 1.1885857582092285, Accuracy: 0.7054567022538553\n",
      "Step: 1405, Loss: 1.166916012763977, Accuracy: 0.7054883831199621\n",
      "Step: 1406, Loss: 1.1707087755203247, Accuracy: 0.7055200189528548\n",
      "Step: 1407, Loss: 1.200362205505371, Accuracy: 0.7055516098484849\n",
      "Step: 1408, Loss: 1.4242225885391235, Accuracy: 0.7053465814998817\n",
      "Step: 1409, Loss: 1.08345627784729, Accuracy: 0.7054964539007093\n",
      "Step: 1410, Loss: 1.1528329849243164, Accuracy: 0.7055279943302623\n",
      "Step: 1411, Loss: 1.2663804292678833, Accuracy: 0.7055004721435316\n",
      "Step: 1412, Loss: 1.3491706848144531, Accuracy: 0.7054140127388535\n",
      "Step: 1413, Loss: 1.0557156801223755, Accuracy: 0.7055044790193306\n",
      "Step: 1414, Loss: 1.2154004573822021, Accuracy: 0.7055359246171967\n",
      "Step: 1415, Loss: 1.0718053579330444, Accuracy: 0.7056261770244822\n",
      "Step: 1416, Loss: 1.003566861152649, Accuracy: 0.7057751117384145\n",
      "Step: 1417, Loss: 1.0252982378005981, Accuracy: 0.7059238363892807\n",
      "Step: 1418, Loss: 1.075997233390808, Accuracy: 0.7060136246182758\n",
      "Step: 1419, Loss: 1.295263409614563, Accuracy: 0.7059272300469484\n",
      "Step: 1420, Loss: 1.1276122331619263, Accuracy: 0.7059582453671124\n",
      "Step: 1421, Loss: 1.2231135368347168, Accuracy: 0.7059306141584623\n",
      "Step: 1422, Loss: 1.0644614696502686, Accuracy: 0.7060201452330757\n",
      "Step: 1423, Loss: 1.1395435333251953, Accuracy: 0.7061095505617978\n",
      "Step: 1424, Loss: 1.1568197011947632, Accuracy: 0.706140350877193\n",
      "Step: 1425, Loss: 1.2332749366760254, Accuracy: 0.7061126694717158\n",
      "Step: 1426, Loss: 1.4560097455978394, Accuracy: 0.7057346414389162\n",
      "Epoch: 8, Val_Accuracy: 0.3560747663551402\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6994794258f543ea9f727b61ed06ce9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1427 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0, Loss: 1.3812702894210815, Accuracy: 0.5\n",
      "Step: 1, Loss: 1.3099004030227661, Accuracy: 0.5416666666666666\n",
      "Step: 2, Loss: 1.026016116142273, Accuracy: 0.6666666666666666\n",
      "Step: 3, Loss: 1.0909024477005005, Accuracy: 0.7083333333333334\n",
      "Step: 4, Loss: 1.0846024751663208, Accuracy: 0.7333333333333333\n",
      "Step: 5, Loss: 1.1418368816375732, Accuracy: 0.7361111111111112\n",
      "Step: 6, Loss: 1.1463398933410645, Accuracy: 0.7380952380952381\n",
      "Step: 7, Loss: 1.3770416975021362, Accuracy: 0.7083333333333334\n",
      "Step: 8, Loss: 1.0808333158493042, Accuracy: 0.7222222222222222\n",
      "Step: 9, Loss: 1.1912815570831299, Accuracy: 0.725\n",
      "Step: 10, Loss: 1.2467681169509888, Accuracy: 0.7196969696969697\n",
      "Step: 11, Loss: 1.2459087371826172, Accuracy: 0.7152777777777778\n",
      "Step: 12, Loss: 1.1899181604385376, Accuracy: 0.7115384615384616\n",
      "Step: 13, Loss: 1.0105644464492798, Accuracy: 0.7261904761904762\n",
      "Step: 14, Loss: 1.0872701406478882, Accuracy: 0.7277777777777777\n",
      "Step: 15, Loss: 1.4670292139053345, Accuracy: 0.7083333333333334\n",
      "Step: 16, Loss: 1.2517403364181519, Accuracy: 0.7058823529411765\n",
      "Step: 17, Loss: 1.408152461051941, Accuracy: 0.6898148148148148\n",
      "Step: 18, Loss: 1.076625108718872, Accuracy: 0.6973684210526315\n",
      "Step: 19, Loss: 1.144426703453064, Accuracy: 0.7\n",
      "Step: 20, Loss: 1.4225897789001465, Accuracy: 0.6904761904761905\n",
      "Step: 21, Loss: 1.302118182182312, Accuracy: 0.6856060606060606\n",
      "Step: 22, Loss: 1.0900088548660278, Accuracy: 0.6920289855072463\n",
      "Step: 23, Loss: 1.204901099205017, Accuracy: 0.6944444444444444\n",
      "Step: 24, Loss: 1.5228943824768066, Accuracy: 0.68\n",
      "Step: 25, Loss: 1.2150191068649292, Accuracy: 0.6794871794871795\n",
      "Step: 26, Loss: 1.2456804513931274, Accuracy: 0.6790123456790124\n",
      "Step: 27, Loss: 1.0735377073287964, Accuracy: 0.6845238095238095\n",
      "Step: 28, Loss: 1.1934723854064941, Accuracy: 0.6839080459770115\n",
      "Step: 29, Loss: 1.1532658338546753, Accuracy: 0.6861111111111111\n",
      "Step: 30, Loss: 1.2418574094772339, Accuracy: 0.6854838709677419\n",
      "Step: 31, Loss: 1.065475344657898, Accuracy: 0.6901041666666666\n",
      "Step: 32, Loss: 1.174294114112854, Accuracy: 0.6919191919191919\n",
      "Step: 33, Loss: 1.1458276510238647, Accuracy: 0.6936274509803921\n",
      "Step: 34, Loss: 1.243475079536438, Accuracy: 0.6928571428571428\n",
      "Step: 35, Loss: 1.1544724702835083, Accuracy: 0.6944444444444444\n",
      "Step: 36, Loss: 1.2737127542495728, Accuracy: 0.6936936936936937\n",
      "Step: 37, Loss: 1.3168411254882812, Accuracy: 0.6907894736842105\n",
      "Step: 38, Loss: 1.14425790309906, Accuracy: 0.6923076923076923\n",
      "Step: 39, Loss: 1.2927385568618774, Accuracy: 0.6895833333333333\n",
      "Step: 40, Loss: 1.27938711643219, Accuracy: 0.6869918699186992\n",
      "Step: 41, Loss: 1.1135591268539429, Accuracy: 0.6884920634920635\n",
      "Step: 42, Loss: 1.2174633741378784, Accuracy: 0.687984496124031\n",
      "Step: 43, Loss: 1.2888298034667969, Accuracy: 0.6856060606060606\n",
      "Step: 44, Loss: 1.1264468431472778, Accuracy: 0.687037037037037\n",
      "Step: 45, Loss: 1.3695937395095825, Accuracy: 0.6829710144927537\n",
      "Step: 46, Loss: 1.1235074996948242, Accuracy: 0.6843971631205674\n",
      "Step: 47, Loss: 1.1724647283554077, Accuracy: 0.6857638888888888\n",
      "Step: 48, Loss: 1.2949727773666382, Accuracy: 0.6836734693877551\n",
      "Step: 49, Loss: 1.2910921573638916, Accuracy: 0.6816666666666666\n",
      "Step: 50, Loss: 1.1950045824050903, Accuracy: 0.6830065359477124\n",
      "Step: 51, Loss: 1.1602513790130615, Accuracy: 0.6842948717948718\n",
      "Step: 52, Loss: 1.156298041343689, Accuracy: 0.6855345911949685\n",
      "Step: 53, Loss: 1.091835618019104, Accuracy: 0.6882716049382716\n",
      "Step: 54, Loss: 1.0981110334396362, Accuracy: 0.6909090909090909\n",
      "Step: 55, Loss: 1.4769620895385742, Accuracy: 0.6860119047619048\n",
      "Step: 56, Loss: 1.217083215713501, Accuracy: 0.685672514619883\n",
      "Step: 57, Loss: 1.3611249923706055, Accuracy: 0.6839080459770115\n",
      "Step: 58, Loss: 1.2962876558303833, Accuracy: 0.6822033898305084\n",
      "Step: 59, Loss: 1.0979477167129517, Accuracy: 0.6847222222222222\n",
      "Step: 60, Loss: 1.0262128114700317, Accuracy: 0.6885245901639344\n",
      "Step: 61, Loss: 1.329836130142212, Accuracy: 0.6868279569892473\n",
      "Step: 62, Loss: 1.0882272720336914, Accuracy: 0.6891534391534392\n",
      "Step: 63, Loss: 1.1486977338790894, Accuracy: 0.6901041666666666\n",
      "Step: 64, Loss: 1.1444650888442993, Accuracy: 0.691025641025641\n",
      "Step: 65, Loss: 1.1772645711898804, Accuracy: 0.6906565656565656\n",
      "Step: 66, Loss: 1.2067838907241821, Accuracy: 0.6915422885572139\n",
      "Step: 67, Loss: 1.2197357416152954, Accuracy: 0.6911764705882353\n",
      "Step: 68, Loss: 0.9994152188301086, Accuracy: 0.6944444444444444\n",
      "Step: 69, Loss: 1.0362415313720703, Accuracy: 0.6976190476190476\n",
      "Step: 70, Loss: 1.0350135564804077, Accuracy: 0.6995305164319249\n",
      "Step: 71, Loss: 1.1910046339035034, Accuracy: 0.6990740740740741\n",
      "Step: 72, Loss: 1.283052921295166, Accuracy: 0.6974885844748858\n",
      "Step: 73, Loss: 1.0780963897705078, Accuracy: 0.6993243243243243\n",
      "Step: 74, Loss: 1.3169949054718018, Accuracy: 0.6977777777777778\n",
      "Step: 75, Loss: 1.36421537399292, Accuracy: 0.6962719298245614\n",
      "Step: 76, Loss: 1.2559834718704224, Accuracy: 0.6958874458874459\n",
      "Step: 77, Loss: 1.0128272771835327, Accuracy: 0.6987179487179487\n",
      "Step: 78, Loss: 1.2366503477096558, Accuracy: 0.6983122362869199\n",
      "Step: 79, Loss: 1.1999398469924927, Accuracy: 0.6979166666666666\n",
      "Step: 80, Loss: 1.0690659284591675, Accuracy: 0.6995884773662552\n",
      "Step: 81, Loss: 1.1201698780059814, Accuracy: 0.7002032520325203\n",
      "Step: 82, Loss: 1.2494255304336548, Accuracy: 0.6997991967871486\n",
      "Step: 83, Loss: 1.1987115144729614, Accuracy: 0.6994047619047619\n",
      "Step: 84, Loss: 1.129542350769043, Accuracy: 0.7\n",
      "Step: 85, Loss: 1.0315804481506348, Accuracy: 0.7025193798449613\n",
      "Step: 86, Loss: 1.210353970527649, Accuracy: 0.7021072796934866\n",
      "Step: 87, Loss: 1.2976967096328735, Accuracy: 0.7007575757575758\n",
      "Step: 88, Loss: 1.0088354349136353, Accuracy: 0.7031835205992509\n",
      "Step: 89, Loss: 1.0803401470184326, Accuracy: 0.7046296296296296\n",
      "Step: 90, Loss: 0.9804649949073792, Accuracy: 0.706959706959707\n",
      "Step: 91, Loss: 1.2713403701782227, Accuracy: 0.7056159420289855\n",
      "Step: 92, Loss: 1.0380821228027344, Accuracy: 0.7078853046594982\n",
      "Step: 93, Loss: 1.1560953855514526, Accuracy: 0.7083333333333334\n",
      "Step: 94, Loss: 1.2405215501785278, Accuracy: 0.7078947368421052\n",
      "Step: 95, Loss: 1.2202297449111938, Accuracy: 0.7074652777777778\n",
      "Step: 96, Loss: 1.1297365427017212, Accuracy: 0.7087628865979382\n",
      "Step: 97, Loss: 1.0605027675628662, Accuracy: 0.7100340136054422\n",
      "Step: 98, Loss: 1.160945177078247, Accuracy: 0.7104377104377104\n",
      "Step: 99, Loss: 1.2464405298233032, Accuracy: 0.71\n",
      "Step: 100, Loss: 1.307451605796814, Accuracy: 0.7087458745874587\n",
      "Step: 101, Loss: 1.381685733795166, Accuracy: 0.7066993464052288\n",
      "Step: 102, Loss: 1.3848567008972168, Accuracy: 0.7046925566343042\n",
      "Step: 103, Loss: 1.3222976922988892, Accuracy: 0.7035256410256411\n",
      "Step: 104, Loss: 1.273594617843628, Accuracy: 0.7031746031746032\n",
      "Step: 105, Loss: 1.0918413400650024, Accuracy: 0.7044025157232704\n",
      "Step: 106, Loss: 1.189756989479065, Accuracy: 0.7040498442367601\n",
      "Step: 107, Loss: 1.0536667108535767, Accuracy: 0.7052469135802469\n",
      "Step: 108, Loss: 1.1326539516448975, Accuracy: 0.7048929663608563\n",
      "Step: 109, Loss: 1.2989248037338257, Accuracy: 0.7037878787878787\n",
      "Step: 110, Loss: 1.0609550476074219, Accuracy: 0.704954954954955\n",
      "Step: 111, Loss: 1.086350917816162, Accuracy: 0.7061011904761905\n",
      "Step: 112, Loss: 1.0935560464859009, Accuracy: 0.7072271386430679\n",
      "Step: 113, Loss: 1.3077445030212402, Accuracy: 0.7068713450292398\n",
      "Step: 114, Loss: 1.149598479270935, Accuracy: 0.7072463768115942\n",
      "Step: 115, Loss: 1.1419627666473389, Accuracy: 0.7076149425287356\n",
      "Step: 116, Loss: 1.2892173528671265, Accuracy: 0.7072649572649573\n",
      "Step: 117, Loss: 1.2246617078781128, Accuracy: 0.7069209039548022\n",
      "Step: 118, Loss: 1.1509418487548828, Accuracy: 0.7072829131652661\n",
      "Step: 119, Loss: 1.212272047996521, Accuracy: 0.7069444444444445\n",
      "Step: 120, Loss: 1.0873945951461792, Accuracy: 0.7079889807162535\n",
      "Step: 121, Loss: 1.1725867986679077, Accuracy: 0.7083333333333334\n",
      "Step: 122, Loss: 1.2600935697555542, Accuracy: 0.7073170731707317\n",
      "Step: 123, Loss: 1.1958729028701782, Accuracy: 0.7076612903225806\n",
      "Step: 124, Loss: 1.3083829879760742, Accuracy: 0.7066666666666667\n",
      "Step: 125, Loss: 1.111486554145813, Accuracy: 0.707010582010582\n",
      "Step: 126, Loss: 1.0948302745819092, Accuracy: 0.7073490813648294\n",
      "Step: 127, Loss: 1.2165241241455078, Accuracy: 0.7076822916666666\n",
      "Step: 128, Loss: 1.2565577030181885, Accuracy: 0.7073643410852714\n",
      "Step: 129, Loss: 1.0930465459823608, Accuracy: 0.7083333333333334\n",
      "Step: 130, Loss: 1.3094028234481812, Accuracy: 0.7073791348600509\n",
      "Step: 131, Loss: 1.392602801322937, Accuracy: 0.7058080808080808\n",
      "Step: 132, Loss: 1.1212241649627686, Accuracy: 0.706140350877193\n",
      "Step: 133, Loss: 1.3166968822479248, Accuracy: 0.7052238805970149\n",
      "Step: 134, Loss: 1.3068314790725708, Accuracy: 0.7043209876543209\n",
      "Step: 135, Loss: 1.1014255285263062, Accuracy: 0.7046568627450981\n",
      "Step: 136, Loss: 1.1519070863723755, Accuracy: 0.7049878345498783\n",
      "Step: 137, Loss: 1.273112177848816, Accuracy: 0.7041062801932367\n",
      "Step: 138, Loss: 1.3960623741149902, Accuracy: 0.7026378896882494\n",
      "Step: 139, Loss: 1.218874216079712, Accuracy: 0.7023809523809523\n",
      "Step: 140, Loss: 1.0525264739990234, Accuracy: 0.7033096926713948\n",
      "Step: 141, Loss: 1.2303444147109985, Accuracy: 0.7030516431924883\n",
      "Step: 142, Loss: 1.3081961870193481, Accuracy: 0.7022144522144522\n",
      "Step: 143, Loss: 1.0551446676254272, Accuracy: 0.703125\n",
      "Step: 144, Loss: 1.1829169988632202, Accuracy: 0.7028735632183908\n",
      "Step: 145, Loss: 0.9765329957008362, Accuracy: 0.704337899543379\n",
      "Step: 146, Loss: 0.9797866940498352, Accuracy: 0.70578231292517\n",
      "Step: 147, Loss: 1.1907380819320679, Accuracy: 0.706081081081081\n",
      "Step: 148, Loss: 1.061942219734192, Accuracy: 0.7069351230425056\n",
      "Step: 149, Loss: 1.1499028205871582, Accuracy: 0.7072222222222222\n",
      "Step: 150, Loss: 1.429052472114563, Accuracy: 0.7052980132450332\n",
      "Step: 151, Loss: 1.143452763557434, Accuracy: 0.7055921052631579\n",
      "Step: 152, Loss: 1.2579224109649658, Accuracy: 0.7047930283224401\n",
      "Step: 153, Loss: 1.050595760345459, Accuracy: 0.7056277056277056\n",
      "Step: 154, Loss: 1.2059160470962524, Accuracy: 0.7053763440860215\n",
      "Step: 155, Loss: 1.1452094316482544, Accuracy: 0.7056623931623932\n",
      "Step: 156, Loss: 1.588321328163147, Accuracy: 0.7032908704883227\n",
      "Step: 157, Loss: 0.9957118630409241, Accuracy: 0.7046413502109705\n",
      "Step: 158, Loss: 1.165481686592102, Accuracy: 0.7049266247379455\n",
      "Step: 159, Loss: 1.0590547323226929, Accuracy: 0.70625\n",
      "Step: 160, Loss: 1.1837393045425415, Accuracy: 0.7065217391304348\n",
      "Step: 161, Loss: 1.1335225105285645, Accuracy: 0.7067901234567902\n",
      "Step: 162, Loss: 1.113992691040039, Accuracy: 0.7070552147239264\n",
      "Step: 163, Loss: 1.2083508968353271, Accuracy: 0.7073170731707317\n",
      "Step: 164, Loss: 1.3363451957702637, Accuracy: 0.7065656565656566\n",
      "Step: 165, Loss: 1.212231993675232, Accuracy: 0.7068273092369478\n",
      "Step: 166, Loss: 1.1427644491195679, Accuracy: 0.7070858283433133\n",
      "Step: 167, Loss: 1.3871221542358398, Accuracy: 0.7058531746031746\n",
      "Step: 168, Loss: 1.1105390787124634, Accuracy: 0.7066074950690335\n",
      "Step: 169, Loss: 1.2965953350067139, Accuracy: 0.7058823529411765\n",
      "Step: 170, Loss: 1.0179063081741333, Accuracy: 0.7071150097465887\n",
      "Step: 171, Loss: 1.1187881231307983, Accuracy: 0.7073643410852714\n",
      "Step: 172, Loss: 1.2311304807662964, Accuracy: 0.7071290944123314\n",
      "Step: 173, Loss: 1.1047215461730957, Accuracy: 0.7078544061302682\n",
      "Step: 174, Loss: 1.2307628393173218, Accuracy: 0.7076190476190476\n",
      "Step: 175, Loss: 1.3500958681106567, Accuracy: 0.7069128787878788\n",
      "Step: 176, Loss: 1.1579604148864746, Accuracy: 0.7071563088512242\n",
      "Step: 177, Loss: 1.1422122716903687, Accuracy: 0.7073970037453183\n",
      "Step: 178, Loss: 1.301879644393921, Accuracy: 0.7067039106145251\n",
      "Step: 179, Loss: 1.1314576864242554, Accuracy: 0.7069444444444445\n",
      "Step: 180, Loss: 1.0302022695541382, Accuracy: 0.7081031307550645\n",
      "Step: 181, Loss: 1.1252492666244507, Accuracy: 0.7087912087912088\n",
      "Step: 182, Loss: 1.2645221948623657, Accuracy: 0.7085610200364298\n",
      "Step: 183, Loss: 1.2523471117019653, Accuracy: 0.7078804347826086\n",
      "Step: 184, Loss: 1.1887263059616089, Accuracy: 0.7081081081081081\n",
      "Step: 185, Loss: 1.0421618223190308, Accuracy: 0.7087813620071685\n",
      "Step: 186, Loss: 1.1928989887237549, Accuracy: 0.7090017825311943\n",
      "Step: 187, Loss: 1.0664558410644531, Accuracy: 0.7096631205673759\n",
      "Step: 188, Loss: 1.075184941291809, Accuracy: 0.7103174603174603\n",
      "Step: 189, Loss: 1.0455132722854614, Accuracy: 0.7114035087719298\n",
      "Step: 190, Loss: 1.3084672689437866, Accuracy: 0.7107329842931938\n",
      "Step: 191, Loss: 1.2193458080291748, Accuracy: 0.7109375\n",
      "Step: 192, Loss: 0.9333168864250183, Accuracy: 0.7124352331606217\n",
      "Step: 193, Loss: 1.208626389503479, Accuracy: 0.7126288659793815\n",
      "Step: 194, Loss: 1.0187509059906006, Accuracy: 0.7136752136752137\n",
      "Step: 195, Loss: 1.1060665845870972, Accuracy: 0.7138605442176871\n",
      "Step: 196, Loss: 1.1980690956115723, Accuracy: 0.7136209813874789\n",
      "Step: 197, Loss: 1.1560640335083008, Accuracy: 0.7138047138047138\n",
      "Step: 198, Loss: 1.16828453540802, Accuracy: 0.7139865996649917\n",
      "Step: 199, Loss: 1.3601077795028687, Accuracy: 0.7133333333333334\n",
      "Step: 200, Loss: 1.267320990562439, Accuracy: 0.7131011608623549\n",
      "Step: 201, Loss: 1.161811351776123, Accuracy: 0.7132838283828383\n",
      "Step: 202, Loss: 1.114182949066162, Accuracy: 0.7138752052545156\n",
      "Step: 203, Loss: 1.1569651365280151, Accuracy: 0.7140522875816994\n",
      "Step: 204, Loss: 1.2694839239120483, Accuracy: 0.7138211382113822\n",
      "Step: 205, Loss: 1.0848822593688965, Accuracy: 0.7144012944983819\n",
      "Step: 206, Loss: 1.3314133882522583, Accuracy: 0.7133655394524959\n",
      "Step: 207, Loss: 1.315014123916626, Accuracy: 0.7127403846153846\n",
      "Step: 208, Loss: 1.1976157426834106, Accuracy: 0.7125199362041468\n",
      "Step: 209, Loss: 0.9920830726623535, Accuracy: 0.7134920634920635\n",
      "Step: 210, Loss: 1.2387784719467163, Accuracy: 0.7132701421800948\n",
      "Step: 211, Loss: 1.0879782438278198, Accuracy: 0.7138364779874213\n",
      "Step: 212, Loss: 1.1267476081848145, Accuracy: 0.7140062597809077\n",
      "Step: 213, Loss: 1.1274062395095825, Accuracy: 0.7141744548286605\n",
      "Step: 214, Loss: 1.2793159484863281, Accuracy: 0.7135658914728682\n",
      "Step: 215, Loss: 1.2519972324371338, Accuracy: 0.7133487654320988\n",
      "Step: 216, Loss: 1.3142584562301636, Accuracy: 0.7127496159754224\n",
      "Step: 217, Loss: 1.2495142221450806, Accuracy: 0.7125382262996942\n",
      "Step: 218, Loss: 1.0513650178909302, Accuracy: 0.713089802130898\n",
      "Step: 219, Loss: 0.9832713007926941, Accuracy: 0.7140151515151515\n",
      "Step: 220, Loss: 1.2032357454299927, Accuracy: 0.7141779788838613\n",
      "Step: 221, Loss: 1.301508903503418, Accuracy: 0.7135885885885885\n",
      "Step: 222, Loss: 1.0785099267959595, Accuracy: 0.7141255605381166\n",
      "Step: 223, Loss: 1.1152722835540771, Accuracy: 0.7146577380952381\n",
      "Step: 224, Loss: 1.1610784530639648, Accuracy: 0.7148148148148148\n",
      "Step: 225, Loss: 1.1685951948165894, Accuracy: 0.7149705014749262\n",
      "Step: 226, Loss: 1.231507658958435, Accuracy: 0.7147577092511013\n",
      "Step: 227, Loss: 0.9493381381034851, Accuracy: 0.7160087719298246\n",
      "Step: 228, Loss: 1.0946112871170044, Accuracy: 0.7165211062590975\n",
      "Step: 229, Loss: 1.1064629554748535, Accuracy: 0.7170289855072464\n",
      "Step: 230, Loss: 1.1608384847640991, Accuracy: 0.7171717171717171\n",
      "Step: 231, Loss: 1.1775882244110107, Accuracy: 0.7173132183908046\n",
      "Step: 232, Loss: 1.2373045682907104, Accuracy: 0.7170958512160229\n",
      "Step: 233, Loss: 1.2252272367477417, Accuracy: 0.7168803418803419\n",
      "Step: 234, Loss: 1.11294686794281, Accuracy: 0.7173758865248226\n",
      "Step: 235, Loss: 1.1281886100769043, Accuracy: 0.7175141242937854\n",
      "Step: 236, Loss: 1.0711232423782349, Accuracy: 0.7180028129395218\n",
      "Step: 237, Loss: 1.1688131093978882, Accuracy: 0.7181372549019608\n",
      "Step: 238, Loss: 1.1256675720214844, Accuracy: 0.7182705718270572\n",
      "Step: 239, Loss: 1.0316418409347534, Accuracy: 0.71875\n",
      "Step: 240, Loss: 1.2562079429626465, Accuracy: 0.7181881051175657\n",
      "Step: 241, Loss: 1.0352071523666382, Accuracy: 0.71900826446281\n",
      "Step: 242, Loss: 1.2751222848892212, Accuracy: 0.7184499314128944\n",
      "Step: 243, Loss: 1.1960941553115845, Accuracy: 0.7185792349726776\n",
      "Step: 244, Loss: 1.2205382585525513, Accuracy: 0.7183673469387755\n",
      "Step: 245, Loss: 1.2149133682250977, Accuracy: 0.7184959349593496\n",
      "Step: 246, Loss: 1.0940839052200317, Accuracy: 0.7189608636977058\n",
      "Step: 247, Loss: 1.1633621454238892, Accuracy: 0.7190860215053764\n",
      "Step: 248, Loss: 1.2264255285263062, Accuracy: 0.7188755020080321\n",
      "Step: 249, Loss: 1.3959932327270508, Accuracy: 0.718\n",
      "Step: 250, Loss: 1.1632276773452759, Accuracy: 0.7181274900398407\n",
      "Step: 251, Loss: 1.3737307786941528, Accuracy: 0.7172619047619048\n",
      "Step: 252, Loss: 1.54953134059906, Accuracy: 0.7157444005270093\n",
      "Step: 253, Loss: 1.1462666988372803, Accuracy: 0.7158792650918635\n",
      "Step: 254, Loss: 1.1149327754974365, Accuracy: 0.7163398692810458\n",
      "Step: 255, Loss: 1.2267132997512817, Accuracy: 0.7161458333333334\n",
      "Step: 256, Loss: 0.9892705082893372, Accuracy: 0.7169260700389105\n",
      "Step: 257, Loss: 1.2342779636383057, Accuracy: 0.7167312661498708\n",
      "Step: 258, Loss: 1.0770689249038696, Accuracy: 0.7171814671814671\n",
      "Step: 259, Loss: 0.9972588419914246, Accuracy: 0.717948717948718\n",
      "Step: 260, Loss: 1.1349674463272095, Accuracy: 0.7183908045977011\n",
      "Step: 261, Loss: 1.1510292291641235, Accuracy: 0.7185114503816794\n",
      "Step: 262, Loss: 1.101773977279663, Accuracy: 0.7189480354879595\n",
      "Step: 263, Loss: 1.321959376335144, Accuracy: 0.7184343434343434\n",
      "Step: 264, Loss: 1.2438198328018188, Accuracy: 0.7182389937106918\n",
      "Step: 265, Loss: 1.1570614576339722, Accuracy: 0.718358395989975\n",
      "Step: 266, Loss: 1.3803726434707642, Accuracy: 0.7175405742821473\n",
      "Step: 267, Loss: 1.3958574533462524, Accuracy: 0.716728855721393\n",
      "Step: 268, Loss: 1.1585742235183716, Accuracy: 0.7168525402726146\n",
      "Step: 269, Loss: 1.1861225366592407, Accuracy: 0.7166666666666667\n",
      "Step: 270, Loss: 1.4274965524673462, Accuracy: 0.7158671586715867\n",
      "Step: 271, Loss: 1.1068867444992065, Accuracy: 0.7162990196078431\n",
      "Step: 272, Loss: 1.2997575998306274, Accuracy: 0.7161172161172161\n",
      "Step: 273, Loss: 1.031952977180481, Accuracy: 0.7165450121654501\n",
      "Step: 274, Loss: 1.1671407222747803, Accuracy: 0.7166666666666667\n",
      "Step: 275, Loss: 1.2046226263046265, Accuracy: 0.7164855072463768\n",
      "Step: 276, Loss: 1.0328890085220337, Accuracy: 0.717208182912154\n",
      "Step: 277, Loss: 1.1849149465560913, Accuracy: 0.717326139088729\n",
      "Step: 278, Loss: 1.2537345886230469, Accuracy: 0.7171445639187575\n",
      "Step: 279, Loss: 1.280530333518982, Accuracy: 0.7166666666666667\n",
      "Step: 280, Loss: 1.2220441102981567, Accuracy: 0.7164887307236062\n",
      "Step: 281, Loss: 1.0390926599502563, Accuracy: 0.716903073286052\n",
      "Step: 282, Loss: 1.1470500230789185, Accuracy: 0.717020023557126\n",
      "Step: 283, Loss: 1.0450373888015747, Accuracy: 0.7177230046948356\n",
      "Step: 284, Loss: 1.324337363243103, Accuracy: 0.7172514619883041\n",
      "Step: 285, Loss: 1.1932475566864014, Accuracy: 0.7170745920745921\n",
      "Step: 286, Loss: 1.1587108373641968, Accuracy: 0.7171893147502904\n",
      "Step: 287, Loss: 1.0223053693771362, Accuracy: 0.7178819444444444\n",
      "Step: 288, Loss: 1.2348254919052124, Accuracy: 0.7177047289504037\n",
      "Step: 289, Loss: 1.0292762517929077, Accuracy: 0.7183908045977011\n",
      "Step: 290, Loss: 1.2224369049072266, Accuracy: 0.718213058419244\n",
      "Step: 291, Loss: 1.1419254541397095, Accuracy: 0.7183219178082192\n",
      "Step: 292, Loss: 1.2627958059310913, Accuracy: 0.7181456200227532\n",
      "Step: 293, Loss: 1.3072854280471802, Accuracy: 0.717687074829932\n",
      "Step: 294, Loss: 1.0829061269760132, Accuracy: 0.7180790960451977\n",
      "Step: 295, Loss: 1.3675481081008911, Accuracy: 0.7173423423423423\n",
      "Step: 296, Loss: 1.2199280261993408, Accuracy: 0.7171717171717171\n",
      "Step: 297, Loss: 1.328074336051941, Accuracy: 0.7167225950782998\n",
      "Step: 298, Loss: 1.2474956512451172, Accuracy: 0.7165551839464883\n",
      "Step: 299, Loss: 1.0121806859970093, Accuracy: 0.7172222222222222\n",
      "Step: 300, Loss: 1.1734817028045654, Accuracy: 0.7173311184939092\n",
      "Step: 301, Loss: 1.0819772481918335, Accuracy: 0.7177152317880795\n",
      "Step: 302, Loss: 1.1466130018234253, Accuracy: 0.7178217821782178\n",
      "Step: 303, Loss: 1.3092172145843506, Accuracy: 0.7173793859649122\n",
      "Step: 304, Loss: 1.0600043535232544, Accuracy: 0.7180327868852459\n",
      "Step: 305, Loss: 0.9930351376533508, Accuracy: 0.718681917211329\n",
      "Step: 306, Loss: 1.305765151977539, Accuracy: 0.7182410423452769\n",
      "Step: 307, Loss: 1.3501296043395996, Accuracy: 0.7178030303030303\n",
      "Step: 308, Loss: 1.0930817127227783, Accuracy: 0.7179072276159655\n",
      "Step: 309, Loss: 1.304071307182312, Accuracy: 0.7174731182795699\n",
      "Step: 310, Loss: 1.3075886964797974, Accuracy: 0.7170418006430869\n",
      "Step: 311, Loss: 1.5818161964416504, Accuracy: 0.7158119658119658\n",
      "Step: 312, Loss: 1.162333607673645, Accuracy: 0.7159211927582535\n",
      "Step: 313, Loss: 1.37847900390625, Accuracy: 0.7152335456475584\n",
      "Step: 314, Loss: 1.2432721853256226, Accuracy: 0.7150793650793651\n",
      "Step: 315, Loss: 1.0967923402786255, Accuracy: 0.7154535864978903\n",
      "Step: 316, Loss: 1.3503046035766602, Accuracy: 0.7150368033648791\n",
      "Step: 317, Loss: 1.3277673721313477, Accuracy: 0.714622641509434\n",
      "Step: 318, Loss: 1.2191109657287598, Accuracy: 0.7144723092998955\n",
      "Step: 319, Loss: 1.1107816696166992, Accuracy: 0.71484375\n",
      "Step: 320, Loss: 1.0739632844924927, Accuracy: 0.7152128764278297\n",
      "Step: 321, Loss: 1.4644932746887207, Accuracy: 0.7142857142857143\n",
      "Step: 322, Loss: 1.1184834241867065, Accuracy: 0.7146542827657378\n",
      "Step: 323, Loss: 1.1373475790023804, Accuracy: 0.7147633744855967\n",
      "Step: 324, Loss: 1.1384400129318237, Accuracy: 0.7148717948717949\n",
      "Step: 325, Loss: 1.2239189147949219, Accuracy: 0.7147239263803681\n",
      "Step: 326, Loss: 1.1799665689468384, Accuracy: 0.7148318042813455\n",
      "Step: 327, Loss: 1.077646017074585, Accuracy: 0.7151930894308943\n",
      "Step: 328, Loss: 1.1213880777359009, Accuracy: 0.7155521783181358\n",
      "Step: 329, Loss: 1.0380769968032837, Accuracy: 0.7161616161616161\n",
      "Step: 330, Loss: 1.0863147974014282, Accuracy: 0.716515609264854\n",
      "Step: 331, Loss: 1.2304688692092896, Accuracy: 0.7163654618473896\n",
      "Step: 332, Loss: 1.1864591836929321, Accuracy: 0.7164664664664665\n",
      "Step: 333, Loss: 1.159080147743225, Accuracy: 0.716566866267465\n",
      "Step: 334, Loss: 1.0855149030685425, Accuracy: 0.7169154228855721\n",
      "Step: 335, Loss: 1.2739341259002686, Accuracy: 0.7165178571428571\n",
      "Step: 336, Loss: 1.1555622816085815, Accuracy: 0.7166172106824926\n",
      "Step: 337, Loss: 1.1347883939743042, Accuracy: 0.716715976331361\n",
      "Step: 338, Loss: 1.164231300354004, Accuracy: 0.7168141592920354\n",
      "Step: 339, Loss: 1.2461804151535034, Accuracy: 0.7166666666666667\n",
      "Step: 340, Loss: 1.2993415594100952, Accuracy: 0.716275659824047\n",
      "Step: 341, Loss: 1.0861728191375732, Accuracy: 0.7166179337231969\n",
      "Step: 342, Loss: 0.993781566619873, Accuracy: 0.717201166180758\n",
      "Step: 343, Loss: 0.9945803284645081, Accuracy: 0.717781007751938\n",
      "Step: 344, Loss: 1.227399468421936, Accuracy: 0.7176328502415459\n",
      "Step: 345, Loss: 1.2477213144302368, Accuracy: 0.7174855491329479\n",
      "Step: 346, Loss: 1.2076066732406616, Accuracy: 0.7173390970220941\n",
      "Step: 347, Loss: 1.2624653577804565, Accuracy: 0.7171934865900383\n",
      "Step: 348, Loss: 1.2570260763168335, Accuracy: 0.7170487106017192\n",
      "Step: 349, Loss: 1.111883521080017, Accuracy: 0.7173809523809523\n",
      "Step: 350, Loss: 1.3160003423690796, Accuracy: 0.7169990503323836\n",
      "Step: 351, Loss: 1.201585054397583, Accuracy: 0.7168560606060606\n",
      "Step: 352, Loss: 1.1549601554870605, Accuracy: 0.7171860245514636\n",
      "Step: 353, Loss: 1.1061128377914429, Accuracy: 0.7172787193973634\n",
      "Step: 354, Loss: 1.2304809093475342, Accuracy: 0.7171361502347418\n",
      "Step: 355, Loss: 1.0278420448303223, Accuracy: 0.7176966292134831\n",
      "Step: 356, Loss: 1.1387759447097778, Accuracy: 0.7177871148459384\n",
      "Step: 357, Loss: 1.10968816280365, Accuracy: 0.7181098696461825\n",
      "Step: 358, Loss: 1.2887356281280518, Accuracy: 0.717966573816156\n",
      "Step: 359, Loss: 1.2289758920669556, Accuracy: 0.7178240740740741\n",
      "Step: 360, Loss: 1.0219238996505737, Accuracy: 0.7183748845798708\n",
      "Step: 361, Loss: 1.0920805931091309, Accuracy: 0.7184622467771639\n",
      "Step: 362, Loss: 1.2967489957809448, Accuracy: 0.7180899908172635\n",
      "Step: 363, Loss: 1.3232395648956299, Accuracy: 0.7174908424908425\n",
      "Step: 364, Loss: 1.168620228767395, Accuracy: 0.7175799086757991\n",
      "Step: 365, Loss: 1.150133490562439, Accuracy: 0.7176684881602914\n",
      "Step: 366, Loss: 1.189673900604248, Accuracy: 0.7177565849227975\n",
      "Step: 367, Loss: 1.0850715637207031, Accuracy: 0.7178442028985508\n",
      "Step: 368, Loss: 1.1782629489898682, Accuracy: 0.7179313459801264\n",
      "Step: 369, Loss: 1.446699619293213, Accuracy: 0.7171171171171171\n",
      "Step: 370, Loss: 1.12548828125, Accuracy: 0.7172057502246182\n",
      "Step: 371, Loss: 1.1398849487304688, Accuracy: 0.7172939068100358\n",
      "Step: 372, Loss: 1.3198261260986328, Accuracy: 0.7167113494191242\n",
      "Step: 373, Loss: 1.1821885108947754, Accuracy: 0.7165775401069518\n",
      "Step: 374, Loss: 1.0692323446273804, Accuracy: 0.7168888888888889\n",
      "Step: 375, Loss: 1.0320103168487549, Accuracy: 0.7174202127659575\n",
      "Step: 376, Loss: 1.3539594411849976, Accuracy: 0.7166224580017684\n",
      "Step: 377, Loss: 1.2133541107177734, Accuracy: 0.7164902998236331\n",
      "Step: 378, Loss: 1.1994825601577759, Accuracy: 0.7165787159190853\n",
      "Step: 379, Loss: 1.209065556526184, Accuracy: 0.7164473684210526\n",
      "Step: 380, Loss: 1.4546561241149902, Accuracy: 0.715660542432196\n",
      "Step: 381, Loss: 1.3220713138580322, Accuracy: 0.7153141361256544\n",
      "Step: 382, Loss: 1.310122013092041, Accuracy: 0.7149695387293299\n",
      "Step: 383, Loss: 1.1070245504379272, Accuracy: 0.7152777777777778\n",
      "Step: 384, Loss: 1.3920825719833374, Accuracy: 0.7147186147186148\n",
      "Step: 385, Loss: 1.2190200090408325, Accuracy: 0.7145941278065631\n",
      "Step: 386, Loss: 1.0820906162261963, Accuracy: 0.7149009474590869\n",
      "Step: 387, Loss: 1.136818528175354, Accuracy: 0.7149914089347079\n",
      "Step: 388, Loss: 1.1033331155776978, Accuracy: 0.7152956298200515\n",
      "Step: 389, Loss: 1.3953989744186401, Accuracy: 0.7147435897435898\n",
      "Step: 390, Loss: 1.0965036153793335, Accuracy: 0.7150468883205456\n",
      "Step: 391, Loss: 1.0766452550888062, Accuracy: 0.7153486394557823\n",
      "Step: 392, Loss: 1.1571826934814453, Accuracy: 0.7154368108566582\n",
      "Step: 393, Loss: 1.1601897478103638, Accuracy: 0.7155245346869712\n",
      "Step: 394, Loss: 1.07927405834198, Accuracy: 0.7158227848101266\n",
      "Step: 395, Loss: 1.1794886589050293, Accuracy: 0.7159090909090909\n",
      "Step: 396, Loss: 1.1889110803604126, Accuracy: 0.7159949622166247\n",
      "Step: 397, Loss: 1.3830903768539429, Accuracy: 0.7156616415410385\n",
      "Step: 398, Loss: 1.1134968996047974, Accuracy: 0.7157477025898078\n",
      "Step: 399, Loss: 1.335650086402893, Accuracy: 0.7154166666666667\n",
      "Step: 400, Loss: 1.2130049467086792, Accuracy: 0.7152950955943475\n",
      "Step: 401, Loss: 1.046932578086853, Accuracy: 0.7155887230514096\n",
      "Step: 402, Loss: 1.3265899419784546, Accuracy: 0.7152605459057072\n",
      "Step: 403, Loss: 1.1586090326309204, Accuracy: 0.7153465346534653\n",
      "Step: 404, Loss: 1.2518454790115356, Accuracy: 0.7152263374485597\n",
      "Step: 405, Loss: 1.0480693578720093, Accuracy: 0.7155172413793104\n",
      "Step: 406, Loss: 1.2735599279403687, Accuracy: 0.7153972153972153\n",
      "Step: 407, Loss: 1.2107350826263428, Accuracy: 0.7154820261437909\n",
      "Step: 408, Loss: 1.1663894653320312, Accuracy: 0.7155664221678891\n",
      "Step: 409, Loss: 1.128906488418579, Accuracy: 0.7156504065040651\n",
      "Step: 410, Loss: 1.2560299634933472, Accuracy: 0.7155312246553123\n",
      "Step: 411, Loss: 1.1442128419876099, Accuracy: 0.7158171521035599\n",
      "Step: 412, Loss: 0.9356098175048828, Accuracy: 0.7165052461662631\n",
      "Step: 413, Loss: 0.9979648590087891, Accuracy: 0.716988727858293\n",
      "Step: 414, Loss: 1.1449893712997437, Accuracy: 0.7170682730923694\n",
      "Step: 415, Loss: 1.2528029680252075, Accuracy: 0.7169471153846154\n",
      "Step: 416, Loss: 1.1021171808242798, Accuracy: 0.7172262190247801\n",
      "Step: 417, Loss: 0.9442576766014099, Accuracy: 0.7179027113237639\n",
      "Step: 418, Loss: 1.0482271909713745, Accuracy: 0.7183770883054893\n",
      "Step: 419, Loss: 1.225472092628479, Accuracy: 0.7182539682539683\n",
      "Step: 420, Loss: 1.082708716392517, Accuracy: 0.7185273159144893\n",
      "Step: 421, Loss: 1.2364543676376343, Accuracy: 0.7184044233807267\n",
      "Step: 422, Loss: 0.9314855933189392, Accuracy: 0.719070133963751\n",
      "Step: 423, Loss: 1.1198891401290894, Accuracy: 0.7191430817610063\n",
      "Step: 424, Loss: 1.244345784187317, Accuracy: 0.7190196078431372\n",
      "Step: 425, Loss: 1.1567703485488892, Accuracy: 0.7190923317683882\n",
      "Step: 426, Loss: 1.022067904472351, Accuracy: 0.7195550351288056\n",
      "Step: 427, Loss: 1.2026032209396362, Accuracy: 0.7194314641744548\n",
      "Step: 428, Loss: 1.2787652015686035, Accuracy: 0.7191142191142191\n",
      "Step: 429, Loss: 1.301573634147644, Accuracy: 0.7187984496124031\n",
      "Step: 430, Loss: 1.1272324323654175, Accuracy: 0.7188708430007734\n",
      "Step: 431, Loss: 1.154944658279419, Accuracy: 0.7189429012345679\n",
      "Step: 432, Loss: 1.4791446924209595, Accuracy: 0.7182448036951501\n",
      "Step: 433, Loss: 1.2933275699615479, Accuracy: 0.7179339477726574\n",
      "Step: 434, Loss: 1.1340911388397217, Accuracy: 0.7181992337164751\n",
      "Step: 435, Loss: 1.1358426809310913, Accuracy: 0.7182721712538226\n",
      "Step: 436, Loss: 1.1714736223220825, Accuracy: 0.7183447749809306\n",
      "Step: 437, Loss: 1.292284369468689, Accuracy: 0.7180365296803652\n",
      "Step: 438, Loss: 1.1560615301132202, Accuracy: 0.7179195140470767\n",
      "Step: 439, Loss: 1.241110920906067, Accuracy: 0.7178030303030303\n",
      "Step: 440, Loss: 1.1080610752105713, Accuracy: 0.7180650037792895\n",
      "Step: 441, Loss: 1.213612675666809, Accuracy: 0.717948717948718\n",
      "Step: 442, Loss: 1.482527256011963, Accuracy: 0.7172686230248307\n",
      "Step: 443, Loss: 0.929697573184967, Accuracy: 0.7179054054054054\n",
      "Step: 444, Loss: 1.0926588773727417, Accuracy: 0.7181647940074907\n",
      "Step: 445, Loss: 1.2870200872421265, Accuracy: 0.7178624813153961\n",
      "Step: 446, Loss: 1.0333093404769897, Accuracy: 0.7183072334079046\n",
      "Step: 447, Loss: 1.113655686378479, Accuracy: 0.7185639880952381\n",
      "Step: 448, Loss: 1.307512640953064, Accuracy: 0.7182628062360802\n",
      "Step: 449, Loss: 1.0890356302261353, Accuracy: 0.7187037037037037\n",
      "Step: 450, Loss: 1.3239277601242065, Accuracy: 0.7184035476718403\n",
      "Step: 451, Loss: 1.4297815561294556, Accuracy: 0.7177359882005899\n",
      "Step: 452, Loss: 1.4331140518188477, Accuracy: 0.7170713760117734\n",
      "Step: 453, Loss: 1.4068676233291626, Accuracy: 0.7165932452276065\n",
      "Step: 454, Loss: 1.1590834856033325, Accuracy: 0.7166666666666667\n",
      "Step: 455, Loss: 1.399196743965149, Accuracy: 0.7161915204678363\n",
      "Step: 456, Loss: 1.1792272329330444, Accuracy: 0.7162654996353027\n",
      "Step: 457, Loss: 0.9931471347808838, Accuracy: 0.7167030567685589\n",
      "Step: 458, Loss: 1.280004858970642, Accuracy: 0.7164124909222949\n",
      "Step: 459, Loss: 1.1286845207214355, Accuracy: 0.7166666666666667\n",
      "Step: 460, Loss: 1.2284760475158691, Accuracy: 0.7165582067968185\n",
      "Step: 461, Loss: 1.1839839220046997, Accuracy: 0.7166305916305916\n",
      "Step: 462, Loss: 1.1708130836486816, Accuracy: 0.7167026637868971\n",
      "Step: 463, Loss: 1.1998835802078247, Accuracy: 0.7165948275862069\n",
      "Step: 464, Loss: 1.2540977001190186, Accuracy: 0.7164874551971326\n",
      "Step: 465, Loss: 1.1831473112106323, Accuracy: 0.7165593705293276\n",
      "Step: 466, Loss: 1.252957820892334, Accuracy: 0.716452533904354\n",
      "Step: 467, Loss: 1.0769858360290527, Accuracy: 0.7167022792022792\n",
      "Step: 468, Loss: 1.04106605052948, Accuracy: 0.7171286425017769\n",
      "Step: 469, Loss: 1.1775039434432983, Accuracy: 0.7171985815602837\n",
      "Step: 470, Loss: 1.2277212142944336, Accuracy: 0.7170912951167728\n",
      "Step: 471, Loss: 1.1229156255722046, Accuracy: 0.7171610169491526\n",
      "Step: 472, Loss: 1.2091766595840454, Accuracy: 0.7170542635658915\n",
      "Step: 473, Loss: 0.9666687846183777, Accuracy: 0.7176511954992968\n",
      "Step: 474, Loss: 1.29324209690094, Accuracy: 0.7173684210526315\n",
      "Step: 475, Loss: 1.187644124031067, Accuracy: 0.717436974789916\n",
      "Step: 476, Loss: 1.261651635169983, Accuracy: 0.717330538085255\n",
      "Step: 477, Loss: 1.0367428064346313, Accuracy: 0.7175732217573222\n",
      "Step: 478, Loss: 1.1260812282562256, Accuracy: 0.7176409185803758\n",
      "Step: 479, Loss: 1.2044347524642944, Accuracy: 0.7177083333333333\n",
      "Step: 480, Loss: 1.22742760181427, Accuracy: 0.7176022176022177\n",
      "Step: 481, Loss: 1.1198095083236694, Accuracy: 0.7178423236514523\n",
      "Step: 482, Loss: 1.1934285163879395, Accuracy: 0.7179089026915114\n",
      "Step: 483, Loss: 1.0501220226287842, Accuracy: 0.7181473829201102\n",
      "Step: 484, Loss: 0.909684956073761, Accuracy: 0.7187285223367698\n",
      "Step: 485, Loss: 1.212943434715271, Accuracy: 0.7186213991769548\n",
      "Step: 486, Loss: 1.1166175603866577, Accuracy: 0.7188569472963724\n",
      "Step: 487, Loss: 1.6190924644470215, Accuracy: 0.717896174863388\n",
      "Step: 488, Loss: 1.001076340675354, Accuracy: 0.7183026584867076\n",
      "Step: 489, Loss: 1.1933026313781738, Accuracy: 0.7183673469387755\n",
      "Step: 490, Loss: 1.2232805490493774, Accuracy: 0.7182620502376104\n",
      "Step: 491, Loss: 0.9982593059539795, Accuracy: 0.7186653116531165\n",
      "Step: 492, Loss: 1.1554663181304932, Accuracy: 0.7187288708586883\n",
      "Step: 493, Loss: 1.2003297805786133, Accuracy: 0.7186234817813765\n",
      "Step: 494, Loss: 1.0113575458526611, Accuracy: 0.719023569023569\n",
      "Step: 495, Loss: 1.2593711614608765, Accuracy: 0.7189180107526881\n",
      "Step: 496, Loss: 1.386841893196106, Accuracy: 0.7184775318578136\n",
      "Step: 497, Loss: 1.1616113185882568, Accuracy: 0.7185408299866132\n",
      "Step: 498, Loss: 1.2479908466339111, Accuracy: 0.7182698730794923\n",
      "Step: 499, Loss: 1.439217209815979, Accuracy: 0.7176666666666667\n",
      "Step: 500, Loss: 1.3813804388046265, Accuracy: 0.7172322022621423\n",
      "Step: 501, Loss: 1.1891566514968872, Accuracy: 0.7171314741035857\n",
      "Step: 502, Loss: 1.2371028661727905, Accuracy: 0.7170311464546058\n",
      "Step: 503, Loss: 1.1699674129486084, Accuracy: 0.7170965608465608\n",
      "Step: 504, Loss: 1.1001688241958618, Accuracy: 0.7173267326732673\n",
      "Step: 505, Loss: 1.1967781782150269, Accuracy: 0.7172266139657444\n",
      "Step: 506, Loss: 1.0722979307174683, Accuracy: 0.7174556213017751\n",
      "Step: 507, Loss: 1.2501856088638306, Accuracy: 0.7173556430446194\n",
      "Step: 508, Loss: 1.0293796062469482, Accuracy: 0.7177472167648985\n",
      "Step: 509, Loss: 1.3370050191879272, Accuracy: 0.717483660130719\n",
      "Step: 510, Loss: 1.0965598821640015, Accuracy: 0.7177103718199609\n",
      "Step: 511, Loss: 1.1578181982040405, Accuracy: 0.7177734375\n",
      "Step: 512, Loss: 1.0426888465881348, Accuracy: 0.7179987004548408\n",
      "Step: 513, Loss: 1.110545039176941, Accuracy: 0.7182230869001297\n",
      "Step: 514, Loss: 1.1625516414642334, Accuracy: 0.718284789644013\n",
      "Step: 515, Loss: 1.1614590883255005, Accuracy: 0.7183462532299741\n",
      "Step: 516, Loss: 1.2227100133895874, Accuracy: 0.7182462927143778\n",
      "Step: 517, Loss: 0.951771080493927, Accuracy: 0.7187902187902188\n",
      "Step: 518, Loss: 1.2818431854248047, Accuracy: 0.718529222864483\n",
      "Step: 519, Loss: 1.506795048713684, Accuracy: 0.717948717948718\n",
      "Step: 520, Loss: 1.2792595624923706, Accuracy: 0.7176903390914907\n",
      "Step: 521, Loss: 1.100988507270813, Accuracy: 0.717911877394636\n",
      "Step: 522, Loss: 1.2738155126571655, Accuracy: 0.7178138942001274\n",
      "Step: 523, Loss: 1.12300705909729, Accuracy: 0.7178753180661578\n",
      "Step: 524, Loss: 1.2558822631835938, Accuracy: 0.7177777777777777\n",
      "Step: 525, Loss: 1.1004562377929688, Accuracy: 0.7179974651457541\n",
      "Step: 526, Loss: 1.0012609958648682, Accuracy: 0.7183744465528147\n",
      "Step: 527, Loss: 1.2726823091506958, Accuracy: 0.7181186868686869\n",
      "Step: 528, Loss: 1.3710392713546753, Accuracy: 0.7177063642091998\n",
      "Step: 529, Loss: 1.3158236742019653, Accuracy: 0.7174528301886792\n",
      "Step: 530, Loss: 1.2143999338150024, Accuracy: 0.7173571876961707\n",
      "Step: 531, Loss: 1.1471675634384155, Accuracy: 0.7174185463659147\n",
      "Step: 532, Loss: 1.212196946144104, Accuracy: 0.717479674796748\n",
      "Step: 533, Loss: 1.2259255647659302, Accuracy: 0.7173845193508115\n",
      "Step: 534, Loss: 1.0076265335083008, Accuracy: 0.7177570093457943\n",
      "Step: 535, Loss: 1.4090232849121094, Accuracy: 0.7171952736318408\n",
      "Step: 536, Loss: 1.2613054513931274, Accuracy: 0.7171011793916822\n",
      "Step: 537, Loss: 1.0362458229064941, Accuracy: 0.7174721189591078\n",
      "Step: 538, Loss: 1.2827317714691162, Accuracy: 0.7172232529375386\n",
      "Step: 539, Loss: 1.4880930185317993, Accuracy: 0.7166666666666667\n",
      "Step: 540, Loss: 1.151708960533142, Accuracy: 0.716728280961183\n",
      "Step: 541, Loss: 1.179323434829712, Accuracy: 0.716789667896679\n",
      "Step: 542, Loss: 1.0596345663070679, Accuracy: 0.7170042971147943\n",
      "Step: 543, Loss: 1.1597286462783813, Accuracy: 0.7170649509803921\n",
      "Step: 544, Loss: 1.2740556001663208, Accuracy: 0.7168195718654434\n",
      "Step: 545, Loss: 0.916696310043335, Accuracy: 0.7173382173382173\n",
      "Step: 546, Loss: 1.3505698442459106, Accuracy: 0.7169408897014016\n",
      "Step: 547, Loss: 1.153719425201416, Accuracy: 0.7170012165450121\n",
      "Step: 548, Loss: 1.0992907285690308, Accuracy: 0.7172131147540983\n",
      "Step: 549, Loss: 1.290759563446045, Accuracy: 0.7169696969696969\n",
      "Step: 550, Loss: 1.188834547996521, Accuracy: 0.7170296430732003\n",
      "Step: 551, Loss: 1.1598056554794312, Accuracy: 0.7170893719806763\n",
      "Step: 552, Loss: 1.1687194108963013, Accuracy: 0.7171488848704038\n",
      "Step: 553, Loss: 1.157119870185852, Accuracy: 0.717208182912154\n",
      "Step: 554, Loss: 1.3535752296447754, Accuracy: 0.7169669669669669\n",
      "Step: 555, Loss: 1.1917818784713745, Accuracy: 0.7170263788968825\n",
      "Step: 556, Loss: 1.2011134624481201, Accuracy: 0.7170855774985039\n",
      "Step: 557, Loss: 1.3940820693969727, Accuracy: 0.7166965352449224\n",
      "Step: 558, Loss: 1.1459726095199585, Accuracy: 0.7169051878354203\n",
      "Step: 559, Loss: 1.1189275979995728, Accuracy: 0.7169642857142857\n",
      "Step: 560, Loss: 1.291248083114624, Accuracy: 0.7167260843731432\n",
      "Step: 561, Loss: 1.1032871007919312, Accuracy: 0.7169335705812574\n",
      "Step: 562, Loss: 1.0488874912261963, Accuracy: 0.7171403197158082\n",
      "Step: 563, Loss: 1.255592703819275, Accuracy: 0.7170508274231678\n",
      "Step: 564, Loss: 1.0005683898925781, Accuracy: 0.7174041297935103\n",
      "Step: 565, Loss: 1.2627309560775757, Accuracy: 0.7171672555948174\n",
      "Step: 566, Loss: 1.1682082414627075, Accuracy: 0.7172251616696061\n",
      "Step: 567, Loss: 1.257962942123413, Accuracy: 0.7171361502347418\n",
      "Step: 568, Loss: 1.1456931829452515, Accuracy: 0.7170474516695958\n",
      "Step: 569, Loss: 1.1274586915969849, Accuracy: 0.7172514619883041\n",
      "Step: 570, Loss: 1.1891142129898071, Accuracy: 0.7171628721541156\n",
      "Step: 571, Loss: 1.1282970905303955, Accuracy: 0.7173659673659674\n",
      "Step: 572, Loss: 1.0683993101119995, Accuracy: 0.7175683536940082\n",
      "Step: 573, Loss: 1.2647641897201538, Accuracy: 0.717479674796748\n",
      "Step: 574, Loss: 1.525183081626892, Accuracy: 0.7168115942028985\n",
      "Step: 575, Loss: 1.1820605993270874, Accuracy: 0.7167245370370371\n",
      "Step: 576, Loss: 1.3128060102462769, Accuracy: 0.7164933564413634\n",
      "Step: 577, Loss: 1.257584810256958, Accuracy: 0.7162629757785467\n",
      "Step: 578, Loss: 1.1458728313446045, Accuracy: 0.716321243523316\n",
      "Step: 579, Loss: 1.2208631038665771, Accuracy: 0.7163793103448276\n",
      "Step: 580, Loss: 1.1564407348632812, Accuracy: 0.7164371772805508\n",
      "Step: 581, Loss: 1.2023338079452515, Accuracy: 0.7163516609392898\n",
      "Step: 582, Loss: 1.245440125465393, Accuracy: 0.7161234991423671\n",
      "Step: 583, Loss: 1.158697485923767, Accuracy: 0.716181506849315\n",
      "Step: 584, Loss: 1.216936469078064, Accuracy: 0.7160968660968661\n",
      "Step: 585, Loss: 1.153159499168396, Accuracy: 0.7161547212741752\n",
      "Step: 586, Loss: 1.0358223915100098, Accuracy: 0.716496308915389\n",
      "Step: 587, Loss: 1.1741849184036255, Accuracy: 0.7165532879818595\n",
      "Step: 588, Loss: 1.1705818176269531, Accuracy: 0.7166100735710244\n",
      "Step: 589, Loss: 1.0605162382125854, Accuracy: 0.7168079096045198\n",
      "Step: 590, Loss: 1.0821141004562378, Accuracy: 0.7170050761421319\n",
      "Step: 591, Loss: 1.1835764646530151, Accuracy: 0.716920045045045\n",
      "Step: 592, Loss: 1.1855610609054565, Accuracy: 0.7168353007307476\n",
      "Step: 593, Loss: 1.2862054109573364, Accuracy: 0.7167508417508418\n",
      "Step: 594, Loss: 1.4817529916763306, Accuracy: 0.7162464985994398\n",
      "Step: 595, Loss: 1.1662417650222778, Accuracy: 0.7163031319910514\n",
      "Step: 596, Loss: 1.1801389455795288, Accuracy: 0.7162199888330542\n",
      "Step: 597, Loss: 1.3107378482818604, Accuracy: 0.7159977703455964\n",
      "Step: 598, Loss: 1.1334762573242188, Accuracy: 0.7160545353366722\n",
      "Step: 599, Loss: 1.2489759922027588, Accuracy: 0.7159722222222222\n",
      "Step: 600, Loss: 1.3982995748519897, Accuracy: 0.7156128674431503\n",
      "Step: 601, Loss: 1.3082605600357056, Accuracy: 0.7153931339977851\n",
      "Step: 602, Loss: 1.282593846321106, Accuracy: 0.7151741293532339\n",
      "Step: 603, Loss: 0.9913256764411926, Accuracy: 0.7155077262693157\n",
      "Step: 604, Loss: 1.131537914276123, Accuracy: 0.715702479338843\n",
      "Step: 605, Loss: 1.2061443328857422, Accuracy: 0.7156215621562156\n",
      "Step: 606, Loss: 1.2278881072998047, Accuracy: 0.7155409115870401\n",
      "Step: 607, Loss: 1.309588074684143, Accuracy: 0.7153234649122807\n",
      "Step: 608, Loss: 1.1423500776290894, Accuracy: 0.7153804050355774\n",
      "Step: 609, Loss: 1.1530307531356812, Accuracy: 0.7154371584699454\n",
      "Step: 610, Loss: 1.3504337072372437, Accuracy: 0.7152209492635024\n",
      "Step: 611, Loss: 1.0661743879318237, Accuracy: 0.7154139433551199\n",
      "Step: 612, Loss: 1.1561182737350464, Accuracy: 0.7154703643284394\n",
      "Step: 613, Loss: 0.9808387160301208, Accuracy: 0.7157980456026058\n",
      "Step: 614, Loss: 1.1687067747116089, Accuracy: 0.7158536585365853\n",
      "Step: 615, Loss: 1.3034545183181763, Accuracy: 0.7156385281385281\n",
      "Step: 616, Loss: 1.174954891204834, Accuracy: 0.7156942193408968\n",
      "Step: 617, Loss: 1.3705638647079468, Accuracy: 0.7153451995685005\n",
      "Step: 618, Loss: 1.416250228881836, Accuracy: 0.7149973074851912\n",
      "Step: 619, Loss: 1.26993989944458, Accuracy: 0.7147849462365592\n",
      "Step: 620, Loss: 1.3640104532241821, Accuracy: 0.714573268921095\n",
      "Step: 621, Loss: 0.9893665909767151, Accuracy: 0.714898177920686\n",
      "Step: 622, Loss: 1.1690253019332886, Accuracy: 0.7149545211342964\n",
      "Step: 623, Loss: 1.093612551689148, Accuracy: 0.7151442307692307\n",
      "Step: 624, Loss: 1.468782901763916, Accuracy: 0.7146666666666667\n",
      "Step: 625, Loss: 1.1830180883407593, Accuracy: 0.7145899893503728\n",
      "Step: 626, Loss: 1.0224411487579346, Accuracy: 0.7149122807017544\n",
      "Step: 627, Loss: 1.145397424697876, Accuracy: 0.714968152866242\n",
      "Step: 628, Loss: 1.3511165380477905, Accuracy: 0.7146263910969793\n",
      "Step: 629, Loss: 1.208601474761963, Accuracy: 0.7145502645502646\n",
      "Step: 630, Loss: 1.0895912647247314, Accuracy: 0.7147385103011094\n",
      "Step: 631, Loss: 1.2357937097549438, Accuracy: 0.7146624472573839\n",
      "Step: 632, Loss: 1.2579896450042725, Accuracy: 0.7145866245392312\n",
      "Step: 633, Loss: 1.0758448839187622, Accuracy: 0.7147739221871714\n",
      "Step: 634, Loss: 1.2647875547409058, Accuracy: 0.7146981627296588\n",
      "Step: 635, Loss: 1.2400379180908203, Accuracy: 0.714622641509434\n",
      "Step: 636, Loss: 1.3140429258346558, Accuracy: 0.7144165358451072\n",
      "Step: 637, Loss: 1.2242021560668945, Accuracy: 0.7143416927899686\n",
      "Step: 638, Loss: 1.1945114135742188, Accuracy: 0.7143974960876369\n",
      "Step: 639, Loss: 1.1156057119369507, Accuracy: 0.7145833333333333\n",
      "Step: 640, Loss: 1.1003150939941406, Accuracy: 0.7147685907436298\n",
      "Step: 641, Loss: 1.033462405204773, Accuracy: 0.7150830737279336\n",
      "Step: 642, Loss: 1.2414368391036987, Accuracy: 0.7150077760497667\n",
      "Step: 643, Loss: 1.1323119401931763, Accuracy: 0.7150621118012422\n",
      "Step: 644, Loss: 1.0950833559036255, Accuracy: 0.7152454780361757\n",
      "Step: 645, Loss: 0.990121603012085, Accuracy: 0.7155572755417957\n",
      "Step: 646, Loss: 1.210680603981018, Accuracy: 0.7154817104585265\n",
      "Step: 647, Loss: 1.4543328285217285, Accuracy: 0.7150205761316872\n",
      "Step: 648, Loss: 1.1008881330490112, Accuracy: 0.7152028762198254\n",
      "Step: 649, Loss: 1.2834314107894897, Accuracy: 0.715\n",
      "Step: 650, Loss: 1.2633970975875854, Accuracy: 0.7149257552483359\n",
      "Step: 651, Loss: 1.1399728059768677, Accuracy: 0.7149795501022495\n",
      "Step: 652, Loss: 1.1320216655731201, Accuracy: 0.7150331801939765\n",
      "Step: 653, Loss: 1.1945425271987915, Accuracy: 0.7150866462793068\n",
      "Step: 654, Loss: 1.0039218664169312, Accuracy: 0.7153944020356234\n",
      "Step: 655, Loss: 0.9995815753936768, Accuracy: 0.7157012195121951\n",
      "Step: 656, Loss: 1.2599328756332397, Accuracy: 0.7156265854895992\n",
      "Step: 657, Loss: 1.2908052206039429, Accuracy: 0.7155521783181358\n",
      "Step: 658, Loss: 1.1592754125595093, Accuracy: 0.7156044511886697\n",
      "Step: 659, Loss: 1.0497552156448364, Accuracy: 0.7159090909090909\n",
      "Step: 660, Loss: 1.188557505607605, Accuracy: 0.7159606656580938\n",
      "Step: 661, Loss: 1.3911328315734863, Accuracy: 0.7156344410876133\n",
      "Step: 662, Loss: 1.0731279850006104, Accuracy: 0.7158119658119658\n",
      "Step: 663, Loss: 1.1984823942184448, Accuracy: 0.7157379518072289\n",
      "Step: 664, Loss: 1.0990278720855713, Accuracy: 0.7159147869674185\n",
      "Step: 665, Loss: 1.1601067781448364, Accuracy: 0.7158408408408409\n",
      "Step: 666, Loss: 1.1949325799942017, Accuracy: 0.7158920539730135\n",
      "Step: 667, Loss: 1.2411855459213257, Accuracy: 0.715818363273453\n",
      "Step: 668, Loss: 1.2039014101028442, Accuracy: 0.715869456900847\n",
      "Step: 669, Loss: 1.0001091957092285, Accuracy: 0.7161691542288557\n",
      "Step: 670, Loss: 1.073122262954712, Accuracy: 0.7163437655240934\n",
      "Step: 671, Loss: 1.28214693069458, Accuracy: 0.7162698412698413\n",
      "Step: 672, Loss: 1.0606645345687866, Accuracy: 0.7164437840515107\n",
      "Step: 673, Loss: 1.0004686117172241, Accuracy: 0.7167408506429278\n",
      "Step: 674, Loss: 1.1701818704605103, Accuracy: 0.7167901234567902\n",
      "Step: 675, Loss: 1.2798094749450684, Accuracy: 0.716715976331361\n",
      "Step: 676, Loss: 1.131566047668457, Accuracy: 0.7167651403249631\n",
      "Step: 677, Loss: 1.226731538772583, Accuracy: 0.7166912487708947\n",
      "Step: 678, Loss: 1.0037459135055542, Accuracy: 0.716985763377516\n",
      "Step: 679, Loss: 1.1859514713287354, Accuracy: 0.7170343137254902\n",
      "Step: 680, Loss: 1.2994823455810547, Accuracy: 0.7168379833578071\n",
      "Step: 681, Loss: 1.155038833618164, Accuracy: 0.7168866080156403\n",
      "Step: 682, Loss: 1.154157280921936, Accuracy: 0.7169350902879453\n",
      "Step: 683, Loss: 1.373702049255371, Accuracy: 0.7166179337231969\n",
      "Step: 684, Loss: 1.1445037126541138, Accuracy: 0.7166666666666667\n",
      "Step: 685, Loss: 1.2251380681991577, Accuracy: 0.7165937803692906\n",
      "Step: 686, Loss: 1.0750945806503296, Accuracy: 0.7167637069383794\n",
      "Step: 687, Loss: 1.3257174491882324, Accuracy: 0.7165697674418605\n",
      "Step: 688, Loss: 1.158214807510376, Accuracy: 0.7167392356071601\n",
      "Step: 689, Loss: 1.3135676383972168, Accuracy: 0.7165458937198068\n",
      "Step: 690, Loss: 1.1814020872116089, Accuracy: 0.716594307766522\n",
      "Step: 691, Loss: 1.340047836303711, Accuracy: 0.7164017341040463\n",
      "Step: 692, Loss: 1.394364356994629, Accuracy: 0.7160894660894661\n",
      "Step: 693, Loss: 1.1140481233596802, Accuracy: 0.7161383285302594\n",
      "Step: 694, Loss: 1.1042035818099976, Accuracy: 0.7163069544364509\n",
      "Step: 695, Loss: 1.147875189781189, Accuracy: 0.7163553639846744\n",
      "Step: 696, Loss: 1.129357933998108, Accuracy: 0.7164036346245816\n",
      "Step: 697, Loss: 1.1919819116592407, Accuracy: 0.7163323782234957\n",
      "Step: 698, Loss: 1.3394556045532227, Accuracy: 0.716142107773009\n",
      "Step: 699, Loss: 1.409839153289795, Accuracy: 0.7158333333333333\n",
      "Step: 700, Loss: 1.3645857572555542, Accuracy: 0.7155254398478365\n",
      "Step: 701, Loss: 1.1780861616134644, Accuracy: 0.7155745489078822\n",
      "Step: 702, Loss: 1.2668744325637817, Accuracy: 0.7153864390706496\n",
      "Step: 703, Loss: 1.0612128973007202, Accuracy: 0.7155539772727273\n",
      "Step: 704, Loss: 1.3290210962295532, Accuracy: 0.7153664302600473\n",
      "Step: 705, Loss: 1.1831426620483398, Accuracy: 0.7154154863078376\n",
      "Step: 706, Loss: 1.081590175628662, Accuracy: 0.7155822725129656\n",
      "Step: 707, Loss: 1.0969502925872803, Accuracy: 0.7157485875706214\n",
      "Step: 708, Loss: 1.0125359296798706, Accuracy: 0.7160319699106723\n",
      "Step: 709, Loss: 1.186529517173767, Accuracy: 0.7160798122065728\n",
      "Step: 710, Loss: 1.0835329294204712, Accuracy: 0.7162447257383966\n",
      "Step: 711, Loss: 1.161241888999939, Accuracy: 0.7162921348314607\n",
      "Step: 712, Loss: 1.1932343244552612, Accuracy: 0.7163394109396914\n",
      "Step: 713, Loss: 1.1392630338668823, Accuracy: 0.7162698412698413\n",
      "Step: 714, Loss: 1.0284814834594727, Accuracy: 0.7165501165501166\n",
      "Step: 715, Loss: 1.2609142065048218, Accuracy: 0.7163640595903166\n",
      "Step: 716, Loss: 1.1160386800765991, Accuracy: 0.7165271966527197\n",
      "Step: 717, Loss: 1.1720536947250366, Accuracy: 0.7164577530176416\n",
      "Step: 718, Loss: 1.191186547279358, Accuracy: 0.7163885025498378\n",
      "Step: 719, Loss: 1.0794166326522827, Accuracy: 0.716550925925926\n",
      "Step: 720, Loss: 1.2346218824386597, Accuracy: 0.7164817383263985\n",
      "Step: 721, Loss: 1.1438517570495605, Accuracy: 0.716528162511542\n",
      "Step: 722, Loss: 1.1692187786102295, Accuracy: 0.7165744582757031\n",
      "Step: 723, Loss: 1.2663055658340454, Accuracy: 0.7165055248618785\n",
      "Step: 724, Loss: 1.0738204717636108, Accuracy: 0.7166666666666667\n",
      "Step: 725, Loss: 1.1330245733261108, Accuracy: 0.716712580348944\n",
      "Step: 726, Loss: 1.1658130884170532, Accuracy: 0.7167583677212288\n",
      "Step: 727, Loss: 1.2742937803268433, Accuracy: 0.7165750915750916\n",
      "Step: 728, Loss: 1.2508889436721802, Accuracy: 0.716506630086877\n",
      "Step: 729, Loss: 1.3914145231246948, Accuracy: 0.7162100456621004\n",
      "Step: 730, Loss: 1.0992945432662964, Accuracy: 0.7163702690378477\n",
      "Step: 731, Loss: 1.2020142078399658, Accuracy: 0.7164162112932605\n",
      "Step: 732, Loss: 1.4803746938705444, Accuracy: 0.7160072760345612\n",
      "Step: 733, Loss: 1.2429805994033813, Accuracy: 0.7159400544959128\n",
      "Step: 734, Loss: 1.1645756959915161, Accuracy: 0.7159863945578231\n",
      "Step: 735, Loss: 1.1930869817733765, Accuracy: 0.7159193840579711\n",
      "Step: 736, Loss: 1.2325457334518433, Accuracy: 0.7158525554047942\n",
      "Step: 737, Loss: 1.1472359895706177, Accuracy: 0.7158988256549232\n",
      "Step: 738, Loss: 1.2441632747650146, Accuracy: 0.7157194406856112\n",
      "Step: 739, Loss: 1.2112263441085815, Accuracy: 0.7156531531531531\n",
      "Step: 740, Loss: 1.3492110967636108, Accuracy: 0.7153621232568601\n",
      "Step: 741, Loss: 1.2350387573242188, Accuracy: 0.7152964959568733\n",
      "Step: 742, Loss: 1.278559684753418, Accuracy: 0.715231045311799\n",
      "Step: 743, Loss: 0.9998355507850647, Accuracy: 0.7155017921146953\n",
      "Step: 744, Loss: 1.2454646825790405, Accuracy: 0.7154362416107383\n",
      "Step: 745, Loss: 1.325706124305725, Accuracy: 0.7152591599642538\n",
      "Step: 746, Loss: 1.0980840921401978, Accuracy: 0.7154172244533691\n",
      "Step: 747, Loss: 1.3919649124145508, Accuracy: 0.7151292335115864\n",
      "Step: 748, Loss: 1.3607412576675415, Accuracy: 0.7148420115709835\n",
      "Step: 749, Loss: 1.2191523313522339, Accuracy: 0.7147777777777777\n",
      "Step: 750, Loss: 1.5464215278625488, Accuracy: 0.7142698624056814\n",
      "Step: 751, Loss: 1.153292179107666, Accuracy: 0.7143173758865248\n",
      "Step: 752, Loss: 1.1763567924499512, Accuracy: 0.714364763169544\n",
      "Step: 753, Loss: 1.4352325201034546, Accuracy: 0.7139699381078691\n",
      "Step: 754, Loss: 1.2018202543258667, Accuracy: 0.7140176600441501\n",
      "Step: 755, Loss: 1.1867457628250122, Accuracy: 0.7140652557319224\n",
      "Step: 756, Loss: 1.0839488506317139, Accuracy: 0.7141127256715103\n",
      "Step: 757, Loss: 1.0673844814300537, Accuracy: 0.7142700087950747\n",
      "Step: 758, Loss: 1.0717064142227173, Accuracy: 0.7144268774703557\n",
      "Step: 759, Loss: 1.017275094985962, Accuracy: 0.7146929824561403\n",
      "Step: 760, Loss: 1.4110931158065796, Accuracy: 0.7144108628996934\n",
      "Step: 761, Loss: 1.0793750286102295, Accuracy: 0.7145669291338582\n",
      "Step: 762, Loss: 1.4325261116027832, Accuracy: 0.7142857142857143\n",
      "Step: 763, Loss: 1.1178439855575562, Accuracy: 0.7143324607329843\n",
      "Step: 764, Loss: 1.0717121362686157, Accuracy: 0.7144880174291939\n",
      "Step: 765, Loss: 1.048309087753296, Accuracy: 0.7146431679721497\n",
      "Step: 766, Loss: 1.2345107793807983, Accuracy: 0.71458061712299\n",
      "Step: 767, Loss: 1.2483664751052856, Accuracy: 0.7145182291666666\n",
      "Step: 768, Loss: 1.1615912914276123, Accuracy: 0.7145643693107933\n",
      "Step: 769, Loss: 1.395115852355957, Accuracy: 0.7142857142857143\n",
      "Step: 770, Loss: 1.137937307357788, Accuracy: 0.7143320363164721\n",
      "Step: 771, Loss: 1.4478836059570312, Accuracy: 0.7139464594127807\n",
      "Step: 772, Loss: 1.2954095602035522, Accuracy: 0.713777490297542\n",
      "Step: 773, Loss: 1.0328925848007202, Accuracy: 0.7139319552110249\n",
      "Step: 774, Loss: 1.1717135906219482, Accuracy: 0.7139784946236559\n",
      "Step: 775, Loss: 1.522926926612854, Accuracy: 0.7134879725085911\n",
      "Step: 776, Loss: 1.1175645589828491, Accuracy: 0.7136422136422137\n",
      "Step: 777, Loss: 1.142683982849121, Accuracy: 0.7136889460154242\n",
      "Step: 778, Loss: 1.3152823448181152, Accuracy: 0.7135216089002995\n",
      "Step: 779, Loss: 1.3180445432662964, Accuracy: 0.7133547008547009\n",
      "Step: 780, Loss: 1.1615707874298096, Accuracy: 0.713401621852326\n",
      "Step: 781, Loss: 1.2652082443237305, Accuracy: 0.7132352941176471\n",
      "Step: 782, Loss: 1.0819928646087646, Accuracy: 0.7133886760323542\n",
      "Step: 783, Loss: 1.0633492469787598, Accuracy: 0.7135416666666666\n",
      "Step: 784, Loss: 1.1515980958938599, Accuracy: 0.713588110403397\n",
      "Step: 785, Loss: 1.2896136045455933, Accuracy: 0.7134223918575063\n",
      "Step: 786, Loss: 1.3372974395751953, Accuracy: 0.7132570944515036\n",
      "Step: 787, Loss: 1.3216733932495117, Accuracy: 0.7130922165820643\n",
      "Step: 788, Loss: 1.1251851320266724, Accuracy: 0.7131389945078158\n",
      "Step: 789, Loss: 1.23108971118927, Accuracy: 0.7130801687763713\n",
      "Step: 790, Loss: 1.157554268836975, Accuracy: 0.7131268436578171\n",
      "Step: 791, Loss: 1.2291027307510376, Accuracy: 0.7130681818181818\n",
      "Step: 792, Loss: 1.1269855499267578, Accuracy: 0.7131147540983607\n",
      "Step: 793, Loss: 1.390684962272644, Accuracy: 0.7128463476070529\n",
      "Step: 794, Loss: 1.197567343711853, Accuracy: 0.7127882599580713\n",
      "Step: 795, Loss: 1.0805052518844604, Accuracy: 0.7129396984924623\n",
      "Step: 796, Loss: 1.3073004484176636, Accuracy: 0.7127770807193643\n",
      "Step: 797, Loss: 1.1640592813491821, Accuracy: 0.7128237259816207\n",
      "Step: 798, Loss: 1.2277112007141113, Accuracy: 0.7127659574468085\n",
      "Step: 799, Loss: 1.113385558128357, Accuracy: 0.7129166666666666\n",
      "Step: 800, Loss: 1.224213719367981, Accuracy: 0.7128589263420724\n",
      "Step: 801, Loss: 1.2761012315750122, Accuracy: 0.7128013300083126\n",
      "Step: 802, Loss: 1.258140206336975, Accuracy: 0.712640099626401\n",
      "Step: 803, Loss: 1.0754960775375366, Accuracy: 0.712790215588723\n",
      "Step: 804, Loss: 1.2981899976730347, Accuracy: 0.7126293995859213\n",
      "Step: 805, Loss: 1.0063351392745972, Accuracy: 0.7128825475599669\n",
      "Step: 806, Loss: 1.3662056922912598, Accuracy: 0.7127220156959934\n",
      "Step: 807, Loss: 1.1236072778701782, Accuracy: 0.7128712871287128\n",
      "Step: 808, Loss: 1.1154025793075562, Accuracy: 0.7130201895344046\n",
      "Step: 809, Loss: 1.0867400169372559, Accuracy: 0.7131687242798354\n",
      "Step: 810, Loss: 1.3714485168457031, Accuracy: 0.7129058775174681\n",
      "Step: 811, Loss: 1.2099024057388306, Accuracy: 0.7129515599343186\n",
      "Step: 812, Loss: 1.2474724054336548, Accuracy: 0.7128946289462894\n",
      "Step: 813, Loss: 1.1183359622955322, Accuracy: 0.7129402129402129\n",
      "Step: 814, Loss: 1.2087664604187012, Accuracy: 0.7128834355828221\n",
      "Step: 815, Loss: 1.236425757408142, Accuracy: 0.7128267973856209\n",
      "Step: 816, Loss: 1.166440725326538, Accuracy: 0.7128722970216238\n",
      "Step: 817, Loss: 1.1709606647491455, Accuracy: 0.712917685411573\n",
      "Step: 818, Loss: 1.2529171705245972, Accuracy: 0.7128612128612128\n",
      "Step: 819, Loss: 1.3160392045974731, Accuracy: 0.7127032520325203\n",
      "Step: 820, Loss: 1.2867532968521118, Accuracy: 0.7125456760048721\n",
      "Step: 821, Loss: 1.3338106870651245, Accuracy: 0.7123884833738848\n",
      "Step: 822, Loss: 1.14887535572052, Accuracy: 0.7124341838801134\n",
      "Step: 823, Loss: 1.1239920854568481, Accuracy: 0.7124797734627831\n",
      "Step: 824, Loss: 1.1224993467330933, Accuracy: 0.7125252525252526\n",
      "Step: 825, Loss: 1.3886089324951172, Accuracy: 0.7122679580306699\n",
      "Step: 826, Loss: 1.4181389808654785, Accuracy: 0.7120112857718662\n",
      "Step: 827, Loss: 1.227464199066162, Accuracy: 0.7119565217391305\n",
      "Step: 828, Loss: 1.1508110761642456, Accuracy: 0.7120024125452352\n",
      "Step: 829, Loss: 1.0774861574172974, Accuracy: 0.71214859437751\n",
      "Step: 830, Loss: 1.3283683061599731, Accuracy: 0.7119935820296831\n",
      "Step: 831, Loss: 1.2338465452194214, Accuracy: 0.7119391025641025\n",
      "Step: 832, Loss: 1.1113845109939575, Accuracy: 0.7120848339335735\n",
      "Step: 833, Loss: 1.114637017250061, Accuracy: 0.7122302158273381\n",
      "Step: 834, Loss: 1.1282353401184082, Accuracy: 0.7122754491017964\n",
      "Step: 835, Loss: 1.1918878555297852, Accuracy: 0.7123205741626795\n",
      "Step: 836, Loss: 1.155350923538208, Accuracy: 0.7123655913978495\n",
      "Step: 837, Loss: 1.0565766096115112, Accuracy: 0.7125099443118537\n",
      "Step: 838, Loss: 1.1700427532196045, Accuracy: 0.712554628526023\n",
      "Step: 839, Loss: 1.1805397272109985, Accuracy: 0.7125992063492064\n",
      "Step: 840, Loss: 1.0282636880874634, Accuracy: 0.7128418549346016\n",
      "Step: 841, Loss: 1.2844173908233643, Accuracy: 0.7126880443388757\n",
      "Step: 842, Loss: 1.190198302268982, Accuracy: 0.7126334519572953\n",
      "Step: 843, Loss: 1.122245192527771, Accuracy: 0.7126777251184834\n",
      "Step: 844, Loss: 1.404566764831543, Accuracy: 0.71232741617357\n",
      "Step: 845, Loss: 1.1511341333389282, Accuracy: 0.7123719464144996\n",
      "Step: 846, Loss: 1.2233326435089111, Accuracy: 0.7123179850452578\n",
      "Step: 847, Loss: 1.1159814596176147, Accuracy: 0.7124606918238994\n",
      "Step: 848, Loss: 1.2090234756469727, Accuracy: 0.7124067530427954\n",
      "Step: 849, Loss: 1.0574949979782104, Accuracy: 0.7125490196078431\n",
      "Step: 850, Loss: 1.1611000299453735, Accuracy: 0.7125930278104191\n",
      "Step: 851, Loss: 1.3449878692626953, Accuracy: 0.7124413145539906\n",
      "Step: 852, Loss: 1.3034907579421997, Accuracy: 0.7122899570144587\n",
      "Step: 853, Loss: 1.164703369140625, Accuracy: 0.7123341139734582\n",
      "Step: 854, Loss: 1.0626674890518188, Accuracy: 0.7124756335282652\n",
      "Step: 855, Loss: 1.1584738492965698, Accuracy: 0.7125194704049844\n",
      "Step: 856, Loss: 1.1677186489105225, Accuracy: 0.7125632049786076\n",
      "Step: 857, Loss: 1.2036668062210083, Accuracy: 0.7125097125097125\n",
      "Step: 858, Loss: 1.214354395866394, Accuracy: 0.7124563445867288\n",
      "Step: 859, Loss: 1.4134597778320312, Accuracy: 0.7122093023255814\n",
      "Step: 860, Loss: 1.2045530080795288, Accuracy: 0.7121564072783585\n",
      "Step: 861, Loss: 1.1261078119277954, Accuracy: 0.712200309358082\n",
      "Step: 862, Loss: 1.4141448736190796, Accuracy: 0.7119544225569718\n",
      "Step: 863, Loss: 1.173577904701233, Accuracy: 0.7119984567901234\n",
      "Step: 864, Loss: 1.2779548168182373, Accuracy: 0.7118497109826589\n",
      "Step: 865, Loss: 1.264661431312561, Accuracy: 0.7117013086989993\n",
      "Step: 866, Loss: 1.1789566278457642, Accuracy: 0.7117454825067282\n",
      "Step: 867, Loss: 1.113714575767517, Accuracy: 0.7117895545314901\n",
      "Step: 868, Loss: 1.15790855884552, Accuracy: 0.7118335251246644\n",
      "Step: 869, Loss: 1.1728386878967285, Accuracy: 0.7118773946360153\n",
      "Step: 870, Loss: 1.1787909269332886, Accuracy: 0.7119211634137007\n",
      "Step: 871, Loss: 1.2775601148605347, Accuracy: 0.7117737003058104\n",
      "Step: 872, Loss: 1.066904067993164, Accuracy: 0.7119129438717068\n",
      "Step: 873, Loss: 1.1146284341812134, Accuracy: 0.7120518688024409\n",
      "Step: 874, Loss: 1.3131048679351807, Accuracy: 0.7119047619047619\n",
      "Step: 875, Loss: 1.3013356924057007, Accuracy: 0.71175799086758\n",
      "Step: 876, Loss: 1.279009222984314, Accuracy: 0.7117065754465982\n",
      "Step: 877, Loss: 1.3459359407424927, Accuracy: 0.7115603644646925\n",
      "Step: 878, Loss: 1.295524001121521, Accuracy: 0.7114144861585134\n",
      "Step: 879, Loss: 1.1021946668624878, Accuracy: 0.7115530303030303\n",
      "Step: 880, Loss: 1.1680570840835571, Accuracy: 0.7115966704502459\n",
      "Step: 881, Loss: 1.3913277387619019, Accuracy: 0.7113567649281936\n",
      "Step: 882, Loss: 1.3315027952194214, Accuracy: 0.711211778029445\n",
      "Step: 883, Loss: 1.2089250087738037, Accuracy: 0.7111613876319759\n",
      "Step: 884, Loss: 1.2372585535049438, Accuracy: 0.7111111111111111\n",
      "Step: 885, Loss: 1.0130434036254883, Accuracy: 0.7113431151241535\n",
      "Step: 886, Loss: 1.3014119863510132, Accuracy: 0.7111987974445697\n",
      "Step: 887, Loss: 1.251123070716858, Accuracy: 0.7111486486486487\n",
      "Step: 888, Loss: 1.3947240114212036, Accuracy: 0.7109111361079865\n",
      "Step: 889, Loss: 1.1975785493850708, Accuracy: 0.7108614232209738\n",
      "Step: 890, Loss: 1.1573017835617065, Accuracy: 0.7109988776655444\n",
      "Step: 891, Loss: 1.1488094329833984, Accuracy: 0.711042600896861\n",
      "Step: 892, Loss: 1.0598598718643188, Accuracy: 0.7111795446061964\n",
      "Step: 893, Loss: 1.4489617347717285, Accuracy: 0.7108501118568232\n",
      "Step: 894, Loss: 1.083650827407837, Accuracy: 0.7109869646182495\n",
      "Step: 895, Loss: 1.378250241279602, Accuracy: 0.7107514880952381\n",
      "Step: 896, Loss: 1.4541116952896118, Accuracy: 0.7104236343366778\n",
      "Step: 897, Loss: 1.1627708673477173, Accuracy: 0.7104677060133631\n",
      "Step: 898, Loss: 1.247092604637146, Accuracy: 0.710418984056359\n",
      "Step: 899, Loss: 1.1633281707763672, Accuracy: 0.710462962962963\n",
      "Step: 900, Loss: 1.4015660285949707, Accuracy: 0.7102293747687755\n",
      "Step: 901, Loss: 1.1339845657348633, Accuracy: 0.7102734663710274\n",
      "Step: 902, Loss: 1.2532035112380981, Accuracy: 0.7102251753414545\n",
      "Step: 903, Loss: 1.2426897287368774, Accuracy: 0.7101769911504425\n",
      "Step: 904, Loss: 1.3030307292938232, Accuracy: 0.710036832412523\n",
      "Step: 905, Loss: 1.290185809135437, Accuracy: 0.7098969830757911\n",
      "Step: 906, Loss: 1.1564117670059204, Accuracy: 0.7099411980889379\n",
      "Step: 907, Loss: 1.4884390830993652, Accuracy: 0.709618208516887\n",
      "Step: 908, Loss: 1.0243208408355713, Accuracy: 0.7098459845984598\n",
      "Step: 909, Loss: 1.066778540611267, Accuracy: 0.709981684981685\n",
      "Step: 910, Loss: 1.1936683654785156, Accuracy: 0.70993413830955\n",
      "Step: 911, Loss: 1.1006063222885132, Accuracy: 0.7100694444444444\n",
      "Step: 912, Loss: 1.331038236618042, Accuracy: 0.7099306316173786\n",
      "Step: 913, Loss: 1.1164411306381226, Accuracy: 0.7099744711889132\n",
      "Step: 914, Loss: 1.0974427461624146, Accuracy: 0.7101092896174863\n",
      "Step: 915, Loss: 1.0757075548171997, Accuracy: 0.7102438136826783\n",
      "Step: 916, Loss: 1.14060640335083, Accuracy: 0.71037804434751\n",
      "Step: 917, Loss: 1.0998481512069702, Accuracy: 0.710511982570806\n",
      "Step: 918, Loss: 1.155861258506775, Accuracy: 0.7105549510337323\n",
      "Step: 919, Loss: 1.2711952924728394, Accuracy: 0.7105072463768116\n",
      "Step: 920, Loss: 1.2455207109451294, Accuracy: 0.7104596453130655\n",
      "Step: 921, Loss: 1.226008653640747, Accuracy: 0.710412147505423\n",
      "Step: 922, Loss: 1.1736057996749878, Accuracy: 0.7104550379198267\n",
      "Step: 923, Loss: 1.1447417736053467, Accuracy: 0.7104978354978355\n",
      "Step: 924, Loss: 1.0781810283660889, Accuracy: 0.7106306306306306\n",
      "Step: 925, Loss: 1.280615210533142, Accuracy: 0.7104931605471563\n",
      "Step: 926, Loss: 1.2254207134246826, Accuracy: 0.7104458827759799\n",
      "Step: 927, Loss: 1.0202796459197998, Accuracy: 0.7106681034482759\n",
      "Step: 928, Loss: 1.0754457712173462, Accuracy: 0.710800143523502\n",
      "Step: 929, Loss: 1.240242838859558, Accuracy: 0.710752688172043\n",
      "Step: 930, Loss: 1.2214726209640503, Accuracy: 0.7107053347654851\n",
      "Step: 931, Loss: 1.0228642225265503, Accuracy: 0.7109263233190272\n",
      "Step: 932, Loss: 1.2717018127441406, Accuracy: 0.710789567702751\n",
      "Step: 933, Loss: 1.2180038690567017, Accuracy: 0.7107423269093505\n",
      "Step: 934, Loss: 1.2922405004501343, Accuracy: 0.7106060606060606\n",
      "Step: 935, Loss: 1.2679880857467651, Accuracy: 0.7105591168091168\n",
      "Step: 936, Loss: 1.0069870948791504, Accuracy: 0.7107790821771611\n",
      "Step: 937, Loss: 1.071972370147705, Accuracy: 0.7109097370291401\n",
      "Step: 938, Loss: 1.1180692911148071, Accuracy: 0.7109513667021654\n",
      "Step: 939, Loss: 1.0030516386032104, Accuracy: 0.7111702127659575\n",
      "Step: 940, Loss: 1.2008856534957886, Accuracy: 0.7111229188806234\n",
      "Step: 941, Loss: 1.039242148399353, Accuracy: 0.7112526539278131\n",
      "Step: 942, Loss: 1.1493093967437744, Accuracy: 0.7113821138211383\n",
      "Step: 943, Loss: 1.1135226488113403, Accuracy: 0.7115112994350282\n",
      "Step: 944, Loss: 1.3101965188980103, Accuracy: 0.7113756613756613\n",
      "Step: 945, Loss: 1.349575400352478, Accuracy: 0.7112403100775194\n",
      "Step: 946, Loss: 0.9884588122367859, Accuracy: 0.7114572333685322\n",
      "Step: 947, Loss: 1.049439787864685, Accuracy: 0.7115857946554149\n",
      "Step: 948, Loss: 1.1435959339141846, Accuracy: 0.7116262732701089\n",
      "Step: 949, Loss: 1.3269026279449463, Accuracy: 0.7114912280701754\n",
      "Step: 950, Loss: 0.9819610714912415, Accuracy: 0.7117069751139152\n",
      "Step: 951, Loss: 1.1898733377456665, Accuracy: 0.7116596638655462\n",
      "Step: 952, Loss: 1.3547768592834473, Accuracy: 0.7114375655823715\n",
      "Step: 953, Loss: 1.088315486907959, Accuracy: 0.7115653389238294\n",
      "Step: 954, Loss: 1.1541837453842163, Accuracy: 0.7116055846422339\n",
      "Step: 955, Loss: 1.4089504480361938, Accuracy: 0.711384239888424\n",
      "Step: 956, Loss: 1.2879676818847656, Accuracy: 0.7113375130616509\n",
      "Step: 957, Loss: 1.0464640855789185, Accuracy: 0.7115518441196939\n",
      "Step: 958, Loss: 1.2418090105056763, Accuracy: 0.7115050399721933\n",
      "Step: 959, Loss: 1.2906008958816528, Accuracy: 0.7113715277777778\n",
      "Step: 960, Loss: 1.0139873027801514, Accuracy: 0.7115851543531044\n",
      "Step: 961, Loss: 1.1150661706924438, Accuracy: 0.7117117117117117\n",
      "Step: 962, Loss: 1.3890079259872437, Accuracy: 0.7114918656974731\n",
      "Step: 963, Loss: 1.0579348802566528, Accuracy: 0.7116182572614108\n",
      "Step: 964, Loss: 1.2345163822174072, Accuracy: 0.7115716753022453\n",
      "Step: 965, Loss: 1.2404886484146118, Accuracy: 0.7115251897860594\n",
      "Step: 966, Loss: 1.221551775932312, Accuracy: 0.7114788004136504\n",
      "Step: 967, Loss: 1.0742582082748413, Accuracy: 0.7116046831955923\n",
      "Step: 968, Loss: 1.4266613721847534, Accuracy: 0.7113003095975232\n",
      "Step: 969, Loss: 1.1778895854949951, Accuracy: 0.711340206185567\n",
      "Step: 970, Loss: 1.1346005201339722, Accuracy: 0.7113800205973223\n",
      "Step: 971, Loss: 1.2079122066497803, Accuracy: 0.7114197530864198\n",
      "Step: 972, Loss: 1.2396352291107178, Accuracy: 0.7113737581363481\n",
      "Step: 973, Loss: 1.194900393486023, Accuracy: 0.7113278576317591\n",
      "Step: 974, Loss: 1.1378117799758911, Accuracy: 0.7113675213675213\n",
      "Step: 975, Loss: 1.1332677602767944, Accuracy: 0.7114071038251366\n",
      "Step: 976, Loss: 1.2869274616241455, Accuracy: 0.7112760150119413\n",
      "Step: 977, Loss: 1.1618818044662476, Accuracy: 0.7113156100886162\n",
      "Step: 978, Loss: 1.1053966283798218, Accuracy: 0.7114402451481103\n",
      "Step: 979, Loss: 1.2437905073165894, Accuracy: 0.7113095238095238\n",
      "Step: 980, Loss: 1.0965420007705688, Accuracy: 0.7114339109751954\n",
      "Step: 981, Loss: 1.315436840057373, Accuracy: 0.7113034623217923\n",
      "Step: 982, Loss: 1.271155834197998, Accuracy: 0.7111732790776535\n",
      "Step: 983, Loss: 1.1130205392837524, Accuracy: 0.7112974254742548\n",
      "Step: 984, Loss: 1.1364939212799072, Accuracy: 0.711336717428088\n",
      "Step: 985, Loss: 1.3072656393051147, Accuracy: 0.7112068965517241\n",
      "Step: 986, Loss: 1.1691166162490845, Accuracy: 0.7112462006079028\n",
      "Step: 987, Loss: 1.247992992401123, Accuracy: 0.7112010796221323\n",
      "Step: 988, Loss: 0.9972020983695984, Accuracy: 0.7114088304684867\n",
      "Step: 989, Loss: 1.4660910367965698, Accuracy: 0.7111111111111111\n",
      "Step: 990, Loss: 1.0578638315200806, Accuracy: 0.7112344433232425\n",
      "Step: 991, Loss: 1.3921564817428589, Accuracy: 0.7110215053763441\n",
      "Step: 992, Loss: 1.2718828916549683, Accuracy: 0.7109768378650554\n",
      "Step: 993, Loss: 1.0800889730453491, Accuracy: 0.7110999329309189\n",
      "Step: 994, Loss: 1.1867314577102661, Accuracy: 0.7111390284757119\n",
      "Step: 995, Loss: 1.1580417156219482, Accuracy: 0.7111780455153949\n",
      "Step: 996, Loss: 1.3263999223709106, Accuracy: 0.7110498161150117\n",
      "Step: 997, Loss: 1.3871265649795532, Accuracy: 0.7108383433533734\n",
      "Step: 998, Loss: 0.9988046288490295, Accuracy: 0.7110443777110443\n",
      "Step: 999, Loss: 1.0291613340377808, Accuracy: 0.7111666666666666\n",
      "Step: 1000, Loss: 1.0850330591201782, Accuracy: 0.7112887112887113\n",
      "Step: 1001, Loss: 1.080652117729187, Accuracy: 0.7114105123087159\n",
      "Step: 1002, Loss: 1.2411625385284424, Accuracy: 0.7113659022931207\n",
      "Step: 1003, Loss: 1.2342904806137085, Accuracy: 0.711238379814077\n",
      "Step: 1004, Loss: 1.316390872001648, Accuracy: 0.7111940298507463\n",
      "Step: 1005, Loss: 1.3395644426345825, Accuracy: 0.7109840954274353\n",
      "Step: 1006, Loss: 1.1474931240081787, Accuracy: 0.7110228401191658\n",
      "Step: 1007, Loss: 1.1981431245803833, Accuracy: 0.7109788359788359\n",
      "Step: 1008, Loss: 1.2604026794433594, Accuracy: 0.7109349190617773\n",
      "Step: 1009, Loss: 1.263209342956543, Accuracy: 0.7108910891089109\n",
      "Step: 1010, Loss: 1.2588056325912476, Accuracy: 0.7108473458621827\n",
      "Step: 1011, Loss: 1.1689238548278809, Accuracy: 0.7108860342555995\n",
      "Step: 1012, Loss: 1.3005293607711792, Accuracy: 0.7107601184600197\n",
      "Step: 1013, Loss: 1.177025318145752, Accuracy: 0.7107988165680473\n",
      "Step: 1014, Loss: 1.166093349456787, Accuracy: 0.7108374384236453\n",
      "Step: 1015, Loss: 1.1268595457077026, Accuracy: 0.7108759842519685\n",
      "Step: 1016, Loss: 1.00944983959198, Accuracy: 0.7110783349721402\n",
      "Step: 1017, Loss: 1.1844136714935303, Accuracy: 0.7110347085789129\n",
      "Step: 1018, Loss: 1.473035216331482, Accuracy: 0.7107458292443573\n",
      "Step: 1019, Loss: 1.146285057067871, Accuracy: 0.7107843137254902\n",
      "Step: 1020, Loss: 1.1523263454437256, Accuracy: 0.710822722820764\n",
      "Step: 1021, Loss: 1.140616536140442, Accuracy: 0.7109425962165689\n",
      "Step: 1022, Loss: 1.082426905632019, Accuracy: 0.7110622352557836\n",
      "Step: 1023, Loss: 1.136136770248413, Accuracy: 0.7111002604166666\n",
      "Step: 1024, Loss: 1.204166293144226, Accuracy: 0.7110569105691057\n",
      "Step: 1025, Loss: 1.2894095182418823, Accuracy: 0.7109324236517219\n",
      "Step: 1026, Loss: 1.3551111221313477, Accuracy: 0.7107270366764038\n",
      "Step: 1027, Loss: 1.1216672658920288, Accuracy: 0.7108463035019456\n",
      "Step: 1028, Loss: 1.1298643350601196, Accuracy: 0.7109653385163589\n",
      "Step: 1029, Loss: 1.0349634885787964, Accuracy: 0.7111650485436893\n",
      "Step: 1030, Loss: 1.3165514469146729, Accuracy: 0.7110410604591012\n",
      "Step: 1031, Loss: 1.4214762449264526, Accuracy: 0.7108365633074936\n",
      "Step: 1032, Loss: 1.596291184425354, Accuracy: 0.7103904485317845\n",
      "Step: 1033, Loss: 1.1050316095352173, Accuracy: 0.7105093488072212\n",
      "Step: 1034, Loss: 1.2143943309783936, Accuracy: 0.7104669887278583\n",
      "Step: 1035, Loss: 1.4335373640060425, Accuracy: 0.7101833976833977\n",
      "Step: 1036, Loss: 1.0354349613189697, Accuracy: 0.7103825136612022\n",
      "Step: 1037, Loss: 1.1817487478256226, Accuracy: 0.7104206807964033\n",
      "Step: 1038, Loss: 1.0937210321426392, Accuracy: 0.7104587744626243\n",
      "Step: 1039, Loss: 1.0861306190490723, Accuracy: 0.7105769230769231\n",
      "Step: 1040, Loss: 1.2279996871948242, Accuracy: 0.7105347422350304\n",
      "Step: 1041, Loss: 1.1901602745056152, Accuracy: 0.710572616762636\n",
      "Step: 1042, Loss: 1.2897380590438843, Accuracy: 0.710450623202301\n",
      "Step: 1043, Loss: 1.3243366479873657, Accuracy: 0.7103288633461047\n",
      "Step: 1044, Loss: 1.2869880199432373, Accuracy: 0.710207336523126\n",
      "Step: 1045, Loss: 1.1626701354980469, Accuracy: 0.7102453792224347\n",
      "Step: 1046, Loss: 1.4217967987060547, Accuracy: 0.7100445717924228\n",
      "Step: 1047, Loss: 1.0485590696334839, Accuracy: 0.7101622137404581\n",
      "Step: 1048, Loss: 1.3088793754577637, Accuracy: 0.7100413091833492\n",
      "Step: 1049, Loss: 1.0876826047897339, Accuracy: 0.7101587301587302\n",
      "Step: 1050, Loss: 1.1788525581359863, Accuracy: 0.7101966381224231\n",
      "Step: 1051, Loss: 1.164152979850769, Accuracy: 0.710234474017744\n",
      "Step: 1052, Loss: 1.2090939283370972, Accuracy: 0.7102722380500158\n",
      "Step: 1053, Loss: 1.2263522148132324, Accuracy: 0.7102308665401644\n",
      "Step: 1054, Loss: 1.221774935722351, Accuracy: 0.7101895734597157\n",
      "Step: 1055, Loss: 1.159713625907898, Accuracy: 0.7102272727272727\n",
      "Step: 1056, Loss: 1.2613917589187622, Accuracy: 0.7101072216966257\n",
      "Step: 1057, Loss: 1.1129463911056519, Accuracy: 0.7101449275362319\n",
      "Step: 1058, Loss: 1.27118718624115, Accuracy: 0.7101038715769594\n",
      "Step: 1059, Loss: 1.2202397584915161, Accuracy: 0.710062893081761\n",
      "Step: 1060, Loss: 1.209797739982605, Accuracy: 0.710100534087339\n",
      "Step: 1061, Loss: 1.155287265777588, Accuracy: 0.7101381042059008\n",
      "Step: 1062, Loss: 1.192010521888733, Accuracy: 0.7100972091564753\n",
      "Step: 1063, Loss: 1.1133085489273071, Accuracy: 0.7102130325814536\n",
      "Step: 1064, Loss: 1.166706919670105, Accuracy: 0.7101721439749609\n",
      "Step: 1065, Loss: 1.3216291666030884, Accuracy: 0.7100531582238899\n",
      "Step: 1066, Loss: 1.1530215740203857, Accuracy: 0.7100905966885348\n",
      "Step: 1067, Loss: 1.2054026126861572, Accuracy: 0.7100499375780275\n",
      "Step: 1068, Loss: 1.4746302366256714, Accuracy: 0.7097754911131899\n",
      "Step: 1069, Loss: 1.2207473516464233, Accuracy: 0.7097352024922119\n",
      "Step: 1070, Loss: 1.198050618171692, Accuracy: 0.7097727980080921\n",
      "Step: 1071, Loss: 1.1451408863067627, Accuracy: 0.7098103233830846\n",
      "Step: 1072, Loss: 0.980992317199707, Accuracy: 0.7100031065548307\n",
      "Step: 1073, Loss: 1.0938488245010376, Accuracy: 0.7101179391682185\n",
      "Step: 1074, Loss: 1.1008844375610352, Accuracy: 0.7102325581395349\n",
      "Step: 1075, Loss: 1.1286916732788086, Accuracy: 0.7102695167286245\n",
      "Step: 1076, Loss: 1.0763416290283203, Accuracy: 0.7103837821108017\n",
      "Step: 1077, Loss: 1.1461938619613647, Accuracy: 0.7104205318491033\n",
      "Step: 1078, Loss: 1.1019054651260376, Accuracy: 0.7105344454742045\n",
      "Step: 1079, Loss: 1.2308205366134644, Accuracy: 0.7104938271604938\n",
      "Step: 1080, Loss: 1.3894543647766113, Accuracy: 0.7102991057662658\n",
      "Step: 1081, Loss: 1.1107286214828491, Accuracy: 0.7104128157732594\n",
      "Step: 1082, Loss: 1.3856568336486816, Accuracy: 0.7102185287780856\n",
      "Step: 1083, Loss: 1.1141835451126099, Accuracy: 0.7103321033210332\n",
      "Step: 1084, Loss: 1.0869758129119873, Accuracy: 0.7104454685099847\n",
      "Step: 1085, Loss: 1.4251927137374878, Accuracy: 0.7102516881522406\n",
      "Step: 1086, Loss: 1.1715787649154663, Accuracy: 0.7102115915363385\n",
      "Step: 1087, Loss: 1.3303481340408325, Accuracy: 0.7100949754901961\n",
      "Step: 1088, Loss: 1.2382210493087769, Accuracy: 0.7100550964187328\n",
      "Step: 1089, Loss: 1.0939067602157593, Accuracy: 0.7101681957186544\n",
      "Step: 1090, Loss: 1.214086890220642, Accuracy: 0.71012832263978\n",
      "Step: 1091, Loss: 1.155588984489441, Accuracy: 0.7101648351648352\n",
      "Step: 1092, Loss: 1.4680185317993164, Accuracy: 0.7098963098505642\n",
      "Step: 1093, Loss: 1.0703861713409424, Accuracy: 0.7100091407678245\n",
      "Step: 1094, Loss: 1.2265992164611816, Accuracy: 0.7099695585996956\n",
      "Step: 1095, Loss: 1.0017294883728027, Accuracy: 0.7101581508515815\n",
      "Step: 1096, Loss: 1.089369773864746, Accuracy: 0.7102704345183835\n",
      "Step: 1097, Loss: 1.2926018238067627, Accuracy: 0.7101548269581056\n",
      "Step: 1098, Loss: 1.0721379518508911, Accuracy: 0.7102669093114953\n",
      "Step: 1099, Loss: 1.0374469757080078, Accuracy: 0.7104545454545454\n",
      "Step: 1100, Loss: 1.231579303741455, Accuracy: 0.710414774447472\n",
      "Step: 1101, Loss: 1.1930654048919678, Accuracy: 0.7104506957047791\n",
      "Step: 1102, Loss: 1.1500600576400757, Accuracy: 0.710486551828347\n",
      "Step: 1103, Loss: 1.144039511680603, Accuracy: 0.710522342995169\n",
      "Step: 1104, Loss: 1.1718801259994507, Accuracy: 0.7104826546003017\n",
      "Step: 1105, Loss: 1.3909980058670044, Accuracy: 0.7102923447860157\n",
      "Step: 1106, Loss: 1.1687983274459839, Accuracy: 0.710328214393255\n",
      "Step: 1107, Loss: 1.3312592506408691, Accuracy: 0.7102135980746089\n",
      "Step: 1108, Loss: 1.0673695802688599, Accuracy: 0.7103246167718665\n",
      "Step: 1109, Loss: 1.2324340343475342, Accuracy: 0.7102852852852853\n",
      "Step: 1110, Loss: 1.0868736505508423, Accuracy: 0.7103960396039604\n",
      "Step: 1111, Loss: 1.069682002067566, Accuracy: 0.7105065947242206\n",
      "Step: 1112, Loss: 1.2442549467086792, Accuracy: 0.7104672057502246\n",
      "Step: 1113, Loss: 1.2238982915878296, Accuracy: 0.7104278874925194\n",
      "Step: 1114, Loss: 1.2900480031967163, Accuracy: 0.7103886397608371\n",
      "Step: 1115, Loss: 1.1515644788742065, Accuracy: 0.7104241338112306\n",
      "Step: 1116, Loss: 1.1776193380355835, Accuracy: 0.7104595643091615\n",
      "Step: 1117, Loss: 1.225392460823059, Accuracy: 0.7104203935599285\n",
      "Step: 1118, Loss: 1.2237966060638428, Accuracy: 0.7103812928209711\n",
      "Step: 1119, Loss: 1.2544981241226196, Accuracy: 0.7103422619047619\n",
      "Step: 1120, Loss: 1.2266613245010376, Accuracy: 0.7103033006244425\n",
      "Step: 1121, Loss: 1.0668169260025024, Accuracy: 0.7104129530600118\n",
      "Step: 1122, Loss: 1.1409579515457153, Accuracy: 0.710522410210745\n",
      "Step: 1123, Loss: 1.0640262365341187, Accuracy: 0.7106316725978647\n",
      "Step: 1124, Loss: 1.189943552017212, Accuracy: 0.7106666666666667\n",
      "Step: 1125, Loss: 1.3106554746627808, Accuracy: 0.7105535820011841\n",
      "Step: 1126, Loss: 1.081789493560791, Accuracy: 0.7106625258799172\n",
      "Step: 1127, Loss: 1.3033301830291748, Accuracy: 0.7105496453900709\n",
      "Step: 1128, Loss: 1.2361812591552734, Accuracy: 0.7105107764983761\n",
      "Step: 1129, Loss: 1.2868053913116455, Accuracy: 0.71047197640118\n",
      "Step: 1130, Loss: 1.275931715965271, Accuracy: 0.7103595638078397\n",
      "Step: 1131, Loss: 1.413699984550476, Accuracy: 0.7101737338044759\n",
      "Step: 1132, Loss: 1.265920639038086, Accuracy: 0.7100617828773168\n",
      "Step: 1133, Loss: 1.2330255508422852, Accuracy: 0.7100235155790712\n",
      "Step: 1134, Loss: 0.9456367492675781, Accuracy: 0.7102790014684288\n",
      "Step: 1135, Loss: 1.2213133573532104, Accuracy: 0.7102406103286385\n",
      "Step: 1136, Loss: 1.3201384544372559, Accuracy: 0.7101289944297859\n",
      "Step: 1137, Loss: 1.1945286989212036, Accuracy: 0.7101640304628002\n",
      "Step: 1138, Loss: 1.26585054397583, Accuracy: 0.7101258413813286\n",
      "Step: 1139, Loss: 1.1532562971115112, Accuracy: 0.7101608187134503\n",
      "Step: 1140, Loss: 0.9088945388793945, Accuracy: 0.7104148407829389\n",
      "Step: 1141, Loss: 1.2302857637405396, Accuracy: 0.7103765323992994\n",
      "Step: 1142, Loss: 1.2223314046859741, Accuracy: 0.7103382910469525\n",
      "Step: 1143, Loss: 1.2345026731491089, Accuracy: 0.7103001165501166\n",
      "Step: 1144, Loss: 1.2442904710769653, Accuracy: 0.7102620087336244\n",
      "Step: 1145, Loss: 1.0814567804336548, Accuracy: 0.710369400814427\n",
      "Step: 1146, Loss: 1.16268789768219, Accuracy: 0.7104039523394362\n",
      "Step: 1147, Loss: 1.1619402170181274, Accuracy: 0.710438443670151\n",
      "Step: 1148, Loss: 1.0764590501785278, Accuracy: 0.7106179286335944\n",
      "Step: 1149, Loss: 1.3133482933044434, Accuracy: 0.7105072463768116\n",
      "Step: 1150, Loss: 1.3589228391647339, Accuracy: 0.7103243556327831\n",
      "Step: 1151, Loss: 1.156518816947937, Accuracy: 0.7103587962962963\n",
      "Step: 1152, Loss: 1.2327173948287964, Accuracy: 0.7103209019947961\n",
      "Step: 1153, Loss: 1.1827253103256226, Accuracy: 0.7103552859618717\n",
      "Step: 1154, Loss: 1.2408794164657593, Accuracy: 0.7102453102453102\n",
      "Step: 1155, Loss: 1.0636879205703735, Accuracy: 0.7103517877739332\n",
      "Step: 1156, Loss: 1.3493732213974, Accuracy: 0.7101699798329012\n",
      "Step: 1157, Loss: 1.333555817604065, Accuracy: 0.7099884858952217\n",
      "Step: 1158, Loss: 1.1125081777572632, Accuracy: 0.7100949094046591\n",
      "Step: 1159, Loss: 1.2016092538833618, Accuracy: 0.7100574712643678\n",
      "Step: 1160, Loss: 1.1441878080368042, Accuracy: 0.710091874820557\n",
      "Step: 1161, Loss: 1.1527596712112427, Accuracy: 0.7101262191623637\n",
      "Step: 1162, Loss: 1.2052767276763916, Accuracy: 0.7100888506735454\n",
      "Step: 1163, Loss: 1.247871994972229, Accuracy: 0.7100515463917526\n",
      "Step: 1164, Loss: 1.2316370010375977, Accuracy: 0.7100143061516452\n",
      "Step: 1165, Loss: 1.1248899698257446, Accuracy: 0.7101200686106347\n",
      "Step: 1166, Loss: 0.9596738815307617, Accuracy: 0.7103684661525278\n",
      "Step: 1167, Loss: 1.341753363609314, Accuracy: 0.710259703196347\n",
      "Step: 1168, Loss: 1.1621222496032715, Accuracy: 0.7102936983176504\n",
      "Step: 1169, Loss: 1.1495102643966675, Accuracy: 0.7103276353276353\n",
      "Step: 1170, Loss: 1.0643996000289917, Accuracy: 0.7104326786222602\n",
      "Step: 1171, Loss: 1.340030550956726, Accuracy: 0.7103242320819113\n",
      "Step: 1172, Loss: 1.1566331386566162, Accuracy: 0.7103580562659847\n",
      "Step: 1173, Loss: 1.5363401174545288, Accuracy: 0.7100369108461102\n",
      "Step: 1174, Loss: 1.384743094444275, Accuracy: 0.7098581560283688\n",
      "Step: 1175, Loss: 1.148510456085205, Accuracy: 0.7098922902494331\n",
      "Step: 1176, Loss: 1.1103509664535522, Accuracy: 0.7099263664684226\n",
      "Step: 1177, Loss: 1.2264864444732666, Accuracy: 0.7098896434634975\n",
      "Step: 1178, Loss: 1.1447762250900269, Accuracy: 0.7099236641221374\n",
      "Step: 1179, Loss: 1.3934036493301392, Accuracy: 0.7097457627118644\n",
      "Step: 1180, Loss: 1.1495262384414673, Accuracy: 0.7097798475867908\n",
      "Step: 1181, Loss: 1.1962512731552124, Accuracy: 0.7097433728144388\n",
      "Step: 1182, Loss: 1.2286792993545532, Accuracy: 0.7097069597069597\n",
      "Step: 1183, Loss: 1.16978120803833, Accuracy: 0.709740990990991\n",
      "Step: 1184, Loss: 1.0211175680160522, Accuracy: 0.709915611814346\n",
      "Step: 1185, Loss: 0.9547128677368164, Accuracy: 0.7101602023608768\n",
      "Step: 1186, Loss: 1.2493515014648438, Accuracy: 0.7101235607975288\n",
      "Step: 1187, Loss: 1.1225727796554565, Accuracy: 0.7102272727272727\n",
      "Step: 1188, Loss: 0.9232704043388367, Accuracy: 0.710470984020185\n",
      "Step: 1189, Loss: 1.152262806892395, Accuracy: 0.7105042016806723\n",
      "Step: 1190, Loss: 1.1095324754714966, Accuracy: 0.7106073327735796\n",
      "Step: 1191, Loss: 1.0542161464691162, Accuracy: 0.7107102908277405\n",
      "Step: 1192, Loss: 1.0605194568634033, Accuracy: 0.71081307627829\n",
      "Step: 1193, Loss: 1.329798698425293, Accuracy: 0.7106365159128978\n",
      "Step: 1194, Loss: 1.4584459066390991, Accuracy: 0.7103905160390516\n",
      "Step: 1195, Loss: 1.0852826833724976, Accuracy: 0.7104933110367893\n",
      "Step: 1196, Loss: 1.2689635753631592, Accuracy: 0.7104566972988026\n",
      "Step: 1197, Loss: 1.257875919342041, Accuracy: 0.7103505843071787\n",
      "Step: 1198, Loss: 1.2270071506500244, Accuracy: 0.7103141506811231\n",
      "Step: 1199, Loss: 1.1360461711883545, Accuracy: 0.7103472222222222\n",
      "Step: 1200, Loss: 1.1186696290969849, Accuracy: 0.7103802386899806\n",
      "Step: 1201, Loss: 1.174883246421814, Accuracy: 0.7104132002218525\n",
      "Step: 1202, Loss: 1.2422573566436768, Accuracy: 0.7103768356885564\n",
      "Step: 1203, Loss: 1.0670851469039917, Accuracy: 0.7104789590254706\n",
      "Step: 1204, Loss: 1.379644513130188, Accuracy: 0.7103042876901798\n",
      "Step: 1205, Loss: 1.1922739744186401, Accuracy: 0.7103372028745163\n",
      "Step: 1206, Loss: 1.3925977945327759, Accuracy: 0.7101629384148025\n",
      "Step: 1207, Loss: 1.0887060165405273, Accuracy: 0.7102649006622517\n",
      "Step: 1208, Loss: 1.4471317529678345, Accuracy: 0.7100909842845327\n",
      "Step: 1209, Loss: 1.1344813108444214, Accuracy: 0.7101239669421487\n",
      "Step: 1210, Loss: 1.4705816507339478, Accuracy: 0.7098816405174787\n",
      "Step: 1211, Loss: 1.116634726524353, Accuracy: 0.7099147414741475\n",
      "Step: 1212, Loss: 1.3764525651931763, Accuracy: 0.7097416872767244\n",
      "Step: 1213, Loss: 1.2336081266403198, Accuracy: 0.7097062053816584\n",
      "Step: 1214, Loss: 0.9792645573616028, Accuracy: 0.7098765432098766\n",
      "Step: 1215, Loss: 1.2511296272277832, Accuracy: 0.7098410087719298\n",
      "Step: 1216, Loss: 1.2981187105178833, Accuracy: 0.7097370583401807\n",
      "Step: 1217, Loss: 1.5973488092422485, Accuracy: 0.7093596059113301\n",
      "Step: 1218, Loss: 1.1219661235809326, Accuracy: 0.7094613070823079\n",
      "Step: 1219, Loss: 1.2521872520446777, Accuracy: 0.7093579234972678\n",
      "Step: 1220, Loss: 1.0535179376602173, Accuracy: 0.7095277095277095\n",
      "Step: 1221, Loss: 1.022025465965271, Accuracy: 0.7096972176759411\n",
      "Step: 1222, Loss: 1.163051724433899, Accuracy: 0.7097301717089125\n",
      "Step: 1223, Loss: 1.2270073890686035, Accuracy: 0.7096949891067538\n",
      "Step: 1224, Loss: 1.3823870420455933, Accuracy: 0.7095238095238096\n",
      "Step: 1225, Loss: 1.0877995491027832, Accuracy: 0.7096247960848288\n",
      "Step: 1226, Loss: 1.2064006328582764, Accuracy: 0.7095897853844064\n",
      "Step: 1227, Loss: 1.1448578834533691, Accuracy: 0.7096226927252985\n",
      "Step: 1228, Loss: 1.060144066810608, Accuracy: 0.7097233523189586\n",
      "Step: 1229, Loss: 1.3007506132125854, Accuracy: 0.7096205962059621\n",
      "Step: 1230, Loss: 1.3791402578353882, Accuracy: 0.7094503113999459\n",
      "Step: 1231, Loss: 1.1369216442108154, Accuracy: 0.7094832251082251\n",
      "Step: 1232, Loss: 1.1482101678848267, Accuracy: 0.7095160854284942\n",
      "Step: 1233, Loss: 1.286997675895691, Accuracy: 0.7094138303619665\n",
      "Step: 1234, Loss: 1.1723910570144653, Accuracy: 0.70944669365722\n",
      "Step: 1235, Loss: 1.1364973783493042, Accuracy: 0.7094795037756203\n",
      "Step: 1236, Loss: 1.3399888277053833, Accuracy: 0.7093775262732417\n",
      "Step: 1237, Loss: 1.3161307573318481, Accuracy: 0.7092757135164244\n",
      "Step: 1238, Loss: 1.1957284212112427, Accuracy: 0.7093085821899381\n",
      "Step: 1239, Loss: 1.0991376638412476, Accuracy: 0.7094086021505376\n",
      "Step: 1240, Loss: 1.0397270917892456, Accuracy: 0.7095756110663444\n",
      "Step: 1241, Loss: 1.2694242000579834, Accuracy: 0.7095410628019324\n",
      "Step: 1242, Loss: 1.1124829053878784, Accuracy: 0.7095736122284795\n",
      "Step: 1243, Loss: 1.0848621129989624, Accuracy: 0.7096730975348339\n",
      "Step: 1244, Loss: 1.237159013748169, Accuracy: 0.7096385542168675\n",
      "Step: 1245, Loss: 1.410008430480957, Accuracy: 0.709470304975923\n",
      "Step: 1246, Loss: 1.30821692943573, Accuracy: 0.7093691526329858\n",
      "Step: 1247, Loss: 1.3825387954711914, Accuracy: 0.7092013888888888\n",
      "Step: 1248, Loss: 1.1810506582260132, Accuracy: 0.7092340539097945\n",
      "Step: 1249, Loss: 1.1066259145736694, Accuracy: 0.7093333333333334\n",
      "Step: 1250, Loss: 1.0643343925476074, Accuracy: 0.7094990674127365\n",
      "Step: 1251, Loss: 1.0066543817520142, Accuracy: 0.7096645367412141\n",
      "Step: 1252, Loss: 1.3003507852554321, Accuracy: 0.7095637137536579\n",
      "Step: 1253, Loss: 1.3333348035812378, Accuracy: 0.7094630515683147\n",
      "Step: 1254, Loss: 1.2879008054733276, Accuracy: 0.7094289508632138\n",
      "Step: 1255, Loss: 1.3128774166107178, Accuracy: 0.7093285562632696\n",
      "Step: 1256, Loss: 0.9681594371795654, Accuracy: 0.7095597984619464\n",
      "Step: 1257, Loss: 1.2365702390670776, Accuracy: 0.709525702172761\n",
      "Step: 1258, Loss: 1.1614593267440796, Accuracy: 0.7095578501456182\n",
      "Step: 1259, Loss: 1.2320185899734497, Accuracy: 0.7095238095238096\n",
      "Step: 1260, Loss: 1.2329741716384888, Accuracy: 0.7094898228918848\n",
      "Step: 1261, Loss: 1.1727495193481445, Accuracy: 0.7095219228737454\n",
      "Step: 1262, Loss: 1.290220856666565, Accuracy: 0.7094220110847189\n",
      "Step: 1263, Loss: 1.0789122581481934, Accuracy: 0.7095200421940928\n",
      "Step: 1264, Loss: 1.2784545421600342, Accuracy: 0.7094202898550724\n",
      "Step: 1265, Loss: 1.2375049591064453, Accuracy: 0.7093865192206424\n",
      "Step: 1266, Loss: 1.1498595476150513, Accuracy: 0.7094185740594581\n",
      "Step: 1267, Loss: 1.2192143201828003, Accuracy: 0.7093848580441641\n",
      "Step: 1268, Loss: 1.0851540565490723, Accuracy: 0.7094825321775676\n",
      "Step: 1269, Loss: 1.1743440628051758, Accuracy: 0.7095144356955381\n",
      "Step: 1270, Loss: 1.2360807657241821, Accuracy: 0.7094807238394965\n",
      "Step: 1271, Loss: 1.0509544610977173, Accuracy: 0.7095780922431866\n",
      "Step: 1272, Loss: 1.1244384050369263, Accuracy: 0.7096753076721655\n",
      "Step: 1273, Loss: 1.2164849042892456, Accuracy: 0.7096415489272632\n",
      "Step: 1274, Loss: 1.3129913806915283, Accuracy: 0.7095424836601307\n",
      "Step: 1275, Loss: 1.133233904838562, Accuracy: 0.7095741901776385\n",
      "Step: 1276, Loss: 1.2578426599502563, Accuracy: 0.7095405899243018\n",
      "Step: 1277, Loss: 1.3249307870864868, Accuracy: 0.7094418362023995\n",
      "Step: 1278, Loss: 0.9921002388000488, Accuracy: 0.7096038571800886\n",
      "Step: 1279, Loss: 1.2334598302841187, Accuracy: 0.7095703125\n",
      "Step: 1280, Loss: 1.4646469354629517, Accuracy: 0.7093416601613323\n",
      "Step: 1281, Loss: 1.1836713552474976, Accuracy: 0.7093083723348934\n",
      "Step: 1282, Loss: 1.0956405401229858, Accuracy: 0.7094050402702\n",
      "Step: 1283, Loss: 1.129858136177063, Accuracy: 0.7095015576323987\n",
      "Step: 1284, Loss: 1.0890089273452759, Accuracy: 0.7095979247730221\n",
      "Step: 1285, Loss: 1.317430853843689, Accuracy: 0.7094997407983411\n",
      "Step: 1286, Loss: 1.2779779434204102, Accuracy: 0.7094664594664595\n",
      "Step: 1287, Loss: 1.3038965463638306, Accuracy: 0.7093038302277432\n",
      "Step: 1288, Loss: 1.1470435857772827, Accuracy: 0.7093354021205068\n",
      "Step: 1289, Loss: 1.433341383934021, Accuracy: 0.709108527131783\n",
      "Step: 1290, Loss: 0.9934980273246765, Accuracy: 0.7092693002840176\n",
      "Step: 1291, Loss: 1.1190440654754639, Accuracy: 0.7093653250773994\n",
      "Step: 1292, Loss: 1.1833748817443848, Accuracy: 0.7093323021397268\n",
      "Step: 1293, Loss: 1.3615535497665405, Accuracy: 0.7091705306543019\n",
      "Step: 1294, Loss: 1.1540939807891846, Accuracy: 0.7091377091377091\n",
      "Step: 1295, Loss: 1.341287612915039, Accuracy: 0.7090406378600823\n",
      "Step: 1296, Loss: 0.9944691061973572, Accuracy: 0.7092007196093549\n",
      "Step: 1297, Loss: 1.0082355737686157, Accuracy: 0.7093605546995377\n",
      "Step: 1298, Loss: 1.1288838386535645, Accuracy: 0.7093918398768283\n",
      "Step: 1299, Loss: 1.0709047317504883, Accuracy: 0.7094871794871795\n",
      "Step: 1300, Loss: 0.9892547130584717, Accuracy: 0.7096464258262875\n",
      "Step: 1301, Loss: 1.0910170078277588, Accuracy: 0.7097414234511009\n",
      "Step: 1302, Loss: 1.2883554697036743, Accuracy: 0.709644410335124\n",
      "Step: 1303, Loss: 1.1580513715744019, Accuracy: 0.7096753578732107\n",
      "Step: 1304, Loss: 1.2208195924758911, Accuracy: 0.7096424010217114\n",
      "Step: 1305, Loss: 1.1143015623092651, Accuracy: 0.7096733027054619\n",
      "Step: 1306, Loss: 1.1601659059524536, Accuracy: 0.7097041571027799\n",
      "Step: 1307, Loss: 1.294061541557312, Accuracy: 0.7096075433231397\n",
      "Step: 1308, Loss: 1.1297036409378052, Accuracy: 0.7096384008148714\n",
      "Step: 1309, Loss: 1.1411770582199097, Accuracy: 0.7096692111959287\n",
      "Step: 1310, Loss: 1.2014222145080566, Accuracy: 0.7096364098652428\n",
      "Step: 1311, Loss: 1.3342041969299316, Accuracy: 0.7094766260162602\n",
      "Step: 1312, Loss: 1.3477445840835571, Accuracy: 0.7093170855547093\n",
      "Step: 1313, Loss: 1.1023060083389282, Accuracy: 0.7094114662607813\n",
      "Step: 1314, Loss: 1.1122294664382935, Accuracy: 0.7095057034220532\n",
      "Step: 1315, Loss: 1.335018277168274, Accuracy: 0.7094098277608916\n",
      "Step: 1316, Loss: 1.1938368082046509, Accuracy: 0.709440647937231\n",
      "Step: 1317, Loss: 1.214309573173523, Accuracy: 0.7094714213454729\n",
      "Step: 1318, Loss: 1.3838200569152832, Accuracy: 0.7093126105635582\n",
      "Step: 1319, Loss: 1.0958220958709717, Accuracy: 0.7094065656565657\n",
      "Step: 1320, Loss: 1.1597505807876587, Accuracy: 0.7094372949785516\n",
      "Step: 1321, Loss: 1.0021544694900513, Accuracy: 0.7095940494200706\n",
      "Step: 1322, Loss: 1.175942063331604, Accuracy: 0.7096245905769716\n",
      "Step: 1323, Loss: 1.0484017133712769, Accuracy: 0.709718026183283\n",
      "Step: 1324, Loss: 1.1649304628372192, Accuracy: 0.709811320754717\n",
      "Step: 1325, Loss: 1.144689917564392, Accuracy: 0.709841628959276\n",
      "Step: 1326, Loss: 1.3031638860702515, Accuracy: 0.7098090931926652\n",
      "Step: 1327, Loss: 1.2266446352005005, Accuracy: 0.7097766064257028\n",
      "Step: 1328, Loss: 1.2612324953079224, Accuracy: 0.7097441685477803\n",
      "Step: 1329, Loss: 1.0867390632629395, Accuracy: 0.7098370927318296\n",
      "Step: 1330, Loss: 1.1776996850967407, Accuracy: 0.7098672677185074\n",
      "Step: 1331, Loss: 1.0499675273895264, Accuracy: 0.70995995995996\n",
      "Step: 1332, Loss: 1.128698706626892, Accuracy: 0.7099899974993749\n",
      "Step: 1333, Loss: 1.1896086931228638, Accuracy: 0.7100199900049975\n",
      "Step: 1334, Loss: 1.3391848802566528, Accuracy: 0.7099250936329587\n",
      "Step: 1335, Loss: 1.3257943391799927, Accuracy: 0.7098303393213573\n",
      "Step: 1336, Loss: 1.128685474395752, Accuracy: 0.7098603839441536\n",
      "Step: 1337, Loss: 1.1220426559448242, Accuracy: 0.7098903836571998\n",
      "Step: 1338, Loss: 1.0660325288772583, Accuracy: 0.709982574060244\n",
      "Step: 1339, Loss: 1.2665345668792725, Accuracy: 0.7098880597014925\n",
      "Step: 1340, Loss: 1.0348843336105347, Accuracy: 0.7099801143425305\n",
      "Step: 1341, Loss: 1.2010127305984497, Accuracy: 0.7099478390461997\n",
      "Step: 1342, Loss: 1.3119574785232544, Accuracy: 0.7098535616778356\n",
      "Step: 1343, Loss: 1.0096393823623657, Accuracy: 0.7100074404761905\n",
      "Step: 1344, Loss: 1.2600568532943726, Accuracy: 0.7099132589838909\n",
      "Step: 1345, Loss: 1.1638940572738647, Accuracy: 0.7099430411094602\n",
      "Step: 1346, Loss: 1.2902928590774536, Accuracy: 0.7098490472655283\n",
      "Step: 1347, Loss: 1.042310118675232, Accuracy: 0.7100024727992087\n",
      "Step: 1348, Loss: 1.149958610534668, Accuracy: 0.7100321225599209\n",
      "Step: 1349, Loss: 1.3127167224884033, Accuracy: 0.7099382716049383\n",
      "Step: 1350, Loss: 1.0430256128311157, Accuracy: 0.7100296076980015\n",
      "Step: 1351, Loss: 1.482398509979248, Accuracy: 0.7098126232741617\n",
      "Step: 1352, Loss: 1.2534983158111572, Accuracy: 0.7097807341709781\n",
      "Step: 1353, Loss: 1.3180280923843384, Accuracy: 0.7096873461349089\n",
      "Step: 1354, Loss: 1.0712544918060303, Accuracy: 0.7097785977859778\n",
      "Step: 1355, Loss: 1.097261905670166, Accuracy: 0.709869714847591\n",
      "Step: 1356, Loss: 1.1383949518203735, Accuracy: 0.7099606976172931\n",
      "Step: 1357, Loss: 1.2217768430709839, Accuracy: 0.7099288168875798\n",
      "Step: 1358, Loss: 1.3200949430465698, Accuracy: 0.7098356634780476\n",
      "Step: 1359, Loss: 1.4550482034683228, Accuracy: 0.7096200980392157\n",
      "Step: 1360, Loss: 1.2362884283065796, Accuracy: 0.7095885378398237\n",
      "Step: 1361, Loss: 1.1465271711349487, Accuracy: 0.709618208516887\n",
      "Step: 1362, Loss: 1.2238998413085938, Accuracy: 0.7095866960136953\n",
      "Step: 1363, Loss: 1.3530505895614624, Accuracy: 0.7094941348973607\n",
      "Step: 1364, Loss: 1.1737030744552612, Accuracy: 0.7094627594627595\n",
      "Step: 1365, Loss: 1.173749327659607, Accuracy: 0.7094924353343094\n",
      "Step: 1366, Loss: 1.2319633960723877, Accuracy: 0.7094611070470617\n",
      "Step: 1367, Loss: 1.287023901939392, Accuracy: 0.7094298245614035\n",
      "Step: 1368, Loss: 1.1370197534561157, Accuracy: 0.7094594594594594\n",
      "Step: 1369, Loss: 1.3281413316726685, Accuracy: 0.7093065693430657\n",
      "Step: 1370, Loss: 1.1596943140029907, Accuracy: 0.7093362509117432\n",
      "Step: 1371, Loss: 1.162346363067627, Accuracy: 0.709365889212828\n",
      "Step: 1372, Loss: 1.0327197313308716, Accuracy: 0.7095168730274338\n",
      "Step: 1373, Loss: 1.2758311033248901, Accuracy: 0.7094856865599224\n",
      "Step: 1374, Loss: 1.2532914876937866, Accuracy: 0.7094545454545454\n",
      "Step: 1375, Loss: 1.5192590951919556, Accuracy: 0.7091812015503876\n",
      "Step: 1376, Loss: 1.1921294927597046, Accuracy: 0.7091503267973857\n",
      "Step: 1377, Loss: 1.2895702123641968, Accuracy: 0.709059022738268\n",
      "Step: 1378, Loss: 1.3607984781265259, Accuracy: 0.7089074208363548\n",
      "Step: 1379, Loss: 1.1625897884368896, Accuracy: 0.7089371980676329\n",
      "Step: 1380, Loss: 1.2383748292922974, Accuracy: 0.7089065894279507\n",
      "Step: 1381, Loss: 1.1633957624435425, Accuracy: 0.7089363241678727\n",
      "Step: 1382, Loss: 1.525725245475769, Accuracy: 0.7086647384912027\n",
      "Step: 1383, Loss: 1.160871148109436, Accuracy: 0.7086946050096339\n",
      "Step: 1384, Loss: 1.3008824586868286, Accuracy: 0.7086040914560771\n",
      "Step: 1385, Loss: 1.196285367012024, Accuracy: 0.7085738335738335\n",
      "Step: 1386, Loss: 1.1513761281967163, Accuracy: 0.7086037010334054\n",
      "Step: 1387, Loss: 1.3176064491271973, Accuracy: 0.7085134486071085\n",
      "Step: 1388, Loss: 1.3297280073165894, Accuracy: 0.7083633309335253\n",
      "Step: 1389, Loss: 1.2747706174850464, Accuracy: 0.708273381294964\n",
      "Step: 1390, Loss: 1.4628242254257202, Accuracy: 0.708063743110472\n",
      "Step: 1391, Loss: 1.0848852396011353, Accuracy: 0.7081537356321839\n",
      "Step: 1392, Loss: 1.3682082891464233, Accuracy: 0.7080043072505384\n",
      "Step: 1393, Loss: 1.131762981414795, Accuracy: 0.7080344332855093\n",
      "Step: 1394, Loss: 1.1152271032333374, Accuracy: 0.7080645161290322\n",
      "Step: 1395, Loss: 1.0574318170547485, Accuracy: 0.7081542502387774\n",
      "Step: 1396, Loss: 1.1093207597732544, Accuracy: 0.7082438558816512\n",
      "Step: 1397, Loss: 1.0639132261276245, Accuracy: 0.7083333333333334\n",
      "Step: 1398, Loss: 1.0811076164245605, Accuracy: 0.7084226828687158\n",
      "Step: 1399, Loss: 1.0465036630630493, Accuracy: 0.7085119047619047\n",
      "Step: 1400, Loss: 0.9310970902442932, Accuracy: 0.7087199619319534\n",
      "Step: 1401, Loss: 1.0776138305664062, Accuracy: 0.708808844507846\n",
      "Step: 1402, Loss: 1.0064102411270142, Accuracy: 0.7089569969113804\n",
      "Step: 1403, Loss: 1.1053365468978882, Accuracy: 0.709045584045584\n",
      "Step: 1404, Loss: 1.193985939025879, Accuracy: 0.7090154211150652\n",
      "Step: 1405, Loss: 1.0164332389831543, Accuracy: 0.7091631104788999\n",
      "Step: 1406, Loss: 1.218348741531372, Accuracy: 0.7091329068941009\n",
      "Step: 1407, Loss: 1.110742211341858, Accuracy: 0.7092211174242424\n",
      "Step: 1408, Loss: 1.1700375080108643, Accuracy: 0.7092500591436006\n",
      "Step: 1409, Loss: 1.3302730321884155, Accuracy: 0.709160756501182\n",
      "Step: 1410, Loss: 1.2312790155410767, Accuracy: 0.7091306402078904\n",
      "Step: 1411, Loss: 1.3459683656692505, Accuracy: 0.7089825306893296\n",
      "Step: 1412, Loss: 1.137156367301941, Accuracy: 0.7090115593300307\n",
      "Step: 1413, Loss: 1.097145438194275, Accuracy: 0.7090994813767091\n",
      "Step: 1414, Loss: 1.1592386960983276, Accuracy: 0.7091283863368669\n",
      "Step: 1415, Loss: 1.285476803779602, Accuracy: 0.7090395480225988\n",
      "Step: 1416, Loss: 1.1701500415802002, Accuracy: 0.7090684544812985\n",
      "Step: 1417, Loss: 0.9918086528778076, Accuracy: 0.7092148566055477\n",
      "Step: 1418, Loss: 0.9455766677856445, Accuracy: 0.7094197791872211\n",
      "Step: 1419, Loss: 1.3553781509399414, Accuracy: 0.7092723004694835\n",
      "Step: 1420, Loss: 1.1083815097808838, Accuracy: 0.7093596059113301\n",
      "Step: 1421, Loss: 1.2285884618759155, Accuracy: 0.7093295827473043\n",
      "Step: 1422, Loss: 1.2561393976211548, Accuracy: 0.7092410400562192\n",
      "Step: 1423, Loss: 1.3306975364685059, Accuracy: 0.7090941011235955\n",
      "Step: 1424, Loss: 1.0959968566894531, Accuracy: 0.7091812865497076\n",
      "Step: 1425, Loss: 1.0807400941848755, Accuracy: 0.7092683496961197\n",
      "Step: 1426, Loss: 1.3658753633499146, Accuracy: 0.7089465078252745\n",
      "Epoch: 9, Val_Accuracy: 0.3897196261682243\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "best_acc = 0\n",
    "for epoch in range(epochs):\n",
    "    accuracy = 0\n",
    "    model.train()\n",
    "    for step,(image,label) in enumerate(tqdm(train_loader)):\n",
    "        image = image.to(device)\n",
    "        label = label.to(device)\n",
    "        output = model(image)\n",
    "        loss = criterion(output,label)\n",
    "        _,predicted = torch.max(output.data,1)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        accuracy = accuracy + (predicted == label).sum().item()\n",
    "        print(f'Step: {step}, Loss: {loss.item()}, Accuracy: {accuracy/((step+1)*batch_size)}')\n",
    "    val_accuracy = val_acc()\n",
    "    print(f'Epoch: {epoch}, Val_Accuracy: {val_accuracy}')\n",
    "    if val_accuracy > best_acc:\n",
    "        best_acc = val_accuracy\n",
    "        torch.save(model.state_dict(),f'model{val_accuracy:.2f}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = train_loader.dataset[0][0].cuda()\n",
    "output = model(image.unsqueeze(0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
